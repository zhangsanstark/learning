{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u7840 \u00b6 \u8bfe\u7a0b\u7406\u5ff5\u4e0e\u5b97\u65e8 \u00b6 \u672c\u7cfb\u5217\u8bfe\u7a0b\u5c06\u5f00\u542f\u4f60\u7684NLP\u4e4b\u65c5\uff0c\u5168\u9762\u4ece\u4f01\u4e1a\u5b9e\u6218\u89d2\u5ea6\u51fa\u53d1\uff0c\u8bfe\u7a0b\u8bbe\u8ba1\u5185\u5bb9\u5bf9\u5e94\u4f01\u4e1a\u5f00\u53d1\u6807\u51c6\u6d41\u7a0b\u548c\u4f01\u4e1a\u53d1\u5c55\u8def\u5f84\uff0c\u52a9\u529b\u4f60\u6210\u4e3a\u4e00\u540d\u771f\u6b63\u7684AI-NLP\u5de5\u7a0b\u5e08\u3002 \u5185\u5bb9\u8bf4\u660e \u00b6 \u672c\u8bfe\u7a0b\u5185\u5bb9\u7ed3\u5408\u5f53\u4e0b\u65f6\u4ee3\u80cc\u666f\uff0c\u66f4\u591a\u5173\u6ce8NLP\u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u8fd9\u4e5f\u5c06\u662f\u672a\u6765\u51e0\u5e74\u751a\u81f3\u51e0\u5341\u5e74NLP\u7684\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u3002 \u5185\u5bb9\u6982\u8981 \u00b6 \u6a21\u5757\u540d\u79f0 \u4e3b\u8981\u5185\u5bb9 \u6848\u4f8b \u6587\u672c\u9884\u5904\u7406 \u6587\u672c\u5904\u7406\u57fa\u672c\u65b9\u6cd5\uff0c\u6587\u672c\u5f20\u91cf\u8868\u793a\uff0c\u6587\u672c\u6570\u636e\u5206\u6790\uff0c\u6587\u672c\u589e\u5f3a\u65b9\u6cd5\u7b49 \u8bcd\u5411\u91cf\u53ef\u89c6\u5316\u5b9e\u9a8c RNN\u53ca\u5176\u53d8\u4f53 RNN, LSTM, GRU\u6a21\u578b\u7684\u4f5c\u7528\u3001\u6784\u5efa\u3001\u4f18\u52a3\u52bf\u6bd4\u8f83\u7b49 \u5168\u7403\u4eba\u540d\u5206\u7c7b\u4efb\u52a1, \u82f1\u8bd1\u6cd5\u7ffb\u8bd1\u4efb\u52a1 Transformer Transformer\u6a21\u578b\u7684\u4f5c\u7528\uff0c\u7ec6\u8282\u539f\u7406\u89e3\u6790\uff0c\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u7b49 \u6784\u5efa\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b \u8fc1\u79fb\u5b66\u4e60 fasttext\u5de5\u5177\u7684\u4f5c\u7528\uff0c\u8fc1\u79fb\u5b66\u4e60\u7406\u8bba\uff0cNLP\u6807\u51c6\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f7f\u7528\u7b49 \u5168\u56fd\u9152\u5e97\u8bc4\u8bba\u60c5\u611f\u5206\u6790\u4efb\u52a1 NLP\u7cbe\u9009\u95ee\u7b54 Transformer\u5e38\u89c1\u95ee\u7b54\u3001BERT\u3001ELMO\u3001GPT\u7ecf\u5178\u9762\u8bd5\u9898","title":"\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u7840"},{"location":"index.html#_1","text":"","title":"\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u7840"},{"location":"index.html#_2","text":"\u672c\u7cfb\u5217\u8bfe\u7a0b\u5c06\u5f00\u542f\u4f60\u7684NLP\u4e4b\u65c5\uff0c\u5168\u9762\u4ece\u4f01\u4e1a\u5b9e\u6218\u89d2\u5ea6\u51fa\u53d1\uff0c\u8bfe\u7a0b\u8bbe\u8ba1\u5185\u5bb9\u5bf9\u5e94\u4f01\u4e1a\u5f00\u53d1\u6807\u51c6\u6d41\u7a0b\u548c\u4f01\u4e1a\u53d1\u5c55\u8def\u5f84\uff0c\u52a9\u529b\u4f60\u6210\u4e3a\u4e00\u540d\u771f\u6b63\u7684AI-NLP\u5de5\u7a0b\u5e08\u3002","title":"\u8bfe\u7a0b\u7406\u5ff5\u4e0e\u5b97\u65e8"},{"location":"index.html#_3","text":"\u672c\u8bfe\u7a0b\u5185\u5bb9\u7ed3\u5408\u5f53\u4e0b\u65f6\u4ee3\u80cc\u666f\uff0c\u66f4\u591a\u5173\u6ce8NLP\u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u8fd9\u4e5f\u5c06\u662f\u672a\u6765\u51e0\u5e74\u751a\u81f3\u51e0\u5341\u5e74NLP\u7684\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u3002","title":"\u5185\u5bb9\u8bf4\u660e"},{"location":"index.html#_4","text":"\u6a21\u5757\u540d\u79f0 \u4e3b\u8981\u5185\u5bb9 \u6848\u4f8b \u6587\u672c\u9884\u5904\u7406 \u6587\u672c\u5904\u7406\u57fa\u672c\u65b9\u6cd5\uff0c\u6587\u672c\u5f20\u91cf\u8868\u793a\uff0c\u6587\u672c\u6570\u636e\u5206\u6790\uff0c\u6587\u672c\u589e\u5f3a\u65b9\u6cd5\u7b49 \u8bcd\u5411\u91cf\u53ef\u89c6\u5316\u5b9e\u9a8c RNN\u53ca\u5176\u53d8\u4f53 RNN, LSTM, GRU\u6a21\u578b\u7684\u4f5c\u7528\u3001\u6784\u5efa\u3001\u4f18\u52a3\u52bf\u6bd4\u8f83\u7b49 \u5168\u7403\u4eba\u540d\u5206\u7c7b\u4efb\u52a1, \u82f1\u8bd1\u6cd5\u7ffb\u8bd1\u4efb\u52a1 Transformer Transformer\u6a21\u578b\u7684\u4f5c\u7528\uff0c\u7ec6\u8282\u539f\u7406\u89e3\u6790\uff0c\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u7b49 \u6784\u5efa\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b \u8fc1\u79fb\u5b66\u4e60 fasttext\u5de5\u5177\u7684\u4f5c\u7528\uff0c\u8fc1\u79fb\u5b66\u4e60\u7406\u8bba\uff0cNLP\u6807\u51c6\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f7f\u7528\u7b49 \u5168\u56fd\u9152\u5e97\u8bc4\u8bba\u60c5\u611f\u5206\u6790\u4efb\u52a1 NLP\u7cbe\u9009\u95ee\u7b54 Transformer\u5e38\u89c1\u95ee\u7b54\u3001BERT\u3001ELMO\u3001GPT\u7ecf\u5178\u9762\u8bd5\u9898","title":"\u5185\u5bb9\u6982\u8981"},{"location":"01_mkdocs_NLP/index.html","text":"\u00b6","title":"Index"},{"location":"01_mkdocs_NLP/index.html#_1","text":"","title":""},{"location":"01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406. \u4e86\u89e3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u53d1\u5c55\u7b80\u53f2. \u4e86\u89e3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u5e94\u7528\u573a\u666f. \u4e86\u89e3\u672c\u6559\u7a0b\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406. 1 \u4ec0\u4e48\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u00b6 \u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08Natural Language Processing, \u7b80\u79f0NLP\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u8bed\u8a00\u5b66\u4e2d\u5173\u6ce8\u4e8e\u8ba1\u7b97\u673a\u4e0e\u4eba\u7c7b\u8bed\u8a00\u95f4\u8f6c\u6362\u7684\u9886\u57df. 2 \u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u53d1\u5c55\u7b80\u53f2 \u00b6 3 \u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u5e94\u7528\u573a\u666f \u00b6 \u8bed\u97f3\u52a9\u624b \u673a\u5668\u7ffb\u8bd1 \u641c\u7d22\u5f15\u64ce \u667a\u80fd\u95ee\u7b54 ... 3.1 \u8bed\u97f3\u52a9\u624b \u00b6 \u79d1\u5927\u8baf\u98de\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u8bbf\u8c08! \u524d1\u5206\u949f\uff0c6\u520645\u79d2 \u96c4\u5b89\u65b0\u533a\u57ce\u5e02\u672a\u6765\u8eab\u4efd\u8ba4\u8bc1\u7cfb\u7edf \u6d3b\u4f53\u68c0\u6d4b 3.2 \u673a\u5668\u7ffb\u8bd1 \u00b6 CCTV\u4e0a\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf, \u8ba9\u4e16\u754c\u804a\u5f97\u6765! 4\u520650\u79d2 \u673a\u5668\u7ffb\u8bd1\u8f6f\u4ef6\u7684\u5a01\u529b","title":"1 \u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8"},{"location":"01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406. \u4e86\u89e3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u53d1\u5c55\u7b80\u53f2. \u4e86\u89e3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u5e94\u7528\u573a\u666f. \u4e86\u89e3\u672c\u6559\u7a0b\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html#1","text":"\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08Natural Language Processing, \u7b80\u79f0NLP\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u8bed\u8a00\u5b66\u4e2d\u5173\u6ce8\u4e8e\u8ba1\u7b97\u673a\u4e0e\u4eba\u7c7b\u8bed\u8a00\u95f4\u8f6c\u6362\u7684\u9886\u57df.","title":"1 \u4ec0\u4e48\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406"},{"location":"01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html#2","text":"","title":"2 \u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u53d1\u5c55\u7b80\u53f2"},{"location":"01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html#3","text":"\u8bed\u97f3\u52a9\u624b \u673a\u5668\u7ffb\u8bd1 \u641c\u7d22\u5f15\u64ce \u667a\u80fd\u95ee\u7b54 ...","title":"3 \u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u5e94\u7528\u573a\u666f"},{"location":"01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html#31","text":"\u79d1\u5927\u8baf\u98de\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u8bbf\u8c08! \u524d1\u5206\u949f\uff0c6\u520645\u79d2 \u96c4\u5b89\u65b0\u533a\u57ce\u5e02\u672a\u6765\u8eab\u4efd\u8ba4\u8bc1\u7cfb\u7edf \u6d3b\u4f53\u68c0\u6d4b","title":"3.1 \u8bed\u97f3\u52a9\u624b"},{"location":"01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html#32","text":"CCTV\u4e0a\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf, \u8ba9\u4e16\u754c\u804a\u5f97\u6765! 4\u520650\u79d2 \u673a\u5668\u7ffb\u8bd1\u8f6f\u4ef6\u7684\u5a01\u529b","title":"3.2 \u673a\u5668\u7ffb\u8bd1"},{"location":"01_mkdocs_NLP/2_%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1\u4e91\u670d\u52a1\u5668\u4f7f\u7528\u7684\u57fa\u672c\u65b9\u6cd5\u3002 \u80cc\u666f\u4ecb\u7ecd \u00b6 \u5f53\u524d\u5e02\u573a\u4e0a\u5bf9\u7a0b\u5e8f\u5458\u7684\u57fa\u672c\u9700\u6c42\u4e4b\u4e00\u5c31\u662f\u80fd\u591f\u4f7f\u7528\u670d\u52a1\u5668\u8fdb\u884c\u5f00\u53d1\u3002\u5728\u7edd\u5927\u591a\u6570\u516c\u53f8\u4e2d\uff0c\u6211\u4eec\u90fd\u4f1a\u4f7f\u7528Centos\u7cfb\u7edf\uff08Linux\u53d1\u884c\u7248\u4e4b\u4e00\uff09\u8fdb\u884c\u5f00\u53d1\uff0c\u56e0\u4e3a\u5b83\u662f\u88ab\u9a8c\u8bc1\u7684\u6700\u7a33\u5b9a\u7684\u4f01\u4e1a\u7ea7\u5f00\u53d1\u670d\u52a1\u5668\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u5b66\u4e60\u4e00\u4e9b\u7b80\u5355\u7684\u547d\u4ee4\uff0c\u6765\u5f00\u542f\u6211\u4eecCentos\u5b66\u4e60\u4e4b\u65c5\u3002 \u57fa\u672c\u64cd\u4f5c \u00b6 \u5047\u8bbe\u4f60\u5df2\u7ecf\u901a\u8fc7\u8fd0\u7ef4\u4eba\u5458\u5f00\u901a\u4e86\u670d\u52a1\u5668\uff0c\u5e76\u83b7\u5f97\u4e86root\u7528\u6237\u6743\u9650\uff08\u5728\u516c\u53f8\u4e2d\uff0c\u4f60\u53ef\u80fd\u5f97\u4e0d\u5230\u8fd9\u4e48\u9ad8\u7684\u6743\u9650\uff09\uff0c\u9700\u8981\u5728\u7ec8\u7aef\u4e2d\u8f93\u5165\u8fd9\u4e9b\u547d\u4ee4\u3002 \u4f7f\u7528ssh\u547d\u4ee4\u767b\u9646\u670d\u52a1\u5668\uff1a ssh root@XX.XX.XXX.XX root@XX.XX.XXX.XX ' s password: # \u767b\u9646\u540e\u4fee\u6539\u5bc6\u7801\uff1a passwd \u66f4\u6539\u7528\u6237 root \u7684\u5bc6\u7801 \u3002 \u65b0\u7684 \u5bc6\u7801\uff1a # \u4fee\u6539\u540e\u91cd\u65b0\u767b\u9646\u5373\u53ef \u67e5\u770b\u6211\u4eec\u5f53\u524d\u7684\u786c\u4ef6\u914d\u7f6e\uff1a # \u67e5\u770b\u5185\u5b58 free -h # \u67e5\u770b\u786c\u76d8 df -h # \u67e5\u770bcpu\u903b\u8f91\u6838 lscpu \u8f93\u51fa\u6548\u679c: total used free shared buff/cache available Mem: 7.4G 265M 3.5G 580K 3.6G 6.8G Swap: 0B 0B 0B \u6587\u4ef6\u7cfb\u7edf \u5bb9\u91cf \u5df2\u7528 \u53ef\u7528 \u5df2\u7528% \u6302\u8f7d\u70b9 devtmpfs 3.7G 0 3.7G 0% /dev tmpfs 3.7G 0 3.7G 0% /dev/shm tmpfs 3.7G 580K 3.7G 1% /run tmpfs 3.7G 0 3.7G 0% /sys/fs/cgroup /dev/vda1 99G 29G 66G 31% / tmpfs 756M 0 756M 0% /run/user/0 Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 4 On-line CPU(s) list: 0-3 Thread(s) per core: 2 Core(s) per socket: 2 \u5ea7\uff1a 1 NUMA \u8282\u70b9\uff1a 1 \u5382\u5546 ID\uff1a GenuineIntel CPU \u7cfb\u5217\uff1a 6 \u578b\u53f7\uff1a 85 \u578b\u53f7\u540d\u79f0\uff1a Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz \u6b65\u8fdb\uff1a 7 CPU MHz\uff1a 2500.000 BogoMIPS\uff1a 5000.00 \u8d85\u7ba1\u7406\u5668\u5382\u5546\uff1a KVM \u865a\u62df\u5316\u7c7b\u578b\uff1a \u5b8c\u5168 L1d \u7f13\u5b58\uff1a 32K L1i \u7f13\u5b58\uff1a 32K L2 \u7f13\u5b58\uff1a 1024K L3 \u7f13\u5b58\uff1a 36608K NUMA \u8282\u70b90 CPU\uff1a 0-3 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni \u67e5\u770b\u8ba1\u7b97\u73af\u5883\uff1a cd /home/ec2-user/ vim README \u4f60\u5c06\u770b\u5230\u6240\u6709\u7684\u865a\u62df\u73af\u5883(\u6211\u4eec\u5fc5\u987b\u5b66\u4f1a\u4f7f\u7528\u865a\u62df\u73af\u5883\uff0c\u8fd9\u5728\u534f\u4f5c\u5f00\u53d1\u4e2d\u975e\u5e38\u91cd\u8981) Please use one of the following commands to start the required environment with the framework of your choice: for MXNet(+Keras2) with Python3 (CUDA 10.1 and Intel MKL-DNN) ____________________________________ source activate mxnet_p36 for MXNet(+Keras2) with Python2 (CUDA 10.1 and Intel MKL-DNN) ____________________________________ source activate mxnet_p27 for MXNet(+AWS Neuron) with Python3 ___________________________________________________ source activate aws_neuron_mxnet_p36 for TensorFlow(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) __________________________ source activate tensorflow_p36 for TensorFlow(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) __________________________ source activate tensorflow_p27 for TensorFlow(+AWS Neuron) with Python3 _________________________________________ source activate aws_neuron_tensorflow_p36 for TensorFlow 2(+Keras2) with Python3 (CUDA 10.1 and Intel MKL-DNN) _______________________ source activate tensorflow2_p36 for TensorFlow 2(+Keras2) with Python2 (CUDA 10.1 and Intel MKL-DNN) _______________________ source activate tensorflow2_p27 for TensorFlow 2.3 with Python3.7 (CUDA 10.2 and Intel MKL-DNN) _____________________ source activate tensorflow2_latest_p37 for PyTorch 1.4 with Python3 (CUDA 10.1 and Intel MKL) _________________________________________ source activate pytorch_p36 for PyTorch 1.4 with Python2 (CUDA 10.1 and Intel MKL) _________________________________________ source activate pytorch_p27 for PyTorch 1.6 with Python3 (CUDA 10.1 and Intel MKL) ________________________________ source activate pytorch_latest_p36 for PyTorch (+AWS Neuron) with Python3 ______________________________________________ source activate aws_neuron_pytorch_p36 for Chainer with Python2 (CUDA 10.0 and Intel iDeep) ___________________________________________ source activate chainer_p27 for Chainer with Python3 (CUDA 10.0 and Intel iDeep) ___________________________________________ source activate chainer_p36 for base Python2 (CUDA 10.0) _______________________________________________________________________ source activate python2 for base Python3 (CUDA 10.0) _______________________________________________________________________ source activate python3 \u5982\u679c\u4f60\u9700\u8981\u4f7f\u7528python3 + torch\u65b0\u7248: source activate pytorch_latest_p36 \u7136\u540e\u7ee7\u7eed\u53ef\u4ee5\u67e5\u770b\u5177\u4f53\u7684python\u548cpip\u7248\u672c\uff1a python3 -V # \u67e5\u770bpip\u7248\u672c pip -V # \u67e5\u770b\u91cd\u70b9\u7684\u79d1\u5b66\u8ba1\u7b97\u5305\uff0ctensorflow\uff0cpytorch\u7b49 pip list \u8f93\u51fa\u6548\u679c: Python 3.6.10 :: Anaconda, Inc. pip 20.0.2 from /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pip (python 3.6) \u67e5\u770b\u56fe\u6570\u636e\u60c5\u51b5\uff1a # \u5f00\u542f\u56fe\u6570\u636e\u5e93\uff0c\u8fd9\u91cc\u540e\u671f\u6211\u4eec\u5c06\u91cd\u70b9\u5b66\u4e60\u7684\u6570\u636e\u5e93 neo4j start # \u5173\u95ed\u6570\u636e\u5e93 neo4j stop \u8f93\u51fa\u6548\u679c: Active database: graph.db Directories in use: home: /var/lib/neo4j config: /etc/neo4j logs: /var/log/neo4j plugins: /var/lib/neo4j/plugins import: /var/lib/neo4j/import data: /var/lib/neo4j/data certificates: /var/lib/neo4j/certificates run: /var/run/neo4j Starting Neo4j. Started neo4j (pid 17565). It is available at http://0.0.0.0:7474/ There may be a short delay until the server is ready. See /var/log/neo4j/neo4j.log for current status. Stopping Neo4j.. stopped \u8fd0\u884c\u4e00\u4e2a\u4f7f\u7528Pytorch\u7684\u7a0b\u5e8f: cd /data python3 pytorch_demo.py \u8f93\u51fa\u6548\u679c: Net( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) \u603b\u7ed3 \u00b6 \u6211\u4eec\u5b66\u4e60\u4e86\u4e00\u4e9b\u7b80\u5355\u7684\u67e5\u770b\u547d\u4ee4\u4ee5\u9002\u5e94\u670d\u52a1\u5668\u73af\u5883\u3002 \u5f53\u7136\u8be5\u955c\u50cf\u670d\u52a1\u5668\u4e0a\u8fd8\u5b89\u88c5\u4e86\u5f88\u591a\u5de5\u5177\uff0c\u5982nginx\uff0credis\uff0csupervisor\u7b49\u7b49\uff0c\u8fd9\u5728\u6211\u4eec\u8bfe\u7a0b\u7684\u540e\u671f\u4f1a\u7528\u5230\u3002 \u81ea\u5df1\u8d2d\u4e70\u670d\u52a1\u5668\u7684\u5c0f\u4f19\u4f34\u4e5f\u4e0d\u7528\u62c5\u5fc3\uff0c\u6211\u4eec\u4e5f\u6709\u5bf9\u5e94\u7684\u5b89\u88c5\u6559\u7a0b\u3002 \u5176\u4ed6\u8bf4\u660e \u00b6 \u4e3a\u4e86\u66f4\u597d\u7684\u8fde\u63a5\u6d77\u5916\u670d\u52a1\u5668\uff0c\u5927\u5bb6\u53ef\u4ee5\u4f7f\u7528 finalshell \u8fdb\u884c\u767b\u9646\u3002\u70b9\u51fb\u8fdb\u884c\u4e0b\u8f7d\uff1a finalshell WINDOWS\u7248\u672c finalshell macOS\u7248\u672c \u670d\u52a1\u5668\u76f8\u5173\u544a\u77e5: 1\uff0c\u670d\u52a1\u5668\u8ba2\u8d2d\u662f\u5b66\u5458\u4e3a\u4e86\u66f4\u597d\u7684\u9002\u5e94\u4f01\u4e1a\u5f00\u53d1\u73af\u5883\uff0c\u66f4\u597d\u7684\u5b9e\u73b0AI\u5f00\u53d1\uff0c\u81ea\u4e3b\u81ea\u613f\u7684\u884c\u4e3a\u3002 2\uff0c\u6559\u5e08\u56e2\u961f\u5c06\u4e3a\u5176\u63d0\u4f9b\u6307\u5b9a\u6570\u91cf\u7684\u670d\u52a1\u5668\uff0c \u914d\u7f6e\u4e3a4\u6838\uff0c8G\u5185\u5b58 100G\u786c\u76d8 ==> 350\u5143/\u6708 3\uff0c\u4e91\u670d\u52a1\u5668\u7531\u4f20\u667a\u4e13\u4e1a\u8fd0\u7ef4\u56e2\u961f\u7ef4\u62a4\uff0c\u5728\u4e0b\u53d1\u7ed9\u5b66\u5458\u524d\uff0c\u914d\u7f6e\u6307\u5b9a\u7684AI\u955c\u50cf\u73af\u5883\u3002 4\uff0c\u4f20\u667aAI\u56e2\u961f\u5e2e\u52a9\u5b66\u5458\u89e3\u51b3\u5b66\u5458\u5728\u521d\u671f\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5404\u7c7b\u8f6f\u4ef6\u95ee\u9898\u3002 5\uff0c\u4f20\u667a\u8fd0\u7ef4\u56e2\u961f\u5e2e\u52a9\u5b66\u5458\u5904\u7406\u4e91\u670d\u52a1\u5668\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5404\u7c7b\u786c\u4ef6\u5b89\u5168\u548c\u8fd0\u8425\u5546\u95ee\u9898\u3002 6\uff0c\u5b66\u5458\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u4e0d\u5f97\u4f7f\u7528\u670d\u52a1\u5668\u8fdb\u884c\u8fdd\u6cd5\u5f00\u53d1\uff08\u642d\u5efa\u8fdd\u89c4\u7f51\u7ad9\uff0c\u9ed1\u5ba2\u653b\u51fb\u7b49\uff09\uff0c\u5426\u5219\u540e\u679c\u81ea\u8d1f\u3002","title":"2 \u4e91\u670d\u52a1\u5668\u4f7f\u7528\u5165\u95e8"},{"location":"01_mkdocs_NLP/2_%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8.html#_1","text":"\u638c\u63e1\u4e91\u670d\u52a1\u5668\u4f7f\u7528\u7684\u57fa\u672c\u65b9\u6cd5\u3002","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"01_mkdocs_NLP/2_%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8.html#_2","text":"\u5f53\u524d\u5e02\u573a\u4e0a\u5bf9\u7a0b\u5e8f\u5458\u7684\u57fa\u672c\u9700\u6c42\u4e4b\u4e00\u5c31\u662f\u80fd\u591f\u4f7f\u7528\u670d\u52a1\u5668\u8fdb\u884c\u5f00\u53d1\u3002\u5728\u7edd\u5927\u591a\u6570\u516c\u53f8\u4e2d\uff0c\u6211\u4eec\u90fd\u4f1a\u4f7f\u7528Centos\u7cfb\u7edf\uff08Linux\u53d1\u884c\u7248\u4e4b\u4e00\uff09\u8fdb\u884c\u5f00\u53d1\uff0c\u56e0\u4e3a\u5b83\u662f\u88ab\u9a8c\u8bc1\u7684\u6700\u7a33\u5b9a\u7684\u4f01\u4e1a\u7ea7\u5f00\u53d1\u670d\u52a1\u5668\u3002\u4e0b\u9762\u6211\u4eec\u5c06\u5b66\u4e60\u4e00\u4e9b\u7b80\u5355\u7684\u547d\u4ee4\uff0c\u6765\u5f00\u542f\u6211\u4eecCentos\u5b66\u4e60\u4e4b\u65c5\u3002","title":"\u80cc\u666f\u4ecb\u7ecd"},{"location":"01_mkdocs_NLP/2_%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8.html#_3","text":"\u5047\u8bbe\u4f60\u5df2\u7ecf\u901a\u8fc7\u8fd0\u7ef4\u4eba\u5458\u5f00\u901a\u4e86\u670d\u52a1\u5668\uff0c\u5e76\u83b7\u5f97\u4e86root\u7528\u6237\u6743\u9650\uff08\u5728\u516c\u53f8\u4e2d\uff0c\u4f60\u53ef\u80fd\u5f97\u4e0d\u5230\u8fd9\u4e48\u9ad8\u7684\u6743\u9650\uff09\uff0c\u9700\u8981\u5728\u7ec8\u7aef\u4e2d\u8f93\u5165\u8fd9\u4e9b\u547d\u4ee4\u3002 \u4f7f\u7528ssh\u547d\u4ee4\u767b\u9646\u670d\u52a1\u5668\uff1a ssh root@XX.XX.XXX.XX root@XX.XX.XXX.XX ' s password: # \u767b\u9646\u540e\u4fee\u6539\u5bc6\u7801\uff1a passwd \u66f4\u6539\u7528\u6237 root \u7684\u5bc6\u7801 \u3002 \u65b0\u7684 \u5bc6\u7801\uff1a # \u4fee\u6539\u540e\u91cd\u65b0\u767b\u9646\u5373\u53ef \u67e5\u770b\u6211\u4eec\u5f53\u524d\u7684\u786c\u4ef6\u914d\u7f6e\uff1a # \u67e5\u770b\u5185\u5b58 free -h # \u67e5\u770b\u786c\u76d8 df -h # \u67e5\u770bcpu\u903b\u8f91\u6838 lscpu \u8f93\u51fa\u6548\u679c: total used free shared buff/cache available Mem: 7.4G 265M 3.5G 580K 3.6G 6.8G Swap: 0B 0B 0B \u6587\u4ef6\u7cfb\u7edf \u5bb9\u91cf \u5df2\u7528 \u53ef\u7528 \u5df2\u7528% \u6302\u8f7d\u70b9 devtmpfs 3.7G 0 3.7G 0% /dev tmpfs 3.7G 0 3.7G 0% /dev/shm tmpfs 3.7G 580K 3.7G 1% /run tmpfs 3.7G 0 3.7G 0% /sys/fs/cgroup /dev/vda1 99G 29G 66G 31% / tmpfs 756M 0 756M 0% /run/user/0 Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 4 On-line CPU(s) list: 0-3 Thread(s) per core: 2 Core(s) per socket: 2 \u5ea7\uff1a 1 NUMA \u8282\u70b9\uff1a 1 \u5382\u5546 ID\uff1a GenuineIntel CPU \u7cfb\u5217\uff1a 6 \u578b\u53f7\uff1a 85 \u578b\u53f7\u540d\u79f0\uff1a Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz \u6b65\u8fdb\uff1a 7 CPU MHz\uff1a 2500.000 BogoMIPS\uff1a 5000.00 \u8d85\u7ba1\u7406\u5668\u5382\u5546\uff1a KVM \u865a\u62df\u5316\u7c7b\u578b\uff1a \u5b8c\u5168 L1d \u7f13\u5b58\uff1a 32K L1i \u7f13\u5b58\uff1a 32K L2 \u7f13\u5b58\uff1a 1024K L3 \u7f13\u5b58\uff1a 36608K NUMA \u8282\u70b90 CPU\uff1a 0-3 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni \u67e5\u770b\u8ba1\u7b97\u73af\u5883\uff1a cd /home/ec2-user/ vim README \u4f60\u5c06\u770b\u5230\u6240\u6709\u7684\u865a\u62df\u73af\u5883(\u6211\u4eec\u5fc5\u987b\u5b66\u4f1a\u4f7f\u7528\u865a\u62df\u73af\u5883\uff0c\u8fd9\u5728\u534f\u4f5c\u5f00\u53d1\u4e2d\u975e\u5e38\u91cd\u8981) Please use one of the following commands to start the required environment with the framework of your choice: for MXNet(+Keras2) with Python3 (CUDA 10.1 and Intel MKL-DNN) ____________________________________ source activate mxnet_p36 for MXNet(+Keras2) with Python2 (CUDA 10.1 and Intel MKL-DNN) ____________________________________ source activate mxnet_p27 for MXNet(+AWS Neuron) with Python3 ___________________________________________________ source activate aws_neuron_mxnet_p36 for TensorFlow(+Keras2) with Python3 (CUDA 10.0 and Intel MKL-DNN) __________________________ source activate tensorflow_p36 for TensorFlow(+Keras2) with Python2 (CUDA 10.0 and Intel MKL-DNN) __________________________ source activate tensorflow_p27 for TensorFlow(+AWS Neuron) with Python3 _________________________________________ source activate aws_neuron_tensorflow_p36 for TensorFlow 2(+Keras2) with Python3 (CUDA 10.1 and Intel MKL-DNN) _______________________ source activate tensorflow2_p36 for TensorFlow 2(+Keras2) with Python2 (CUDA 10.1 and Intel MKL-DNN) _______________________ source activate tensorflow2_p27 for TensorFlow 2.3 with Python3.7 (CUDA 10.2 and Intel MKL-DNN) _____________________ source activate tensorflow2_latest_p37 for PyTorch 1.4 with Python3 (CUDA 10.1 and Intel MKL) _________________________________________ source activate pytorch_p36 for PyTorch 1.4 with Python2 (CUDA 10.1 and Intel MKL) _________________________________________ source activate pytorch_p27 for PyTorch 1.6 with Python3 (CUDA 10.1 and Intel MKL) ________________________________ source activate pytorch_latest_p36 for PyTorch (+AWS Neuron) with Python3 ______________________________________________ source activate aws_neuron_pytorch_p36 for Chainer with Python2 (CUDA 10.0 and Intel iDeep) ___________________________________________ source activate chainer_p27 for Chainer with Python3 (CUDA 10.0 and Intel iDeep) ___________________________________________ source activate chainer_p36 for base Python2 (CUDA 10.0) _______________________________________________________________________ source activate python2 for base Python3 (CUDA 10.0) _______________________________________________________________________ source activate python3 \u5982\u679c\u4f60\u9700\u8981\u4f7f\u7528python3 + torch\u65b0\u7248: source activate pytorch_latest_p36 \u7136\u540e\u7ee7\u7eed\u53ef\u4ee5\u67e5\u770b\u5177\u4f53\u7684python\u548cpip\u7248\u672c\uff1a python3 -V # \u67e5\u770bpip\u7248\u672c pip -V # \u67e5\u770b\u91cd\u70b9\u7684\u79d1\u5b66\u8ba1\u7b97\u5305\uff0ctensorflow\uff0cpytorch\u7b49 pip list \u8f93\u51fa\u6548\u679c: Python 3.6.10 :: Anaconda, Inc. pip 20.0.2 from /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pip (python 3.6) \u67e5\u770b\u56fe\u6570\u636e\u60c5\u51b5\uff1a # \u5f00\u542f\u56fe\u6570\u636e\u5e93\uff0c\u8fd9\u91cc\u540e\u671f\u6211\u4eec\u5c06\u91cd\u70b9\u5b66\u4e60\u7684\u6570\u636e\u5e93 neo4j start # \u5173\u95ed\u6570\u636e\u5e93 neo4j stop \u8f93\u51fa\u6548\u679c: Active database: graph.db Directories in use: home: /var/lib/neo4j config: /etc/neo4j logs: /var/log/neo4j plugins: /var/lib/neo4j/plugins import: /var/lib/neo4j/import data: /var/lib/neo4j/data certificates: /var/lib/neo4j/certificates run: /var/run/neo4j Starting Neo4j. Started neo4j (pid 17565). It is available at http://0.0.0.0:7474/ There may be a short delay until the server is ready. See /var/log/neo4j/neo4j.log for current status. Stopping Neo4j.. stopped \u8fd0\u884c\u4e00\u4e2a\u4f7f\u7528Pytorch\u7684\u7a0b\u5e8f: cd /data python3 pytorch_demo.py \u8f93\u51fa\u6548\u679c: Net( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) )","title":"\u57fa\u672c\u64cd\u4f5c"},{"location":"01_mkdocs_NLP/2_%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8.html#_4","text":"\u6211\u4eec\u5b66\u4e60\u4e86\u4e00\u4e9b\u7b80\u5355\u7684\u67e5\u770b\u547d\u4ee4\u4ee5\u9002\u5e94\u670d\u52a1\u5668\u73af\u5883\u3002 \u5f53\u7136\u8be5\u955c\u50cf\u670d\u52a1\u5668\u4e0a\u8fd8\u5b89\u88c5\u4e86\u5f88\u591a\u5de5\u5177\uff0c\u5982nginx\uff0credis\uff0csupervisor\u7b49\u7b49\uff0c\u8fd9\u5728\u6211\u4eec\u8bfe\u7a0b\u7684\u540e\u671f\u4f1a\u7528\u5230\u3002 \u81ea\u5df1\u8d2d\u4e70\u670d\u52a1\u5668\u7684\u5c0f\u4f19\u4f34\u4e5f\u4e0d\u7528\u62c5\u5fc3\uff0c\u6211\u4eec\u4e5f\u6709\u5bf9\u5e94\u7684\u5b89\u88c5\u6559\u7a0b\u3002","title":"\u603b\u7ed3"},{"location":"01_mkdocs_NLP/2_%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8.html#_5","text":"\u4e3a\u4e86\u66f4\u597d\u7684\u8fde\u63a5\u6d77\u5916\u670d\u52a1\u5668\uff0c\u5927\u5bb6\u53ef\u4ee5\u4f7f\u7528 finalshell \u8fdb\u884c\u767b\u9646\u3002\u70b9\u51fb\u8fdb\u884c\u4e0b\u8f7d\uff1a finalshell WINDOWS\u7248\u672c finalshell macOS\u7248\u672c \u670d\u52a1\u5668\u76f8\u5173\u544a\u77e5: 1\uff0c\u670d\u52a1\u5668\u8ba2\u8d2d\u662f\u5b66\u5458\u4e3a\u4e86\u66f4\u597d\u7684\u9002\u5e94\u4f01\u4e1a\u5f00\u53d1\u73af\u5883\uff0c\u66f4\u597d\u7684\u5b9e\u73b0AI\u5f00\u53d1\uff0c\u81ea\u4e3b\u81ea\u613f\u7684\u884c\u4e3a\u3002 2\uff0c\u6559\u5e08\u56e2\u961f\u5c06\u4e3a\u5176\u63d0\u4f9b\u6307\u5b9a\u6570\u91cf\u7684\u670d\u52a1\u5668\uff0c \u914d\u7f6e\u4e3a4\u6838\uff0c8G\u5185\u5b58 100G\u786c\u76d8 ==> 350\u5143/\u6708 3\uff0c\u4e91\u670d\u52a1\u5668\u7531\u4f20\u667a\u4e13\u4e1a\u8fd0\u7ef4\u56e2\u961f\u7ef4\u62a4\uff0c\u5728\u4e0b\u53d1\u7ed9\u5b66\u5458\u524d\uff0c\u914d\u7f6e\u6307\u5b9a\u7684AI\u955c\u50cf\u73af\u5883\u3002 4\uff0c\u4f20\u667aAI\u56e2\u961f\u5e2e\u52a9\u5b66\u5458\u89e3\u51b3\u5b66\u5458\u5728\u521d\u671f\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5404\u7c7b\u8f6f\u4ef6\u95ee\u9898\u3002 5\uff0c\u4f20\u667a\u8fd0\u7ef4\u56e2\u961f\u5e2e\u52a9\u5b66\u5458\u5904\u7406\u4e91\u670d\u52a1\u5668\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5404\u7c7b\u786c\u4ef6\u5b89\u5168\u548c\u8fd0\u8425\u5546\u95ee\u9898\u3002 6\uff0c\u5b66\u5458\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u4e0d\u5f97\u4f7f\u7528\u670d\u52a1\u5668\u8fdb\u884c\u8fdd\u6cd5\u5f00\u53d1\uff08\u642d\u5efa\u8fdd\u89c4\u7f51\u7ad9\uff0c\u9ed1\u5ba2\u653b\u51fb\u7b49\uff09\uff0c\u5426\u5219\u540e\u679c\u81ea\u8d1f\u3002","title":"\u5176\u4ed6\u8bf4\u660e"},{"location":"02_mkdocs_preprocess/index.html","text":"\u00b6","title":"Index"},{"location":"02_mkdocs_preprocess/index.html#_1","text":"","title":""},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6587\u672c\u9884\u5904\u7406\u76f8\u5173\u5185\u5bb9 1 \u6587\u672c\u9884\u5904\u7406\u53ca\u5176\u4f5c\u7528 \u00b6 \u6587\u672c\u8bed\u6599\u5728\u8f93\u9001\u7ed9\u6a21\u578b\u524d\u4e00\u822c\u9700\u8981\u4e00\u7cfb\u5217\u7684\u9884\u5904\u7406\u5de5\u4f5c, \u624d\u80fd\u7b26\u5408\u6a21\u578b\u8f93\u5165\u7684\u8981\u6c42, \u5982: \u5c06\u6587\u672c\u8f6c\u5316\u6210\u6a21\u578b\u9700\u8981\u7684\u5f20\u91cf, \u89c4\u8303\u5f20\u91cf\u7684\u5c3a\u5bf8\u7b49, \u800c\u4e14\u79d1\u5b66\u7684\u6587\u672c\u9884\u5904\u7406\u73af\u8282\u8fd8\u5c06\u6709\u6548\u6307\u5bfc\u6a21\u578b\u8d85\u53c2\u6570\u7684\u9009\u62e9, \u63d0\u5347\u6a21\u578b\u7684\u8bc4\u4f30\u6307\u6807. 2 \u6587\u672c\u9884\u5904\u7406\u4e2d\u5305\u542b\u7684\u4e3b\u8981\u73af\u8282 \u00b6 \u6587\u672c\u5904\u7406\u7684\u57fa\u672c\u65b9\u6cd5 \u6587\u672c\u5f20\u91cf\u8868\u793a\u65b9\u6cd5 \u6587\u672c\u8bed\u6599\u7684\u6570\u636e\u5206\u6790 \u6587\u672c\u7279\u5f81\u5904\u7406 \u6570\u636e\u589e\u5f3a\u65b9\u6cd5 2.1 \u6587\u672c\u5904\u7406\u7684\u57fa\u672c\u65b9\u6cd5 \u00b6 \u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b 2.2 \u6587\u672c\u5f20\u91cf\u8868\u793a\u65b9\u6cd5 \u00b6 one-hot\u7f16\u7801 Word2vec Word Embedding 2.3 \u6587\u672c\u8bed\u6599\u7684\u6570\u636e\u5206\u6790 \u00b6 \u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u8bcd\u9891\u7edf\u8ba1\u4e0e\u5173\u952e\u8bcd\u8bcd\u4e91 2.4 \u6587\u672c\u7279\u5f81\u5904\u7406 \u00b6 \u6dfb\u52a0n-gram\u7279\u5f81 \u6587\u672c\u957f\u5ea6\u89c4\u8303 2.5 \u6570\u636e\u589e\u5f3a\u65b9\u6cd5 \u00b6 \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5 2.6 \u91cd\u8981\u8bf4\u660e \u00b6 \u5728\u5b9e\u9645\u751f\u4ea7\u5e94\u7528\u4e2d, \u6211\u4eec\u6700\u5e38\u4f7f\u7528\u7684\u4e24\u79cd\u8bed\u8a00\u662f\u4e2d\u6587\u548c\u82f1\u6587\uff0c\u56e0\u6b64\u6587\u672c\u9884\u5904\u7406\u90e8\u5206\u7684\u5185\u5bb9\u90fd\u5c06\u9488\u5bf9\u8fd9\u4e24\u79cd\u8bed\u8a00\u8fdb\u884c\u8bb2\u89e3.","title":"1 \u8ba4\u8bc6\u6587\u672c\u9884\u5904\u7406"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#_1","text":"\u4e86\u89e3\u6587\u672c\u9884\u5904\u7406\u76f8\u5173\u5185\u5bb9","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#1","text":"\u6587\u672c\u8bed\u6599\u5728\u8f93\u9001\u7ed9\u6a21\u578b\u524d\u4e00\u822c\u9700\u8981\u4e00\u7cfb\u5217\u7684\u9884\u5904\u7406\u5de5\u4f5c, \u624d\u80fd\u7b26\u5408\u6a21\u578b\u8f93\u5165\u7684\u8981\u6c42, \u5982: \u5c06\u6587\u672c\u8f6c\u5316\u6210\u6a21\u578b\u9700\u8981\u7684\u5f20\u91cf, \u89c4\u8303\u5f20\u91cf\u7684\u5c3a\u5bf8\u7b49, \u800c\u4e14\u79d1\u5b66\u7684\u6587\u672c\u9884\u5904\u7406\u73af\u8282\u8fd8\u5c06\u6709\u6548\u6307\u5bfc\u6a21\u578b\u8d85\u53c2\u6570\u7684\u9009\u62e9, \u63d0\u5347\u6a21\u578b\u7684\u8bc4\u4f30\u6307\u6807.","title":"1 \u6587\u672c\u9884\u5904\u7406\u53ca\u5176\u4f5c\u7528"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#2","text":"\u6587\u672c\u5904\u7406\u7684\u57fa\u672c\u65b9\u6cd5 \u6587\u672c\u5f20\u91cf\u8868\u793a\u65b9\u6cd5 \u6587\u672c\u8bed\u6599\u7684\u6570\u636e\u5206\u6790 \u6587\u672c\u7279\u5f81\u5904\u7406 \u6570\u636e\u589e\u5f3a\u65b9\u6cd5","title":"2 \u6587\u672c\u9884\u5904\u7406\u4e2d\u5305\u542b\u7684\u4e3b\u8981\u73af\u8282"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#21","text":"\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b","title":"2.1 \u6587\u672c\u5904\u7406\u7684\u57fa\u672c\u65b9\u6cd5"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#22","text":"one-hot\u7f16\u7801 Word2vec Word Embedding","title":"2.2 \u6587\u672c\u5f20\u91cf\u8868\u793a\u65b9\u6cd5"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#23","text":"\u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u8bcd\u9891\u7edf\u8ba1\u4e0e\u5173\u952e\u8bcd\u8bcd\u4e91","title":"2.3 \u6587\u672c\u8bed\u6599\u7684\u6570\u636e\u5206\u6790"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#24","text":"\u6dfb\u52a0n-gram\u7279\u5f81 \u6587\u672c\u957f\u5ea6\u89c4\u8303","title":"2.4 \u6587\u672c\u7279\u5f81\u5904\u7406"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#25","text":"\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5","title":"2.5 \u6570\u636e\u589e\u5f3a\u65b9\u6cd5"},{"location":"02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html#26","text":"\u5728\u5b9e\u9645\u751f\u4ea7\u5e94\u7528\u4e2d, \u6211\u4eec\u6700\u5e38\u4f7f\u7528\u7684\u4e24\u79cd\u8bed\u8a00\u662f\u4e2d\u6587\u548c\u82f1\u6587\uff0c\u56e0\u6b64\u6587\u672c\u9884\u5904\u7406\u90e8\u5206\u7684\u5185\u5bb9\u90fd\u5c06\u9488\u5bf9\u8fd9\u4e24\u79cd\u8bed\u8a00\u8fdb\u884c\u8bb2\u89e3.","title":"2.6 \u91cd\u8981\u8bf4\u660e"},{"location":"02_mkdocs_preprocess/2%20%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662f\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u53ca\u5176\u5b83\u4eec\u7684\u4f5c\u7528. \u638c\u63e1\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6d41\u884c\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5. 1 \u4ec0\u4e48\u662f\u5206\u8bcd \u00b6 \u5206\u8bcd\u5c31\u662f\u5c06\u8fde\u7eed\u7684\u5b57\u5e8f\u5217\u6309\u7167\u4e00\u5b9a\u7684\u89c4\u8303\u91cd\u65b0\u7ec4\u5408\u6210\u8bcd\u5e8f\u5217\u7684\u8fc7\u7a0b\u3002\u5728\u82f1\u6587\u7684\u884c\u6587\u4e2d\uff0c\u5355\u8bcd\u4e4b\u95f4\u662f\u4ee5\u7a7a\u683c\u4f5c\u4e3a\u81ea\u7136\u5206\u754c\u7b26\u7684\uff0c\u800c\u4e2d\u6587\u53ea\u662f\u5b57\u3001\u53e5\u548c\u6bb5\u80fd\u901a\u8fc7\u660e\u663e\u7684\u5206\u754c\u7b26\u6765\u7b80\u5355\u5212\u754c\uff0c\u552f\u72ec\u8bcd\u6ca1\u6709\u4e00\u4e2a\u5f62\u5f0f\u4e0a\u7684\u5206\u754c\u7b26\u3002\u5206\u8bcd\u8fc7\u7a0b\u5c31\u662f\u627e\u5230\u8fd9\u6837\u5206\u754c\u7b26\u7684\u8fc7\u7a0b. \u4e3e\u4e2a\u4f8b\u5b50: \u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd ['\u4f20\u667a', '\u6559\u80b2', '\u662f', '\u4e00\u5bb6', '\u4e0a\u5e02\u516c\u53f8', '\uff0c', '\u65d7\u4e0b', '\u6709', '\u9ed1\u9a6c', '\u7a0b\u5e8f\u5458', '\u54c1\u724c', '\u3002', '\u6211', '\u662f', '\u5728', '\u9ed1\u9a6c', '\u8fd9\u91cc', '\u5b66\u4e60', '\u4eba\u5de5\u667a\u80fd'] \u5206\u8bcd\u7684\u4f5c\u7528: \u8bcd\u4f5c\u4e3a\u8bed\u8a00\u8bed\u4e49\u7406\u89e3\u7684\u6700\u5c0f\u5355\u5143, \u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u8bed\u8a00\u7684\u57fa\u7840. \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1, \u5982\u81ea\u52a8\u95ee\u7b54, \u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u6d41\u884c\u4e2d\u6587\u5206\u8bcd\u5de5\u5177jieba: \u613f\u666f: \u201c\u7ed3\u5df4\u201d\u4e2d\u6587\u5206\u8bcd, \u505a\u6700\u597d\u7684 Python \u4e2d\u6587\u5206\u8bcd\u7ec4\u4ef6. jieba\u7684\u7279\u6027: \u652f\u6301\u591a\u79cd\u5206\u8bcd\u6a21\u5f0f \u7cbe\u786e\u6a21\u5f0f \u5168\u6a21\u5f0f \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f \u652f\u6301\u4e2d\u6587\u7e41\u4f53\u5206\u8bcd \u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 jieba\u7684\u5b89\u88c5: pip install jieba jieba\u7684\u4f7f\u7528: \u7cbe\u786e\u6a21\u5f0f\u5206\u8bcd: \u8bd5\u56fe\u5c06\u53e5\u5b50\u6700\u7cbe\u786e\u5730\u5207\u5f00\uff0c\u9002\u5408\u6587\u672c\u5206\u6790. import jieba content = \"\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd\" # \u7cbe\u786e\u6a21\u578b\uff1a\u8bd5\u56fe\u5c06\u53e5\u5b50\u6700\u7cbe\u786e\u5730\u5207\u5f00\uff0c\u9002\u5408\u6587\u672c\u5206\u6790\u3002\u4e5f\u5c5e\u4e8e\u9ed8\u8ba4\u6a21\u5f0f jieba . cut ( content , cut_all = False ) # cut_all\u9ed8\u8ba4\u4e3aFalse # \u5c06\u8fd4\u56de\u4e00\u4e2a\u751f\u6210\u5668\u5bf9\u8c61 < generator object Tokenizer . cut at 0x7f8d9053e650 > # \u82e5\u9700\u76f4\u63a5\u8fd4\u56de\u5217\u8868\u5185\u5bb9, \u4f7f\u7528jieba.lcut\u5373\u53ef jieba . lcut ( content , cut_all = False ) [ '\u4f20\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' ] \u5168\u6a21\u5f0f\u5206\u8bcd: \u628a\u53e5\u5b50\u4e2d\u6240\u6709\u7684\u53ef\u4ee5\u6210\u8bcd\u7684\u8bcd\u8bed\u90fd\u626b\u63cf\u51fa\u6765, \u901f\u5ea6\u975e\u5e38\u5feb\uff0c\u4f46\u662f\u4e0d\u80fd\u6d88\u9664\u6b67\u4e49\u3002 # \u82e5\u9700\u76f4\u63a5\u8fd4\u56de\u5217\u8868\u5185\u5bb9, \u4f7f\u7528jieba.lcut\u5373\u53ef jieba . lcut ( content , cut_all = True ) [ '\u4f20' , '\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02' , '\u4e0a\u5e02\u516c\u53f8' , '\u516c\u53f8' , '' , '' , '\u65d7\u4e0b' , '\u4e0b\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '' , '' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5' , '\u4eba\u5de5\u667a\u80fd' , '\u667a\u80fd' ] # \u6ce8\u610f1\uff1a\u4eba\u5de5\u667a\u80fd\u5168\u6a21\u578b\u5206\u6210\u4e09\u4e2a\u8bcd # \u6ce8\u610f2\uff1a\u9017\u53f7\u548c\u53e5\u53f7\u4e5f\u7ed9\u5206\u6210\u4e86\u8bcd \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\u5206\u8bcd: \u5728\u7cbe\u786e\u6a21\u5f0f\u7684\u57fa\u7840\u4e0a\uff0c\u5bf9\u957f\u8bcd\u518d\u6b21\u5207\u5206\uff0c\u63d0\u9ad8\u53ec\u56de\u7387\uff0c\u9002\u5408\u7528\u4e8e\u641c\u7d22\u5f15\u64ce\u5206\u8bcd\u3002 import jieba content = \"\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd\" jieba . cut_for_search ( content ) # \u5c06\u8fd4\u56de\u4e00\u4e2a\u751f\u6210\u5668\u5bf9\u8c61 < generator object Tokenizer . cut_for_search at 0x7f8d90e5a550 > # \u82e5\u9700\u76f4\u63a5\u8fd4\u56de\u5217\u8868\u5185\u5bb9, \u4f7f\u7528jieba.lcut_for_search\u5373\u53ef jieba . lcut_for_search ( content ) [ '\u4f20\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02' , '\u516c\u53f8' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5' , '\u667a\u80fd' , '\u4eba\u5de5\u667a\u80fd' ] # \u5bf9'\u65e0\u7ebf\u7535'\u7b49\u8f83\u957f\u8bcd\u6c47\u90fd\u8fdb\u884c\u4e86\u518d\u6b21\u5206\u8bcd. \u4e2d\u6587\u7e41\u4f53\u5206\u8bcd: \u9488\u5bf9\u4e2d\u56fd\u9999\u6e2f, \u53f0\u6e7e\u5730\u533a\u7684\u7e41\u4f53\u6587\u672c\u8fdb\u884c\u5206\u8bcd\u3002 import jieba content = \"\u7169\u60f1\u5373\u662f\u83e9\u63d0\uff0c\u6211\u66ab\u4e14\u4e0d\u63d0\" jieba . lcut ( content ) [ '\u7169\u60f1' , '\u5373' , '\u662f' , '\u83e9\u63d0' , '\uff0c' , '\u6211' , '\u66ab\u4e14' , '\u4e0d' , '\u63d0' ] \u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178: \u6dfb\u52a0\u81ea\u5b9a\u4e49\u8bcd\u5178\u540e, jieba\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u8bcd\u5178\u4e2d\u51fa\u73b0\u7684\u8bcd\u6c47\uff0c\u63d0\u5347\u6574\u4f53\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002 \u8bcd\u5178\u683c\u5f0f: \u6bcf\u4e00\u884c\u5206\u4e09\u90e8\u5206\uff1a\u8bcd\u8bed\u3001\u8bcd\u9891\uff08\u53ef\u7701\u7565\uff09\u3001\u8bcd\u6027\uff08\u53ef\u7701\u7565\uff09\uff0c\u7528\u7a7a\u683c\u9694\u5f00\uff0c\u987a\u5e8f\u4e0d\u53ef\u98a0\u5012\u3002 \u8bcd\u5178\u6837\u5f0f\u5982\u4e0b, \u5177\u4f53\u8bcd\u6027\u542b\u4e49\u8bf7\u53c2\u7167 7 jieba\u8bcd\u6027\u5bf9\u7167\u8868 , \u5c06\u8be5\u8bcd\u5178\u5b58\u4e3auserdict.txt, \u65b9\u4fbf\u4e4b\u540e\u52a0\u8f7d\u4f7f\u7528\u3002 # \u683c\u5f0f\uff1aword1 freq1 word_type1 \u9ed1\u9a6c\u7a0b\u5e8f\u5458 5 n \u4f20\u667a\u6559\u80b2 6 n \u4eba\u5de5\u667a\u80fd 7 nz \u5b66\u4e60 3 \u4e0a\u5e02 3 import jieba sentence = '\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd' # 1 \u6ca1\u6709\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 mydata = jieba . lcut ( sentence , cut_all = False ) print ( 'mydata-->' , mydata ) # 2 \u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 jieba . load_userdict ( \"./userdict.txt\" ) mydata2 = jieba . lcut ( sentence , cut_all = False ) print ( 'mydata2-->' , mydata2 ) # \u6ca1\u6709\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178\u7684\u5206\u8bcd\u6548\u679c mydata --> [ '\u4f20\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' ] # \u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178\u7684\u5206\u8bcd\u6548\u679c mydata2 --> [ '\u4f20\u667a\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' ] 2 \u4ec0\u4e48\u662f\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u00b6 \u547d\u540d\u5b9e\u4f53: \u901a\u5e38\u6211\u4eec\u5c06\u4eba\u540d, \u5730\u540d, \u673a\u6784\u540d\u7b49\u4e13\u6709\u540d\u8bcd\u7edf\u79f0\u547d\u540d\u5b9e\u4f53. \u5982: \u5468\u6770\u4f26, \u9ed1\u5c71\u53bf, \u5b54\u5b50\u5b66\u9662, 24\u8f8a\u65b9\u94a2\u77eb\u76f4\u673a. \u987e\u540d\u601d\u4e49, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b(Named Entity Recognition\uff0c\u7b80\u79f0NER)\u5c31\u662f\u8bc6\u522b\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u547d\u540d\u5b9e\u4f53. \u4e3e\u4e2a\u4f8b\u5b50: \u9c81\u8fc5, \u6d59\u6c5f\u7ecd\u5174\u4eba, \u4e94\u56db\u65b0\u6587\u5316\u8fd0\u52a8\u7684\u91cd\u8981\u53c2\u4e0e\u8005, \u4ee3\u8868\u4f5c\u671d\u82b1\u5915\u62fe. ==> \u9c81\u8fc5(\u4eba\u540d) / \u6d59\u6c5f\u7ecd\u5174(\u5730\u540d)\u4eba / \u4e94\u56db\u65b0\u6587\u5316\u8fd0\u52a8(\u4e13\u6709\u540d\u8bcd) / \u91cd\u8981\u53c2\u4e0e\u8005 / \u4ee3\u8868\u4f5c / \u671d\u82b1\u5915\u62fe(\u4e13\u6709\u540d\u8bcd) \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u4f5c\u7528: \u540c\u8bcd\u6c47\u4e00\u6837, \u547d\u540d\u5b9e\u4f53\u4e5f\u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u7684\u57fa\u7840\u5355\u5143, \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. 3 \u4ec0\u4e48\u662f\u8bcd\u6027\u6807\u6ce8 \u00b6 \u8bcd\u6027: \u8bed\u8a00\u4e2d\u5bf9\u8bcd\u7684\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u4ee5\u8bed\u6cd5\u7279\u5f81\u4e3a\u4e3b\u8981\u4f9d\u636e\u3001\u517c\u987e\u8bcd\u6c47\u610f\u4e49\u5bf9\u8bcd\u8fdb\u884c\u5212\u5206\u7684\u7ed3\u679c, \u5e38\u89c1\u7684\u8bcd\u6027\u670914\u79cd, \u5982: \u540d\u8bcd, \u52a8\u8bcd, \u5f62\u5bb9\u8bcd\u7b49. \u987e\u540d\u601d\u4e49, \u8bcd\u6027\u6807\u6ce8(Part-Of-Speech tagging, \u7b80\u79f0POS)\u5c31\u662f\u6807\u6ce8\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u6bcf\u4e2a\u8bcd\u6c47\u7684\u8bcd\u6027. \u4e3e\u4e2a\u4f8b\u5b50: \u6211\u7231\u81ea\u7136\u8bed\u8a00\u5904\u7406 ==> \u6211/rr, \u7231/v, \u81ea\u7136\u8bed\u8a00/n, \u5904\u7406/vn rr: \u4eba\u79f0\u4ee3\u8bcd v: \u52a8\u8bcd n: \u540d\u8bcd vn: \u52a8\u540d\u8bcd \u8bcd\u6027\u6807\u6ce8\u7684\u4f5c\u7528: \u8bcd\u6027\u6807\u6ce8\u4ee5\u5206\u8bcd\u4e3a\u57fa\u7840, \u662f\u5bf9\u6587\u672c\u8bed\u8a00\u7684\u53e6\u4e00\u4e2a\u89d2\u5ea6\u7684\u7406\u89e3, \u56e0\u6b64\u4e5f\u5e38\u5e38\u6210\u4e3aAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u4f7f\u7528jieba\u8fdb\u884c\u4e2d\u6587\u8bcd\u6027\u6807\u6ce8: import jieba.posseg as pseg pseg . lcut ( \"\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\" ) [ pair ( '\u6211' , 'r' ), pair ( '\u7231' , 'v' ), pair ( '\u5317\u4eac' , 'ns' ), pair ( '\u5929\u5b89\u95e8' , 'ns' )] # \u7ed3\u679c\u8fd4\u56de\u4e00\u4e2a\u88c5\u6709pair\u5143\u7ec4\u7684\u5217\u8868, \u6bcf\u4e2apair\u5143\u7ec4\u4e2d\u5206\u522b\u662f\u8bcd\u6c47\u53ca\u5176\u5bf9\u5e94\u7684\u8bcd\u6027, \u5177\u4f53\u8bcd\u6027\u542b\u4e49\u8bf7\u53c2\u7167[\u9644\u5f55: jieba\u8bcd\u6027\u5bf9\u7167\u8868]() 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u5206\u8bcd: \u5206\u8bcd\u5c31\u662f\u5c06\u8fde\u7eed\u7684\u5b57\u5e8f\u5217\u6309\u7167\u4e00\u5b9a\u7684\u89c4\u8303\u91cd\u65b0\u7ec4\u5408\u6210\u8bcd\u5e8f\u5217\u7684\u8fc7\u7a0b\u3002\u6211\u4eec\u77e5\u9053\uff0c\u5728\u82f1\u6587\u7684\u884c\u6587\u4e2d\uff0c\u5355\u8bcd\u4e4b\u95f4\u662f\u4ee5\u7a7a\u683c\u4f5c\u4e3a\u81ea\u7136\u5206\u754c\u7b26\u7684\uff0c\u800c\u4e2d\u6587\u53ea\u662f\u5b57\u3001\u53e5\u548c\u6bb5\u80fd\u901a\u8fc7\u660e\u663e\u7684\u5206\u754c\u7b26\u6765\u7b80\u5355\u5212\u754c\uff0c\u552f\u72ec\u8bcd\u6ca1\u6709\u4e00\u4e2a\u5f62\u5f0f\u4e0a\u7684\u5206\u754c\u7b26, \u5206\u8bcd\u8fc7\u7a0b\u5c31\u662f\u627e\u5230\u8fd9\u6837\u5206\u754c\u7b26\u7684\u8fc7\u7a0b. \u5b66\u4e60\u4e86\u5206\u8bcd\u7684\u4f5c\u7528: \u8bcd\u4f5c\u4e3a\u8bed\u8a00\u8bed\u4e49\u7406\u89e3\u7684\u6700\u5c0f\u5355\u5143, \u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u8bed\u8a00\u7684\u57fa\u7840. \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1, \u5982\u81ea\u52a8\u95ee\u7b54, \u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u5b66\u4e60\u4e86\u6d41\u884c\u4e2d\u6587\u5206\u8bcd\u5de5\u5177jieba: \u652f\u6301\u591a\u79cd\u5206\u8bcd\u6a21\u5f0f: \u7cbe\u786e\u6a21\u5f0f, \u5168\u6a21\u5f0f, \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f \u652f\u6301\u4e2d\u6587\u7e41\u4f53\u5206\u8bcd \u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 \u5b66\u4e60\u4e86jieba\u5de5\u5177\u7684\u5b89\u88c5\u548c\u5206\u8bcd\u4f7f\u7528. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u547d\u540d\u5b9e\u4f53\u8bc6\u522b: \u547d\u540d\u5b9e\u4f53: \u901a\u5e38\u6211\u4eec\u5c06\u4eba\u540d, \u5730\u540d, \u673a\u6784\u540d\u7b49\u4e13\u6709\u540d\u8bcd\u7edf\u79f0\u547d\u540d\u5b9e\u4f53. \u5982: \u5468\u6770\u4f26, \u9ed1\u5c71\u53bf, \u5b54\u5b50\u5b66\u9662, 24\u8f8a\u65b9\u94a2\u77eb\u76f4\u673a. \u987e\u540d\u601d\u4e49, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b(Named Entity Recognition\uff0c\u7b80\u79f0NER)\u5c31\u662f\u8bc6\u522b\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u547d\u540d\u5b9e\u4f53. \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u4f5c\u7528: \u540c\u8bcd\u6c47\u4e00\u6837, \u547d\u540d\u5b9e\u4f53\u4e5f\u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u7684\u57fa\u7840\u5355\u5143, \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u8bcd\u6027\u6807\u6ce8: \u8bcd\u6027: \u8bed\u8a00\u4e2d\u5bf9\u8bcd\u7684\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u4ee5\u8bed\u6cd5\u7279\u5f81\u4e3a\u4e3b\u8981\u4f9d\u636e\u3001\u517c\u987e\u8bcd\u6c47\u610f\u4e49\u5bf9\u8bcd\u8fdb\u884c\u5212\u5206\u7684\u7ed3\u679c, \u5e38\u89c1\u7684\u8bcd\u6027\u670914\u79cd, \u5982: \u540d\u8bcd, \u52a8\u8bcd, \u5f62\u5bb9\u8bcd\u7b49. \u987e\u540d\u601d\u4e49, \u8bcd\u6027\u6807\u6ce8(Part-Of-Speech tagging, \u7b80\u79f0POS)\u5c31\u662f\u6807\u6ce8\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u6bcf\u4e2a\u8bcd\u6c47\u7684\u8bcd\u6027. \u5b66\u4e60\u4e86\u8bcd\u6027\u6807\u6ce8\u7684\u4f5c\u7528: \u8bcd\u6027\u6807\u6ce8\u4ee5\u5206\u8bcd\u4e3a\u57fa\u7840, \u662f\u5bf9\u6587\u672c\u8bed\u8a00\u7684\u53e6\u4e00\u4e2a\u89d2\u5ea6\u7684\u7406\u89e3, \u56e0\u6b64\u4e5f\u5e38\u5e38\u6210\u4e3aAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u5b66\u4e60\u4e86\u4f7f\u7528jieba\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8.","title":"2 \u6587\u672c\u5904\u7406\u7684\u57fa\u672c\u65b9\u6cd5"},{"location":"02_mkdocs_preprocess/2%20%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662f\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u53ca\u5176\u5b83\u4eec\u7684\u4f5c\u7528. \u638c\u63e1\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6d41\u884c\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"02_mkdocs_preprocess/2%20%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95.html#1","text":"\u5206\u8bcd\u5c31\u662f\u5c06\u8fde\u7eed\u7684\u5b57\u5e8f\u5217\u6309\u7167\u4e00\u5b9a\u7684\u89c4\u8303\u91cd\u65b0\u7ec4\u5408\u6210\u8bcd\u5e8f\u5217\u7684\u8fc7\u7a0b\u3002\u5728\u82f1\u6587\u7684\u884c\u6587\u4e2d\uff0c\u5355\u8bcd\u4e4b\u95f4\u662f\u4ee5\u7a7a\u683c\u4f5c\u4e3a\u81ea\u7136\u5206\u754c\u7b26\u7684\uff0c\u800c\u4e2d\u6587\u53ea\u662f\u5b57\u3001\u53e5\u548c\u6bb5\u80fd\u901a\u8fc7\u660e\u663e\u7684\u5206\u754c\u7b26\u6765\u7b80\u5355\u5212\u754c\uff0c\u552f\u72ec\u8bcd\u6ca1\u6709\u4e00\u4e2a\u5f62\u5f0f\u4e0a\u7684\u5206\u754c\u7b26\u3002\u5206\u8bcd\u8fc7\u7a0b\u5c31\u662f\u627e\u5230\u8fd9\u6837\u5206\u754c\u7b26\u7684\u8fc7\u7a0b. \u4e3e\u4e2a\u4f8b\u5b50: \u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd ['\u4f20\u667a', '\u6559\u80b2', '\u662f', '\u4e00\u5bb6', '\u4e0a\u5e02\u516c\u53f8', '\uff0c', '\u65d7\u4e0b', '\u6709', '\u9ed1\u9a6c', '\u7a0b\u5e8f\u5458', '\u54c1\u724c', '\u3002', '\u6211', '\u662f', '\u5728', '\u9ed1\u9a6c', '\u8fd9\u91cc', '\u5b66\u4e60', '\u4eba\u5de5\u667a\u80fd'] \u5206\u8bcd\u7684\u4f5c\u7528: \u8bcd\u4f5c\u4e3a\u8bed\u8a00\u8bed\u4e49\u7406\u89e3\u7684\u6700\u5c0f\u5355\u5143, \u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u8bed\u8a00\u7684\u57fa\u7840. \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1, \u5982\u81ea\u52a8\u95ee\u7b54, \u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u6d41\u884c\u4e2d\u6587\u5206\u8bcd\u5de5\u5177jieba: \u613f\u666f: \u201c\u7ed3\u5df4\u201d\u4e2d\u6587\u5206\u8bcd, \u505a\u6700\u597d\u7684 Python \u4e2d\u6587\u5206\u8bcd\u7ec4\u4ef6. jieba\u7684\u7279\u6027: \u652f\u6301\u591a\u79cd\u5206\u8bcd\u6a21\u5f0f \u7cbe\u786e\u6a21\u5f0f \u5168\u6a21\u5f0f \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f \u652f\u6301\u4e2d\u6587\u7e41\u4f53\u5206\u8bcd \u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 jieba\u7684\u5b89\u88c5: pip install jieba jieba\u7684\u4f7f\u7528: \u7cbe\u786e\u6a21\u5f0f\u5206\u8bcd: \u8bd5\u56fe\u5c06\u53e5\u5b50\u6700\u7cbe\u786e\u5730\u5207\u5f00\uff0c\u9002\u5408\u6587\u672c\u5206\u6790. import jieba content = \"\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd\" # \u7cbe\u786e\u6a21\u578b\uff1a\u8bd5\u56fe\u5c06\u53e5\u5b50\u6700\u7cbe\u786e\u5730\u5207\u5f00\uff0c\u9002\u5408\u6587\u672c\u5206\u6790\u3002\u4e5f\u5c5e\u4e8e\u9ed8\u8ba4\u6a21\u5f0f jieba . cut ( content , cut_all = False ) # cut_all\u9ed8\u8ba4\u4e3aFalse # \u5c06\u8fd4\u56de\u4e00\u4e2a\u751f\u6210\u5668\u5bf9\u8c61 < generator object Tokenizer . cut at 0x7f8d9053e650 > # \u82e5\u9700\u76f4\u63a5\u8fd4\u56de\u5217\u8868\u5185\u5bb9, \u4f7f\u7528jieba.lcut\u5373\u53ef jieba . lcut ( content , cut_all = False ) [ '\u4f20\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' ] \u5168\u6a21\u5f0f\u5206\u8bcd: \u628a\u53e5\u5b50\u4e2d\u6240\u6709\u7684\u53ef\u4ee5\u6210\u8bcd\u7684\u8bcd\u8bed\u90fd\u626b\u63cf\u51fa\u6765, \u901f\u5ea6\u975e\u5e38\u5feb\uff0c\u4f46\u662f\u4e0d\u80fd\u6d88\u9664\u6b67\u4e49\u3002 # \u82e5\u9700\u76f4\u63a5\u8fd4\u56de\u5217\u8868\u5185\u5bb9, \u4f7f\u7528jieba.lcut\u5373\u53ef jieba . lcut ( content , cut_all = True ) [ '\u4f20' , '\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02' , '\u4e0a\u5e02\u516c\u53f8' , '\u516c\u53f8' , '' , '' , '\u65d7\u4e0b' , '\u4e0b\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '' , '' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5' , '\u4eba\u5de5\u667a\u80fd' , '\u667a\u80fd' ] # \u6ce8\u610f1\uff1a\u4eba\u5de5\u667a\u80fd\u5168\u6a21\u578b\u5206\u6210\u4e09\u4e2a\u8bcd # \u6ce8\u610f2\uff1a\u9017\u53f7\u548c\u53e5\u53f7\u4e5f\u7ed9\u5206\u6210\u4e86\u8bcd \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f\u5206\u8bcd: \u5728\u7cbe\u786e\u6a21\u5f0f\u7684\u57fa\u7840\u4e0a\uff0c\u5bf9\u957f\u8bcd\u518d\u6b21\u5207\u5206\uff0c\u63d0\u9ad8\u53ec\u56de\u7387\uff0c\u9002\u5408\u7528\u4e8e\u641c\u7d22\u5f15\u64ce\u5206\u8bcd\u3002 import jieba content = \"\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd\" jieba . cut_for_search ( content ) # \u5c06\u8fd4\u56de\u4e00\u4e2a\u751f\u6210\u5668\u5bf9\u8c61 < generator object Tokenizer . cut_for_search at 0x7f8d90e5a550 > # \u82e5\u9700\u76f4\u63a5\u8fd4\u56de\u5217\u8868\u5185\u5bb9, \u4f7f\u7528jieba.lcut_for_search\u5373\u53ef jieba . lcut_for_search ( content ) [ '\u4f20\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02' , '\u516c\u53f8' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5' , '\u667a\u80fd' , '\u4eba\u5de5\u667a\u80fd' ] # \u5bf9'\u65e0\u7ebf\u7535'\u7b49\u8f83\u957f\u8bcd\u6c47\u90fd\u8fdb\u884c\u4e86\u518d\u6b21\u5206\u8bcd. \u4e2d\u6587\u7e41\u4f53\u5206\u8bcd: \u9488\u5bf9\u4e2d\u56fd\u9999\u6e2f, \u53f0\u6e7e\u5730\u533a\u7684\u7e41\u4f53\u6587\u672c\u8fdb\u884c\u5206\u8bcd\u3002 import jieba content = \"\u7169\u60f1\u5373\u662f\u83e9\u63d0\uff0c\u6211\u66ab\u4e14\u4e0d\u63d0\" jieba . lcut ( content ) [ '\u7169\u60f1' , '\u5373' , '\u662f' , '\u83e9\u63d0' , '\uff0c' , '\u6211' , '\u66ab\u4e14' , '\u4e0d' , '\u63d0' ] \u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178: \u6dfb\u52a0\u81ea\u5b9a\u4e49\u8bcd\u5178\u540e, jieba\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u8bcd\u5178\u4e2d\u51fa\u73b0\u7684\u8bcd\u6c47\uff0c\u63d0\u5347\u6574\u4f53\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002 \u8bcd\u5178\u683c\u5f0f: \u6bcf\u4e00\u884c\u5206\u4e09\u90e8\u5206\uff1a\u8bcd\u8bed\u3001\u8bcd\u9891\uff08\u53ef\u7701\u7565\uff09\u3001\u8bcd\u6027\uff08\u53ef\u7701\u7565\uff09\uff0c\u7528\u7a7a\u683c\u9694\u5f00\uff0c\u987a\u5e8f\u4e0d\u53ef\u98a0\u5012\u3002 \u8bcd\u5178\u6837\u5f0f\u5982\u4e0b, \u5177\u4f53\u8bcd\u6027\u542b\u4e49\u8bf7\u53c2\u7167 7 jieba\u8bcd\u6027\u5bf9\u7167\u8868 , \u5c06\u8be5\u8bcd\u5178\u5b58\u4e3auserdict.txt, \u65b9\u4fbf\u4e4b\u540e\u52a0\u8f7d\u4f7f\u7528\u3002 # \u683c\u5f0f\uff1aword1 freq1 word_type1 \u9ed1\u9a6c\u7a0b\u5e8f\u5458 5 n \u4f20\u667a\u6559\u80b2 6 n \u4eba\u5de5\u667a\u80fd 7 nz \u5b66\u4e60 3 \u4e0a\u5e02 3 import jieba sentence = '\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd' # 1 \u6ca1\u6709\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 mydata = jieba . lcut ( sentence , cut_all = False ) print ( 'mydata-->' , mydata ) # 2 \u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 jieba . load_userdict ( \"./userdict.txt\" ) mydata2 = jieba . lcut ( sentence , cut_all = False ) print ( 'mydata2-->' , mydata2 ) # \u6ca1\u6709\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178\u7684\u5206\u8bcd\u6548\u679c mydata --> [ '\u4f20\u667a' , '\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' ] # \u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178\u7684\u5206\u8bcd\u6548\u679c mydata2 --> [ '\u4f20\u667a\u6559\u80b2' , '\u662f' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u9ed1\u9a6c\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u6211' , '\u662f' , '\u5728' , '\u9ed1\u9a6c' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' ]","title":"1 \u4ec0\u4e48\u662f\u5206\u8bcd"},{"location":"02_mkdocs_preprocess/2%20%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95.html#2","text":"\u547d\u540d\u5b9e\u4f53: \u901a\u5e38\u6211\u4eec\u5c06\u4eba\u540d, \u5730\u540d, \u673a\u6784\u540d\u7b49\u4e13\u6709\u540d\u8bcd\u7edf\u79f0\u547d\u540d\u5b9e\u4f53. \u5982: \u5468\u6770\u4f26, \u9ed1\u5c71\u53bf, \u5b54\u5b50\u5b66\u9662, 24\u8f8a\u65b9\u94a2\u77eb\u76f4\u673a. \u987e\u540d\u601d\u4e49, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b(Named Entity Recognition\uff0c\u7b80\u79f0NER)\u5c31\u662f\u8bc6\u522b\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u547d\u540d\u5b9e\u4f53. \u4e3e\u4e2a\u4f8b\u5b50: \u9c81\u8fc5, \u6d59\u6c5f\u7ecd\u5174\u4eba, \u4e94\u56db\u65b0\u6587\u5316\u8fd0\u52a8\u7684\u91cd\u8981\u53c2\u4e0e\u8005, \u4ee3\u8868\u4f5c\u671d\u82b1\u5915\u62fe. ==> \u9c81\u8fc5(\u4eba\u540d) / \u6d59\u6c5f\u7ecd\u5174(\u5730\u540d)\u4eba / \u4e94\u56db\u65b0\u6587\u5316\u8fd0\u52a8(\u4e13\u6709\u540d\u8bcd) / \u91cd\u8981\u53c2\u4e0e\u8005 / \u4ee3\u8868\u4f5c / \u671d\u82b1\u5915\u62fe(\u4e13\u6709\u540d\u8bcd) \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u4f5c\u7528: \u540c\u8bcd\u6c47\u4e00\u6837, \u547d\u540d\u5b9e\u4f53\u4e5f\u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u7684\u57fa\u7840\u5355\u5143, \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282.","title":"2 \u4ec0\u4e48\u662f\u547d\u540d\u5b9e\u4f53\u8bc6\u522b"},{"location":"02_mkdocs_preprocess/2%20%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95.html#3","text":"\u8bcd\u6027: \u8bed\u8a00\u4e2d\u5bf9\u8bcd\u7684\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u4ee5\u8bed\u6cd5\u7279\u5f81\u4e3a\u4e3b\u8981\u4f9d\u636e\u3001\u517c\u987e\u8bcd\u6c47\u610f\u4e49\u5bf9\u8bcd\u8fdb\u884c\u5212\u5206\u7684\u7ed3\u679c, \u5e38\u89c1\u7684\u8bcd\u6027\u670914\u79cd, \u5982: \u540d\u8bcd, \u52a8\u8bcd, \u5f62\u5bb9\u8bcd\u7b49. \u987e\u540d\u601d\u4e49, \u8bcd\u6027\u6807\u6ce8(Part-Of-Speech tagging, \u7b80\u79f0POS)\u5c31\u662f\u6807\u6ce8\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u6bcf\u4e2a\u8bcd\u6c47\u7684\u8bcd\u6027. \u4e3e\u4e2a\u4f8b\u5b50: \u6211\u7231\u81ea\u7136\u8bed\u8a00\u5904\u7406 ==> \u6211/rr, \u7231/v, \u81ea\u7136\u8bed\u8a00/n, \u5904\u7406/vn rr: \u4eba\u79f0\u4ee3\u8bcd v: \u52a8\u8bcd n: \u540d\u8bcd vn: \u52a8\u540d\u8bcd \u8bcd\u6027\u6807\u6ce8\u7684\u4f5c\u7528: \u8bcd\u6027\u6807\u6ce8\u4ee5\u5206\u8bcd\u4e3a\u57fa\u7840, \u662f\u5bf9\u6587\u672c\u8bed\u8a00\u7684\u53e6\u4e00\u4e2a\u89d2\u5ea6\u7684\u7406\u89e3, \u56e0\u6b64\u4e5f\u5e38\u5e38\u6210\u4e3aAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u4f7f\u7528jieba\u8fdb\u884c\u4e2d\u6587\u8bcd\u6027\u6807\u6ce8: import jieba.posseg as pseg pseg . lcut ( \"\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\" ) [ pair ( '\u6211' , 'r' ), pair ( '\u7231' , 'v' ), pair ( '\u5317\u4eac' , 'ns' ), pair ( '\u5929\u5b89\u95e8' , 'ns' )] # \u7ed3\u679c\u8fd4\u56de\u4e00\u4e2a\u88c5\u6709pair\u5143\u7ec4\u7684\u5217\u8868, \u6bcf\u4e2apair\u5143\u7ec4\u4e2d\u5206\u522b\u662f\u8bcd\u6c47\u53ca\u5176\u5bf9\u5e94\u7684\u8bcd\u6027, \u5177\u4f53\u8bcd\u6027\u542b\u4e49\u8bf7\u53c2\u7167[\u9644\u5f55: jieba\u8bcd\u6027\u5bf9\u7167\u8868]()","title":"3 \u4ec0\u4e48\u662f\u8bcd\u6027\u6807\u6ce8"},{"location":"02_mkdocs_preprocess/2%20%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95.html#4","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u5206\u8bcd: \u5206\u8bcd\u5c31\u662f\u5c06\u8fde\u7eed\u7684\u5b57\u5e8f\u5217\u6309\u7167\u4e00\u5b9a\u7684\u89c4\u8303\u91cd\u65b0\u7ec4\u5408\u6210\u8bcd\u5e8f\u5217\u7684\u8fc7\u7a0b\u3002\u6211\u4eec\u77e5\u9053\uff0c\u5728\u82f1\u6587\u7684\u884c\u6587\u4e2d\uff0c\u5355\u8bcd\u4e4b\u95f4\u662f\u4ee5\u7a7a\u683c\u4f5c\u4e3a\u81ea\u7136\u5206\u754c\u7b26\u7684\uff0c\u800c\u4e2d\u6587\u53ea\u662f\u5b57\u3001\u53e5\u548c\u6bb5\u80fd\u901a\u8fc7\u660e\u663e\u7684\u5206\u754c\u7b26\u6765\u7b80\u5355\u5212\u754c\uff0c\u552f\u72ec\u8bcd\u6ca1\u6709\u4e00\u4e2a\u5f62\u5f0f\u4e0a\u7684\u5206\u754c\u7b26, \u5206\u8bcd\u8fc7\u7a0b\u5c31\u662f\u627e\u5230\u8fd9\u6837\u5206\u754c\u7b26\u7684\u8fc7\u7a0b. \u5b66\u4e60\u4e86\u5206\u8bcd\u7684\u4f5c\u7528: \u8bcd\u4f5c\u4e3a\u8bed\u8a00\u8bed\u4e49\u7406\u89e3\u7684\u6700\u5c0f\u5355\u5143, \u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u8bed\u8a00\u7684\u57fa\u7840. \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1, \u5982\u81ea\u52a8\u95ee\u7b54, \u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u5b66\u4e60\u4e86\u6d41\u884c\u4e2d\u6587\u5206\u8bcd\u5de5\u5177jieba: \u652f\u6301\u591a\u79cd\u5206\u8bcd\u6a21\u5f0f: \u7cbe\u786e\u6a21\u5f0f, \u5168\u6a21\u5f0f, \u641c\u7d22\u5f15\u64ce\u6a21\u5f0f \u652f\u6301\u4e2d\u6587\u7e41\u4f53\u5206\u8bcd \u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u8bcd\u5178 \u5b66\u4e60\u4e86jieba\u5de5\u5177\u7684\u5b89\u88c5\u548c\u5206\u8bcd\u4f7f\u7528. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u547d\u540d\u5b9e\u4f53\u8bc6\u522b: \u547d\u540d\u5b9e\u4f53: \u901a\u5e38\u6211\u4eec\u5c06\u4eba\u540d, \u5730\u540d, \u673a\u6784\u540d\u7b49\u4e13\u6709\u540d\u8bcd\u7edf\u79f0\u547d\u540d\u5b9e\u4f53. \u5982: \u5468\u6770\u4f26, \u9ed1\u5c71\u53bf, \u5b54\u5b50\u5b66\u9662, 24\u8f8a\u65b9\u94a2\u77eb\u76f4\u673a. \u987e\u540d\u601d\u4e49, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b(Named Entity Recognition\uff0c\u7b80\u79f0NER)\u5c31\u662f\u8bc6\u522b\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u547d\u540d\u5b9e\u4f53. \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u4f5c\u7528: \u540c\u8bcd\u6c47\u4e00\u6837, \u547d\u540d\u5b9e\u4f53\u4e5f\u662f\u4eba\u7c7b\u7406\u89e3\u6587\u672c\u7684\u57fa\u7840\u5355\u5143, \u56e0\u6b64\u4e5f\u662fAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u8bcd\u6027\u6807\u6ce8: \u8bcd\u6027: \u8bed\u8a00\u4e2d\u5bf9\u8bcd\u7684\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u4ee5\u8bed\u6cd5\u7279\u5f81\u4e3a\u4e3b\u8981\u4f9d\u636e\u3001\u517c\u987e\u8bcd\u6c47\u610f\u4e49\u5bf9\u8bcd\u8fdb\u884c\u5212\u5206\u7684\u7ed3\u679c, \u5e38\u89c1\u7684\u8bcd\u6027\u670914\u79cd, \u5982: \u540d\u8bcd, \u52a8\u8bcd, \u5f62\u5bb9\u8bcd\u7b49. \u987e\u540d\u601d\u4e49, \u8bcd\u6027\u6807\u6ce8(Part-Of-Speech tagging, \u7b80\u79f0POS)\u5c31\u662f\u6807\u6ce8\u51fa\u4e00\u6bb5\u6587\u672c\u4e2d\u6bcf\u4e2a\u8bcd\u6c47\u7684\u8bcd\u6027. \u5b66\u4e60\u4e86\u8bcd\u6027\u6807\u6ce8\u7684\u4f5c\u7528: \u8bcd\u6027\u6807\u6ce8\u4ee5\u5206\u8bcd\u4e3a\u57fa\u7840, \u662f\u5bf9\u6587\u672c\u8bed\u8a00\u7684\u53e6\u4e00\u4e2a\u89d2\u5ea6\u7684\u7406\u89e3, \u56e0\u6b64\u4e5f\u5e38\u5e38\u6210\u4e3aAI\u89e3\u51b3NLP\u9886\u57df\u9ad8\u9636\u4efb\u52a1\u7684\u91cd\u8981\u57fa\u7840\u73af\u8282. \u5b66\u4e60\u4e86\u4f7f\u7528jieba\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8.","title":"4 \u5c0f\u7ed3"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662f\u6587\u672c\u5f20\u91cf\u8868\u793a\u53ca\u5176\u4f5c\u7528. \u638c\u63e1\u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u51e0\u79cd\u65b9\u6cd5\u53ca\u5176\u5b9e\u73b0. 1 \u6587\u672c\u5f20\u91cf\u8868\u793a \u00b6 \u5c06\u4e00\u6bb5\u6587\u672c\u4f7f\u7528\u5f20\u91cf\u8fdb\u884c\u8868\u793a\uff0c\u5176\u4e2d\u4e00\u822c\u5c06\u8bcd\u6c47\u8868\u793a\u6210\u5411\u91cf\uff0c\u79f0\u4f5c\u8bcd\u5411\u91cf\uff0c\u518d\u7531\u5404\u4e2a\u8bcd\u5411\u91cf\u6309\u987a\u5e8f\u7ec4\u6210\u77e9\u9635\u5f62\u6210\u6587\u672c\u8868\u793a. \u4e3e\u4e2a\u4f8b\u5b50: [\"\u4eba\u751f\", \"\u8be5\", \"\u5982\u4f55\", \"\u8d77\u5934\"] ==> # \u6bcf\u4e2a\u8bcd\u5bf9\u5e94\u77e9\u9635\u4e2d\u7684\u4e00\u4e2a\u5411\u91cf [[1.32, 4,32, 0,32, 5.2], [3.1, 5.43, 0.34, 3.2], [3.21, 5.32, 2, 4.32], [2.54, 7.32, 5.12, 9.54]] \u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u4f5c\u7528: \u5c06\u6587\u672c\u8868\u793a\u6210\u5f20\u91cf\uff08\u77e9\u9635\uff09\u5f62\u5f0f\uff0c\u80fd\u591f\u4f7f\u8bed\u8a00\u6587\u672c\u53ef\u4ee5\u4f5c\u4e3a\u8ba1\u7b97\u673a\u5904\u7406\u7a0b\u5e8f\u7684\u8f93\u5165\uff0c\u8fdb\u884c\u63a5\u4e0b\u6765\u4e00\u7cfb\u5217\u7684\u89e3\u6790\u5de5\u4f5c. \u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u65b9\u6cd5: one-hot\u7f16\u7801 Word2vec Word Embedding 2 one-hot\u8bcd\u5411\u91cf\u8868\u793a \u00b6 \u53c8\u79f0\u72ec\u70ed\u7f16\u7801\uff0c\u5c06\u6bcf\u4e2a\u8bcd\u8868\u793a\u6210\u5177\u6709n\u4e2a\u5143\u7d20\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u8bcd\u5411\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u662f1\uff0c\u5176\u4ed6\u5143\u7d20\u90fd\u662f0\uff0c\u4e0d\u540c\u8bcd\u6c47\u5143\u7d20\u4e3a0\u7684\u4f4d\u7f6e\u4e0d\u540c\uff0c\u5176\u4e2dn\u7684\u5927\u5c0f\u662f\u6574\u4e2a\u8bed\u6599\u4e2d\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570. \u4e3e\u4e2a\u4f8b\u5b50: [\"\u6539\u53d8\", \"\u8981\", \"\u5982\u4f55\", \"\u8d77\u624b\"]` ==> [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]] onehot\u7f16\u7801\u5b9e\u73b0: \u8fdb\u884conehot\u7f16\u7801: import jieba # \u5bfc\u5165keras\u4e2d\u7684\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer from tensorflow.keras.preprocessing.text import Tokenizer # \u5bfc\u5165\u7528\u4e8e\u5bf9\u8c61\u4fdd\u5b58\u4e0e\u52a0\u8f7d\u7684joblib from sklearn.externals import joblib # \u601d\u8def\u5206\u6790 \u751f\u6210onehot # 1 \u51c6\u5907\u8bed\u6599 vocabs # 2 \u5b9e\u4f8b\u5316\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer, \u4f7f\u7528\u6620\u5c04\u5668\u62df\u5408\u73b0\u6709\u6587\u672c\u6570\u636e (\u5185\u90e8\u751f\u6210 index_word word_index) # 2-1 \u6ce8\u610fidx\u5e8f\u53f7-1 # 3 \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503c zero_list\uff0c\u751f\u6210onehot # 4 \u4f7f\u7528joblib\u5de5\u5177\u4fdd\u5b58\u6620\u5c04\u5668 joblib.dump() def dm_onehot_gen (): # 1 \u51c6\u5907\u8bed\u6599 vocabs vocabs = { \"\u5468\u6770\u4f26\" , \"\u9648\u5955\u8fc5\" , \"\u738b\u529b\u5b8f\" , \"\u674e\u5b97\u76db\" , \"\u5434\u4ea6\u51e1\" , \"\u9e7f\u6657\" } # 2 \u5b9e\u4f8b\u5316\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer, \u4f7f\u7528\u6620\u5c04\u5668\u62df\u5408\u73b0\u6709\u6587\u672c\u6570\u636e (\u5185\u90e8\u751f\u6210 index_word word_index) # 2-1 \u6ce8\u610fidx\u5e8f\u53f7-1 mytokenizer = Tokenizer () mytokenizer . fit_on_texts ( vocabs ) # 3 \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503c zero_list\uff0c\u751f\u6210onehot for vocab in vocabs : zero_list = [ 0 ] * len ( vocabs ) idx = mytokenizer . word_index [ vocab ] - 1 zero_list [ idx ] = 1 print ( vocab , '\u7684onehot\u7f16\u7801\u662f' , zero_list ) # 4 \u4f7f\u7528joblib\u5de5\u5177\u4fdd\u5b58\u6620\u5c04\u5668 joblib.dump() mypath = './mytokenizer' joblib . dump ( mytokenizer , mypath ) print ( '\u4fdd\u5b58mytokenizer End' ) # \u6ce8\u610f5-1 \u5b57\u5178\u6ca1\u6709\u987a\u5e8f onehot\u7f16\u7801\u6ca1\u6709\u987a\u5e8f []-\u6709\u5e8f {}-\u65e0\u5e8f \u533a\u522b # \u6ce8\u610f5-2 \u5b57\u5178\u6709\u7684\u5355\u8bcd\u624d\u6709idx idx\u4ece1\u5f00\u59cb # \u6ce8\u610f5-3 \u67e5\u8be2\u6ca1\u6709\u6ce8\u518c\u7684\u8bcd\u4f1a\u6709\u5f02\u5e38 eg: \u72d7\u86cb print ( mytokenizer . word_index ) print ( mytokenizer . index_word ) \u8f93\u51fa\u6548\u679c: \u9648\u5955\u8fc5 \u7684onehot\u7f16\u7801\u662f [1, 0, 0, 0, 0, 0] \u738b\u529b\u5b8f \u7684onehot\u7f16\u7801\u662f [0, 1, 0, 0, 0, 0] \u9e7f\u6657 \u7684onehot\u7f16\u7801\u662f [0, 0, 1, 0, 0, 0] \u5468\u6770\u4f26 \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 1, 0, 0] \u674e\u5b97\u76db \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 0, 1, 0] \u5434\u4ea6\u51e1 \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 0, 0, 1] \u4fdd\u5b58mytokenizer End {'\u9648\u5955\u8fc5': 1, '\u738b\u529b\u5b8f': 2, '\u9e7f\u6657': 3, '\u5468\u6770\u4f26': 4, '\u674e\u5b97\u76db': 5, '\u5434\u4ea6\u51e1': 6} {1: '\u9648\u5955\u8fc5', 2: '\u738b\u529b\u5b8f', 3: '\u9e7f\u6657', 4: '\u5468\u6770\u4f26', 5: '\u674e\u5b97\u76db', 6: '\u5434\u4ea6\u51e1'} onehot\u7f16\u7801\u5668\u7684\u4f7f\u7528: # \u601d\u8def\u5206\u6790 # 1 \u52a0\u8f7d\u5df2\u4fdd\u5b58\u7684\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer joblib.load(mypath) # 2 \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503czero_list\uff0c\u751f\u6210onehot \u4ee5token\u4e3a'\u674e\u5b97\u76db' # 3 token = \"\u72d7\u86cb\" \u4f1a\u51fa\u73b0\u5f02\u5e38 def dm_onehot_use (): vocabs = { \"\u5468\u6770\u4f26\" , \"\u9648\u5955\u8fc5\" , \"\u738b\u529b\u5b8f\" , \"\u674e\u5b97\u76db\" , \"\u5434\u4ea6\u51e1\" , \"\u9e7f\u6657\" } # 1 \u52a0\u8f7d\u5df2\u4fdd\u5b58\u7684\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer joblib.load(mypath) mypath = './mytokenizer' mytokenizer = joblib . load ( mypath ) # 2 \u7f16\u7801token\u4e3a\"\u674e\u5b97\u76db\" \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503c zero_list\uff0c\u751f\u6210onehot token = \"\u674e\u5b97\u76db\" zero_list = [ 0 ] * len ( vocabs ) idx = mytokenizer . word_index [ token ] - 1 zero_list [ idx ] = 1 print ( token , '\u7684onehot\u7f16\u7801\u662f' , zero_list ) \u8f93\u51fa\u6548\u679c: \u674e\u5b97\u76db \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 0, 1, 0] one-hot\u7f16\u7801\u7684\u4f18\u52a3\u52bf\uff1a \u4f18\u52bf\uff1a\u64cd\u4f5c\u7b80\u5355\uff0c\u5bb9\u6613\u7406\u89e3. \u52a3\u52bf\uff1a\u5b8c\u5168\u5272\u88c2\u4e86\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u800c\u4e14\u5728\u5927\u8bed\u6599\u96c6\u4e0b\uff0c\u6bcf\u4e2a\u5411\u91cf\u7684\u957f\u5ea6\u8fc7\u5927\uff0c\u5360\u636e\u5927\u91cf\u5185\u5b58. \u6b63\u56e0\u4e3aone-hot\u7f16\u7801\u660e\u663e\u7684\u52a3\u52bf\uff0c\u8fd9\u79cd\u7f16\u7801\u65b9\u5f0f\u88ab\u5e94\u7528\u7684\u5730\u65b9\u8d8a\u6765\u8d8a\u5c11\uff0c\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u63a5\u4e0b\u6765\u6211\u4eec\u8981\u5b66\u4e60\u7684\u7a20\u5bc6\u5411\u91cf\u7684\u8868\u793a\u65b9\u6cd5word2vec\u548cword embedding. 3 word2vec\u6a21\u578b \u00b6 3.1 \u6a21\u578b\u4ecb\u7ecd \u00b6 word2vec\u662f\u4e00\u79cd\u6d41\u884c\u7684\u5c06\u8bcd\u6c47\u8868\u793a\u6210\u5411\u91cf\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5, \u8be5\u8fc7\u7a0b\u5c06\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b, \u5c06\u7f51\u7edc\u53c2\u6570\u4f5c\u4e3a\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a, \u5b83\u5305\u542bCBOW\u548cskipgram\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f. CBOW(Continuous bag of words)\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u4e0a\u4e0b\u6587\u8bcd\u6c47\u9884\u6d4b\u76ee\u6807\u8bcd\u6c47. \u5206\u6790: \u56fe\u4e2d\u7a97\u53e3\u5927\u5c0f\u4e3a9, \u4f7f\u7528\u524d\u540e4\u4e2a\u8bcd\u6c47\u5bf9\u76ee\u6807\u8bcd\u6c47\u8fdb\u884c\u9884\u6d4b. CBOW\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope can set\uff0c\u56e0\u4e3a\u662fCBOW\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528Hope\u548cset\u4f5c\u4e3a\u8f93\u5165\uff0ccan\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0ccan\uff0cset\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u6bcf\u4e2aone-hot\u7f16\u7801\u7684\u5355\u8bcd\u4e0e\u5404\u81ea\u7684\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58\u4e4b\u540e\u518d\u76f8\u52a0, \u5f97\u5230\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635\u4e0e\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3, \u6240\u6709\u7684\u53d8\u6362\u77e9\u9635\u5171\u4eab\u53c2\u6570)\u76f8\u4e58, \u5f97\u52305x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eec\u771f\u6b63\u7684\u76ee\u6807\u77e9\u9635\u5373can\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21\u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. skipgram\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u76ee\u6807\u8bcd\u6c47\u9884\u6d4b\u4e0a\u4e0b\u6587\u8bcd\u6c47. \u5206\u6790: \u56fe\u4e2d\u7a97\u53e3\u5927\u5c0f\u4e3a9, \u4f7f\u7528\u76ee\u6807\u8bcd\u6c47\u5bf9\u524d\u540e\u56db\u4e2a\u8bcd\u6c47\u8fdb\u884c\u9884\u6d4b. skipgram\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope can set\uff0c\u56e0\u4e3a\u662fskipgram\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528can\u4f5c\u4e3a\u8f93\u5165 \uff0cHope\u548cset\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0ccan\uff0cset\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u5c06can\u7684one-hot\u7f16\u7801\u4e0e\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58, \u5f97\u5230\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635\u4e0e\u591a\u4e2a\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3)\u76f8\u4e58, \u5f97\u5230\u591a\u4e2a5x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eecHope\u548cset\u5bf9\u5e94\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21 \u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635\u5373\u53c2\u6570\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. \u8bcd\u5411\u91cf\u7684\u68c0\u7d22\u83b7\u53d6 \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5b8c\u6bd5\u540e\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u77e9\u9635w\u5c31\u6211\u4eec\u7684\u60f3\u8981\u8bcd\u5411\u91cf\u3002\u5982\u4f55\u68c0\u7d22\u67d01\u4e2a\u5355\u8bcd\u7684\u5411\u91cf\u5462\uff1f\u4ee5CBOW\u65b9\u5f0f\u4e3e\u4f8b\u8bf4\u660e\u5982\u4f55\u68c0\u7d22a\u5355\u8bcd\u7684\u8bcd\u5411\u91cf\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1aa\u7684onehot\u7f16\u7801[10000]\uff0c\u7528\u53c2\u6570\u77e9\u9635[3,5] * a\u7684onehot\u7f16\u7801[10000]\uff0c\u53ef\u4ee5\u628a\u53c2\u6570\u77e9\u9635\u7684\u7b2c1\u5217\u53c2\u6570\u7ed9\u53d6\u51fa\u6765\uff0c\u8fd9\u4e2a[3,1]\u7684\u503c\u5c31\u662fa\u7684\u8bcd\u5411\u91cf\u3002 3.2 word2vec\u7684\u8bad\u7ec3\u548c\u4f7f\u7528 \u00b6 \u7b2c\u4e00\u6b65: \u83b7\u53d6\u8bad\u7ec3\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d 1 \u83b7\u53d6\u8bad\u7ec3\u6570\u636e \u00b6 \u6570\u636e\u6765\u6e90\uff1a http://mattmahoney.net/dc/enwik9.zip \u5728\u8fd9\u91cc, \u6211\u4eec\u5c06\u7814\u7a76\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u90e8\u5206\u7f51\u9875\u4fe1\u606f, \u5b83\u7684\u5927\u5c0f\u5728300M\u5de6\u53f3\u3002\u8fd9\u4e9b\u8bed\u6599\u5df2\u7ecf\u88ab\u51c6\u5907\u597d, \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7Matt Mahoney\u7684\u7f51\u7ad9\u4e0b\u8f7d\u3002 \u6ce8\u610f\uff1a\u539f\u59cb\u6570\u636e\u96c6\u5df2\u7ecf\u653e\u5728/root/data/enwik9.zip\uff0c\u89e3\u538b\u540e\u6570\u636e\u4e3a/root/data/enwik9\uff0c\u9884\u5904\u7406\u540e\u7684\u6570\u636e\u4e3a/root/data/fil9 \u67e5\u770b\u539f\u59cb\u6570\u636e: $ head -10 data/enwik9 # \u539f\u59cb\u6570\u636e\u5c06\u8f93\u51fa\u5f88\u591a\u5305\u542bXML/HTML\u683c\u5f0f\u7684\u5185\u5bb9, \u8fd9\u4e9b\u5185\u5bb9\u5e76\u4e0d\u662f\u6211\u4eec\u9700\u8981\u7684 <mediawiki xmlns = \"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi = \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation = \"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version = \"0.3\" xml:lang = \"en\" > <siteinfo> <sitename>Wikipedia</sitename> <base>http://en.wikipedia.org/wiki/Main_Page</base> <generator>MediaWiki 1 .6alpha</generator> < case >first-letter</case> <namespaces> <namespace key = \"-2\" >Media</namespace> <namespace key = \"-1\" >Special</namespace> <namespace key = \"0\" /> \u539f\u59cb\u6570\u636e\u5904\u7406: # \u4f7f\u7528wikifil.pl\u6587\u4ef6\u5904\u7406\u811a\u672c\u6765\u6e05\u9664XML/HTML\u683c\u5f0f\u7684\u5185\u5bb9 # perl wikifil.pl data/enwik9 > data/fil9 #\u8be5\u547d\u4ee4\u5df2\u7ecf\u6267\u884c \u67e5\u770b\u9884\u5904\u7406\u540e\u7684\u6570\u636e: # \u67e5\u770b\u524d80\u4e2a\u5b57\u7b26 head -c 80 data/fil9 # \u8f93\u51fa\u7ed3\u679c\u4e3a\u7531\u7a7a\u683c\u5206\u5272\u7684\u5355\u8bcd anarchism originated as a term of abuse first used against early working class 2 \u8bcd\u5411\u91cf\u7684\u8bad\u7ec3\u4fdd\u5b58\u52a0\u8f7d \u00b6 fasttext \u662f facebook \u5f00\u6e90\u7684\u4e00\u4e2a\u8bcd\u5411\u91cf\u4e0e\u6587\u672c\u5206\u7c7b\u5de5\u5177\u3002\u4e0b\u9762\u662f\u8be5\u5de5\u5177\u5305\u7684\u5b89\u88c5\u65b9\u6cd5 # \u8bad\u7ec3\u8bcd\u5411\u91cf\u5de5\u5177\u5e93\u7684\u5b89\u88c5 # \u65b9\u6cd51 \u7b80\u6d01\u7248 pip install fasttext # \u65b9\u6cd52\uff1a\u6e90\u7801\u5b89\u88c5(\u63a8\u8350) # \u4ee5linux\u5b89\u88c5\u4e3a\u4f8b\uff1a \u76ee\u5f55\u5207\u6362\u5230\u865a\u62df\u5f00\u53d1\u73af\u5883\u76ee\u5f55\u4e0b\uff0c\u518d\u6267\u884cgit clone \u64cd\u4f5c git clone https://github.com/facebookresearch/fastText.git cd fastText # \u4f7f\u7528pip\u5b89\u88c5python\u4e2d\u7684fasttext\u5de5\u5177\u5305 sudo pip install . # \u5bfc\u5165fasttext import fasttext def dm_fasttext_train_save_load (): # 1 \u4f7f\u7528train_unsupervised(\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5) \u8bad\u7ec3\u8bcd\u5411\u91cf mymodel = fasttext . train_unsupervised ( './data/fil9' ) print ( '\u8bad\u7ec3\u8bcd\u5411\u91cf ok' ) # 2 save_model()\u4fdd\u5b58\u5df2\u7ecf\u8bad\u7ec3\u597d\u8bcd\u5411\u91cf # \u6ce8\u610f\uff0c\u8be5\u884c\u4ee3\u7801\u6267\u884c\u8017\u65f6\u5f88\u957f mymodel . save_model ( \"./data/fil9.bin\" ) print ( '\u4fdd\u5b58\u8bcd\u5411\u91cf ok' ) # 3 \u6a21\u578b\u52a0\u8f7d mymodel = fasttext . load_model ( './data/fil9.bin' ) print ( '\u52a0\u8f7d\u8bcd\u5411\u91cf ok' ) # \u6b65\u9aa41\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a \u6709\u6548\u8bad\u7ec3\u8bcd\u6c47\u91cf\u4e3a124M , \u5171218316\u4e2a\u5355\u8bcd Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 53996 lr : 0.000000 loss : 0.734999 ETA : 0 h 0 m 3 \u67e5\u770b\u5355\u8bcd\u5bf9\u5e94\u7684\u8bcd\u5411\u91cf \u00b6 # \u901a\u8fc7get_word_vector\u65b9\u6cd5\u6765\u83b7\u5f97\u6307\u5b9a\u8bcd\u6c47\u7684\u8bcd\u5411\u91cf, \u9ed8\u8ba4\u8bcd\u5411\u91cf\u8bad\u7ec3\u51fa\u6765\u662f1\u4e2a\u5355\u8bcd100\u7279\u5f81 def dm_fasttext_get_word_vector (): mymodel = fasttext . load_model ( './data/fil9.bin' ) myvector = mymodel . get_word_vector ( 'the' ) print ( 'myvector->' , type ( myvector ), myvector . shape , myvector ) # \u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a array ([ - 0.03087516 , 0.09221972 , 0.17660329 , 0.17308897 , 0.12863874 , 0.13912526 , - 0.09851588 , 0.00739991 , 0.37038437 , - 0.00845221 , ... - 0.21184735 , - 0.05048715 , - 0.34571868 , 0.23765688 , 0.23726143 ], dtype = float32 ) 4 \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u00b6 # \u68c0\u67e5\u5355\u8bcd\u5411\u91cf\u8d28\u91cf\u7684\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\u5c31\u662f\u67e5\u770b\u5176\u90bb\u8fd1\u5355\u8bcd, \u901a\u8fc7\u6211\u4eec\u4e3b\u89c2\u6765\u5224\u65ad\u8fd9\u4e9b\u90bb\u8fd1\u5355\u8bcd\u662f\u5426\u4e0e\u76ee\u6807\u5355\u8bcd\u76f8\u5173\u6765\u7c97\u7565\u8bc4\u5b9a\u6a21\u578b\u6548\u679c\u597d\u574f. # \u67e5\u627e\"\u8fd0\u52a8\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\"\u4f53\u80b2\u7f51\", \"\u8fd0\u52a8\u6c7d\u8f66\", \"\u8fd0\u52a8\u670d\"\u7b49. >>> model . get_nearest_neighbors ( 'sports' ) [( 0.8414610624313354 , 'sportsnet' ), ( 0.8134572505950928 , 'sport' ), ( 0.8100415468215942 , 'sportscars' ), ( 0.8021156787872314 , 'sportsground' ), ( 0.7889881134033203 , 'sportswomen' ), ( 0.7863013744354248 , 'sportsplex' ), ( 0.7786710262298584 , 'sporty' ), ( 0.7696356177330017 , 'sportscar' ), ( 0.7619683146476746 , 'sportswear' ), ( 0.7600985765457153 , 'sportin' )] # \u67e5\u627e\"\u97f3\u4e50\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u97f3\u4e50\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'music' ) [( 0.8908010125160217 , 'emusic' ), ( 0.8464668393135071 , 'musicmoz' ), ( 0.8444250822067261 , 'musics' ), ( 0.8113634586334229 , 'allmusic' ), ( 0.8106718063354492 , 'musices' ), ( 0.8049437999725342 , 'musicam' ), ( 0.8004694581031799 , 'musicom' ), ( 0.7952923774719238 , 'muchmusic' ), ( 0.7852965593338013 , 'musicweb' ), ( 0.7767147421836853 , 'musico' )] # \u67e5\u627e\"\u5c0f\u72d7\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u5c0f\u72d7\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'dog' ) [( 0.8456876873970032 , 'catdog' ), ( 0.7480780482292175 , 'dogcow' ), ( 0.7289096117019653 , 'sleddog' ), ( 0.7269964218139648 , 'hotdog' ), ( 0.7114801406860352 , 'sheepdog' ), ( 0.6947550773620605 , 'dogo' ), ( 0.6897546648979187 , 'bodog' ), ( 0.6621081829071045 , 'maddog' ), ( 0.6605004072189331 , 'dogs' ), ( 0.6398137211799622 , 'dogpile' )] 5 \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u00b6 # \u5728\u8bad\u7ec3\u8bcd\u5411\u91cf\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u53ef\u4ee5\u8bbe\u5b9a\u5f88\u591a\u5e38\u7528\u8d85\u53c2\u6570\u6765\u8c03\u8282\u6211\u4eec\u7684\u6a21\u578b\u6548\u679c, \u5982: # \u65e0\u76d1\u7763\u8bad\u7ec3\u6a21\u5f0f: 'skipgram' \u6216\u8005 'cbow', \u9ed8\u8ba4\u4e3a'skipgram', \u5728\u5b9e\u8df5\u4e2d\uff0cskipgram\u6a21\u5f0f\u5728\u5229\u7528\u5b50\u8bcd\u65b9\u9762\u6bd4cbow\u66f4\u597d. # \u8bcd\u5d4c\u5165\u7ef4\u5ea6dim: \u9ed8\u8ba4\u4e3a100, \u4f46\u968f\u7740\u8bed\u6599\u5e93\u7684\u589e\u5927, \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5f80\u5f80\u4e5f\u8981\u66f4\u5927. # \u6570\u636e\u5faa\u73af\u6b21\u6570epoch: \u9ed8\u8ba4\u4e3a5, \u4f46\u5f53\u4f60\u7684\u6570\u636e\u96c6\u8db3\u591f\u5927, \u53ef\u80fd\u4e0d\u9700\u8981\u90a3\u4e48\u591a\u6b21. # \u5b66\u4e60\u7387lr: \u9ed8\u8ba4\u4e3a0.05, \u6839\u636e\u7ecf\u9a8c, \u5efa\u8bae\u9009\u62e9[0.01\uff0c1]\u8303\u56f4\u5185. # \u4f7f\u7528\u7684\u7ebf\u7a0b\u6570thread: \u9ed8\u8ba4\u4e3a12\u4e2a\u7ebf\u7a0b, \u4e00\u822c\u5efa\u8bae\u548c\u4f60\u7684cpu\u6838\u6570\u76f8\u540c. >>> model = fasttext . train_unsupervised ( 'data/fil9' , \"cbow\" , dim = 300 , epoch = 1 , lr = 0.1 , thread = 8 ) Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 49523 lr : 0.000000 avg . loss : 1.777205 ETA : 0 h 0 m 0 s 4 \u8bcd\u5d4c\u5165word embedding\u4ecb\u7ecd \u00b6 \u901a\u8fc7\u4e00\u5b9a\u7684\u65b9\u5f0f\u5c06\u8bcd\u6c47\u6620\u5c04\u5230\u6307\u5b9a\u7ef4\u5ea6(\u4e00\u822c\u662f\u66f4\u9ad8\u7ef4\u5ea6)\u7684\u7a7a\u95f4. \u5e7f\u4e49\u7684word embedding\u5305\u62ec\u6240\u6709\u5bc6\u96c6\u8bcd\u6c47\u5411\u91cf\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5982\u4e4b\u524d\u5b66\u4e60\u7684word2vec, \u5373\u53ef\u8ba4\u4e3a\u662fword embedding\u7684\u4e00\u79cd. \u72ed\u4e49\u7684word embedding\u662f\u6307\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u52a0\u5165\u7684embedding\u5c42, \u5bf9\u6574\u4e2a\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u7684\u540c\u65f6\u4ea7\u751f\u7684embedding\u77e9\u9635(embedding\u5c42\u7684\u53c2\u6570), \u8fd9\u4e2aembedding\u77e9\u9635\u5c31\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6240\u6709\u8f93\u5165\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a\u7ec4\u6210\u7684\u77e9\u9635. word embedding\u7684\u53ef\u89c6\u5316\u5206\u6790: \u901a\u8fc7\u4f7f\u7528tensorboard\u53ef\u89c6\u5316\u5d4c\u5165\u7684\u8bcd\u5411\u91cf. import torch from tensorflow.keras.preprocessing.text import Tokenizer from torch.utils.tensorboard import SummaryWriter import jieba import torch.nn as nn # \u6ce8\u610f\uff1a # fs = tf.io.gfile.get_filesystem(save_path) # AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem' # \u9519\u8bef\u539f\u56e0\u5206\u6790\uff1a # 1 from tensorboard.compat import tf \u4f7f\u7528\u4e86tf \u5982\u679c\u5b89\u88c5tensorflow\uff0c\u9ed8\u8ba4\u4f1a\u8c03\u7528\u5b83tf\u7684api\u51fd\u6570 import tensorflow as tf import tensorboard as tb tf . io . gfile = tb . compat . tensorflow_stub . io . gfile # \u5b9e\u9a8c\uff1ann.Embedding\u5c42\u8bcd\u5411\u91cf\u53ef\u89c6\u5316\u5206\u6790 # 1 \u5bf9\u53e5\u5b50\u5206\u8bcd word_list # 2 \u5bf9\u53e5\u5b50word2id\u6c42my_token_list\uff0c\u5bf9\u53e5\u5b50\u6587\u672c\u6570\u503c\u5316sentence2id # 3 \u521b\u5efann.Embedding\u5c42\uff0c\u67e5\u770b\u6bcf\u4e2atoken\u7684\u8bcd\u5411\u91cf\u6570\u636e # 4 \u521b\u5efaSummaryWriter\u5bf9\u8c61, \u53ef\u89c6\u5316\u8bcd\u5411\u91cf # \u8bcd\u5411\u91cf\u77e9\u9635embd.weight.data \u548c \u8bcd\u5411\u91cf\u5355\u8bcd\u5217\u8868my_token_list\u6dfb\u52a0\u5230SummaryWriter\u5bf9\u8c61\u4e2d # summarywriter.add_embedding(embd.weight.data, my_token_list) # 5 \u901a\u8fc7tensorboard\u89c2\u5bdf\u8bcd\u5411\u91cf\u76f8\u4f3c\u6027 # 6 \u4e5f\u53ef\u901a\u8fc7\u7a0b\u5e8f\uff0c\u4ecenn.Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf def dm02_nnembeding_show (): # 1 \u5bf9\u53e5\u5b50\u5206\u8bcd word_list sentence1 = '\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd' sentence2 = \"\u6211\u7231\u81ea\u7136\u8bed\u8a00\u5904\u7406\" sentences = [ sentence1 , sentence2 ] word_list = [] for s in sentences : word_list . append ( jieba . lcut ( s )) # print('word_list--->', word_list) # 2 \u5bf9\u53e5\u5b50word2id\u6c42my_token_list\uff0c\u5bf9\u53e5\u5b50\u6587\u672c\u6570\u503c\u5316sentence2id mytokenizer = Tokenizer () mytokenizer . fit_on_texts ( word_list ) # print(mytokenizer.index_word, mytokenizer.word_index) # \u6253\u5370my_token_list my_token_list = mytokenizer . index_word . values () print ( 'my_token_list-->' , my_token_list ) # \u6253\u5370\u6587\u672c\u6570\u503c\u5316\u4ee5\u540e\u7684\u53e5\u5b50 sentence2id = mytokenizer . texts_to_sequences ( word_list ) print ( 'sentence2id--->' , sentence2id , len ( sentence2id )) # 3 \u521b\u5efann.Embedding\u5c42 embd = nn . Embedding ( num_embeddings = len ( my_token_list ), embedding_dim = 8 ) # print(\"embd--->\", embd) # print('nn.Embedding\u5c42\u8bcd\u5411\u91cf\u77e9\u9635-->', embd.weight.data, embd.weight.data.shape, type(embd.weight.data)) # 4 \u521b\u5efaSummaryWriter\u5bf9\u8c61 \u8bcd\u5411\u91cf\u77e9\u9635embd.weight.data \u548c \u8bcd\u5411\u91cf\u5355\u8bcd\u5217\u8868my_token_list summarywriter = SummaryWriter () summarywriter . add_embedding ( embd . weight . data , my_token_list ) summarywriter . close () # 5 \u901a\u8fc7tensorboard\u89c2\u5bdf\u8bcd\u5411\u91cf\u76f8\u4f3c\u6027 # cd \u7a0b\u5e8f\u7684\u5f53\u524d\u76ee\u5f55\u4e0b\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4 # \u542f\u52a8tensorboard\u670d\u52a1 tensorboard --logdir=runs --host 0.0.0.0 # \u901a\u8fc7\u6d4f\u89c8\u5668\uff0c\u67e5\u770b\u8bcd\u5411\u91cf\u53ef\u89c6\u5316\u6548\u679c http://127.0.0.1:6006 print ( '\u4ecenn.Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf' ) # # 6 \u4ecenn.Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf for idx in range ( len ( mytokenizer . index_word )): tmpvec = embd ( torch . tensor ( idx )) print ( ' %4s ' % ( mytokenizer . index_word [ idx + 1 ]), tmpvec . detach () . numpy ()) \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c my_token_list --> dict_values ([ '\u662f' , '\u9ed1\u9a6c' , '\u6211' , '\u4f20\u667a' , '\u6559\u80b2' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u5728' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' , '\u7231' , '\u81ea\u7136\u8bed\u8a00' , '\u5904\u7406' ]) sentence2id ---> [[ 4 , 5 , 1 , 6 , 7 , 8 , 9 , 10 , 2 , 11 , 12 , 13 , 3 , 1 , 14 , 2 , 15 , 16 , 17 ], [ 3 , 18 , 19 , 20 ]] 2 \u4ecenn . Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf \u662f [ 0.46067393 - 0.9049023 - 0.03143226 - 0.32443136 0.03115687 - 1.3352231 - 0.08336695 - 2.4732168 ] \u9ed1\u9a6c [ 0.66760564 0.08703537 0.23735243 1.5896837 - 1.8869231 0.22520915 - 1.0676078 - 0.7654686 ] \u6211 [ - 0.9093167 - 0.6114051 - 0.6825029 0.9269122 0.5208822 2.294128 - 0.11160549 - 0.34862307 ] \u4f20\u667a [ - 1.1552105 - 0.4274638 - 0.8121502 - 1.4969801 - 1.3328248 - 1.0934378 0.6707438 - 1.1796173 ] \u6559\u80b2 [ 0.01580311 - 1.1884228 0.59364647 1.5387698 - 1.0822943 0.36760855 - 0.4652998 - 0.57378227 ] \u4e00\u5bb6 [ - 1.1898873 - 0.42482868 - 1.9391155 - 1.5678993 - 1.6960118 0.22525501 - 1.0754168 0.41797593 ] \u4e0a\u5e02\u516c\u53f8 [ 0.590556 2.4274144 1.6698223 - 0.9776848 - 0.6119061 0.4434897 - 2.3726876 - 0.2607738 ] \uff0c [ - 0.17568143 1.0074369 0.2571488 1.8940887 - 0.5383494 0.65416646 0.63454026 0.6235991 ] \u65d7\u4e0b [ 2.8400452 - 1.0096515 2.247107 0.30006626 - 1.2687006 0.05855403 0.01199368 - 0.6156502 ] \u6709 [ 0.89320636 - 0.43819678 1.0345292 1.3546743 - 1.4238662 - 1.6994532 0.30445674 2.673923 ] \u7a0b\u5e8f\u5458 [ 1.2147354 0.24878891 0.36161897 0.37458655 - 0.48264053 - 0.0141514 1.2033817 0.7899459 ] \u54c1\u724c [ 0.59799325 - 0.01371854 0.0628166 - 1.4829391 0.39795023 - 0.39259398 - 0.60923046 0.54170054 ] \u3002 [ 0.59599686 1.6038656 - 0.10832139 0.25223547 0.37193906 1.1944667 - 0.91253406 0.6869221 ] \u5728 [ - 1.161504 2.6963246 - 0.6087775 0.9399654 0.8480068 0.684357 0.96156543 - 0.3541162 ] \u8fd9\u91cc [ 0.1034054 - 0.01949253 0.8989019 1.61057 - 1.5983531 0.17945968 - 0.17572908 - 0.9724814 ] \u5b66\u4e60 [ - 1.3899843 - 1.0846052 - 1.1301199 - 0.4078141 0.40511298 0.6562911 0.9231357 - 0.34704337 ] \u4eba\u5de5\u667a\u80fd [ - 1.4966388 - 1.0905199 1.001238 - 0.75254333 - 1.4210068 - 1.854177 1.0471514 - 0.27140012 ] \u7231 [ - 1.5254552 0.6189947 1.2703396 - 0.4826037 - 1.4928672 0.8320283 1.7333516 0.16908517 ] \u81ea\u7136\u8bed\u8a00 [ - 0.3856235 - 1.2193452 0.9991112 - 1.5821775 0.45017946 - 0.66064674 0.08045111 0.62901515 ] \u5904\u7406 [ 1.5062869 1.3156213 - 0.21295634 0.47610474 0.08946162 0.57107806 - 1.0727187 0.16396333 ] \u8bcd\u5411\u91cf\u548c\u8bcd\u663e\u793a\u6807\u7b7e \u5199\u5165\u78c1\u76d8ok \u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u67e5\u770b ./ runs \u76ee\u5f55 \u5728\u7ec8\u7aef\u542f\u52a8tensorboard\u670d\u52a1: $ cd ~ $ tensorboard --logdir = runs --host 0 .0.0.0 # \u901a\u8fc7http://192.168.88.161:6006\u8bbf\u95ee\u6d4f\u89c8\u5668\u53ef\u89c6\u5316\u9875\u9762 \u6d4f\u89c8\u5668\u5c55\u793a\u5e76\u53ef\u4ee5\u4f7f\u7528\u53f3\u4fa7\u8fd1\u90bb\u8bcd\u6c47\u529f\u80fd\u68c0\u9a8c\u6548\u679c: 5 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6587\u672c\u5f20\u91cf\u8868\u793a: \u5c06\u4e00\u6bb5\u6587\u672c\u4f7f\u7528\u5f20\u91cf\u8fdb\u884c\u8868\u793a\uff0c\u5176\u4e2d\u4e00\u822c\u5c06\u8bcd\u6c47\u4e3a\u8868\u793a\u6210\u5411\u91cf\uff0c\u79f0\u4f5c\u8bcd\u5411\u91cf\uff0c\u518d\u7531\u5404\u4e2a\u8bcd\u5411\u91cf\u6309\u987a\u5e8f\u7ec4\u6210\u77e9\u9635\u5f62\u6210\u6587\u672c\u8868\u793a. \u5b66\u4e60\u4e86\u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u4f5c\u7528: \u5c06\u6587\u672c\u8868\u793a\u6210\u5f20\u91cf\uff08\u77e9\u9635\uff09\u5f62\u5f0f\uff0c\u80fd\u591f\u4f7f\u8bed\u8a00\u6587\u672c\u53ef\u4ee5\u4f5c\u4e3a\u8ba1\u7b97\u673a\u5904\u7406\u7a0b\u5e8f\u7684\u8f93\u5165\uff0c\u8fdb\u884c\u63a5\u4e0b\u6765\u4e00\u7cfb\u5217\u7684\u89e3\u6790\u5de5\u4f5c. \u5b66\u4e60\u4e86\u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u65b9\u6cd5: one-hot\u7f16\u7801 Word2vec Word Embedding \u4ec0\u4e48\u662fone-hot\u8bcd\u5411\u91cf\u8868\u793a: \u53c8\u79f0\u72ec\u70ed\u7f16\u7801\uff0c\u5c06\u6bcf\u4e2a\u8bcd\u8868\u793a\u6210\u5177\u6709n\u4e2a\u5143\u7d20\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u8bcd\u5411\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u662f1\uff0c\u5176\u4ed6\u5143\u7d20\u90fd\u662f0\uff0c\u4e0d\u540c\u8bcd\u6c47\u5143\u7d20\u4e3a0\u7684\u4f4d\u7f6e\u4e0d\u540c\uff0c\u5176\u4e2dn\u7684\u5927\u5c0f\u662f\u6574\u4e2a\u8bed\u6599\u4e2d\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570. \u5b66\u4e60\u4e86onehot\u7f16\u7801\u5b9e\u73b0. \u5b66\u4e60\u4e86one-hot\u7f16\u7801\u7684\u4f18\u52a3\u52bf\uff1a \u4f18\u52bf\uff1a\u64cd\u4f5c\u7b80\u5355\uff0c\u5bb9\u6613\u7406\u89e3. \u52a3\u52bf\uff1a\u5b8c\u5168\u5272\u88c2\u4e86\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u800c\u4e14\u5728\u5927\u8bed\u6599\u96c6\u4e0b\uff0c\u6bcf\u4e2a\u5411\u91cf\u7684\u957f\u5ea6\u8fc7\u5927\uff0c\u5360\u636e\u5927\u91cf\u5185\u5b58. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fword2vec: \u662f\u4e00\u79cd\u6d41\u884c\u7684\u5c06\u8bcd\u6c47\u8868\u793a\u6210\u5411\u91cf\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5, \u8be5\u8fc7\u7a0b\u5c06\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b, \u5c06\u7f51\u7edc\u53c2\u6570\u4f5c\u4e3a\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a, \u5b83\u5305\u542bCBOW\u548cskipgram\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f. \u5b66\u4e60\u4e86CBOW(Continuous bag of words)\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u4e0a\u4e0b\u6587\u8bcd\u6c47\u9884\u6d4b\u76ee\u6807\u8bcd\u6c47. \u5b66\u4e60\u4e86CBOW\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope you set\uff0c\u56e0\u4e3a\u662fCBOW\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528Hope\u548cset\u4f5c\u4e3a\u8f93\u5165\uff0cyou\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0cset\uff0cyou\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u6bcf\u4e2aone-hot\u7f16\u7801\u7684\u5355\u8bcd\u4e0e\u5404\u81ea\u7684\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58\u4e4b\u540e\u518d\u76f8\u52a0, \u5f97\u5230\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635\u4e0e\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3, \u6240\u6709\u7684\u53d8\u6362\u77e9\u9635\u5171\u4eab\u53c2\u6570)\u76f8\u4e58, \u5f97\u52305x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eec\u771f\u6b63\u7684\u76ee\u6807\u77e9\u9635\u5373you\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21\u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. \u5b66\u4e60\u4e86skipgram\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u76ee\u6807\u8bcd\u6c47\u9884\u6d4b\u4e0a\u4e0b\u6587\u8bcd\u6c47. \u5b66\u4e60\u4e86skipgram\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope you set\uff0c\u56e0\u4e3a\u662fskipgram\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528you\u4f5c\u4e3a\u8f93\u5165 \uff0chope\u548cset\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0cset\uff0cyou\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u5c06you\u7684one-hot\u7f16\u7801\u4e0e\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58, \u5f97\u5230\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635\u4e0e\u591a\u4e2a\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3)\u76f8\u4e58, \u5f97\u5230\u591a\u4e2a5x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eechope\u548cset\u5bf9\u5e94\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21 \u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635\u5373\u53c2\u6570\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. \u5b66\u4e60\u4e86\u4f7f\u7528fasttext\u5de5\u5177\u5b9e\u73b0word2vec\u7684\u8bad\u7ec3\u548c\u4f7f\u7528: \u7b2c\u4e00\u6b65: \u83b7\u53d6\u8bad\u7ec3\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fword embedding(\u8bcd\u5d4c\u5165): \u901a\u8fc7\u4e00\u5b9a\u7684\u65b9\u5f0f\u5c06\u8bcd\u6c47\u6620\u5c04\u5230\u6307\u5b9a\u7ef4\u5ea6(\u4e00\u822c\u662f\u66f4\u9ad8\u7ef4\u5ea6)\u7684\u7a7a\u95f4. \u5e7f\u4e49\u7684word embedding\u5305\u62ec\u6240\u6709\u5bc6\u96c6\u8bcd\u6c47\u5411\u91cf\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5982\u4e4b\u524d\u5b66\u4e60\u7684word2vec, \u5373\u53ef\u8ba4\u4e3a\u662fword embedding\u7684\u4e00\u79cd. \u72ed\u4e49\u7684word embedding\u662f\u6307\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u52a0\u5165\u7684embedding\u5c42, \u5bf9\u6574\u4e2a\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u7684\u540c\u65f6\u4ea7\u751f\u7684embedding\u77e9\u9635(embedding\u5c42\u7684\u53c2\u6570), \u8fd9\u4e2aembedding\u77e9\u9635\u5c31\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6240\u6709\u8f93\u5165\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a\u7ec4\u6210\u7684\u77e9\u9635. \u5b66\u4e60\u4e86word embedding\u7684\u53ef\u89c6\u5316\u5206\u6790: \u901a\u8fc7\u4f7f\u7528tensorboard\u53ef\u89c6\u5316\u5d4c\u5165\u7684\u8bcd\u5411\u91cf. \u5728\u7ec8\u7aef\u542f\u52a8tensorboard\u670d\u52a1. \u6d4f\u89c8\u5668\u5c55\u793a\u5e76\u53ef\u4ee5\u4f7f\u7528\u53f3\u4fa7\u8fd1\u90bb\u8bcd\u6c47\u529f\u80fd\u68c0\u9a8c\u6548\u679c.","title":"3 \u6587\u672c\u5f20\u91cf\u8868\u793a\u65b9\u6cd5"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662f\u6587\u672c\u5f20\u91cf\u8868\u793a\u53ca\u5176\u4f5c\u7528. \u638c\u63e1\u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u51e0\u79cd\u65b9\u6cd5\u53ca\u5176\u5b9e\u73b0.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#1","text":"\u5c06\u4e00\u6bb5\u6587\u672c\u4f7f\u7528\u5f20\u91cf\u8fdb\u884c\u8868\u793a\uff0c\u5176\u4e2d\u4e00\u822c\u5c06\u8bcd\u6c47\u8868\u793a\u6210\u5411\u91cf\uff0c\u79f0\u4f5c\u8bcd\u5411\u91cf\uff0c\u518d\u7531\u5404\u4e2a\u8bcd\u5411\u91cf\u6309\u987a\u5e8f\u7ec4\u6210\u77e9\u9635\u5f62\u6210\u6587\u672c\u8868\u793a. \u4e3e\u4e2a\u4f8b\u5b50: [\"\u4eba\u751f\", \"\u8be5\", \"\u5982\u4f55\", \"\u8d77\u5934\"] ==> # \u6bcf\u4e2a\u8bcd\u5bf9\u5e94\u77e9\u9635\u4e2d\u7684\u4e00\u4e2a\u5411\u91cf [[1.32, 4,32, 0,32, 5.2], [3.1, 5.43, 0.34, 3.2], [3.21, 5.32, 2, 4.32], [2.54, 7.32, 5.12, 9.54]] \u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u4f5c\u7528: \u5c06\u6587\u672c\u8868\u793a\u6210\u5f20\u91cf\uff08\u77e9\u9635\uff09\u5f62\u5f0f\uff0c\u80fd\u591f\u4f7f\u8bed\u8a00\u6587\u672c\u53ef\u4ee5\u4f5c\u4e3a\u8ba1\u7b97\u673a\u5904\u7406\u7a0b\u5e8f\u7684\u8f93\u5165\uff0c\u8fdb\u884c\u63a5\u4e0b\u6765\u4e00\u7cfb\u5217\u7684\u89e3\u6790\u5de5\u4f5c. \u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u65b9\u6cd5: one-hot\u7f16\u7801 Word2vec Word Embedding","title":"1 \u6587\u672c\u5f20\u91cf\u8868\u793a"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#2-one-hot","text":"\u53c8\u79f0\u72ec\u70ed\u7f16\u7801\uff0c\u5c06\u6bcf\u4e2a\u8bcd\u8868\u793a\u6210\u5177\u6709n\u4e2a\u5143\u7d20\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u8bcd\u5411\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u662f1\uff0c\u5176\u4ed6\u5143\u7d20\u90fd\u662f0\uff0c\u4e0d\u540c\u8bcd\u6c47\u5143\u7d20\u4e3a0\u7684\u4f4d\u7f6e\u4e0d\u540c\uff0c\u5176\u4e2dn\u7684\u5927\u5c0f\u662f\u6574\u4e2a\u8bed\u6599\u4e2d\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570. \u4e3e\u4e2a\u4f8b\u5b50: [\"\u6539\u53d8\", \"\u8981\", \"\u5982\u4f55\", \"\u8d77\u624b\"]` ==> [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]] onehot\u7f16\u7801\u5b9e\u73b0: \u8fdb\u884conehot\u7f16\u7801: import jieba # \u5bfc\u5165keras\u4e2d\u7684\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer from tensorflow.keras.preprocessing.text import Tokenizer # \u5bfc\u5165\u7528\u4e8e\u5bf9\u8c61\u4fdd\u5b58\u4e0e\u52a0\u8f7d\u7684joblib from sklearn.externals import joblib # \u601d\u8def\u5206\u6790 \u751f\u6210onehot # 1 \u51c6\u5907\u8bed\u6599 vocabs # 2 \u5b9e\u4f8b\u5316\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer, \u4f7f\u7528\u6620\u5c04\u5668\u62df\u5408\u73b0\u6709\u6587\u672c\u6570\u636e (\u5185\u90e8\u751f\u6210 index_word word_index) # 2-1 \u6ce8\u610fidx\u5e8f\u53f7-1 # 3 \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503c zero_list\uff0c\u751f\u6210onehot # 4 \u4f7f\u7528joblib\u5de5\u5177\u4fdd\u5b58\u6620\u5c04\u5668 joblib.dump() def dm_onehot_gen (): # 1 \u51c6\u5907\u8bed\u6599 vocabs vocabs = { \"\u5468\u6770\u4f26\" , \"\u9648\u5955\u8fc5\" , \"\u738b\u529b\u5b8f\" , \"\u674e\u5b97\u76db\" , \"\u5434\u4ea6\u51e1\" , \"\u9e7f\u6657\" } # 2 \u5b9e\u4f8b\u5316\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer, \u4f7f\u7528\u6620\u5c04\u5668\u62df\u5408\u73b0\u6709\u6587\u672c\u6570\u636e (\u5185\u90e8\u751f\u6210 index_word word_index) # 2-1 \u6ce8\u610fidx\u5e8f\u53f7-1 mytokenizer = Tokenizer () mytokenizer . fit_on_texts ( vocabs ) # 3 \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503c zero_list\uff0c\u751f\u6210onehot for vocab in vocabs : zero_list = [ 0 ] * len ( vocabs ) idx = mytokenizer . word_index [ vocab ] - 1 zero_list [ idx ] = 1 print ( vocab , '\u7684onehot\u7f16\u7801\u662f' , zero_list ) # 4 \u4f7f\u7528joblib\u5de5\u5177\u4fdd\u5b58\u6620\u5c04\u5668 joblib.dump() mypath = './mytokenizer' joblib . dump ( mytokenizer , mypath ) print ( '\u4fdd\u5b58mytokenizer End' ) # \u6ce8\u610f5-1 \u5b57\u5178\u6ca1\u6709\u987a\u5e8f onehot\u7f16\u7801\u6ca1\u6709\u987a\u5e8f []-\u6709\u5e8f {}-\u65e0\u5e8f \u533a\u522b # \u6ce8\u610f5-2 \u5b57\u5178\u6709\u7684\u5355\u8bcd\u624d\u6709idx idx\u4ece1\u5f00\u59cb # \u6ce8\u610f5-3 \u67e5\u8be2\u6ca1\u6709\u6ce8\u518c\u7684\u8bcd\u4f1a\u6709\u5f02\u5e38 eg: \u72d7\u86cb print ( mytokenizer . word_index ) print ( mytokenizer . index_word ) \u8f93\u51fa\u6548\u679c: \u9648\u5955\u8fc5 \u7684onehot\u7f16\u7801\u662f [1, 0, 0, 0, 0, 0] \u738b\u529b\u5b8f \u7684onehot\u7f16\u7801\u662f [0, 1, 0, 0, 0, 0] \u9e7f\u6657 \u7684onehot\u7f16\u7801\u662f [0, 0, 1, 0, 0, 0] \u5468\u6770\u4f26 \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 1, 0, 0] \u674e\u5b97\u76db \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 0, 1, 0] \u5434\u4ea6\u51e1 \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 0, 0, 1] \u4fdd\u5b58mytokenizer End {'\u9648\u5955\u8fc5': 1, '\u738b\u529b\u5b8f': 2, '\u9e7f\u6657': 3, '\u5468\u6770\u4f26': 4, '\u674e\u5b97\u76db': 5, '\u5434\u4ea6\u51e1': 6} {1: '\u9648\u5955\u8fc5', 2: '\u738b\u529b\u5b8f', 3: '\u9e7f\u6657', 4: '\u5468\u6770\u4f26', 5: '\u674e\u5b97\u76db', 6: '\u5434\u4ea6\u51e1'} onehot\u7f16\u7801\u5668\u7684\u4f7f\u7528: # \u601d\u8def\u5206\u6790 # 1 \u52a0\u8f7d\u5df2\u4fdd\u5b58\u7684\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer joblib.load(mypath) # 2 \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503czero_list\uff0c\u751f\u6210onehot \u4ee5token\u4e3a'\u674e\u5b97\u76db' # 3 token = \"\u72d7\u86cb\" \u4f1a\u51fa\u73b0\u5f02\u5e38 def dm_onehot_use (): vocabs = { \"\u5468\u6770\u4f26\" , \"\u9648\u5955\u8fc5\" , \"\u738b\u529b\u5b8f\" , \"\u674e\u5b97\u76db\" , \"\u5434\u4ea6\u51e1\" , \"\u9e7f\u6657\" } # 1 \u52a0\u8f7d\u5df2\u4fdd\u5b58\u7684\u8bcd\u6c47\u6620\u5c04\u5668Tokenizer joblib.load(mypath) mypath = './mytokenizer' mytokenizer = joblib . load ( mypath ) # 2 \u7f16\u7801token\u4e3a\"\u674e\u5b97\u76db\" \u67e5\u8be2\u5355\u8bcdidx \u8d4b\u503c zero_list\uff0c\u751f\u6210onehot token = \"\u674e\u5b97\u76db\" zero_list = [ 0 ] * len ( vocabs ) idx = mytokenizer . word_index [ token ] - 1 zero_list [ idx ] = 1 print ( token , '\u7684onehot\u7f16\u7801\u662f' , zero_list ) \u8f93\u51fa\u6548\u679c: \u674e\u5b97\u76db \u7684onehot\u7f16\u7801\u662f [0, 0, 0, 0, 1, 0] one-hot\u7f16\u7801\u7684\u4f18\u52a3\u52bf\uff1a \u4f18\u52bf\uff1a\u64cd\u4f5c\u7b80\u5355\uff0c\u5bb9\u6613\u7406\u89e3. \u52a3\u52bf\uff1a\u5b8c\u5168\u5272\u88c2\u4e86\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u800c\u4e14\u5728\u5927\u8bed\u6599\u96c6\u4e0b\uff0c\u6bcf\u4e2a\u5411\u91cf\u7684\u957f\u5ea6\u8fc7\u5927\uff0c\u5360\u636e\u5927\u91cf\u5185\u5b58. \u6b63\u56e0\u4e3aone-hot\u7f16\u7801\u660e\u663e\u7684\u52a3\u52bf\uff0c\u8fd9\u79cd\u7f16\u7801\u65b9\u5f0f\u88ab\u5e94\u7528\u7684\u5730\u65b9\u8d8a\u6765\u8d8a\u5c11\uff0c\u53d6\u800c\u4ee3\u4e4b\u7684\u662f\u63a5\u4e0b\u6765\u6211\u4eec\u8981\u5b66\u4e60\u7684\u7a20\u5bc6\u5411\u91cf\u7684\u8868\u793a\u65b9\u6cd5word2vec\u548cword embedding.","title":"2 one-hot\u8bcd\u5411\u91cf\u8868\u793a"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#3-word2vec","text":"","title":"3 word2vec\u6a21\u578b"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#31","text":"word2vec\u662f\u4e00\u79cd\u6d41\u884c\u7684\u5c06\u8bcd\u6c47\u8868\u793a\u6210\u5411\u91cf\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5, \u8be5\u8fc7\u7a0b\u5c06\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b, \u5c06\u7f51\u7edc\u53c2\u6570\u4f5c\u4e3a\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a, \u5b83\u5305\u542bCBOW\u548cskipgram\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f. CBOW(Continuous bag of words)\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u4e0a\u4e0b\u6587\u8bcd\u6c47\u9884\u6d4b\u76ee\u6807\u8bcd\u6c47. \u5206\u6790: \u56fe\u4e2d\u7a97\u53e3\u5927\u5c0f\u4e3a9, \u4f7f\u7528\u524d\u540e4\u4e2a\u8bcd\u6c47\u5bf9\u76ee\u6807\u8bcd\u6c47\u8fdb\u884c\u9884\u6d4b. CBOW\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope can set\uff0c\u56e0\u4e3a\u662fCBOW\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528Hope\u548cset\u4f5c\u4e3a\u8f93\u5165\uff0ccan\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0ccan\uff0cset\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u6bcf\u4e2aone-hot\u7f16\u7801\u7684\u5355\u8bcd\u4e0e\u5404\u81ea\u7684\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58\u4e4b\u540e\u518d\u76f8\u52a0, \u5f97\u5230\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635\u4e0e\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3, \u6240\u6709\u7684\u53d8\u6362\u77e9\u9635\u5171\u4eab\u53c2\u6570)\u76f8\u4e58, \u5f97\u52305x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eec\u771f\u6b63\u7684\u76ee\u6807\u77e9\u9635\u5373can\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21\u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. skipgram\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u76ee\u6807\u8bcd\u6c47\u9884\u6d4b\u4e0a\u4e0b\u6587\u8bcd\u6c47. \u5206\u6790: \u56fe\u4e2d\u7a97\u53e3\u5927\u5c0f\u4e3a9, \u4f7f\u7528\u76ee\u6807\u8bcd\u6c47\u5bf9\u524d\u540e\u56db\u4e2a\u8bcd\u6c47\u8fdb\u884c\u9884\u6d4b. skipgram\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope can set\uff0c\u56e0\u4e3a\u662fskipgram\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528can\u4f5c\u4e3a\u8f93\u5165 \uff0cHope\u548cset\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0ccan\uff0cset\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u5c06can\u7684one-hot\u7f16\u7801\u4e0e\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58, \u5f97\u5230\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635\u4e0e\u591a\u4e2a\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3)\u76f8\u4e58, \u5f97\u5230\u591a\u4e2a5x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eecHope\u548cset\u5bf9\u5e94\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21 \u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635\u5373\u53c2\u6570\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. \u8bcd\u5411\u91cf\u7684\u68c0\u7d22\u83b7\u53d6 \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5b8c\u6bd5\u540e\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u77e9\u9635w\u5c31\u6211\u4eec\u7684\u60f3\u8981\u8bcd\u5411\u91cf\u3002\u5982\u4f55\u68c0\u7d22\u67d01\u4e2a\u5355\u8bcd\u7684\u5411\u91cf\u5462\uff1f\u4ee5CBOW\u65b9\u5f0f\u4e3e\u4f8b\u8bf4\u660e\u5982\u4f55\u68c0\u7d22a\u5355\u8bcd\u7684\u8bcd\u5411\u91cf\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1aa\u7684onehot\u7f16\u7801[10000]\uff0c\u7528\u53c2\u6570\u77e9\u9635[3,5] * a\u7684onehot\u7f16\u7801[10000]\uff0c\u53ef\u4ee5\u628a\u53c2\u6570\u77e9\u9635\u7684\u7b2c1\u5217\u53c2\u6570\u7ed9\u53d6\u51fa\u6765\uff0c\u8fd9\u4e2a[3,1]\u7684\u503c\u5c31\u662fa\u7684\u8bcd\u5411\u91cf\u3002","title":"3.1 \u6a21\u578b\u4ecb\u7ecd"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#32-word2vec","text":"\u7b2c\u4e00\u6b65: \u83b7\u53d6\u8bad\u7ec3\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d","title":"3.2 word2vec\u7684\u8bad\u7ec3\u548c\u4f7f\u7528"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#1_1","text":"\u6570\u636e\u6765\u6e90\uff1a http://mattmahoney.net/dc/enwik9.zip \u5728\u8fd9\u91cc, \u6211\u4eec\u5c06\u7814\u7a76\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u90e8\u5206\u7f51\u9875\u4fe1\u606f, \u5b83\u7684\u5927\u5c0f\u5728300M\u5de6\u53f3\u3002\u8fd9\u4e9b\u8bed\u6599\u5df2\u7ecf\u88ab\u51c6\u5907\u597d, \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7Matt Mahoney\u7684\u7f51\u7ad9\u4e0b\u8f7d\u3002 \u6ce8\u610f\uff1a\u539f\u59cb\u6570\u636e\u96c6\u5df2\u7ecf\u653e\u5728/root/data/enwik9.zip\uff0c\u89e3\u538b\u540e\u6570\u636e\u4e3a/root/data/enwik9\uff0c\u9884\u5904\u7406\u540e\u7684\u6570\u636e\u4e3a/root/data/fil9 \u67e5\u770b\u539f\u59cb\u6570\u636e: $ head -10 data/enwik9 # \u539f\u59cb\u6570\u636e\u5c06\u8f93\u51fa\u5f88\u591a\u5305\u542bXML/HTML\u683c\u5f0f\u7684\u5185\u5bb9, \u8fd9\u4e9b\u5185\u5bb9\u5e76\u4e0d\u662f\u6211\u4eec\u9700\u8981\u7684 <mediawiki xmlns = \"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi = \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation = \"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version = \"0.3\" xml:lang = \"en\" > <siteinfo> <sitename>Wikipedia</sitename> <base>http://en.wikipedia.org/wiki/Main_Page</base> <generator>MediaWiki 1 .6alpha</generator> < case >first-letter</case> <namespaces> <namespace key = \"-2\" >Media</namespace> <namespace key = \"-1\" >Special</namespace> <namespace key = \"0\" /> \u539f\u59cb\u6570\u636e\u5904\u7406: # \u4f7f\u7528wikifil.pl\u6587\u4ef6\u5904\u7406\u811a\u672c\u6765\u6e05\u9664XML/HTML\u683c\u5f0f\u7684\u5185\u5bb9 # perl wikifil.pl data/enwik9 > data/fil9 #\u8be5\u547d\u4ee4\u5df2\u7ecf\u6267\u884c \u67e5\u770b\u9884\u5904\u7406\u540e\u7684\u6570\u636e: # \u67e5\u770b\u524d80\u4e2a\u5b57\u7b26 head -c 80 data/fil9 # \u8f93\u51fa\u7ed3\u679c\u4e3a\u7531\u7a7a\u683c\u5206\u5272\u7684\u5355\u8bcd anarchism originated as a term of abuse first used against early working class","title":"1 \u83b7\u53d6\u8bad\u7ec3\u6570\u636e"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#2","text":"fasttext \u662f facebook \u5f00\u6e90\u7684\u4e00\u4e2a\u8bcd\u5411\u91cf\u4e0e\u6587\u672c\u5206\u7c7b\u5de5\u5177\u3002\u4e0b\u9762\u662f\u8be5\u5de5\u5177\u5305\u7684\u5b89\u88c5\u65b9\u6cd5 # \u8bad\u7ec3\u8bcd\u5411\u91cf\u5de5\u5177\u5e93\u7684\u5b89\u88c5 # \u65b9\u6cd51 \u7b80\u6d01\u7248 pip install fasttext # \u65b9\u6cd52\uff1a\u6e90\u7801\u5b89\u88c5(\u63a8\u8350) # \u4ee5linux\u5b89\u88c5\u4e3a\u4f8b\uff1a \u76ee\u5f55\u5207\u6362\u5230\u865a\u62df\u5f00\u53d1\u73af\u5883\u76ee\u5f55\u4e0b\uff0c\u518d\u6267\u884cgit clone \u64cd\u4f5c git clone https://github.com/facebookresearch/fastText.git cd fastText # \u4f7f\u7528pip\u5b89\u88c5python\u4e2d\u7684fasttext\u5de5\u5177\u5305 sudo pip install . # \u5bfc\u5165fasttext import fasttext def dm_fasttext_train_save_load (): # 1 \u4f7f\u7528train_unsupervised(\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5) \u8bad\u7ec3\u8bcd\u5411\u91cf mymodel = fasttext . train_unsupervised ( './data/fil9' ) print ( '\u8bad\u7ec3\u8bcd\u5411\u91cf ok' ) # 2 save_model()\u4fdd\u5b58\u5df2\u7ecf\u8bad\u7ec3\u597d\u8bcd\u5411\u91cf # \u6ce8\u610f\uff0c\u8be5\u884c\u4ee3\u7801\u6267\u884c\u8017\u65f6\u5f88\u957f mymodel . save_model ( \"./data/fil9.bin\" ) print ( '\u4fdd\u5b58\u8bcd\u5411\u91cf ok' ) # 3 \u6a21\u578b\u52a0\u8f7d mymodel = fasttext . load_model ( './data/fil9.bin' ) print ( '\u52a0\u8f7d\u8bcd\u5411\u91cf ok' ) # \u6b65\u9aa41\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a \u6709\u6548\u8bad\u7ec3\u8bcd\u6c47\u91cf\u4e3a124M , \u5171218316\u4e2a\u5355\u8bcd Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 53996 lr : 0.000000 loss : 0.734999 ETA : 0 h 0 m","title":"2 \u8bcd\u5411\u91cf\u7684\u8bad\u7ec3\u4fdd\u5b58\u52a0\u8f7d"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#3","text":"# \u901a\u8fc7get_word_vector\u65b9\u6cd5\u6765\u83b7\u5f97\u6307\u5b9a\u8bcd\u6c47\u7684\u8bcd\u5411\u91cf, \u9ed8\u8ba4\u8bcd\u5411\u91cf\u8bad\u7ec3\u51fa\u6765\u662f1\u4e2a\u5355\u8bcd100\u7279\u5f81 def dm_fasttext_get_word_vector (): mymodel = fasttext . load_model ( './data/fil9.bin' ) myvector = mymodel . get_word_vector ( 'the' ) print ( 'myvector->' , type ( myvector ), myvector . shape , myvector ) # \u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a array ([ - 0.03087516 , 0.09221972 , 0.17660329 , 0.17308897 , 0.12863874 , 0.13912526 , - 0.09851588 , 0.00739991 , 0.37038437 , - 0.00845221 , ... - 0.21184735 , - 0.05048715 , - 0.34571868 , 0.23765688 , 0.23726143 ], dtype = float32 )","title":"3 \u67e5\u770b\u5355\u8bcd\u5bf9\u5e94\u7684\u8bcd\u5411\u91cf"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#4","text":"# \u68c0\u67e5\u5355\u8bcd\u5411\u91cf\u8d28\u91cf\u7684\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\u5c31\u662f\u67e5\u770b\u5176\u90bb\u8fd1\u5355\u8bcd, \u901a\u8fc7\u6211\u4eec\u4e3b\u89c2\u6765\u5224\u65ad\u8fd9\u4e9b\u90bb\u8fd1\u5355\u8bcd\u662f\u5426\u4e0e\u76ee\u6807\u5355\u8bcd\u76f8\u5173\u6765\u7c97\u7565\u8bc4\u5b9a\u6a21\u578b\u6548\u679c\u597d\u574f. # \u67e5\u627e\"\u8fd0\u52a8\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\"\u4f53\u80b2\u7f51\", \"\u8fd0\u52a8\u6c7d\u8f66\", \"\u8fd0\u52a8\u670d\"\u7b49. >>> model . get_nearest_neighbors ( 'sports' ) [( 0.8414610624313354 , 'sportsnet' ), ( 0.8134572505950928 , 'sport' ), ( 0.8100415468215942 , 'sportscars' ), ( 0.8021156787872314 , 'sportsground' ), ( 0.7889881134033203 , 'sportswomen' ), ( 0.7863013744354248 , 'sportsplex' ), ( 0.7786710262298584 , 'sporty' ), ( 0.7696356177330017 , 'sportscar' ), ( 0.7619683146476746 , 'sportswear' ), ( 0.7600985765457153 , 'sportin' )] # \u67e5\u627e\"\u97f3\u4e50\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u97f3\u4e50\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'music' ) [( 0.8908010125160217 , 'emusic' ), ( 0.8464668393135071 , 'musicmoz' ), ( 0.8444250822067261 , 'musics' ), ( 0.8113634586334229 , 'allmusic' ), ( 0.8106718063354492 , 'musices' ), ( 0.8049437999725342 , 'musicam' ), ( 0.8004694581031799 , 'musicom' ), ( 0.7952923774719238 , 'muchmusic' ), ( 0.7852965593338013 , 'musicweb' ), ( 0.7767147421836853 , 'musico' )] # \u67e5\u627e\"\u5c0f\u72d7\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u5c0f\u72d7\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'dog' ) [( 0.8456876873970032 , 'catdog' ), ( 0.7480780482292175 , 'dogcow' ), ( 0.7289096117019653 , 'sleddog' ), ( 0.7269964218139648 , 'hotdog' ), ( 0.7114801406860352 , 'sheepdog' ), ( 0.6947550773620605 , 'dogo' ), ( 0.6897546648979187 , 'bodog' ), ( 0.6621081829071045 , 'maddog' ), ( 0.6605004072189331 , 'dogs' ), ( 0.6398137211799622 , 'dogpile' )]","title":"4 \u6a21\u578b\u6548\u679c\u68c0\u9a8c"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#5","text":"# \u5728\u8bad\u7ec3\u8bcd\u5411\u91cf\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u53ef\u4ee5\u8bbe\u5b9a\u5f88\u591a\u5e38\u7528\u8d85\u53c2\u6570\u6765\u8c03\u8282\u6211\u4eec\u7684\u6a21\u578b\u6548\u679c, \u5982: # \u65e0\u76d1\u7763\u8bad\u7ec3\u6a21\u5f0f: 'skipgram' \u6216\u8005 'cbow', \u9ed8\u8ba4\u4e3a'skipgram', \u5728\u5b9e\u8df5\u4e2d\uff0cskipgram\u6a21\u5f0f\u5728\u5229\u7528\u5b50\u8bcd\u65b9\u9762\u6bd4cbow\u66f4\u597d. # \u8bcd\u5d4c\u5165\u7ef4\u5ea6dim: \u9ed8\u8ba4\u4e3a100, \u4f46\u968f\u7740\u8bed\u6599\u5e93\u7684\u589e\u5927, \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5f80\u5f80\u4e5f\u8981\u66f4\u5927. # \u6570\u636e\u5faa\u73af\u6b21\u6570epoch: \u9ed8\u8ba4\u4e3a5, \u4f46\u5f53\u4f60\u7684\u6570\u636e\u96c6\u8db3\u591f\u5927, \u53ef\u80fd\u4e0d\u9700\u8981\u90a3\u4e48\u591a\u6b21. # \u5b66\u4e60\u7387lr: \u9ed8\u8ba4\u4e3a0.05, \u6839\u636e\u7ecf\u9a8c, \u5efa\u8bae\u9009\u62e9[0.01\uff0c1]\u8303\u56f4\u5185. # \u4f7f\u7528\u7684\u7ebf\u7a0b\u6570thread: \u9ed8\u8ba4\u4e3a12\u4e2a\u7ebf\u7a0b, \u4e00\u822c\u5efa\u8bae\u548c\u4f60\u7684cpu\u6838\u6570\u76f8\u540c. >>> model = fasttext . train_unsupervised ( 'data/fil9' , \"cbow\" , dim = 300 , epoch = 1 , lr = 0.1 , thread = 8 ) Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 49523 lr : 0.000000 avg . loss : 1.777205 ETA : 0 h 0 m 0 s","title":"5 \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#4-word-embedding","text":"\u901a\u8fc7\u4e00\u5b9a\u7684\u65b9\u5f0f\u5c06\u8bcd\u6c47\u6620\u5c04\u5230\u6307\u5b9a\u7ef4\u5ea6(\u4e00\u822c\u662f\u66f4\u9ad8\u7ef4\u5ea6)\u7684\u7a7a\u95f4. \u5e7f\u4e49\u7684word embedding\u5305\u62ec\u6240\u6709\u5bc6\u96c6\u8bcd\u6c47\u5411\u91cf\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5982\u4e4b\u524d\u5b66\u4e60\u7684word2vec, \u5373\u53ef\u8ba4\u4e3a\u662fword embedding\u7684\u4e00\u79cd. \u72ed\u4e49\u7684word embedding\u662f\u6307\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u52a0\u5165\u7684embedding\u5c42, \u5bf9\u6574\u4e2a\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u7684\u540c\u65f6\u4ea7\u751f\u7684embedding\u77e9\u9635(embedding\u5c42\u7684\u53c2\u6570), \u8fd9\u4e2aembedding\u77e9\u9635\u5c31\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6240\u6709\u8f93\u5165\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a\u7ec4\u6210\u7684\u77e9\u9635. word embedding\u7684\u53ef\u89c6\u5316\u5206\u6790: \u901a\u8fc7\u4f7f\u7528tensorboard\u53ef\u89c6\u5316\u5d4c\u5165\u7684\u8bcd\u5411\u91cf. import torch from tensorflow.keras.preprocessing.text import Tokenizer from torch.utils.tensorboard import SummaryWriter import jieba import torch.nn as nn # \u6ce8\u610f\uff1a # fs = tf.io.gfile.get_filesystem(save_path) # AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem' # \u9519\u8bef\u539f\u56e0\u5206\u6790\uff1a # 1 from tensorboard.compat import tf \u4f7f\u7528\u4e86tf \u5982\u679c\u5b89\u88c5tensorflow\uff0c\u9ed8\u8ba4\u4f1a\u8c03\u7528\u5b83tf\u7684api\u51fd\u6570 import tensorflow as tf import tensorboard as tb tf . io . gfile = tb . compat . tensorflow_stub . io . gfile # \u5b9e\u9a8c\uff1ann.Embedding\u5c42\u8bcd\u5411\u91cf\u53ef\u89c6\u5316\u5206\u6790 # 1 \u5bf9\u53e5\u5b50\u5206\u8bcd word_list # 2 \u5bf9\u53e5\u5b50word2id\u6c42my_token_list\uff0c\u5bf9\u53e5\u5b50\u6587\u672c\u6570\u503c\u5316sentence2id # 3 \u521b\u5efann.Embedding\u5c42\uff0c\u67e5\u770b\u6bcf\u4e2atoken\u7684\u8bcd\u5411\u91cf\u6570\u636e # 4 \u521b\u5efaSummaryWriter\u5bf9\u8c61, \u53ef\u89c6\u5316\u8bcd\u5411\u91cf # \u8bcd\u5411\u91cf\u77e9\u9635embd.weight.data \u548c \u8bcd\u5411\u91cf\u5355\u8bcd\u5217\u8868my_token_list\u6dfb\u52a0\u5230SummaryWriter\u5bf9\u8c61\u4e2d # summarywriter.add_embedding(embd.weight.data, my_token_list) # 5 \u901a\u8fc7tensorboard\u89c2\u5bdf\u8bcd\u5411\u91cf\u76f8\u4f3c\u6027 # 6 \u4e5f\u53ef\u901a\u8fc7\u7a0b\u5e8f\uff0c\u4ecenn.Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf def dm02_nnembeding_show (): # 1 \u5bf9\u53e5\u5b50\u5206\u8bcd word_list sentence1 = '\u4f20\u667a\u6559\u80b2\u662f\u4e00\u5bb6\u4e0a\u5e02\u516c\u53f8\uff0c\u65d7\u4e0b\u6709\u9ed1\u9a6c\u7a0b\u5e8f\u5458\u54c1\u724c\u3002\u6211\u662f\u5728\u9ed1\u9a6c\u8fd9\u91cc\u5b66\u4e60\u4eba\u5de5\u667a\u80fd' sentence2 = \"\u6211\u7231\u81ea\u7136\u8bed\u8a00\u5904\u7406\" sentences = [ sentence1 , sentence2 ] word_list = [] for s in sentences : word_list . append ( jieba . lcut ( s )) # print('word_list--->', word_list) # 2 \u5bf9\u53e5\u5b50word2id\u6c42my_token_list\uff0c\u5bf9\u53e5\u5b50\u6587\u672c\u6570\u503c\u5316sentence2id mytokenizer = Tokenizer () mytokenizer . fit_on_texts ( word_list ) # print(mytokenizer.index_word, mytokenizer.word_index) # \u6253\u5370my_token_list my_token_list = mytokenizer . index_word . values () print ( 'my_token_list-->' , my_token_list ) # \u6253\u5370\u6587\u672c\u6570\u503c\u5316\u4ee5\u540e\u7684\u53e5\u5b50 sentence2id = mytokenizer . texts_to_sequences ( word_list ) print ( 'sentence2id--->' , sentence2id , len ( sentence2id )) # 3 \u521b\u5efann.Embedding\u5c42 embd = nn . Embedding ( num_embeddings = len ( my_token_list ), embedding_dim = 8 ) # print(\"embd--->\", embd) # print('nn.Embedding\u5c42\u8bcd\u5411\u91cf\u77e9\u9635-->', embd.weight.data, embd.weight.data.shape, type(embd.weight.data)) # 4 \u521b\u5efaSummaryWriter\u5bf9\u8c61 \u8bcd\u5411\u91cf\u77e9\u9635embd.weight.data \u548c \u8bcd\u5411\u91cf\u5355\u8bcd\u5217\u8868my_token_list summarywriter = SummaryWriter () summarywriter . add_embedding ( embd . weight . data , my_token_list ) summarywriter . close () # 5 \u901a\u8fc7tensorboard\u89c2\u5bdf\u8bcd\u5411\u91cf\u76f8\u4f3c\u6027 # cd \u7a0b\u5e8f\u7684\u5f53\u524d\u76ee\u5f55\u4e0b\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4 # \u542f\u52a8tensorboard\u670d\u52a1 tensorboard --logdir=runs --host 0.0.0.0 # \u901a\u8fc7\u6d4f\u89c8\u5668\uff0c\u67e5\u770b\u8bcd\u5411\u91cf\u53ef\u89c6\u5316\u6548\u679c http://127.0.0.1:6006 print ( '\u4ecenn.Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf' ) # # 6 \u4ecenn.Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf for idx in range ( len ( mytokenizer . index_word )): tmpvec = embd ( torch . tensor ( idx )) print ( ' %4s ' % ( mytokenizer . index_word [ idx + 1 ]), tmpvec . detach () . numpy ()) \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c my_token_list --> dict_values ([ '\u662f' , '\u9ed1\u9a6c' , '\u6211' , '\u4f20\u667a' , '\u6559\u80b2' , '\u4e00\u5bb6' , '\u4e0a\u5e02\u516c\u53f8' , '\uff0c' , '\u65d7\u4e0b' , '\u6709' , '\u7a0b\u5e8f\u5458' , '\u54c1\u724c' , '\u3002' , '\u5728' , '\u8fd9\u91cc' , '\u5b66\u4e60' , '\u4eba\u5de5\u667a\u80fd' , '\u7231' , '\u81ea\u7136\u8bed\u8a00' , '\u5904\u7406' ]) sentence2id ---> [[ 4 , 5 , 1 , 6 , 7 , 8 , 9 , 10 , 2 , 11 , 12 , 13 , 3 , 1 , 14 , 2 , 15 , 16 , 17 ], [ 3 , 18 , 19 , 20 ]] 2 \u4ecenn . Embedding\u5c42\u4e2d\u6839\u636eidx\u62ff\u8bcd\u5411\u91cf \u662f [ 0.46067393 - 0.9049023 - 0.03143226 - 0.32443136 0.03115687 - 1.3352231 - 0.08336695 - 2.4732168 ] \u9ed1\u9a6c [ 0.66760564 0.08703537 0.23735243 1.5896837 - 1.8869231 0.22520915 - 1.0676078 - 0.7654686 ] \u6211 [ - 0.9093167 - 0.6114051 - 0.6825029 0.9269122 0.5208822 2.294128 - 0.11160549 - 0.34862307 ] \u4f20\u667a [ - 1.1552105 - 0.4274638 - 0.8121502 - 1.4969801 - 1.3328248 - 1.0934378 0.6707438 - 1.1796173 ] \u6559\u80b2 [ 0.01580311 - 1.1884228 0.59364647 1.5387698 - 1.0822943 0.36760855 - 0.4652998 - 0.57378227 ] \u4e00\u5bb6 [ - 1.1898873 - 0.42482868 - 1.9391155 - 1.5678993 - 1.6960118 0.22525501 - 1.0754168 0.41797593 ] \u4e0a\u5e02\u516c\u53f8 [ 0.590556 2.4274144 1.6698223 - 0.9776848 - 0.6119061 0.4434897 - 2.3726876 - 0.2607738 ] \uff0c [ - 0.17568143 1.0074369 0.2571488 1.8940887 - 0.5383494 0.65416646 0.63454026 0.6235991 ] \u65d7\u4e0b [ 2.8400452 - 1.0096515 2.247107 0.30006626 - 1.2687006 0.05855403 0.01199368 - 0.6156502 ] \u6709 [ 0.89320636 - 0.43819678 1.0345292 1.3546743 - 1.4238662 - 1.6994532 0.30445674 2.673923 ] \u7a0b\u5e8f\u5458 [ 1.2147354 0.24878891 0.36161897 0.37458655 - 0.48264053 - 0.0141514 1.2033817 0.7899459 ] \u54c1\u724c [ 0.59799325 - 0.01371854 0.0628166 - 1.4829391 0.39795023 - 0.39259398 - 0.60923046 0.54170054 ] \u3002 [ 0.59599686 1.6038656 - 0.10832139 0.25223547 0.37193906 1.1944667 - 0.91253406 0.6869221 ] \u5728 [ - 1.161504 2.6963246 - 0.6087775 0.9399654 0.8480068 0.684357 0.96156543 - 0.3541162 ] \u8fd9\u91cc [ 0.1034054 - 0.01949253 0.8989019 1.61057 - 1.5983531 0.17945968 - 0.17572908 - 0.9724814 ] \u5b66\u4e60 [ - 1.3899843 - 1.0846052 - 1.1301199 - 0.4078141 0.40511298 0.6562911 0.9231357 - 0.34704337 ] \u4eba\u5de5\u667a\u80fd [ - 1.4966388 - 1.0905199 1.001238 - 0.75254333 - 1.4210068 - 1.854177 1.0471514 - 0.27140012 ] \u7231 [ - 1.5254552 0.6189947 1.2703396 - 0.4826037 - 1.4928672 0.8320283 1.7333516 0.16908517 ] \u81ea\u7136\u8bed\u8a00 [ - 0.3856235 - 1.2193452 0.9991112 - 1.5821775 0.45017946 - 0.66064674 0.08045111 0.62901515 ] \u5904\u7406 [ 1.5062869 1.3156213 - 0.21295634 0.47610474 0.08946162 0.57107806 - 1.0727187 0.16396333 ] \u8bcd\u5411\u91cf\u548c\u8bcd\u663e\u793a\u6807\u7b7e \u5199\u5165\u78c1\u76d8ok \u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u67e5\u770b ./ runs \u76ee\u5f55 \u5728\u7ec8\u7aef\u542f\u52a8tensorboard\u670d\u52a1: $ cd ~ $ tensorboard --logdir = runs --host 0 .0.0.0 # \u901a\u8fc7http://192.168.88.161:6006\u8bbf\u95ee\u6d4f\u89c8\u5668\u53ef\u89c6\u5316\u9875\u9762 \u6d4f\u89c8\u5668\u5c55\u793a\u5e76\u53ef\u4ee5\u4f7f\u7528\u53f3\u4fa7\u8fd1\u90bb\u8bcd\u6c47\u529f\u80fd\u68c0\u9a8c\u6548\u679c:","title":"4 \u8bcd\u5d4c\u5165word embedding\u4ecb\u7ecd"},{"location":"02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html#5_1","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6587\u672c\u5f20\u91cf\u8868\u793a: \u5c06\u4e00\u6bb5\u6587\u672c\u4f7f\u7528\u5f20\u91cf\u8fdb\u884c\u8868\u793a\uff0c\u5176\u4e2d\u4e00\u822c\u5c06\u8bcd\u6c47\u4e3a\u8868\u793a\u6210\u5411\u91cf\uff0c\u79f0\u4f5c\u8bcd\u5411\u91cf\uff0c\u518d\u7531\u5404\u4e2a\u8bcd\u5411\u91cf\u6309\u987a\u5e8f\u7ec4\u6210\u77e9\u9635\u5f62\u6210\u6587\u672c\u8868\u793a. \u5b66\u4e60\u4e86\u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u4f5c\u7528: \u5c06\u6587\u672c\u8868\u793a\u6210\u5f20\u91cf\uff08\u77e9\u9635\uff09\u5f62\u5f0f\uff0c\u80fd\u591f\u4f7f\u8bed\u8a00\u6587\u672c\u53ef\u4ee5\u4f5c\u4e3a\u8ba1\u7b97\u673a\u5904\u7406\u7a0b\u5e8f\u7684\u8f93\u5165\uff0c\u8fdb\u884c\u63a5\u4e0b\u6765\u4e00\u7cfb\u5217\u7684\u89e3\u6790\u5de5\u4f5c. \u5b66\u4e60\u4e86\u6587\u672c\u5f20\u91cf\u8868\u793a\u7684\u65b9\u6cd5: one-hot\u7f16\u7801 Word2vec Word Embedding \u4ec0\u4e48\u662fone-hot\u8bcd\u5411\u91cf\u8868\u793a: \u53c8\u79f0\u72ec\u70ed\u7f16\u7801\uff0c\u5c06\u6bcf\u4e2a\u8bcd\u8868\u793a\u6210\u5177\u6709n\u4e2a\u5143\u7d20\u7684\u5411\u91cf\uff0c\u8fd9\u4e2a\u8bcd\u5411\u91cf\u4e2d\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u662f1\uff0c\u5176\u4ed6\u5143\u7d20\u90fd\u662f0\uff0c\u4e0d\u540c\u8bcd\u6c47\u5143\u7d20\u4e3a0\u7684\u4f4d\u7f6e\u4e0d\u540c\uff0c\u5176\u4e2dn\u7684\u5927\u5c0f\u662f\u6574\u4e2a\u8bed\u6599\u4e2d\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570. \u5b66\u4e60\u4e86onehot\u7f16\u7801\u5b9e\u73b0. \u5b66\u4e60\u4e86one-hot\u7f16\u7801\u7684\u4f18\u52a3\u52bf\uff1a \u4f18\u52bf\uff1a\u64cd\u4f5c\u7b80\u5355\uff0c\u5bb9\u6613\u7406\u89e3. \u52a3\u52bf\uff1a\u5b8c\u5168\u5272\u88c2\u4e86\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u800c\u4e14\u5728\u5927\u8bed\u6599\u96c6\u4e0b\uff0c\u6bcf\u4e2a\u5411\u91cf\u7684\u957f\u5ea6\u8fc7\u5927\uff0c\u5360\u636e\u5927\u91cf\u5185\u5b58. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fword2vec: \u662f\u4e00\u79cd\u6d41\u884c\u7684\u5c06\u8bcd\u6c47\u8868\u793a\u6210\u5411\u91cf\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5, \u8be5\u8fc7\u7a0b\u5c06\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b, \u5c06\u7f51\u7edc\u53c2\u6570\u4f5c\u4e3a\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a, \u5b83\u5305\u542bCBOW\u548cskipgram\u4e24\u79cd\u8bad\u7ec3\u6a21\u5f0f. \u5b66\u4e60\u4e86CBOW(Continuous bag of words)\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u4e0a\u4e0b\u6587\u8bcd\u6c47\u9884\u6d4b\u76ee\u6807\u8bcd\u6c47. \u5b66\u4e60\u4e86CBOW\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope you set\uff0c\u56e0\u4e3a\u662fCBOW\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528Hope\u548cset\u4f5c\u4e3a\u8f93\u5165\uff0cyou\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0cset\uff0cyou\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u6bcf\u4e2aone-hot\u7f16\u7801\u7684\u5355\u8bcd\u4e0e\u5404\u81ea\u7684\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58\u4e4b\u540e\u518d\u76f8\u52a0, \u5f97\u5230\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u4e0a\u4e0b\u6587\u8868\u793a\u77e9\u9635\u4e0e\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3, \u6240\u6709\u7684\u53d8\u6362\u77e9\u9635\u5171\u4eab\u53c2\u6570)\u76f8\u4e58, \u5f97\u52305x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eec\u771f\u6b63\u7684\u76ee\u6807\u77e9\u9635\u5373you\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21\u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. \u5b66\u4e60\u4e86skipgram\u6a21\u5f0f: \u7ed9\u5b9a\u4e00\u6bb5\u7528\u4e8e\u8bad\u7ec3\u7684\u6587\u672c\u8bed\u6599, \u518d\u9009\u5b9a\u67d0\u6bb5\u957f\u5ea6(\u7a97\u53e3)\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61, \u4f7f\u7528\u76ee\u6807\u8bcd\u6c47\u9884\u6d4b\u4e0a\u4e0b\u6587\u8bcd\u6c47. \u5b66\u4e60\u4e86skipgram\u6a21\u5f0f\u4e0b\u7684word2vec\u8fc7\u7a0b\u8bf4\u660e: \u5047\u8bbe\u6211\u4eec\u7ed9\u5b9a\u7684\u8bad\u7ec3\u8bed\u6599\u53ea\u6709\u4e00\u53e5\u8bdd: Hope can set you free (\u613f\u4f60\u81ea\u7531\u6210\u957f)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a3\uff0c\u56e0\u6b64\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u6765\u81eaHope you set\uff0c\u56e0\u4e3a\u662fskipgram\u6a21\u5f0f\uff0c\u6240\u4ee5\u5c06\u4f7f\u7528you\u4f5c\u4e3a\u8f93\u5165 \uff0chope\u548cset\u4f5c\u4e3a\u8f93\u51fa\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c Hope\uff0cset\uff0cyou\u7b49\u8bcd\u6c47\u90fd\u4f7f\u7528\u5b83\u4eec\u7684one-hot\u7f16\u7801. \u5982\u56fe\u6240\u793a: \u5c06you\u7684one-hot\u7f16\u7801\u4e0e\u53d8\u6362\u77e9\u9635(\u5373\u53c2\u6570\u77e9\u96353x5, \u8fd9\u91cc\u76843\u662f\u6307\u6700\u540e\u5f97\u5230\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6)\u76f8\u4e58, \u5f97\u5230\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635(3x1). \u63a5\u7740, \u5c06\u76ee\u6807\u8bcd\u6c47\u8868\u793a\u77e9\u9635\u4e0e\u591a\u4e2a\u53d8\u6362\u77e9\u9635(\u53c2\u6570\u77e9\u96355x3)\u76f8\u4e58, \u5f97\u5230\u591a\u4e2a5x1\u7684\u7ed3\u679c\u77e9\u9635, \u5b83\u5c06\u4e0e\u6211\u4eechope\u548cset\u5bf9\u5e94\u7684one-hot\u7f16\u7801\u77e9\u9635(5x1)\u8fdb\u884c\u635f\u5931\u7684\u8ba1\u7b97, \u7136\u540e\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\u5b8c\u6210\u4e00\u6b21\u6a21 \u578b\u8fed\u4ee3. \u6700\u540e\u7a97\u53e3\u6309\u5e8f\u5411\u540e\u79fb\u52a8\uff0c\u91cd\u65b0\u66f4\u65b0\u53c2\u6570\uff0c\u76f4\u5230\u6240\u6709\u8bed\u6599\u88ab\u904d\u5386\u5b8c\u6210\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635\u5373\u53c2\u6570\u77e9\u9635(3x5)\uff0c\u8fd9\u4e2a\u53d8\u6362\u77e9\u9635\u4e0e\u6bcf\u4e2a\u8bcd\u6c47\u7684one-hot\u7f16\u7801(5x1)\u76f8\u4e58\uff0c\u5f97\u5230\u76843x1\u7684\u77e9\u9635\u5c31\u662f\u8be5\u8bcd\u6c47\u7684word2vec\u5f20\u91cf\u8868\u793a. \u5b66\u4e60\u4e86\u4f7f\u7528fasttext\u5de5\u5177\u5b9e\u73b0word2vec\u7684\u8bad\u7ec3\u548c\u4f7f\u7528: \u7b2c\u4e00\u6b65: \u83b7\u53d6\u8bad\u7ec3\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fword embedding(\u8bcd\u5d4c\u5165): \u901a\u8fc7\u4e00\u5b9a\u7684\u65b9\u5f0f\u5c06\u8bcd\u6c47\u6620\u5c04\u5230\u6307\u5b9a\u7ef4\u5ea6(\u4e00\u822c\u662f\u66f4\u9ad8\u7ef4\u5ea6)\u7684\u7a7a\u95f4. \u5e7f\u4e49\u7684word embedding\u5305\u62ec\u6240\u6709\u5bc6\u96c6\u8bcd\u6c47\u5411\u91cf\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5982\u4e4b\u524d\u5b66\u4e60\u7684word2vec, \u5373\u53ef\u8ba4\u4e3a\u662fword embedding\u7684\u4e00\u79cd. \u72ed\u4e49\u7684word embedding\u662f\u6307\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u52a0\u5165\u7684embedding\u5c42, \u5bf9\u6574\u4e2a\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u7684\u540c\u65f6\u4ea7\u751f\u7684embedding\u77e9\u9635(embedding\u5c42\u7684\u53c2\u6570), \u8fd9\u4e2aembedding\u77e9\u9635\u5c31\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6240\u6709\u8f93\u5165\u8bcd\u6c47\u7684\u5411\u91cf\u8868\u793a\u7ec4\u6210\u7684\u77e9\u9635. \u5b66\u4e60\u4e86word embedding\u7684\u53ef\u89c6\u5316\u5206\u6790: \u901a\u8fc7\u4f7f\u7528tensorboard\u53ef\u89c6\u5316\u5d4c\u5165\u7684\u8bcd\u5411\u91cf. \u5728\u7ec8\u7aef\u542f\u52a8tensorboard\u670d\u52a1. \u6d4f\u89c8\u5668\u5c55\u793a\u5e76\u53ef\u4ee5\u4f7f\u7528\u53f3\u4fa7\u8fd1\u90bb\u8bcd\u6c47\u529f\u80fd\u68c0\u9a8c\u6548\u679c.","title":"5 \u5c0f\u7ed3"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6587\u672c\u6570\u636e\u5206\u6790\u7684\u4f5c\u7528. \u638c\u63e1\u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5. 1 \u6587\u4ef6\u6570\u636e\u5206\u6790\u4ecb\u7ecd \u00b6 \u6587\u672c\u6570\u636e\u5206\u6790\u7684\u4f5c\u7528: \u6587\u672c\u6570\u636e\u5206\u6790\u80fd\u591f\u6709\u6548\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u6570\u636e\u8bed\u6599, \u5feb\u901f\u68c0\u67e5\u51fa\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u7684\u95ee\u9898, \u5e76\u6307\u5bfc\u4e4b\u540e\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e00\u4e9b\u8d85\u53c2\u6570\u7684\u9009\u62e9. \u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5: \u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u8bcd\u9891\u7edf\u8ba1\u4e0e\u5173\u952e\u8bcd\u8bcd\u4e91 2 \u6570\u636e\u96c6\u8bf4\u660e \u00b6 \u6211\u4eec\u5c06\u57fa\u4e8e\u771f\u5b9e\u7684\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u8bed\u6599\u6765\u8bb2\u89e3\u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5. \u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u8bed\u6599: \u5c5e\u4e8e\u4e8c\u5206\u7c7b\u7684\u4e2d\u6587\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8be5\u8bed\u6599\u5b58\u653e\u5728\"./cn_data\"\u76ee\u5f55\u4e0b. \u5176\u4e2dtrain.tsv\u4ee3\u8868\u8bad\u7ec3\u96c6, dev.tsv\u4ee3\u8868\u9a8c\u8bc1\u96c6, \u4e8c\u8005\u6570\u636e\u6837\u5f0f\u76f8\u540c. train.tsv\u6570\u636e\u6837\u5f0f: sentence label \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d,\u9910\u5385\u4e0d\u5206\u5438\u70df\u533a.\u623f\u95f4\u4e0d\u5206\u6709\u65e0\u70df\u623f. 0 \u53bb\u7684\u65f6\u5019 ,\u9152\u5e97\u5927\u5385\u548c\u9910\u5385\u5728\u88c5\u4fee,\u611f\u89c9\u5927\u5385\u6709\u70b9\u6324.\u7531\u4e8e\u9910\u5385\u88c5\u4fee\u672c\u6765\u8be5\u4eab\u53d7\u7684\u65e9\u996d,\u4e5f\u6ca1\u6709\u4eab\u53d7(\u4ed6\u4eec\u662f8\u70b9\u5f00\u59cb\u6bcf\u4e2a\u623f\u95f4\u9001,\u4f46\u662f\u6211\u65f6\u95f4\u6765\u4e0d\u53ca\u4e86)\u4e0d\u8fc7\u524d\u53f0\u670d\u52a1\u5458\u6001\u5ea6\u597d! 1 \u6709\u5f88\u957f\u65f6\u95f4\u6ca1\u6709\u5728\u897f\u85cf\u5927\u53a6\u4f4f\u4e86\uff0c\u4ee5\u524d\u53bb\u5317\u4eac\u5728\u8fd9\u91cc\u4f4f\u7684\u8f83\u591a\u3002\u8fd9\u6b21\u4f4f\u8fdb\u6765\u53d1\u73b0\u6362\u4e86\u6db2\u6676\u7535\u89c6\uff0c\u4f46\u7f51\u7edc\u4e0d\u662f\u5f88\u597d\uff0c\u4ed6\u4eec\u81ea\u5df1\u8bf4\u662f\u6536\u8d39\u7684\u539f\u56e0\u9020\u6210\u7684\u3002\u5176\u5b83\u8fd8\u597d\u3002 1 \u975e\u5e38\u597d\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u4f4f\u7684\u662f\u8c6a\u534e\u6d77\u666f\u623f\uff0c\u6253\u5f00\u7a97\u6237\u5c31\u53ef\u4ee5\u770b\u89c1\u6808\u6865\u548c\u6d77\u666f\u3002\u8bb0\u5f97\u5f88\u65e9\u4ee5\u524d\u4e5f\u4f4f\u8fc7\uff0c\u73b0\u5728\u91cd\u65b0\u88c5\u4fee\u4e86\u3002\u603b\u7684\u6765\u8bf4\u6bd4\u8f83\u6ee1\u610f\uff0c\u4ee5\u540e\u8fd8\u4f1a\u4f4f 1 \u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u623f\u95f4\u5c0f\u4e86\u4e00\u70b9\uff0c\u4f46\u662f\u5e72\u51c0\u6574\u6d01\uff0c\u5f88\u6709\u9999\u6e2f\u7684\u7279\u8272\uff0c\u6027\u4ef7\u6bd4\u8f83\u9ad8\uff0c\u63a8\u8350\u4e00\u4e0b\u54e6 1 \u9152\u5e97\u7684\u88c5\u4fee\u6bd4\u8f83\u9648\u65e7\uff0c\u623f\u95f4\u7684\u9694\u97f3\uff0c\u4e3b\u8981\u662f\u536b\u751f\u95f4\u7684\u9694\u97f3\u975e\u5e38\u5dee\uff0c\u53ea\u80fd\u7b97\u662f\u4e00\u822c\u7684 0 \u9152\u5e97\u6709\u70b9\u65e7\uff0c\u623f\u95f4\u6bd4\u8f83\u5c0f\uff0c\u4f46\u9152\u5e97\u7684\u4f4d\u5b50\u4e0d\u9519\uff0c\u5c31\u5728\u6d77\u8fb9\uff0c\u53ef\u4ee5\u76f4\u63a5\u53bb\u6e38\u6cf3\u30028\u697c\u7684\u6d77\u666f\u6253\u5f00\u7a97\u6237\u5c31\u662f\u6d77\u3002\u5982\u679c\u60f3\u4f4f\u5728\u70ed\u95f9\u7684\u5730\u5e26\uff0c\u8fd9\u91cc\u4e0d\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\uff0c\u4e0d\u8fc7\u5a01\u6d77\u57ce\u5e02\u771f\u7684\u6bd4\u8f83\u5c0f\uff0c\u6253\u8f66\u8fd8\u662f\u76f8\u5f53\u4fbf\u5b9c\u7684\u3002\u665a\u4e0a\u9152\u5e97\u95e8\u53e3\u51fa\u79df\u8f66\u6bd4\u8f83\u5c11\u3002 1 \u4f4d\u7f6e\u5f88\u597d\uff0c\u8d70\u8def\u5230\u6587\u5e99\u3001\u6e05\u51c9\u5bfa5\u5206\u949f\u90fd\u7528\u4e0d\u4e86\uff0c\u5468\u8fb9\u516c\u4ea4\u8f66\u5f88\u591a\u5f88\u65b9\u4fbf\uff0c\u5c31\u662f\u51fa\u79df\u8f66\u4e0d\u592a\u7231\u53bb\uff08\u8001\u57ce\u533a\u8def\u7a84\u7231\u5835\u8f66\uff09\uff0c\u56e0\u4e3a\u662f\u8001\u5bbe\u9986\u6240\u4ee5\u8bbe\u65bd\u8981\u9648\u65e7\u4e9b\uff0c 1 \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 0 train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u5177\u6709\u611f\u60c5\u8272\u5f69\u7684\u8bc4\u8bba\u6587\u672c; \u7b2c\u4e8c\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u662f\u79ef\u6781\u6216\u8005\u6d88\u6781\u7684\u8bc4\u8bba, 0\u4ee3\u8868\u6d88\u6781, 1\u4ee3\u8868\u79ef\u6781. 3 \u83b7\u53d6\u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u00b6 # \u5bfc\u5165\u5fc5\u5907\u5de5\u5177\u5305 import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # \u601d\u8def\u5206\u6790 : \u83b7\u53d6\u6807\u7b7e\u6570\u91cf\u5206\u5e03 # 0 \u4ec0\u4e48\u6807\u7b7e\u6570\u91cf\u5206\u5e03\uff1a\u6c42\u6807\u7b7e0\u6709\u591a\u5c11\u4e2a \u6807\u7b7e1\u6709\u591a\u5c11\u4e2a \u6807\u7b7e2\u6709\u591a\u5c11\u4e2a # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') # 2 pd.read_csv(path, sep='\\t') \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e # 3 sns.countplot() \u7edf\u8ba1label\u6807\u7b7e\u76840\u30011\u5206\u7ec4\u6570\u91cf # 4 \u753b\u56fe\u5c55\u793a plt.title() plt.show() # \u6ce8\u610f1\uff1asns.countplot()\u76f8\u5f53\u4e8eselect * from tab1 group by def dm_label_sns_countplot (): # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') plt . style . use ( 'fivethirtyeight' ) # 2 pd.read_csv \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) dev_data = pd . read_csv ( filepath_or_buffer = './cn_data/dev.tsv' , sep = ' \\t ' ) # 3 sns.countplot() \u7edf\u8ba1label\u6807\u7b7e\u76840\u30011\u5206\u7ec4\u6570\u91cf sns . countplot ( x = 'label' , data = train_data ) # 4 \u753b\u56fe\u5c55\u793a plt.title() plt.show() plt . title ( 'train_label' ) plt . show () # \u9a8c\u8bc1\u96c6\u4e0a\u6807\u7b7e\u7684\u6570\u91cf\u5206\u5e03 # 3-2 sns.countplot() \u7edf\u8ba1label\u6807\u7b7e\u76840\u30011\u5206\u7ec4\u6570\u91cf sns . countplot ( x = 'label' , data = dev_data ) # 4-2 \u753b\u56fe\u5c55\u793a plt.title() plt.show() plt . title ( 'dev_label' ) plt . show () \u8bad\u7ec3\u96c6\u6807\u7b7e\u6570\u91cf\u5206\u5e03: \u9a8c\u8bc1\u96c6\u6807\u7b7e\u6570\u91cf\u5206\u5e03: \u5206\u6790: \u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u4e2d, \u6211\u4eec\u4e00\u822c\u4f7f\u7528ACC\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807, \u82e5\u60f3\u5c06ACC\u7684\u57fa\u7ebf\u5b9a\u4e49\u572850%\u5de6\u53f3, \u5219\u9700\u8981\u6211\u4eec\u7684\u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u7ef4\u6301\u57281:1\u5de6\u53f3, \u5426\u5219\u5c31\u8981\u8fdb\u884c\u5fc5\u8981\u7684\u6570\u636e\u589e\u5f3a\u6216\u6570\u636e\u5220\u51cf. \u4e0a\u56fe\u4e2d\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6\u6b63\u8d1f\u6837\u672c\u90fd\u7a0d\u6709\u4e0d\u5747\u8861, \u53ef\u4ee5\u8fdb\u884c\u4e00\u4e9b\u6570\u636e\u589e\u5f3a. 4 \u83b7\u53d6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u00b6 # \u601d\u8def\u5206\u6790 : \u83b7\u53d6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03 -\u7ed8\u5236\u53e5\u5b50\u957f\u5ea6\u5206\u5e03-\u67f1\u72b6\u56fe \u53e5\u5b50\u957f\u5ea6\u5206\u5e03-\u5bc6\u5ea6\u66f2\u7ebf\u56fe # 0 \u4ec0\u4e48\u662f\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\uff1a\u6c42\u957f\u5ea6\u4e3a50\u7684\u6709\u591a\u5c11\u4e2a \u957f\u5ea651\u7684\u6709\u591a\u5c11\u4e2a \u957f\u5ea6\u4e3a52\u7684\u6709\u591a\u5c11\u4e2a # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') # 2 pd.read_csv(path, sep='\\t') \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e # 3 \u65b0\u589e\u6570\u636e\u957f\u5ea6\u5217\uff1atrain_data['sentence_length'] = list(map(lambda x:len(x) , ...)) # 4-1 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u67f1\u72b6\u56fe sns.countplot(x='sentence_length', data=train_data) # \u753b\u56fe\u5c55\u793a plt.xticks([]) plt.show() # 4-2 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u66f2\u7ebf\u56fe sns.displot(x='sentence_length', data=train_data) # \u753b\u56fe\u5c55\u793a plt.yticks([]) plt.show() def dm_len_sns_countplot_distplot (): # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') plt . style . use ( 'fivethirtyeight' ) # 2 pd.read_csv \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) dev_data = pd . read_csv ( filepath_or_buffer = './cn_data/dev.tsv' , sep = ' \\t ' ) # 3 \u6c42\u6570\u636e\u957f\u5ea6\u5217 \u7136\u540e\u6c42\u6570\u636e\u957f\u5ea6\u7684\u5206\u5e03 train_data [ 'sentence_length' ] = list ( map ( lambda x : len ( x ), train_data [ 'sentence' ])) # 4 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u67f1\u72b6\u56fe sns . countplot ( x = 'sentence_length' , data = train_data ) # sns.countplot(x=train_data['sentence_length']) plt . xticks ([]) # x\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f # plt.title('sentence_length countplot') plt . show () # 5 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u66f2\u7ebf\u56fe sns . displot ( x = 'sentence_length' , data = train_data ) # sns.displot(x=train_data['sentence_length']) plt . yticks ([]) # y\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f plt . show () # \u9a8c\u8bc1\u96c6 # 3 \u6c42\u6570\u636e\u957f\u5ea6\u5217 \u7136\u540e\u6c42\u6570\u636e\u957f\u5ea6\u7684\u5206\u5e03 dev_data [ 'sentence_length' ] = list ( map ( lambda x : len ( x ), dev_data [ 'sentence' ])) # 4 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u67f1\u72b6\u56fe sns . countplot ( x = 'sentence_length' , data = dev_data ) # sns.countplot(x=dev_data['sentence_length']) plt . xticks ([]) # x\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f # plt.title('sentence_length countplot') plt . show () # 5 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u66f2\u7ebf\u56fe sns . displot ( x = 'sentence_length' , data = dev_data ) # sns.displot(x=dev_data['sentence_length']) plt . yticks ([]) # y\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f plt . show () \u8bad\u7ec3\u96c6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03: \u9a8c\u8bc1\u96c6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03: \u5206\u6790: \u901a\u8fc7\u7ed8\u5236\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\u56fe, \u53ef\u4ee5\u5f97\u77e5\u6211\u4eec\u7684\u8bed\u6599\u4e2d\u5927\u90e8\u5206\u53e5\u5b50\u957f\u5ea6\u7684\u5206\u5e03\u8303\u56f4, \u56e0\u4e3a\u6a21\u578b\u7684\u8f93\u5165\u8981\u6c42\u4e3a\u56fa\u5b9a\u5c3a\u5bf8\u7684\u5f20\u91cf\uff0c\u5408\u7406\u7684\u957f\u5ea6\u8303\u56f4\u5bf9\u4e4b\u540e\u8fdb\u884c\u53e5\u5b50\u622a\u65ad\u8865\u9f50(\u89c4\u8303\u957f\u5ea6)\u8d77\u5230\u5173\u952e\u7684\u6307\u5bfc\u4f5c\u7528. \u4e0a\u56fe\u4e2d\u5927\u90e8\u5206\u53e5\u5b50\u957f\u5ea6\u7684\u8303\u56f4\u5927\u81f4\u4e3a20-250\u4e4b\u95f4. 5 \u83b7\u53d6\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u5206\u5e03 \u00b6 # \u83b7\u53d6\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u5206\u5e03\uff0c\u4e5f\u5c31\u662f\u6309\u7167x\u6b63\u8d1f\u6837\u672c\u8fdb\u884c\u5206\u7ec4 \u518d\u6309\u7167y\u957f\u5ea6\u8fdb\u884c\u6563\u70b9\u56fe # train_data['sentence_length'] = list(map(lambda x: len(x), train_data['sentence'])) # sns.stripplot(y='sentence_length', x='label', data=train_data) def dm03_sns_stripplot (): # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') plt . style . use ( 'fivethirtyeight' ) # 2 pd.read_csv \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) dev_data = pd . read_csv ( filepath_or_buffer = './cn_data/dev.tsv' , sep = ' \\t ' ) # 3 \u6c42\u6570\u636e\u957f\u5ea6\u5217 \u7136\u540e\u6c42\u6570\u636e\u957f\u5ea6\u7684\u5206\u5e03 train_data [ 'sentence_length' ] = list ( map ( lambda x : len ( x ), train_data [ 'sentence' ])) # 4 \u7edf\u8ba1\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u56fe \uff08\u5bf9train_data\u6570\u636e\uff0c\u6309\u7167label\u8fdb\u884c\u5206\u7ec4\uff0c\u7edf\u8ba1\u6b63\u6837\u672c\u6563\u70b9\u56fe\uff09 sns . stripplot ( y = 'sentence_length' , x = 'label' , data = train_data ) plt . show () sns . stripplot ( y = 'sentence_length' , x = 'label' , data = dev_data ) plt . show () \u8bad\u7ec3\u96c6\u4e0a\u6b63\u8d1f\u6837\u672c\u7684\u957f\u5ea6\u6563\u70b9\u5206\u5e03: \u9a8c\u8bc1\u96c6\u4e0a\u6b63\u8d1f\u6837\u672c\u7684\u957f\u5ea6\u6563\u70b9\u5206\u5e03: \u5206\u6790: \u901a\u8fc7\u67e5\u770b\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u56fe, \u53ef\u4ee5\u6709\u6548\u5b9a\u4f4d\u5f02\u5e38\u70b9\u7684\u51fa\u73b0\u4f4d\u7f6e, \u5e2e\u52a9\u6211\u4eec\u66f4\u51c6\u786e\u8fdb\u884c\u4eba\u5de5\u8bed\u6599\u5ba1\u67e5. \u4e0a\u56fe\u4e2d\u5728\u8bad\u7ec3\u96c6\u6b63\u6837\u672c\u4e2d\u51fa\u73b0\u4e86\u5f02\u5e38\u70b9, \u5b83\u7684\u53e5\u5b50\u957f\u5ea6\u8fd13500\u5de6\u53f3, \u9700\u8981\u6211\u4eec\u4eba\u5de5\u5ba1\u67e5. 6 \u83b7\u53d6\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u7edf\u8ba1 \u00b6 # \u5bfc\u5165jieba\u7528\u4e8e\u5206\u8bcd # \u5bfc\u5165chain\u65b9\u6cd5\u7528\u4e8e\u6241\u5e73\u5316\u5217\u8868 import jieba from itertools import chain # \u8fdb\u884c\u8bad\u7ec3\u96c6\u7684\u53e5\u5b50\u8fdb\u884c\u5206\u8bcd, \u5e76\u7edf\u8ba1\u51fa\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570 train_vocab = set ( chain ( * map ( lambda x : jieba . lcut ( x ), train_data [ \"sentence\" ]))) print ( \"\u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a\" , len ( train_vocab )) # \u8fdb\u884c\u9a8c\u8bc1\u96c6\u7684\u53e5\u5b50\u8fdb\u884c\u5206\u8bcd, \u5e76\u7edf\u8ba1\u51fa\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570 valid_vocab = set ( chain ( * map ( lambda x : jieba . lcut ( x ), valid_data [ \"sentence\" ]))) print ( \"\u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a\" , len ( valid_vocab )) \u8f93\u51fa\u6548\u679c: \u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a 12147 \u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a 6857 7 \u83b7\u53d6\u8bad\u7ec3\u96c6\u9ad8\u9891\u5f62\u5bb9\u8bcd\u8bcd\u4e91 \u00b6 # \u4f7f\u7528jieba\u4e2d\u7684\u8bcd\u6027\u6807\u6ce8\u529f\u80fd import jieba.posseg as pseg from wordcloud import WordCloud # \u6bcf\u53e5\u8bdd\u4ea7\u751f\u5f62\u5bb9\u8bcd\u5217\u8868 def get_a_list ( text ): r = [] # \u4f7f\u7528jieba\u7684\u8bcd\u6027\u6807\u6ce8\u65b9\u6cd5\u5207\u5206\u6587\u672c \u627e\u5230\u5f62\u5bb9\u8bcd\u5b58\u5165\u5230\u5217\u8868\u4e2d\u8fd4\u56de for g in pseg . lcut ( text ): if g . flag == \"a\" : r . append ( g . word ) return r # \u6839\u636e\u8bcd\u4e91\u5217\u8868\u4ea7\u751f\u8bcd\u4e91 def get_word_cloud ( keywords_list ): # \u5b9e\u4f8b\u5316\u8bcd\u4e91\u751f\u6210\u5668\u5bf9\u8c61 wordcloud = WordCloud ( font_path = \"./SimHei.ttf\" , max_words = 100 , background_color = 'white' ) # \u51c6\u5907\u6570\u636e keywords_string = \" \" . join ( keywords_list ) # \u4ea7\u751f\u8bcd\u4e91 wordcloud . generate ( keywords_string ) # \u753b\u56fe plt . figure () plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( 'off' ) plt . show () # \u601d\u8def\u5206\u6790 \u8bad\u7ec3\u96c6\u6b63\u6837\u672c\u8bcd\u4e91 \u8bad\u7ec3\u96c6\u8d1f\u6837\u672c\u8bcd\u4e91 # 1 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0a\u6b63\u6837\u672c p_train_data # eg: \u5148\u4f7f\u7528\u903b\u8f91==\u64cd\u4f5c\u68c0\u7d22\u7b26\u5408\u6b63\u6837\u672c train_data[train_data['label'] == 1] # 2 \u83b7\u53d6\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd p_a_train_vocab = chain(*map(a,b)) # 3 \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 def dm_word_cloud (): # 1 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0a\u6b63\u6837\u672cp_train_data # eg: \u5148\u4f7f\u7528\u903b\u8f91==\u64cd\u4f5c\u68c0\u7d22\u7b26\u5408\u6b63\u6837\u672c train_data[train_data['label'] == 1] train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) p_train_data = train_data [ train_data [ 'label' ] == 1 ][ 'sentence' ] # 2 \u83b7\u53d6\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd p_a_train_vocab = chain(*map(a,b)) p_a_train_vocab = chain ( * map ( lambda x : get_a_list ( x ) , p_train_data )) # print(p_a_train_vocab) # print(list(p_a_train_vocab)) # 3 \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 get_word_cloud ( p_a_train_vocab ) print ( '*' * 60 ) # \u8bad\u7ec3\u96c6\u8d1f\u6837\u672c\u8bcd\u4e91 n_train_data = train_data [ train_data [ 'label' ] == 0 ][ 'sentence' ] # 2 \u83b7\u53d6\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd p_a_train_vocab = chain(*map(a,b)) n_a_train_vocab = chain ( * map ( lambda x : get_a_list ( x ) , n_train_data ) ) # print(n_a_dev_vocab) # print(list(n_a_dev_vocab)) # 3 \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 get_word_cloud ( n_a_train_vocab ) \u8bad\u7ec3\u96c6\u6b63\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91: \u8bad\u7ec3\u96c6\u8d1f\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91: 8 \u83b7\u53d6\u9a8c\u8bc1\u96c6\u5f62\u5bb9\u8bcd\u8bcd\u4e91 \u00b6 # \u83b7\u5f97\u9a8c\u8bc1\u96c6\u4e0a\u6b63\u6837\u672c p_valid_data = valid_data [ valid_data [ \"label\" ] == 1 ][ \"sentence\" ] # \u5bf9\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd valid_p_a_vocab = chain ( * map ( lambda x : get_a_list ( x ), p_valid_data )) #print(train_p_n_vocab) # \u83b7\u5f97\u9a8c\u8bc1\u96c6\u4e0a\u8d1f\u6837\u672c n_valid_data = valid_data [ valid_data [ \"label\" ] == 0 ][ \"sentence\" ] # \u83b7\u53d6\u8d1f\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd valid_n_a_vocab = chain ( * map ( lambda x : get_a_list ( x ), n_valid_data )) # \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 get_word_cloud ( valid_p_a_vocab ) get_word_cloud ( valid_n_a_vocab ) \u9a8c\u8bc1\u96c6\u6b63\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91: \u9a8c\u8bc1\u96c6\u8d1f\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91: \u5206\u6790: \u6839\u636e\u9ad8\u9891\u5f62\u5bb9\u8bcd\u8bcd\u4e91\u663e\u793a, \u6211\u4eec\u53ef\u4ee5\u5bf9\u5f53\u524d\u8bed\u6599\u8d28\u91cf\u8fdb\u884c\u7b80\u5355\u8bc4\u4f30, \u540c\u65f6\u5bf9\u8fdd\u53cd\u8bed\u6599\u6807\u7b7e\u542b\u4e49\u7684\u8bcd\u6c47\u8fdb\u884c\u4eba\u5de5\u5ba1\u67e5\u548c\u4fee\u6b63, \u6765\u4fdd\u8bc1\u7edd\u5927\u591a\u6570\u8bed\u6599\u7b26\u5408\u8bad\u7ec3\u6807\u51c6. \u4e0a\u56fe\u4e2d\u7684\u6b63\u6837\u672c\u5927\u591a\u6570\u662f\u8912\u4e49\u8bcd, \u800c\u8d1f\u6837\u672c\u5927\u591a\u6570\u662f\u8d2c\u4e49\u8bcd, \u57fa\u672c\u7b26\u5408\u8981\u6c42, \u4f46\u662f\u8d1f\u6837\u672c\u8bcd\u4e91\u4e2d\u4e5f\u5b58\u5728\"\u4fbf\u5229\"\u8fd9\u6837\u7684\u8912\u4e49\u8bcd, \u56e0\u6b64\u53ef\u4ee5\u4eba\u5de5\u8fdb\u884c\u5ba1\u67e5. 9 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u6587\u672c\u6570\u636e\u5206\u6790\u7684\u4f5c\u7528: \u6587\u672c\u6570\u636e\u5206\u6790\u80fd\u591f\u6709\u6548\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u6570\u636e\u8bed\u6599, \u5feb\u901f\u68c0\u67e5\u51fa\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u7684\u95ee\u9898, \u5e76\u6307\u5bfc\u4e4b\u540e\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e00\u4e9b\u8d85\u53c2\u6570\u7684\u9009\u62e9. \u5b66\u4e60\u4e86\u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5: \u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u8bcd\u9891\u7edf\u8ba1\u4e0e\u5173\u952e\u8bcd\u8bcd\u4e91 \u5b66\u4e60\u4e86\u57fa\u4e8e\u771f\u5b9e\u7684\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u8bed\u6599\u8fdb\u884c\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5. \u83b7\u5f97\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u83b7\u53d6\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u83b7\u53d6\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u5206\u5e03 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u7edf\u8ba1 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0a\u6b63\u8d1f\u7684\u6837\u672c\u7684\u9ad8\u9891\u5f62\u5bb9\u8bcd\u8bcd\u4e91","title":"4 \u6587\u672c\u6570\u636e\u5206\u6790"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#_1","text":"\u4e86\u89e3\u6587\u672c\u6570\u636e\u5206\u6790\u7684\u4f5c\u7528. \u638c\u63e1\u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#1","text":"\u6587\u672c\u6570\u636e\u5206\u6790\u7684\u4f5c\u7528: \u6587\u672c\u6570\u636e\u5206\u6790\u80fd\u591f\u6709\u6548\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u6570\u636e\u8bed\u6599, \u5feb\u901f\u68c0\u67e5\u51fa\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u7684\u95ee\u9898, \u5e76\u6307\u5bfc\u4e4b\u540e\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e00\u4e9b\u8d85\u53c2\u6570\u7684\u9009\u62e9. \u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5: \u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u8bcd\u9891\u7edf\u8ba1\u4e0e\u5173\u952e\u8bcd\u8bcd\u4e91","title":"1 \u6587\u4ef6\u6570\u636e\u5206\u6790\u4ecb\u7ecd"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#2","text":"\u6211\u4eec\u5c06\u57fa\u4e8e\u771f\u5b9e\u7684\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u8bed\u6599\u6765\u8bb2\u89e3\u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5. \u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u8bed\u6599: \u5c5e\u4e8e\u4e8c\u5206\u7c7b\u7684\u4e2d\u6587\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8be5\u8bed\u6599\u5b58\u653e\u5728\"./cn_data\"\u76ee\u5f55\u4e0b. \u5176\u4e2dtrain.tsv\u4ee3\u8868\u8bad\u7ec3\u96c6, dev.tsv\u4ee3\u8868\u9a8c\u8bc1\u96c6, \u4e8c\u8005\u6570\u636e\u6837\u5f0f\u76f8\u540c. train.tsv\u6570\u636e\u6837\u5f0f: sentence label \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d,\u9910\u5385\u4e0d\u5206\u5438\u70df\u533a.\u623f\u95f4\u4e0d\u5206\u6709\u65e0\u70df\u623f. 0 \u53bb\u7684\u65f6\u5019 ,\u9152\u5e97\u5927\u5385\u548c\u9910\u5385\u5728\u88c5\u4fee,\u611f\u89c9\u5927\u5385\u6709\u70b9\u6324.\u7531\u4e8e\u9910\u5385\u88c5\u4fee\u672c\u6765\u8be5\u4eab\u53d7\u7684\u65e9\u996d,\u4e5f\u6ca1\u6709\u4eab\u53d7(\u4ed6\u4eec\u662f8\u70b9\u5f00\u59cb\u6bcf\u4e2a\u623f\u95f4\u9001,\u4f46\u662f\u6211\u65f6\u95f4\u6765\u4e0d\u53ca\u4e86)\u4e0d\u8fc7\u524d\u53f0\u670d\u52a1\u5458\u6001\u5ea6\u597d! 1 \u6709\u5f88\u957f\u65f6\u95f4\u6ca1\u6709\u5728\u897f\u85cf\u5927\u53a6\u4f4f\u4e86\uff0c\u4ee5\u524d\u53bb\u5317\u4eac\u5728\u8fd9\u91cc\u4f4f\u7684\u8f83\u591a\u3002\u8fd9\u6b21\u4f4f\u8fdb\u6765\u53d1\u73b0\u6362\u4e86\u6db2\u6676\u7535\u89c6\uff0c\u4f46\u7f51\u7edc\u4e0d\u662f\u5f88\u597d\uff0c\u4ed6\u4eec\u81ea\u5df1\u8bf4\u662f\u6536\u8d39\u7684\u539f\u56e0\u9020\u6210\u7684\u3002\u5176\u5b83\u8fd8\u597d\u3002 1 \u975e\u5e38\u597d\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u4f4f\u7684\u662f\u8c6a\u534e\u6d77\u666f\u623f\uff0c\u6253\u5f00\u7a97\u6237\u5c31\u53ef\u4ee5\u770b\u89c1\u6808\u6865\u548c\u6d77\u666f\u3002\u8bb0\u5f97\u5f88\u65e9\u4ee5\u524d\u4e5f\u4f4f\u8fc7\uff0c\u73b0\u5728\u91cd\u65b0\u88c5\u4fee\u4e86\u3002\u603b\u7684\u6765\u8bf4\u6bd4\u8f83\u6ee1\u610f\uff0c\u4ee5\u540e\u8fd8\u4f1a\u4f4f 1 \u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u623f\u95f4\u5c0f\u4e86\u4e00\u70b9\uff0c\u4f46\u662f\u5e72\u51c0\u6574\u6d01\uff0c\u5f88\u6709\u9999\u6e2f\u7684\u7279\u8272\uff0c\u6027\u4ef7\u6bd4\u8f83\u9ad8\uff0c\u63a8\u8350\u4e00\u4e0b\u54e6 1 \u9152\u5e97\u7684\u88c5\u4fee\u6bd4\u8f83\u9648\u65e7\uff0c\u623f\u95f4\u7684\u9694\u97f3\uff0c\u4e3b\u8981\u662f\u536b\u751f\u95f4\u7684\u9694\u97f3\u975e\u5e38\u5dee\uff0c\u53ea\u80fd\u7b97\u662f\u4e00\u822c\u7684 0 \u9152\u5e97\u6709\u70b9\u65e7\uff0c\u623f\u95f4\u6bd4\u8f83\u5c0f\uff0c\u4f46\u9152\u5e97\u7684\u4f4d\u5b50\u4e0d\u9519\uff0c\u5c31\u5728\u6d77\u8fb9\uff0c\u53ef\u4ee5\u76f4\u63a5\u53bb\u6e38\u6cf3\u30028\u697c\u7684\u6d77\u666f\u6253\u5f00\u7a97\u6237\u5c31\u662f\u6d77\u3002\u5982\u679c\u60f3\u4f4f\u5728\u70ed\u95f9\u7684\u5730\u5e26\uff0c\u8fd9\u91cc\u4e0d\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\uff0c\u4e0d\u8fc7\u5a01\u6d77\u57ce\u5e02\u771f\u7684\u6bd4\u8f83\u5c0f\uff0c\u6253\u8f66\u8fd8\u662f\u76f8\u5f53\u4fbf\u5b9c\u7684\u3002\u665a\u4e0a\u9152\u5e97\u95e8\u53e3\u51fa\u79df\u8f66\u6bd4\u8f83\u5c11\u3002 1 \u4f4d\u7f6e\u5f88\u597d\uff0c\u8d70\u8def\u5230\u6587\u5e99\u3001\u6e05\u51c9\u5bfa5\u5206\u949f\u90fd\u7528\u4e0d\u4e86\uff0c\u5468\u8fb9\u516c\u4ea4\u8f66\u5f88\u591a\u5f88\u65b9\u4fbf\uff0c\u5c31\u662f\u51fa\u79df\u8f66\u4e0d\u592a\u7231\u53bb\uff08\u8001\u57ce\u533a\u8def\u7a84\u7231\u5835\u8f66\uff09\uff0c\u56e0\u4e3a\u662f\u8001\u5bbe\u9986\u6240\u4ee5\u8bbe\u65bd\u8981\u9648\u65e7\u4e9b\uff0c 1 \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 0 train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u5177\u6709\u611f\u60c5\u8272\u5f69\u7684\u8bc4\u8bba\u6587\u672c; \u7b2c\u4e8c\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u662f\u79ef\u6781\u6216\u8005\u6d88\u6781\u7684\u8bc4\u8bba, 0\u4ee3\u8868\u6d88\u6781, 1\u4ee3\u8868\u79ef\u6781.","title":"2 \u6570\u636e\u96c6\u8bf4\u660e"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#3","text":"# \u5bfc\u5165\u5fc5\u5907\u5de5\u5177\u5305 import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # \u601d\u8def\u5206\u6790 : \u83b7\u53d6\u6807\u7b7e\u6570\u91cf\u5206\u5e03 # 0 \u4ec0\u4e48\u6807\u7b7e\u6570\u91cf\u5206\u5e03\uff1a\u6c42\u6807\u7b7e0\u6709\u591a\u5c11\u4e2a \u6807\u7b7e1\u6709\u591a\u5c11\u4e2a \u6807\u7b7e2\u6709\u591a\u5c11\u4e2a # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') # 2 pd.read_csv(path, sep='\\t') \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e # 3 sns.countplot() \u7edf\u8ba1label\u6807\u7b7e\u76840\u30011\u5206\u7ec4\u6570\u91cf # 4 \u753b\u56fe\u5c55\u793a plt.title() plt.show() # \u6ce8\u610f1\uff1asns.countplot()\u76f8\u5f53\u4e8eselect * from tab1 group by def dm_label_sns_countplot (): # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') plt . style . use ( 'fivethirtyeight' ) # 2 pd.read_csv \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) dev_data = pd . read_csv ( filepath_or_buffer = './cn_data/dev.tsv' , sep = ' \\t ' ) # 3 sns.countplot() \u7edf\u8ba1label\u6807\u7b7e\u76840\u30011\u5206\u7ec4\u6570\u91cf sns . countplot ( x = 'label' , data = train_data ) # 4 \u753b\u56fe\u5c55\u793a plt.title() plt.show() plt . title ( 'train_label' ) plt . show () # \u9a8c\u8bc1\u96c6\u4e0a\u6807\u7b7e\u7684\u6570\u91cf\u5206\u5e03 # 3-2 sns.countplot() \u7edf\u8ba1label\u6807\u7b7e\u76840\u30011\u5206\u7ec4\u6570\u91cf sns . countplot ( x = 'label' , data = dev_data ) # 4-2 \u753b\u56fe\u5c55\u793a plt.title() plt.show() plt . title ( 'dev_label' ) plt . show () \u8bad\u7ec3\u96c6\u6807\u7b7e\u6570\u91cf\u5206\u5e03: \u9a8c\u8bc1\u96c6\u6807\u7b7e\u6570\u91cf\u5206\u5e03: \u5206\u6790: \u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u4e2d, \u6211\u4eec\u4e00\u822c\u4f7f\u7528ACC\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807, \u82e5\u60f3\u5c06ACC\u7684\u57fa\u7ebf\u5b9a\u4e49\u572850%\u5de6\u53f3, \u5219\u9700\u8981\u6211\u4eec\u7684\u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u7ef4\u6301\u57281:1\u5de6\u53f3, \u5426\u5219\u5c31\u8981\u8fdb\u884c\u5fc5\u8981\u7684\u6570\u636e\u589e\u5f3a\u6216\u6570\u636e\u5220\u51cf. \u4e0a\u56fe\u4e2d\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6\u6b63\u8d1f\u6837\u672c\u90fd\u7a0d\u6709\u4e0d\u5747\u8861, \u53ef\u4ee5\u8fdb\u884c\u4e00\u4e9b\u6570\u636e\u589e\u5f3a.","title":"3 \u83b7\u53d6\u6807\u7b7e\u6570\u91cf\u5206\u5e03"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#4","text":"# \u601d\u8def\u5206\u6790 : \u83b7\u53d6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03 -\u7ed8\u5236\u53e5\u5b50\u957f\u5ea6\u5206\u5e03-\u67f1\u72b6\u56fe \u53e5\u5b50\u957f\u5ea6\u5206\u5e03-\u5bc6\u5ea6\u66f2\u7ebf\u56fe # 0 \u4ec0\u4e48\u662f\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\uff1a\u6c42\u957f\u5ea6\u4e3a50\u7684\u6709\u591a\u5c11\u4e2a \u957f\u5ea651\u7684\u6709\u591a\u5c11\u4e2a \u957f\u5ea6\u4e3a52\u7684\u6709\u591a\u5c11\u4e2a # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') # 2 pd.read_csv(path, sep='\\t') \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e # 3 \u65b0\u589e\u6570\u636e\u957f\u5ea6\u5217\uff1atrain_data['sentence_length'] = list(map(lambda x:len(x) , ...)) # 4-1 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u67f1\u72b6\u56fe sns.countplot(x='sentence_length', data=train_data) # \u753b\u56fe\u5c55\u793a plt.xticks([]) plt.show() # 4-2 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u66f2\u7ebf\u56fe sns.displot(x='sentence_length', data=train_data) # \u753b\u56fe\u5c55\u793a plt.yticks([]) plt.show() def dm_len_sns_countplot_distplot (): # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') plt . style . use ( 'fivethirtyeight' ) # 2 pd.read_csv \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) dev_data = pd . read_csv ( filepath_or_buffer = './cn_data/dev.tsv' , sep = ' \\t ' ) # 3 \u6c42\u6570\u636e\u957f\u5ea6\u5217 \u7136\u540e\u6c42\u6570\u636e\u957f\u5ea6\u7684\u5206\u5e03 train_data [ 'sentence_length' ] = list ( map ( lambda x : len ( x ), train_data [ 'sentence' ])) # 4 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u67f1\u72b6\u56fe sns . countplot ( x = 'sentence_length' , data = train_data ) # sns.countplot(x=train_data['sentence_length']) plt . xticks ([]) # x\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f # plt.title('sentence_length countplot') plt . show () # 5 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u66f2\u7ebf\u56fe sns . displot ( x = 'sentence_length' , data = train_data ) # sns.displot(x=train_data['sentence_length']) plt . yticks ([]) # y\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f plt . show () # \u9a8c\u8bc1\u96c6 # 3 \u6c42\u6570\u636e\u957f\u5ea6\u5217 \u7136\u540e\u6c42\u6570\u636e\u957f\u5ea6\u7684\u5206\u5e03 dev_data [ 'sentence_length' ] = list ( map ( lambda x : len ( x ), dev_data [ 'sentence' ])) # 4 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u67f1\u72b6\u56fe sns . countplot ( x = 'sentence_length' , data = dev_data ) # sns.countplot(x=dev_data['sentence_length']) plt . xticks ([]) # x\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f # plt.title('sentence_length countplot') plt . show () # 5 \u7ed8\u5236\u6570\u636e\u957f\u5ea6\u5206\u5e03\u56fe-\u66f2\u7ebf\u56fe sns . displot ( x = 'sentence_length' , data = dev_data ) # sns.displot(x=dev_data['sentence_length']) plt . yticks ([]) # y\u8f74\u4e0a\u4e0d\u8981\u63d0\u793a\u4fe1\u606f plt . show () \u8bad\u7ec3\u96c6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03: \u9a8c\u8bc1\u96c6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03: \u5206\u6790: \u901a\u8fc7\u7ed8\u5236\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\u56fe, \u53ef\u4ee5\u5f97\u77e5\u6211\u4eec\u7684\u8bed\u6599\u4e2d\u5927\u90e8\u5206\u53e5\u5b50\u957f\u5ea6\u7684\u5206\u5e03\u8303\u56f4, \u56e0\u4e3a\u6a21\u578b\u7684\u8f93\u5165\u8981\u6c42\u4e3a\u56fa\u5b9a\u5c3a\u5bf8\u7684\u5f20\u91cf\uff0c\u5408\u7406\u7684\u957f\u5ea6\u8303\u56f4\u5bf9\u4e4b\u540e\u8fdb\u884c\u53e5\u5b50\u622a\u65ad\u8865\u9f50(\u89c4\u8303\u957f\u5ea6)\u8d77\u5230\u5173\u952e\u7684\u6307\u5bfc\u4f5c\u7528. \u4e0a\u56fe\u4e2d\u5927\u90e8\u5206\u53e5\u5b50\u957f\u5ea6\u7684\u8303\u56f4\u5927\u81f4\u4e3a20-250\u4e4b\u95f4.","title":"4 \u83b7\u53d6\u53e5\u5b50\u957f\u5ea6\u5206\u5e03"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#5","text":"# \u83b7\u53d6\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u5206\u5e03\uff0c\u4e5f\u5c31\u662f\u6309\u7167x\u6b63\u8d1f\u6837\u672c\u8fdb\u884c\u5206\u7ec4 \u518d\u6309\u7167y\u957f\u5ea6\u8fdb\u884c\u6563\u70b9\u56fe # train_data['sentence_length'] = list(map(lambda x: len(x), train_data['sentence'])) # sns.stripplot(y='sentence_length', x='label', data=train_data) def dm03_sns_stripplot (): # 1 \u8bbe\u7f6e\u663e\u793a\u98ce\u683cplt.style.use('fivethirtyeight') plt . style . use ( 'fivethirtyeight' ) # 2 pd.read_csv \u8bfb\u8bad\u7ec3\u96c6 \u9a8c\u8bc1\u96c6\u6570\u636e train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) dev_data = pd . read_csv ( filepath_or_buffer = './cn_data/dev.tsv' , sep = ' \\t ' ) # 3 \u6c42\u6570\u636e\u957f\u5ea6\u5217 \u7136\u540e\u6c42\u6570\u636e\u957f\u5ea6\u7684\u5206\u5e03 train_data [ 'sentence_length' ] = list ( map ( lambda x : len ( x ), train_data [ 'sentence' ])) # 4 \u7edf\u8ba1\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u56fe \uff08\u5bf9train_data\u6570\u636e\uff0c\u6309\u7167label\u8fdb\u884c\u5206\u7ec4\uff0c\u7edf\u8ba1\u6b63\u6837\u672c\u6563\u70b9\u56fe\uff09 sns . stripplot ( y = 'sentence_length' , x = 'label' , data = train_data ) plt . show () sns . stripplot ( y = 'sentence_length' , x = 'label' , data = dev_data ) plt . show () \u8bad\u7ec3\u96c6\u4e0a\u6b63\u8d1f\u6837\u672c\u7684\u957f\u5ea6\u6563\u70b9\u5206\u5e03: \u9a8c\u8bc1\u96c6\u4e0a\u6b63\u8d1f\u6837\u672c\u7684\u957f\u5ea6\u6563\u70b9\u5206\u5e03: \u5206\u6790: \u901a\u8fc7\u67e5\u770b\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u56fe, \u53ef\u4ee5\u6709\u6548\u5b9a\u4f4d\u5f02\u5e38\u70b9\u7684\u51fa\u73b0\u4f4d\u7f6e, \u5e2e\u52a9\u6211\u4eec\u66f4\u51c6\u786e\u8fdb\u884c\u4eba\u5de5\u8bed\u6599\u5ba1\u67e5. \u4e0a\u56fe\u4e2d\u5728\u8bad\u7ec3\u96c6\u6b63\u6837\u672c\u4e2d\u51fa\u73b0\u4e86\u5f02\u5e38\u70b9, \u5b83\u7684\u53e5\u5b50\u957f\u5ea6\u8fd13500\u5de6\u53f3, \u9700\u8981\u6211\u4eec\u4eba\u5de5\u5ba1\u67e5.","title":"5 \u83b7\u53d6\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u5206\u5e03"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#6","text":"# \u5bfc\u5165jieba\u7528\u4e8e\u5206\u8bcd # \u5bfc\u5165chain\u65b9\u6cd5\u7528\u4e8e\u6241\u5e73\u5316\u5217\u8868 import jieba from itertools import chain # \u8fdb\u884c\u8bad\u7ec3\u96c6\u7684\u53e5\u5b50\u8fdb\u884c\u5206\u8bcd, \u5e76\u7edf\u8ba1\u51fa\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570 train_vocab = set ( chain ( * map ( lambda x : jieba . lcut ( x ), train_data [ \"sentence\" ]))) print ( \"\u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a\" , len ( train_vocab )) # \u8fdb\u884c\u9a8c\u8bc1\u96c6\u7684\u53e5\u5b50\u8fdb\u884c\u5206\u8bcd, \u5e76\u7edf\u8ba1\u51fa\u4e0d\u540c\u8bcd\u6c47\u7684\u603b\u6570 valid_vocab = set ( chain ( * map ( lambda x : jieba . lcut ( x ), valid_data [ \"sentence\" ]))) print ( \"\u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a\" , len ( valid_vocab )) \u8f93\u51fa\u6548\u679c: \u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a 12147 \u8bad\u7ec3\u96c6\u5171\u5305\u542b\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u4e3a\uff1a 6857","title":"6 \u83b7\u53d6\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u7edf\u8ba1"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#7","text":"# \u4f7f\u7528jieba\u4e2d\u7684\u8bcd\u6027\u6807\u6ce8\u529f\u80fd import jieba.posseg as pseg from wordcloud import WordCloud # \u6bcf\u53e5\u8bdd\u4ea7\u751f\u5f62\u5bb9\u8bcd\u5217\u8868 def get_a_list ( text ): r = [] # \u4f7f\u7528jieba\u7684\u8bcd\u6027\u6807\u6ce8\u65b9\u6cd5\u5207\u5206\u6587\u672c \u627e\u5230\u5f62\u5bb9\u8bcd\u5b58\u5165\u5230\u5217\u8868\u4e2d\u8fd4\u56de for g in pseg . lcut ( text ): if g . flag == \"a\" : r . append ( g . word ) return r # \u6839\u636e\u8bcd\u4e91\u5217\u8868\u4ea7\u751f\u8bcd\u4e91 def get_word_cloud ( keywords_list ): # \u5b9e\u4f8b\u5316\u8bcd\u4e91\u751f\u6210\u5668\u5bf9\u8c61 wordcloud = WordCloud ( font_path = \"./SimHei.ttf\" , max_words = 100 , background_color = 'white' ) # \u51c6\u5907\u6570\u636e keywords_string = \" \" . join ( keywords_list ) # \u4ea7\u751f\u8bcd\u4e91 wordcloud . generate ( keywords_string ) # \u753b\u56fe plt . figure () plt . imshow ( wordcloud , interpolation = \"bilinear\" ) plt . axis ( 'off' ) plt . show () # \u601d\u8def\u5206\u6790 \u8bad\u7ec3\u96c6\u6b63\u6837\u672c\u8bcd\u4e91 \u8bad\u7ec3\u96c6\u8d1f\u6837\u672c\u8bcd\u4e91 # 1 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0a\u6b63\u6837\u672c p_train_data # eg: \u5148\u4f7f\u7528\u903b\u8f91==\u64cd\u4f5c\u68c0\u7d22\u7b26\u5408\u6b63\u6837\u672c train_data[train_data['label'] == 1] # 2 \u83b7\u53d6\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd p_a_train_vocab = chain(*map(a,b)) # 3 \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 def dm_word_cloud (): # 1 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0a\u6b63\u6837\u672cp_train_data # eg: \u5148\u4f7f\u7528\u903b\u8f91==\u64cd\u4f5c\u68c0\u7d22\u7b26\u5408\u6b63\u6837\u672c train_data[train_data['label'] == 1] train_data = pd . read_csv ( filepath_or_buffer = './cn_data/train.tsv' , sep = ' \\t ' ) p_train_data = train_data [ train_data [ 'label' ] == 1 ][ 'sentence' ] # 2 \u83b7\u53d6\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd p_a_train_vocab = chain(*map(a,b)) p_a_train_vocab = chain ( * map ( lambda x : get_a_list ( x ) , p_train_data )) # print(p_a_train_vocab) # print(list(p_a_train_vocab)) # 3 \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 get_word_cloud ( p_a_train_vocab ) print ( '*' * 60 ) # \u8bad\u7ec3\u96c6\u8d1f\u6837\u672c\u8bcd\u4e91 n_train_data = train_data [ train_data [ 'label' ] == 0 ][ 'sentence' ] # 2 \u83b7\u53d6\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd p_a_train_vocab = chain(*map(a,b)) n_a_train_vocab = chain ( * map ( lambda x : get_a_list ( x ) , n_train_data ) ) # print(n_a_dev_vocab) # print(list(n_a_dev_vocab)) # 3 \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 get_word_cloud ( n_a_train_vocab ) \u8bad\u7ec3\u96c6\u6b63\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91: \u8bad\u7ec3\u96c6\u8d1f\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91:","title":"7 \u83b7\u53d6\u8bad\u7ec3\u96c6\u9ad8\u9891\u5f62\u5bb9\u8bcd\u8bcd\u4e91"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#8","text":"# \u83b7\u5f97\u9a8c\u8bc1\u96c6\u4e0a\u6b63\u6837\u672c p_valid_data = valid_data [ valid_data [ \"label\" ] == 1 ][ \"sentence\" ] # \u5bf9\u6b63\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd valid_p_a_vocab = chain ( * map ( lambda x : get_a_list ( x ), p_valid_data )) #print(train_p_n_vocab) # \u83b7\u5f97\u9a8c\u8bc1\u96c6\u4e0a\u8d1f\u6837\u672c n_valid_data = valid_data [ valid_data [ \"label\" ] == 0 ][ \"sentence\" ] # \u83b7\u53d6\u8d1f\u6837\u672c\u7684\u6bcf\u4e2a\u53e5\u5b50\u7684\u5f62\u5bb9\u8bcd valid_n_a_vocab = chain ( * map ( lambda x : get_a_list ( x ), n_valid_data )) # \u8c03\u7528\u7ed8\u5236\u8bcd\u4e91\u51fd\u6570 get_word_cloud ( valid_p_a_vocab ) get_word_cloud ( valid_n_a_vocab ) \u9a8c\u8bc1\u96c6\u6b63\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91: \u9a8c\u8bc1\u96c6\u8d1f\u6837\u672c\u5f62\u5bb9\u8bcd\u8bcd\u4e91: \u5206\u6790: \u6839\u636e\u9ad8\u9891\u5f62\u5bb9\u8bcd\u8bcd\u4e91\u663e\u793a, \u6211\u4eec\u53ef\u4ee5\u5bf9\u5f53\u524d\u8bed\u6599\u8d28\u91cf\u8fdb\u884c\u7b80\u5355\u8bc4\u4f30, \u540c\u65f6\u5bf9\u8fdd\u53cd\u8bed\u6599\u6807\u7b7e\u542b\u4e49\u7684\u8bcd\u6c47\u8fdb\u884c\u4eba\u5de5\u5ba1\u67e5\u548c\u4fee\u6b63, \u6765\u4fdd\u8bc1\u7edd\u5927\u591a\u6570\u8bed\u6599\u7b26\u5408\u8bad\u7ec3\u6807\u51c6. \u4e0a\u56fe\u4e2d\u7684\u6b63\u6837\u672c\u5927\u591a\u6570\u662f\u8912\u4e49\u8bcd, \u800c\u8d1f\u6837\u672c\u5927\u591a\u6570\u662f\u8d2c\u4e49\u8bcd, \u57fa\u672c\u7b26\u5408\u8981\u6c42, \u4f46\u662f\u8d1f\u6837\u672c\u8bcd\u4e91\u4e2d\u4e5f\u5b58\u5728\"\u4fbf\u5229\"\u8fd9\u6837\u7684\u8912\u4e49\u8bcd, \u56e0\u6b64\u53ef\u4ee5\u4eba\u5de5\u8fdb\u884c\u5ba1\u67e5.","title":"8 \u83b7\u53d6\u9a8c\u8bc1\u96c6\u5f62\u5bb9\u8bcd\u8bcd\u4e91"},{"location":"02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html#9","text":"\u5b66\u4e60\u4e86\u6587\u672c\u6570\u636e\u5206\u6790\u7684\u4f5c\u7528: \u6587\u672c\u6570\u636e\u5206\u6790\u80fd\u591f\u6709\u6548\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u6570\u636e\u8bed\u6599, \u5feb\u901f\u68c0\u67e5\u51fa\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u7684\u95ee\u9898, \u5e76\u6307\u5bfc\u4e4b\u540e\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e00\u4e9b\u8d85\u53c2\u6570\u7684\u9009\u62e9. \u5b66\u4e60\u4e86\u5e38\u7528\u7684\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5: \u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u8bcd\u9891\u7edf\u8ba1\u4e0e\u5173\u952e\u8bcd\u8bcd\u4e91 \u5b66\u4e60\u4e86\u57fa\u4e8e\u771f\u5b9e\u7684\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u8bed\u6599\u8fdb\u884c\u51e0\u79cd\u6587\u672c\u6570\u636e\u5206\u6790\u65b9\u6cd5. \u83b7\u5f97\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u6807\u7b7e\u6570\u91cf\u5206\u5e03 \u83b7\u53d6\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u53e5\u5b50\u957f\u5ea6\u5206\u5e03 \u83b7\u53d6\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u957f\u5ea6\u6563\u70b9\u5206\u5e03 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u4e0d\u540c\u8bcd\u6c47\u603b\u6570\u7edf\u8ba1 \u83b7\u5f97\u8bad\u7ec3\u96c6\u4e0a\u6b63\u8d1f\u7684\u6837\u672c\u7684\u9ad8\u9891\u5f62\u5bb9\u8bcd\u8bcd\u4e91","title":"9 \u5c0f\u7ed3"},{"location":"02_mkdocs_preprocess/5%20%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u4f5c\u7528. \u638c\u63e1\u5b9e\u73b0\u5e38\u89c1\u7684\u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u5177\u4f53\u65b9\u6cd5. \u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u4f5c\u7528: \u6587\u672c\u7279\u5f81\u5904\u7406\u5305\u62ec\u4e3a\u8bed\u6599\u6dfb\u52a0\u5177\u6709\u666e\u9002\u6027\u7684\u6587\u672c\u7279\u5f81, \u5982:n-gram\u7279\u5f81, \u4ee5\u53ca\u5bf9\u52a0\u5165\u7279\u5f81\u4e4b\u540e\u7684\u6587\u672c\u8bed\u6599\u8fdb\u884c\u5fc5\u8981\u7684\u5904\u7406, \u5982: \u957f\u5ea6\u89c4\u8303. \u8fd9\u4e9b\u7279\u5f81\u5904\u7406\u5de5\u4f5c\u80fd\u591f\u6709\u6548\u7684\u5c06\u91cd\u8981\u7684\u6587\u672c\u7279\u5f81\u52a0\u5165\u6a21\u578b\u8bad\u7ec3\u4e2d, \u589e\u5f3a\u6a21\u578b\u8bc4\u4f30\u6307\u6807. \u5e38\u89c1\u7684\u6587\u672c\u7279\u5f81\u5904\u7406\u65b9\u6cd5: \u6dfb\u52a0n-gram\u7279\u5f81 \u6587\u672c\u957f\u5ea6\u89c4\u8303 1 \u4ec0\u4e48\u662fn-gram\u7279\u5f81 \u00b6 \u7ed9\u5b9a\u4e00\u6bb5\u6587\u672c\u5e8f\u5217, \u5176\u4e2dn\u4e2a\u8bcd\u6216\u5b57\u7684\u76f8\u90bb\u5171\u73b0\u7279\u5f81\u5373n-gram\u7279\u5f81, \u5e38\u7528\u7684n-gram\u7279\u5f81\u662fbi-gram\u548ctri-gram\u7279\u5f81, \u5206\u522b\u5bf9\u5e94n\u4e3a2\u548c3. \u4e3e\u4e2a\u4f8b\u5b50: \u5047\u8bbe\u7ed9\u5b9a\u5206\u8bcd\u5217\u8868: [\"\u662f\u8c01\", \"\u6572\u52a8\", \"\u6211\u5fc3\"] \u5bf9\u5e94\u7684\u6570\u503c\u6620\u5c04\u5217\u8868\u4e3a: [1, 34, 21] \u6211\u4eec\u53ef\u4ee5\u8ba4\u4e3a\u6570\u503c\u6620\u5c04\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u6570\u5b57\u662f\u8bcd\u6c47\u7279\u5f81. \u9664\u6b64\u4e4b\u5916, \u6211\u4eec\u8fd8\u53ef\u4ee5\u628a\"\u662f\u8c01\"\u548c\"\u6572\u52a8\"\u4e24\u4e2a\u8bcd\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb\u4e5f\u4f5c\u4e3a\u4e00\u79cd\u7279\u5f81\u52a0\u5165\u5230\u5e8f\u5217\u5217\u8868\u4e2d, \u5047\u8bbe1000\u5c31\u4ee3\u8868\"\u662f\u8c01\"\u548c\"\u6572\u52a8\"\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb \u6b64\u65f6\u6570\u503c\u6620\u5c04\u5217\u8868\u5c31\u53d8\u6210\u4e86\u5305\u542b2-gram\u7279\u5f81\u7684\u7279\u5f81\u5217\u8868: [1, 34, 21, 1000] \u8fd9\u91cc\u7684\"\u662f\u8c01\"\u548c\"\u6572\u52a8\"\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb\u5c31\u662fbi-gram\u7279\u5f81\u4e2d\u7684\u4e00\u4e2a. \"\u6572\u52a8\"\u548c\"\u6211\u5fc3\"\u4e5f\u662f\u5171\u73b0\u4e14\u76f8\u90bb\u7684\u4e24\u4e2a\u8bcd\u6c47, \u56e0\u6b64\u5b83\u4eec\u4e5f\u662fbi-gram\u7279\u5f81. \u5047\u8bbe1001\u4ee3\u8868\"\u6572\u52a8\"\u548c\"\u6211\u5fc3\"\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb \u90a3\u4e48, \u6700\u540e\u539f\u59cb\u7684\u6570\u503c\u6620\u5c04\u5217\u8868 [1, 34, 21] \u6dfb\u52a0\u4e86bi-gram\u7279\u5f81\u4e4b\u540e\u5c31\u53d8\u6210\u4e86 [1, 34, 21, 1000, 1001] \u63d0\u53d6n-gram\u7279\u5f81: # \u4e00\u822cn-gram\u4e2d\u7684n\u53d62\u6216\u80053, \u8fd9\u91cc\u53d62\u4e3a\u4f8b ngram_range = 2 def create_ngram_set ( input_list ): \"\"\" description: \u4ece\u6570\u503c\u5217\u8868\u4e2d\u63d0\u53d6\u6240\u6709\u7684n-gram\u7279\u5f81 :param input_list: \u8f93\u5165\u7684\u6570\u503c\u5217\u8868, \u53ef\u4ee5\u770b\u4f5c\u662f\u8bcd\u6c47\u6620\u5c04\u540e\u7684\u5217\u8868, \u91cc\u9762\u6bcf\u4e2a\u6570\u5b57\u7684\u53d6\u503c\u8303\u56f4\u4e3a[1, 25000] :return: n-gram\u7279\u5f81\u7ec4\u6210\u7684\u96c6\u5408 eg: >>> create_ngram_set([1, 3, 2, 1, 5, 3]) {(3, 2), (1, 3), (2, 1), (1, 5), (5, 3)} \"\"\" return set ( zip ( * [ input_list [ i :] for i in range ( ngram_range )])) \u8c03\u7528: input_list = [1, 3, 2, 1, 5, 3] res = create_ngram_set(input_list) print(res) \u8f93\u51fa\u6548\u679c: # \u8be5\u8f93\u5165\u5217\u8868\u7684\u6240\u6709bi-gram\u7279\u5f81 {(3, 2), (1, 3), (2, 1), (1, 5), (5, 3)} 2 \u6587\u672c\u957f\u5ea6\u89c4\u8303\u53ca\u5176\u4f5c\u7528 \u00b6 \u4e00\u822c\u6a21\u578b\u7684\u8f93\u5165\u9700\u8981\u7b49\u5c3a\u5bf8\u5927\u5c0f\u7684\u77e9\u9635, \u56e0\u6b64\u5728\u8fdb\u5165\u6a21\u578b\u524d\u9700\u8981\u5bf9\u6bcf\u6761\u6587\u672c\u6570\u503c\u6620\u5c04\u540e\u7684\u957f\u5ea6\u8fdb\u884c\u89c4\u8303, \u6b64\u65f6\u5c06\u6839\u636e\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\u5206\u6790\u51fa\u8986\u76d6\u7edd\u5927\u591a\u6570\u6587\u672c\u7684\u5408\u7406\u957f\u5ea6, \u5bf9\u8d85\u957f\u6587\u672c\u8fdb\u884c\u622a\u65ad, \u5bf9\u4e0d\u8db3\u6587\u672c\u8fdb\u884c\u8865\u9f50(\u4e00\u822c\u4f7f\u7528\u6570\u5b570), \u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f\u6587\u672c\u957f\u5ea6\u89c4\u8303. \u6587\u672c\u957f\u5ea6\u89c4\u8303\u7684\u5b9e\u73b0: from tensorflow.keras.preprocessing import sequence # cutlen\u6839\u636e\u6570\u636e\u5206\u6790\u4e2d\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\uff0c\u8986\u76d690%\u5de6\u53f3\u8bed\u6599\u7684\u6700\u77ed\u957f\u5ea6. # \u8fd9\u91cc\u5047\u5b9acutlen\u4e3a10 cutlen = 10 def padding ( x_train ): \"\"\" description: \u5bf9\u8f93\u5165\u6587\u672c\u5f20\u91cf\u8fdb\u884c\u957f\u5ea6\u89c4\u8303 :param x_train: \u6587\u672c\u7684\u5f20\u91cf\u8868\u793a, \u5f62\u5982: [[1, 32, 32, 61], [2, 54, 21, 7, 19]] :return: \u8fdb\u884c\u622a\u65ad\u8865\u9f50\u540e\u7684\u6587\u672c\u5f20\u91cf\u8868\u793a \"\"\" # \u4f7f\u7528sequence.pad_sequences\u5373\u53ef\u5b8c\u6210 return sequence . pad_sequences ( x_train , cutlen ) \u8c03\u7528: # \u5047\u5b9ax_train\u91cc\u9762\u6709\u4e24\u6761\u6587\u672c, \u4e00\u6761\u957f\u5ea6\u5927\u4e8e10, \u4e00\u5929\u5c0f\u4e8e10 x_train = [[ 1 , 23 , 5 , 32 , 55 , 63 , 2 , 21 , 78 , 32 , 23 , 1 ], [ 2 , 32 , 1 , 23 , 1 ]] res = padding ( x_train ) print ( res ) \u8f93\u51fa\u6548\u679c: [[ 5 32 55 63 2 21 78 32 23 1] [ 0 0 0 0 0 2 32 1 23 1]] 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u4f5c\u7528: \u6587\u672c\u7279\u5f81\u5904\u7406\u5305\u62ec\u4e3a\u8bed\u6599\u6dfb\u52a0\u5177\u6709\u666e\u9002\u6027\u7684\u6587\u672c\u7279\u5f81, \u5982:n-gram\u7279\u5f81, \u4ee5\u53ca\u5bf9\u52a0\u5165\u7279\u5f81\u4e4b\u540e\u7684\u6587\u672c\u8bed\u6599\u8fdb\u884c\u5fc5\u8981\u7684\u5904\u7406, \u5982: \u957f\u5ea6\u89c4\u8303. \u8fd9\u4e9b\u7279\u5f81\u5904\u7406\u5de5\u4f5c\u80fd\u591f\u6709\u6548\u7684\u5c06\u91cd\u8981\u7684\u6587\u672c\u7279\u5f81\u52a0\u5165\u6a21\u578b\u8bad\u7ec3\u4e2d, \u589e\u5f3a\u6a21\u578b\u8bc4\u4f30\u6307\u6807. \u5b66\u4e60\u4e86\u5e38\u89c1\u7684\u6587\u672c\u7279\u5f81\u5904\u7406\u65b9\u6cd5: \u6dfb\u52a0n-gram\u7279\u5f81 \u6587\u672c\u957f\u5ea6\u89c4\u8303 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fn-gram\u7279\u5f81: \u7ed9\u5b9a\u4e00\u6bb5\u6587\u672c\u5e8f\u5217, \u5176\u4e2dn\u4e2a\u8bcd\u6216\u5b57\u7684\u76f8\u90bb\u5171\u73b0\u7279\u5f81\u5373n-gram\u7279\u5f81, \u5e38\u7528\u7684n-gram\u7279\u5f81\u662fbi-gram\u548ctri-gram\u7279\u5f81, \u5206\u522b\u5bf9\u5e94n\u4e3a2\u548c3. \u5b66\u4e60\u4e86\u63d0\u53d6n-gram\u7279\u5f81\u7684\u51fd\u6570: create_ngram_set \u5b66\u4e60\u4e86\u6587\u672c\u957f\u5ea6\u89c4\u8303\u53ca\u5176\u4f5c\u7528: \u4e00\u822c\u6a21\u578b\u7684\u8f93\u5165\u9700\u8981\u7b49\u5c3a\u5bf8\u5927\u5c0f\u7684\u77e9\u9635, \u56e0\u6b64\u5728\u8fdb\u5165\u6a21\u578b\u524d\u9700\u8981\u5bf9\u6bcf\u6761\u6587\u672c\u6570\u503c\u6620\u5c04\u540e\u7684\u957f\u5ea6\u8fdb\u884c\u89c4\u8303, \u6b64\u65f6\u5c06\u6839\u636e\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\u5206\u6790\u51fa\u8986\u76d6\u7edd\u5927\u591a\u6570\u6587\u672c\u7684\u5408\u7406\u957f\u5ea6, \u5bf9\u8d85\u957f\u6587\u672c\u8fdb\u884c\u622a\u65ad, \u5bf9\u4e0d\u8db3\u6587\u672c\u8fdb\u884c\u8865\u9f50(\u4e00\u822c\u4f7f\u7528\u6570\u5b570), \u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f\u6587\u672c\u957f\u5ea6\u89c4\u8303. \u5b66\u4e60\u4e86\u6587\u672c\u957f\u5ea6\u89c4\u8303\u7684\u5b9e\u73b0\u51fd\u6570: padding","title":"5 \u6587\u672c\u7279\u5f81\u5904\u7406"},{"location":"02_mkdocs_preprocess/5%20%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86.html#_1","text":"\u4e86\u89e3\u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u4f5c\u7528. \u638c\u63e1\u5b9e\u73b0\u5e38\u89c1\u7684\u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u5177\u4f53\u65b9\u6cd5. \u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u4f5c\u7528: \u6587\u672c\u7279\u5f81\u5904\u7406\u5305\u62ec\u4e3a\u8bed\u6599\u6dfb\u52a0\u5177\u6709\u666e\u9002\u6027\u7684\u6587\u672c\u7279\u5f81, \u5982:n-gram\u7279\u5f81, \u4ee5\u53ca\u5bf9\u52a0\u5165\u7279\u5f81\u4e4b\u540e\u7684\u6587\u672c\u8bed\u6599\u8fdb\u884c\u5fc5\u8981\u7684\u5904\u7406, \u5982: \u957f\u5ea6\u89c4\u8303. \u8fd9\u4e9b\u7279\u5f81\u5904\u7406\u5de5\u4f5c\u80fd\u591f\u6709\u6548\u7684\u5c06\u91cd\u8981\u7684\u6587\u672c\u7279\u5f81\u52a0\u5165\u6a21\u578b\u8bad\u7ec3\u4e2d, \u589e\u5f3a\u6a21\u578b\u8bc4\u4f30\u6307\u6807. \u5e38\u89c1\u7684\u6587\u672c\u7279\u5f81\u5904\u7406\u65b9\u6cd5: \u6dfb\u52a0n-gram\u7279\u5f81 \u6587\u672c\u957f\u5ea6\u89c4\u8303","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"02_mkdocs_preprocess/5%20%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86.html#1-n-gram","text":"\u7ed9\u5b9a\u4e00\u6bb5\u6587\u672c\u5e8f\u5217, \u5176\u4e2dn\u4e2a\u8bcd\u6216\u5b57\u7684\u76f8\u90bb\u5171\u73b0\u7279\u5f81\u5373n-gram\u7279\u5f81, \u5e38\u7528\u7684n-gram\u7279\u5f81\u662fbi-gram\u548ctri-gram\u7279\u5f81, \u5206\u522b\u5bf9\u5e94n\u4e3a2\u548c3. \u4e3e\u4e2a\u4f8b\u5b50: \u5047\u8bbe\u7ed9\u5b9a\u5206\u8bcd\u5217\u8868: [\"\u662f\u8c01\", \"\u6572\u52a8\", \"\u6211\u5fc3\"] \u5bf9\u5e94\u7684\u6570\u503c\u6620\u5c04\u5217\u8868\u4e3a: [1, 34, 21] \u6211\u4eec\u53ef\u4ee5\u8ba4\u4e3a\u6570\u503c\u6620\u5c04\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u6570\u5b57\u662f\u8bcd\u6c47\u7279\u5f81. \u9664\u6b64\u4e4b\u5916, \u6211\u4eec\u8fd8\u53ef\u4ee5\u628a\"\u662f\u8c01\"\u548c\"\u6572\u52a8\"\u4e24\u4e2a\u8bcd\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb\u4e5f\u4f5c\u4e3a\u4e00\u79cd\u7279\u5f81\u52a0\u5165\u5230\u5e8f\u5217\u5217\u8868\u4e2d, \u5047\u8bbe1000\u5c31\u4ee3\u8868\"\u662f\u8c01\"\u548c\"\u6572\u52a8\"\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb \u6b64\u65f6\u6570\u503c\u6620\u5c04\u5217\u8868\u5c31\u53d8\u6210\u4e86\u5305\u542b2-gram\u7279\u5f81\u7684\u7279\u5f81\u5217\u8868: [1, 34, 21, 1000] \u8fd9\u91cc\u7684\"\u662f\u8c01\"\u548c\"\u6572\u52a8\"\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb\u5c31\u662fbi-gram\u7279\u5f81\u4e2d\u7684\u4e00\u4e2a. \"\u6572\u52a8\"\u548c\"\u6211\u5fc3\"\u4e5f\u662f\u5171\u73b0\u4e14\u76f8\u90bb\u7684\u4e24\u4e2a\u8bcd\u6c47, \u56e0\u6b64\u5b83\u4eec\u4e5f\u662fbi-gram\u7279\u5f81. \u5047\u8bbe1001\u4ee3\u8868\"\u6572\u52a8\"\u548c\"\u6211\u5fc3\"\u5171\u540c\u51fa\u73b0\u4e14\u76f8\u90bb \u90a3\u4e48, \u6700\u540e\u539f\u59cb\u7684\u6570\u503c\u6620\u5c04\u5217\u8868 [1, 34, 21] \u6dfb\u52a0\u4e86bi-gram\u7279\u5f81\u4e4b\u540e\u5c31\u53d8\u6210\u4e86 [1, 34, 21, 1000, 1001] \u63d0\u53d6n-gram\u7279\u5f81: # \u4e00\u822cn-gram\u4e2d\u7684n\u53d62\u6216\u80053, \u8fd9\u91cc\u53d62\u4e3a\u4f8b ngram_range = 2 def create_ngram_set ( input_list ): \"\"\" description: \u4ece\u6570\u503c\u5217\u8868\u4e2d\u63d0\u53d6\u6240\u6709\u7684n-gram\u7279\u5f81 :param input_list: \u8f93\u5165\u7684\u6570\u503c\u5217\u8868, \u53ef\u4ee5\u770b\u4f5c\u662f\u8bcd\u6c47\u6620\u5c04\u540e\u7684\u5217\u8868, \u91cc\u9762\u6bcf\u4e2a\u6570\u5b57\u7684\u53d6\u503c\u8303\u56f4\u4e3a[1, 25000] :return: n-gram\u7279\u5f81\u7ec4\u6210\u7684\u96c6\u5408 eg: >>> create_ngram_set([1, 3, 2, 1, 5, 3]) {(3, 2), (1, 3), (2, 1), (1, 5), (5, 3)} \"\"\" return set ( zip ( * [ input_list [ i :] for i in range ( ngram_range )])) \u8c03\u7528: input_list = [1, 3, 2, 1, 5, 3] res = create_ngram_set(input_list) print(res) \u8f93\u51fa\u6548\u679c: # \u8be5\u8f93\u5165\u5217\u8868\u7684\u6240\u6709bi-gram\u7279\u5f81 {(3, 2), (1, 3), (2, 1), (1, 5), (5, 3)}","title":"1 \u4ec0\u4e48\u662fn-gram\u7279\u5f81"},{"location":"02_mkdocs_preprocess/5%20%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86.html#2","text":"\u4e00\u822c\u6a21\u578b\u7684\u8f93\u5165\u9700\u8981\u7b49\u5c3a\u5bf8\u5927\u5c0f\u7684\u77e9\u9635, \u56e0\u6b64\u5728\u8fdb\u5165\u6a21\u578b\u524d\u9700\u8981\u5bf9\u6bcf\u6761\u6587\u672c\u6570\u503c\u6620\u5c04\u540e\u7684\u957f\u5ea6\u8fdb\u884c\u89c4\u8303, \u6b64\u65f6\u5c06\u6839\u636e\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\u5206\u6790\u51fa\u8986\u76d6\u7edd\u5927\u591a\u6570\u6587\u672c\u7684\u5408\u7406\u957f\u5ea6, \u5bf9\u8d85\u957f\u6587\u672c\u8fdb\u884c\u622a\u65ad, \u5bf9\u4e0d\u8db3\u6587\u672c\u8fdb\u884c\u8865\u9f50(\u4e00\u822c\u4f7f\u7528\u6570\u5b570), \u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f\u6587\u672c\u957f\u5ea6\u89c4\u8303. \u6587\u672c\u957f\u5ea6\u89c4\u8303\u7684\u5b9e\u73b0: from tensorflow.keras.preprocessing import sequence # cutlen\u6839\u636e\u6570\u636e\u5206\u6790\u4e2d\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\uff0c\u8986\u76d690%\u5de6\u53f3\u8bed\u6599\u7684\u6700\u77ed\u957f\u5ea6. # \u8fd9\u91cc\u5047\u5b9acutlen\u4e3a10 cutlen = 10 def padding ( x_train ): \"\"\" description: \u5bf9\u8f93\u5165\u6587\u672c\u5f20\u91cf\u8fdb\u884c\u957f\u5ea6\u89c4\u8303 :param x_train: \u6587\u672c\u7684\u5f20\u91cf\u8868\u793a, \u5f62\u5982: [[1, 32, 32, 61], [2, 54, 21, 7, 19]] :return: \u8fdb\u884c\u622a\u65ad\u8865\u9f50\u540e\u7684\u6587\u672c\u5f20\u91cf\u8868\u793a \"\"\" # \u4f7f\u7528sequence.pad_sequences\u5373\u53ef\u5b8c\u6210 return sequence . pad_sequences ( x_train , cutlen ) \u8c03\u7528: # \u5047\u5b9ax_train\u91cc\u9762\u6709\u4e24\u6761\u6587\u672c, \u4e00\u6761\u957f\u5ea6\u5927\u4e8e10, \u4e00\u5929\u5c0f\u4e8e10 x_train = [[ 1 , 23 , 5 , 32 , 55 , 63 , 2 , 21 , 78 , 32 , 23 , 1 ], [ 2 , 32 , 1 , 23 , 1 ]] res = padding ( x_train ) print ( res ) \u8f93\u51fa\u6548\u679c: [[ 5 32 55 63 2 21 78 32 23 1] [ 0 0 0 0 0 2 32 1 23 1]]","title":"2 \u6587\u672c\u957f\u5ea6\u89c4\u8303\u53ca\u5176\u4f5c\u7528"},{"location":"02_mkdocs_preprocess/5%20%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86.html#3","text":"\u5b66\u4e60\u4e86\u6587\u672c\u7279\u5f81\u5904\u7406\u7684\u4f5c\u7528: \u6587\u672c\u7279\u5f81\u5904\u7406\u5305\u62ec\u4e3a\u8bed\u6599\u6dfb\u52a0\u5177\u6709\u666e\u9002\u6027\u7684\u6587\u672c\u7279\u5f81, \u5982:n-gram\u7279\u5f81, \u4ee5\u53ca\u5bf9\u52a0\u5165\u7279\u5f81\u4e4b\u540e\u7684\u6587\u672c\u8bed\u6599\u8fdb\u884c\u5fc5\u8981\u7684\u5904\u7406, \u5982: \u957f\u5ea6\u89c4\u8303. \u8fd9\u4e9b\u7279\u5f81\u5904\u7406\u5de5\u4f5c\u80fd\u591f\u6709\u6548\u7684\u5c06\u91cd\u8981\u7684\u6587\u672c\u7279\u5f81\u52a0\u5165\u6a21\u578b\u8bad\u7ec3\u4e2d, \u589e\u5f3a\u6a21\u578b\u8bc4\u4f30\u6307\u6807. \u5b66\u4e60\u4e86\u5e38\u89c1\u7684\u6587\u672c\u7279\u5f81\u5904\u7406\u65b9\u6cd5: \u6dfb\u52a0n-gram\u7279\u5f81 \u6587\u672c\u957f\u5ea6\u89c4\u8303 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fn-gram\u7279\u5f81: \u7ed9\u5b9a\u4e00\u6bb5\u6587\u672c\u5e8f\u5217, \u5176\u4e2dn\u4e2a\u8bcd\u6216\u5b57\u7684\u76f8\u90bb\u5171\u73b0\u7279\u5f81\u5373n-gram\u7279\u5f81, \u5e38\u7528\u7684n-gram\u7279\u5f81\u662fbi-gram\u548ctri-gram\u7279\u5f81, \u5206\u522b\u5bf9\u5e94n\u4e3a2\u548c3. \u5b66\u4e60\u4e86\u63d0\u53d6n-gram\u7279\u5f81\u7684\u51fd\u6570: create_ngram_set \u5b66\u4e60\u4e86\u6587\u672c\u957f\u5ea6\u89c4\u8303\u53ca\u5176\u4f5c\u7528: \u4e00\u822c\u6a21\u578b\u7684\u8f93\u5165\u9700\u8981\u7b49\u5c3a\u5bf8\u5927\u5c0f\u7684\u77e9\u9635, \u56e0\u6b64\u5728\u8fdb\u5165\u6a21\u578b\u524d\u9700\u8981\u5bf9\u6bcf\u6761\u6587\u672c\u6570\u503c\u6620\u5c04\u540e\u7684\u957f\u5ea6\u8fdb\u884c\u89c4\u8303, \u6b64\u65f6\u5c06\u6839\u636e\u53e5\u5b50\u957f\u5ea6\u5206\u5e03\u5206\u6790\u51fa\u8986\u76d6\u7edd\u5927\u591a\u6570\u6587\u672c\u7684\u5408\u7406\u957f\u5ea6, \u5bf9\u8d85\u957f\u6587\u672c\u8fdb\u884c\u622a\u65ad, \u5bf9\u4e0d\u8db3\u6587\u672c\u8fdb\u884c\u8865\u9f50(\u4e00\u822c\u4f7f\u7528\u6570\u5b570), \u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u662f\u6587\u672c\u957f\u5ea6\u89c4\u8303. \u5b66\u4e60\u4e86\u6587\u672c\u957f\u5ea6\u89c4\u8303\u7684\u5b9e\u73b0\u51fd\u6570: padding","title":"3 \u5c0f\u7ed3"},{"location":"02_mkdocs_preprocess/6%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6587\u672c\u6570\u636e\u589e\u5f3a\u7684\u4f5c\u7528. \u638c\u63e1\u5b9e\u73b0\u5e38\u89c1\u7684\u6587\u672c\u6570\u636e\u589e\u5f3a\u7684\u5177\u4f53\u65b9\u6cd5. \u5e38\u89c1\u7684\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5: \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5 1 \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5 \u00b6 \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u76ee\u524d\u662f\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u9762\u6548\u679c\u8f83\u597d\u7684\u589e\u5f3a\u65b9\u6cd5, \u4e00\u822c\u57fa\u4e8egoogle\u3001\u6709\u9053\u7b49\u7ffb\u8bd1\u63a5\u53e3, \u5c06\u6587\u672c\u6570\u636e\u7ffb\u8bd1\u6210\u53e6\u5916\u4e00\u79cd\u8bed\u8a00(\u4e00\u822c\u9009\u62e9\u5c0f\u8bed\u79cd),\u4e4b\u540e\u518d\u7ffb\u8bd1\u56de\u539f\u8bed\u8a00, \u5373\u53ef\u8ba4\u4e3a\u5f97\u5230\u4e0e\u4e0e\u539f\u8bed\u6599\u540c\u6807\u7b7e\u7684\u65b0\u8bed\u6599, \u65b0\u8bed\u6599\u52a0\u5165\u5230\u539f\u6570\u636e\u96c6\u4e2d\u5373\u53ef\u8ba4\u4e3a\u662f\u5bf9\u539f\u6570\u636e\u96c6\u6570\u636e\u589e\u5f3a. \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u4f18\u52bf: \u64cd\u4f5c\u7b80\u4fbf, \u83b7\u5f97\u65b0\u8bed\u6599\u8d28\u91cf\u9ad8. \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b58\u5728\u7684\u95ee\u9898: \u5728\u77ed\u6587\u672c\u56de\u8bd1\u8fc7\u7a0b\u4e2d, \u65b0\u8bed\u6599\u4e0e\u539f\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u5f88\u9ad8\u7684\u91cd\u590d\u7387, \u5e76\u4e0d\u80fd\u6709\u6548\u589e\u5927\u6837\u672c\u7684\u7279\u5f81\u7a7a\u95f4. \u9ad8\u91cd\u590d\u7387\u89e3\u51b3\u529e\u6cd5: \u8fdb\u884c\u8fde\u7eed\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1, \u5982: \u4e2d\u6587\u2192\u97e9\u6587\u2192\u65e5\u8bed\u2192\u82f1\u6587\u2192\u4e2d\u6587, \u6839\u636e\u7ecf\u9a8c, \u6700\u591a\u53ea\u91c7\u75283\u6b21\u8fde\u7eed\u7ffb\u8bd1, \u66f4\u591a\u7684\u7ffb\u8bd1\u6b21\u6570\u5c06\u4ea7\u751f\u6548\u7387\u4f4e\u4e0b, \u8bed\u4e49\u5931\u771f\u7b49\u95ee\u9898. \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\uff08\u57fa\u4e8e\u6709\u9053\u7ffb\u8bd1\u63a5\u53e3\uff09: # \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 import requests # \u601d\u8def\u5206\u6790 # 1 \u5b9a\u4e49\u9700\u8981\u8bbf\u95ee\u7684\u6709\u9053\u7ffb\u8bd1API\u63a5\u53e3--url # 2 \u5b9a\u4e49\u9700\u8981\u7ffb\u8bd1\u7684\u6587\u672c\uff1atext # 3 \u5b9a\u4e49data\u6570\u636e\uff1afrom\u4ee3\u8868\u539f\u59cb\u8bed\u8a00, to\u4ee3\u8868\u76ee\u6807\u8bed\u8a00, i\u4ee3\u8868\u9700\u8981\u7ffb\u8bd1\u7684\u6587\u672c, doctype:\u6587\u672c\u7684\u7c7b\u578b # 4 requests.post(url=url, params=data)\u5373\u4ee3\u8868\u8bbf\u95eeapi\u63a5\u53e3\u7684\u65b9\u6cd5 def dm_translate (): url = 'http://fanyi.youdao.com/translate' # \u7b2c\u4e00\u6b21\u7ffb\u8bd1\uff0c\u76ee\u6807\u8bed\u8a00\u82f1\u6587 text1 = '\u8fd9\u4e2a\u4ef7\u683c\u975e\u5e38\u4fbf\u5b9c' data1 = { 'from' : 'zh-CHS' , 'to' : 'en' , 'i' : text1 , 'doctype' : 'json' } response1 = requests . post ( url = url , params = data1 ) res1 = response1 . json () # \u6253\u5370\u7b2c\u4e00\u6b21\u7ffb\u8bd1\u7ed3\u679c print ( res1 ) # \u7b2c\u4e8c\u6b21\u7ffb\u8bd1\uff0c \u76ee\u6807\u8bed\u8a00\u4e2d\u6587 text2 = 'The price is very cheap' data2 = { 'from' : 'en' , 'to' : 'zh-CHS' , 'i' : text2 , 'doctype' : 'json' } response2 = requests . post ( url = url , params = data2 ) res2 = response2 . json () # \u6253\u5370\u7b2c\u4e8c\u6b21\u7ffb\u8bd1\u7ed3\u679c print ( res2 ) \u8f93\u51fa\u7ed3\u679c\u5c55\u793a: \u7b2c\u4e00\u6b21\u7ffb\u8bd1\u7ed3\u679c\uff1a{'type': 'ZH_CN2EN', 'errorCode': 0, 'elapsedTime': 1, 'translateResult': [[{'src': '\u8fd9\u4e2a\u4ef7\u683c\u975e\u5e38\u4fbf\u5b9c', 'tgt': 'The price is very cheap'}]]} \u7b2c\u4e8c\u6b21\u7ffb\u8bd1\u7ed3\u679c\uff1a{'type': 'EN2ZH_CN', 'errorCode': 0, 'elapsedTime': 1, 'translateResult': [[{'src': 'The price is very cheap', 'tgt': '\u4ef7\u683c\u975e\u5e38\u4fbf\u5b9c'}]]} \u8bed\u8a00\u53ca\u5176\u5bf9\u5e94\u7f16\u7801\uff1a 'AUTO': '\u81ea\u52a8\u68c0\u6d4b\u8bed\u8a00' 'zh-CHS': '\u4e2d\u6587', 'en': '\u82f1\u6587' 'ja': '\u65e5\u8bed' 'ko': '\u97e9\u8bed' 'fr': '\u6cd5\u8bed' 'de': '\u5fb7\u8bed' 2 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u5e38\u89c1\u7684\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5: \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5: \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u76ee\u524d\u662f\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u9762\u6548\u679c\u8f83\u597d\u7684\u589e\u5f3a\u65b9\u6cd5, \u4e00\u822c\u57fa\u4e8egoogle\u3001\u6709\u9053\u7b49\u7ffb\u8bd1\u63a5\u53e3, \u5c06\u6587\u672c\u6570\u636e\u7ffb\u8bd1\u6210\u53e6\u5916\u4e00\u79cd\u8bed\u8a00(\u4e00\u822c\u9009\u62e9\u5c0f\u8bed\u79cd),\u4e4b\u540e\u518d\u7ffb\u8bd1\u56de\u539f\u8bed\u8a00, \u5373\u53ef\u8ba4\u4e3a\u5f97\u5230\u4e0e\u4e0e\u539f\u8bed\u6599\u540c\u6807\u7b7e\u7684\u65b0\u8bed\u6599, \u65b0\u8bed\u6599\u52a0\u5165\u5230\u539f\u6570\u636e\u96c6\u4e2d\u5373\u53ef\u8ba4\u4e3a\u662f\u5bf9\u539f\u6570\u636e\u96c6\u6570\u636e\u589e\u5f3a. \u5b66\u4e60\u4e86\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u4f18\u52bf: \u64cd\u4f5c\u7b80\u4fbf, \u83b7\u5f97\u65b0\u8bed\u6599\u8d28\u91cf\u9ad8. \u5b66\u4e60\u4e86\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b58\u5728\u7684\u95ee\u9898: \u5728\u77ed\u6587\u672c\u56de\u8bd1\u8fc7\u7a0b\u4e2d, \u65b0\u8bed\u6599\u4e0e\u539f\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u5f88\u9ad8\u7684\u91cd\u590d\u7387, \u5e76\u4e0d\u80fd\u6709\u6548\u589e\u5927\u6837\u672c\u7684\u7279\u5f81\u7a7a\u95f4. \u5b66\u4e60\u4e86\u9ad8\u91cd\u590d\u7387\u89e3\u51b3\u529e\u6cd5: \u8fdb\u884c\u8fde\u7eed\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1, \u5982: \u4e2d\u6587\u2192\u97e9\u6587\u2192\u65e5\u8bed\u2192\u82f1\u6587\u2192\u4e2d\u6587, \u6839\u636e\u7ecf\u9a8c, \u6700\u591a\u53ea\u91c7\u75283\u6b21\u8fde\u7eed\u7ffb\u8bd1, \u66f4\u591a\u7684\u7ffb\u8bd1\u6b21\u6570\u5c06\u4ea7\u751f\u6548\u7387\u4f4e\u4e0b, \u8bed\u4e49\u5931\u771f\u7b49\u95ee\u9898. \u5b66\u4e60\u4e86\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b9e\u73b0.","title":"6 \u6587\u672c\u6570\u636e\u589e\u5f3a"},{"location":"02_mkdocs_preprocess/6%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.html#_1","text":"\u4e86\u89e3\u6587\u672c\u6570\u636e\u589e\u5f3a\u7684\u4f5c\u7528. \u638c\u63e1\u5b9e\u73b0\u5e38\u89c1\u7684\u6587\u672c\u6570\u636e\u589e\u5f3a\u7684\u5177\u4f53\u65b9\u6cd5. \u5e38\u89c1\u7684\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5: \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"02_mkdocs_preprocess/6%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.html#1","text":"\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u76ee\u524d\u662f\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u9762\u6548\u679c\u8f83\u597d\u7684\u589e\u5f3a\u65b9\u6cd5, \u4e00\u822c\u57fa\u4e8egoogle\u3001\u6709\u9053\u7b49\u7ffb\u8bd1\u63a5\u53e3, \u5c06\u6587\u672c\u6570\u636e\u7ffb\u8bd1\u6210\u53e6\u5916\u4e00\u79cd\u8bed\u8a00(\u4e00\u822c\u9009\u62e9\u5c0f\u8bed\u79cd),\u4e4b\u540e\u518d\u7ffb\u8bd1\u56de\u539f\u8bed\u8a00, \u5373\u53ef\u8ba4\u4e3a\u5f97\u5230\u4e0e\u4e0e\u539f\u8bed\u6599\u540c\u6807\u7b7e\u7684\u65b0\u8bed\u6599, \u65b0\u8bed\u6599\u52a0\u5165\u5230\u539f\u6570\u636e\u96c6\u4e2d\u5373\u53ef\u8ba4\u4e3a\u662f\u5bf9\u539f\u6570\u636e\u96c6\u6570\u636e\u589e\u5f3a. \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u4f18\u52bf: \u64cd\u4f5c\u7b80\u4fbf, \u83b7\u5f97\u65b0\u8bed\u6599\u8d28\u91cf\u9ad8. \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b58\u5728\u7684\u95ee\u9898: \u5728\u77ed\u6587\u672c\u56de\u8bd1\u8fc7\u7a0b\u4e2d, \u65b0\u8bed\u6599\u4e0e\u539f\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u5f88\u9ad8\u7684\u91cd\u590d\u7387, \u5e76\u4e0d\u80fd\u6709\u6548\u589e\u5927\u6837\u672c\u7684\u7279\u5f81\u7a7a\u95f4. \u9ad8\u91cd\u590d\u7387\u89e3\u51b3\u529e\u6cd5: \u8fdb\u884c\u8fde\u7eed\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1, \u5982: \u4e2d\u6587\u2192\u97e9\u6587\u2192\u65e5\u8bed\u2192\u82f1\u6587\u2192\u4e2d\u6587, \u6839\u636e\u7ecf\u9a8c, \u6700\u591a\u53ea\u91c7\u75283\u6b21\u8fde\u7eed\u7ffb\u8bd1, \u66f4\u591a\u7684\u7ffb\u8bd1\u6b21\u6570\u5c06\u4ea7\u751f\u6548\u7387\u4f4e\u4e0b, \u8bed\u4e49\u5931\u771f\u7b49\u95ee\u9898. \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\uff08\u57fa\u4e8e\u6709\u9053\u7ffb\u8bd1\u63a5\u53e3\uff09: # \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 import requests # \u601d\u8def\u5206\u6790 # 1 \u5b9a\u4e49\u9700\u8981\u8bbf\u95ee\u7684\u6709\u9053\u7ffb\u8bd1API\u63a5\u53e3--url # 2 \u5b9a\u4e49\u9700\u8981\u7ffb\u8bd1\u7684\u6587\u672c\uff1atext # 3 \u5b9a\u4e49data\u6570\u636e\uff1afrom\u4ee3\u8868\u539f\u59cb\u8bed\u8a00, to\u4ee3\u8868\u76ee\u6807\u8bed\u8a00, i\u4ee3\u8868\u9700\u8981\u7ffb\u8bd1\u7684\u6587\u672c, doctype:\u6587\u672c\u7684\u7c7b\u578b # 4 requests.post(url=url, params=data)\u5373\u4ee3\u8868\u8bbf\u95eeapi\u63a5\u53e3\u7684\u65b9\u6cd5 def dm_translate (): url = 'http://fanyi.youdao.com/translate' # \u7b2c\u4e00\u6b21\u7ffb\u8bd1\uff0c\u76ee\u6807\u8bed\u8a00\u82f1\u6587 text1 = '\u8fd9\u4e2a\u4ef7\u683c\u975e\u5e38\u4fbf\u5b9c' data1 = { 'from' : 'zh-CHS' , 'to' : 'en' , 'i' : text1 , 'doctype' : 'json' } response1 = requests . post ( url = url , params = data1 ) res1 = response1 . json () # \u6253\u5370\u7b2c\u4e00\u6b21\u7ffb\u8bd1\u7ed3\u679c print ( res1 ) # \u7b2c\u4e8c\u6b21\u7ffb\u8bd1\uff0c \u76ee\u6807\u8bed\u8a00\u4e2d\u6587 text2 = 'The price is very cheap' data2 = { 'from' : 'en' , 'to' : 'zh-CHS' , 'i' : text2 , 'doctype' : 'json' } response2 = requests . post ( url = url , params = data2 ) res2 = response2 . json () # \u6253\u5370\u7b2c\u4e8c\u6b21\u7ffb\u8bd1\u7ed3\u679c print ( res2 ) \u8f93\u51fa\u7ed3\u679c\u5c55\u793a: \u7b2c\u4e00\u6b21\u7ffb\u8bd1\u7ed3\u679c\uff1a{'type': 'ZH_CN2EN', 'errorCode': 0, 'elapsedTime': 1, 'translateResult': [[{'src': '\u8fd9\u4e2a\u4ef7\u683c\u975e\u5e38\u4fbf\u5b9c', 'tgt': 'The price is very cheap'}]]} \u7b2c\u4e8c\u6b21\u7ffb\u8bd1\u7ed3\u679c\uff1a{'type': 'EN2ZH_CN', 'errorCode': 0, 'elapsedTime': 1, 'translateResult': [[{'src': 'The price is very cheap', 'tgt': '\u4ef7\u683c\u975e\u5e38\u4fbf\u5b9c'}]]} \u8bed\u8a00\u53ca\u5176\u5bf9\u5e94\u7f16\u7801\uff1a 'AUTO': '\u81ea\u52a8\u68c0\u6d4b\u8bed\u8a00' 'zh-CHS': '\u4e2d\u6587', 'en': '\u82f1\u6587' 'ja': '\u65e5\u8bed' 'ko': '\u97e9\u8bed' 'fr': '\u6cd5\u8bed' 'de': '\u5fb7\u8bed'","title":"1 \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5"},{"location":"02_mkdocs_preprocess/6%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.html#2","text":"\u5b66\u4e60\u4e86\u5e38\u89c1\u7684\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5: \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u6cd5: \u56de\u8bd1\u6570\u636e\u589e\u5f3a\u76ee\u524d\u662f\u6587\u672c\u6570\u636e\u589e\u5f3a\u65b9\u9762\u6548\u679c\u8f83\u597d\u7684\u589e\u5f3a\u65b9\u6cd5, \u4e00\u822c\u57fa\u4e8egoogle\u3001\u6709\u9053\u7b49\u7ffb\u8bd1\u63a5\u53e3, \u5c06\u6587\u672c\u6570\u636e\u7ffb\u8bd1\u6210\u53e6\u5916\u4e00\u79cd\u8bed\u8a00(\u4e00\u822c\u9009\u62e9\u5c0f\u8bed\u79cd),\u4e4b\u540e\u518d\u7ffb\u8bd1\u56de\u539f\u8bed\u8a00, \u5373\u53ef\u8ba4\u4e3a\u5f97\u5230\u4e0e\u4e0e\u539f\u8bed\u6599\u540c\u6807\u7b7e\u7684\u65b0\u8bed\u6599, \u65b0\u8bed\u6599\u52a0\u5165\u5230\u539f\u6570\u636e\u96c6\u4e2d\u5373\u53ef\u8ba4\u4e3a\u662f\u5bf9\u539f\u6570\u636e\u96c6\u6570\u636e\u589e\u5f3a. \u5b66\u4e60\u4e86\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u4f18\u52bf: \u64cd\u4f5c\u7b80\u4fbf, \u83b7\u5f97\u65b0\u8bed\u6599\u8d28\u91cf\u9ad8. \u5b66\u4e60\u4e86\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b58\u5728\u7684\u95ee\u9898: \u5728\u77ed\u6587\u672c\u56de\u8bd1\u8fc7\u7a0b\u4e2d, \u65b0\u8bed\u6599\u4e0e\u539f\u8bed\u6599\u53ef\u80fd\u5b58\u5728\u5f88\u9ad8\u7684\u91cd\u590d\u7387, \u5e76\u4e0d\u80fd\u6709\u6548\u589e\u5927\u6837\u672c\u7684\u7279\u5f81\u7a7a\u95f4. \u5b66\u4e60\u4e86\u9ad8\u91cd\u590d\u7387\u89e3\u51b3\u529e\u6cd5: \u8fdb\u884c\u8fde\u7eed\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1, \u5982: \u4e2d\u6587\u2192\u97e9\u6587\u2192\u65e5\u8bed\u2192\u82f1\u6587\u2192\u4e2d\u6587, \u6839\u636e\u7ecf\u9a8c, \u6700\u591a\u53ea\u91c7\u75283\u6b21\u8fde\u7eed\u7ffb\u8bd1, \u66f4\u591a\u7684\u7ffb\u8bd1\u6b21\u6570\u5c06\u4ea7\u751f\u6548\u7387\u4f4e\u4e0b, \u8bed\u4e49\u5931\u771f\u7b49\u95ee\u9898. \u5b66\u4e60\u4e86\u56de\u8bd1\u6570\u636e\u589e\u5f3a\u5b9e\u73b0.","title":"2 \u5c0f\u7ed3"},{"location":"02_mkdocs_preprocess/7%20jieba%E8%AF%8D%E6%80%A7%E5%AF%B9%E7%85%A7%E8%A1%A8.html","text":"jieba\u8bcd\u6027\u5bf9\u7167\u8868: - a \u5f62\u5bb9\u8bcd - ad \u526f\u5f62\u8bcd - ag \u5f62\u5bb9\u8bcd\u6027\u8bed\u7d20 - an \u540d\u5f62\u8bcd - b \u533a\u522b\u8bcd - c \u8fde\u8bcd - d \u526f\u8bcd - df - dg \u526f\u8bed\u7d20 - e \u53f9\u8bcd - f \u65b9\u4f4d\u8bcd - g \u8bed\u7d20 - h \u524d\u63a5\u6210\u5206 - i \u6210\u8bed - j \u7b80\u79f0\u7565\u79f0 - k \u540e\u63a5\u6210\u5206 - l \u4e60\u7528\u8bed - m \u6570\u8bcd - mg - mq \u6570\u91cf\u8bcd - n \u540d\u8bcd - ng \u540d\u8bcd\u6027\u8bed\u7d20 - nr \u4eba\u540d - nrfg - nrt - ns \u5730\u540d - nt \u673a\u6784\u56e2\u4f53\u540d - nz \u5176\u4ed6\u4e13\u540d - o \u62df\u58f0\u8bcd - p \u4ecb\u8bcd - q \u91cf\u8bcd - r \u4ee3\u8bcd - rg \u4ee3\u8bcd\u6027\u8bed\u7d20 - rr \u4eba\u79f0\u4ee3\u8bcd - rz \u6307\u793a\u4ee3\u8bcd - s \u5904\u6240\u8bcd - t \u65f6\u95f4\u8bcd - tg \u65f6\u8bed\u7d20 - u \u52a9\u8bcd - ud \u7ed3\u6784\u52a9\u8bcd \u5f97 - ug \u65f6\u6001\u52a9\u8bcd - uj \u7ed3\u6784\u52a9\u8bcd \u7684 - ul \u65f6\u6001\u52a9\u8bcd \u4e86 - uv \u7ed3\u6784\u52a9\u8bcd \u5730 - uz \u65f6\u6001\u52a9\u8bcd \u7740 - v \u52a8\u8bcd - vd \u526f\u52a8\u8bcd - vg \u52a8\u8bcd\u6027\u8bed\u7d20 - vi \u4e0d\u53ca\u7269\u52a8\u8bcd - vn \u540d\u52a8\u8bcd - vq - x \u975e\u8bed\u7d20\u8bcd - y \u8bed\u6c14\u8bcd - z \u72b6\u6001\u8bcd - zg hanlp\u8bcd\u6027\u5bf9\u7167\u8868: \u3010Proper Noun\u2014\u2014NR\uff0c\u4e13\u6709\u540d\u8bcd\u3011 \u3010Temporal Noun\u2014\u2014NT\uff0c\u65f6\u95f4\u540d\u8bcd\u3011 \u3010Localizer\u2014\u2014LC\uff0c\u5b9a\u4f4d\u8bcd\u3011\u5982\u201c\u5185\u201d\uff0c\u201c\u5de6\u53f3\u201d \u3010Pronoun\u2014\u2014PN\uff0c\u4ee3\u8bcd\u3011 \u3010Determiner\u2014\u2014DT\uff0c\u9650\u5b9a\u8bcd\u3011\u5982\u201c\u8fd9\u201d\uff0c\u201c\u5168\u4f53\u201d \u3010Cardinal Number\u2014\u2014CD\uff0c\u91cf\u8bcd\u3011 \u3010Ordinal Number\u2014\u2014OD\uff0c\u6b21\u5e8f\u8bcd\u3011\u5982\u201c\u7b2c\u4e09\u5341\u4e00\u201d \u3010Measure word\u2014\u2014M\uff0c\u5355\u4f4d\u8bcd\u3011\u5982\u201c\u676f\u201d \u3010Verb\uff1aVA\uff0cVC\uff0cVE\uff0cVV\uff0c\u52a8\u8bcd\u3011 \u3010Adverb\uff1aAD\uff0c\u526f\u8bcd\u3011\u5982\u201c\u8fd1\u201d\uff0c\u201c\u6781\u5927\u201d \u3010Preposition\uff1aP\uff0c\u4ecb\u8bcd\u3011\u5982\u201c\u968f\u7740\u201d \u3010Subordinating conjunctions\uff1aCS\uff0c\u4ece\u5c5e\u8fde\u8bcd\u3011 \u3010Conjuctions\uff1aCC\uff0c\u8fde\u8bcd\u3011\u5982\u201c\u548c\u201d \u3010Particle\uff1aDEC,DEG,DEV,DER,AS,SP,ETC,MSP\uff0c\u5c0f\u54c1\u8bcd\u3011\u5982\u201c\u7684\u8bdd\u201d \u3010Interjections\uff1aIJ\uff0c\u611f\u53f9\u8bcd\u3011\u5982\u201c\u54c8\u201d \u3010onomatopoeia\uff1aON\uff0c\u62df\u58f0\u8bcd\u3011\u5982\u201c\u54d7\u5566\u5566\u201d \u3010Other Noun-modifier\uff1aJJ\u3011\u5982\u201c\u53d1\u7a3f/JJ \u65f6\u95f4/NN\u201d \u3010Punctuation\uff1aPU\uff0c\u6807\u70b9\u7b26\u53f7\u3011 \u3010Foreign word\uff1aFW\uff0c\u5916\u56fd\u8bcd\u8bed\u3011\u5982\u201cOK","title":"7 jieba\u8bcd\u6027\u5bf9\u7167\u8868"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6709\u5173\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u548c\u6709\u5173\u6570\u636e. \u638c\u63e1\u4f7f\u7528\u6d45\u5c42\u7f51\u7edc\u6784\u5efa\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u5668\u7684\u5b9e\u73b0\u8fc7\u7a0b. 1 \u6848\u4f8b\u8bf4\u660e \u00b6 \u5173\u4e8e\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1: \u4ee5\u4e00\u6bb5\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u6587\u672c\u63cf\u8ff0\u5185\u5bb9\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u5c5e\u4e8e\u54ea\u4e00\u79cd\u7c7b\u578b\u7684\u65b0\u95fb, \u8fd9\u662f\u5178\u578b\u7684\u6587\u672c\u5206\u7c7b\u95ee\u9898, \u6211\u4eec\u8fd9\u91cc\u5047\u5b9a\u6bcf\u79cd\u7c7b\u578b\u662f\u4e92\u65a5\u7684, \u5373\u6587\u672c\u63cf\u8ff0\u6709\u4e14\u53ea\u6709\u4e00\u79cd\u7c7b\u578b. \u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u6570\u636e: \u6570\u636e\u6587\u4ef6\u9884\u89c8: # \u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/ag_news_csv\u4e0b - data/ - ag_news_csv/ classes.txt readme.txt test.csv train.csv \u6587\u4ef6\u8bf4\u660e: train.csv\u8868\u793a\u8bad\u7ec3\u6570\u636e, \u517112\u4e07\u6761\u6570\u636e; test.csv\u8868\u793a\u9a8c\u8bc1\u6570\u636e, \u51717600\u6761\u6570\u636e; classes.txt\u662f\u6807\u7b7e(\u65b0\u95fb\u4e3b\u9898)\u542b\u4e49\u6587\u4ef6, \u91cc\u9762\u6709\u56db\u4e2a\u5355\u8bcd'World', 'Sports', 'Business', 'Sci/Tech'\u4ee3\u8868\u65b0\u95fb\u7684\u56db\u4e2a\u4e3b\u9898, readme.txt\u662f\u8be5\u6570\u636e\u96c6\u7684\u82f1\u6587\u8bf4\u660e. train.csv\u9884\u89c8: \"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\" \"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\" \"3\",\"Oil and Economy Cloud Stocks' Outlook (Reuters)\",\"Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\" \"3\",\"Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)\",\"Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\" \"3\",\"Oil prices soar to all-time record, posing new menace to US economy (AFP)\",\"AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\" \"3\",\"Stocks End Up, But Near Year Lows (Reuters)\",\"Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\" \"3\",\"Money Funds Fell in Latest Week (AP)\",\"AP - Assets of the nation's retail money market mutual funds fell by #36;1.17 billion in the latest week to #36;849.98 trillion, the Investment Company Institute said Thursday.\" \"3\",\"Fed minutes show dissent over inflation (USATODAY.com)\",\"USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.\" \"3\",\"Safety Net (Forbes.com)\",\"Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"\"buying insurance was the furthest thing from my mind,\"\" says Riley.\" \"3\",\"Wall St. Bears Claw Back Into the Black\",\" NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again.\" \u6587\u4ef6\u5185\u5bb9\u8bf4\u660e: train.csv\u5171\u75313\u5217\u7ec4\u6210, \u4f7f\u7528','\u8fdb\u884c\u5206\u9694, \u5206\u522b\u4ee3\u8868: \u6807\u7b7e, \u65b0\u95fb\u6807\u9898, \u65b0\u95fb\u7b80\u8ff0; \u5176\u4e2d\u6807\u7b7e\u7528\"1\", \"2\", \"3\", \"4\"\u8868\u793a, \u4f9d\u6b21\u5bf9\u5e94classes\u4e2d\u7684\u5185\u5bb9. test.csv\u4e0etrain.csv\u5185\u5bb9\u683c\u5f0f\u4e0e\u542b\u4e49\u76f8\u540c. \u4ece\u672c\u5730\u8fdb\u884c\u6570\u636e\u7684\u52a0\u8f7d\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b from torchtext.legacy.datasets.text_classification import _csv_iterator , _create_data_from_iterator , TextClassificationDataset from torchtext.utils import extract_archive from torchtext.vocab import build_vocab_from_iterator , Vocab # \u4ece\u672c\u5730\u52a0\u8f7d\u6570\u636e\u7684\u65b9\u5f0f\uff0c\u672c\u5730\u6570\u636e\u5728\u865a\u62df\u673a/root/data/ag_news_csv\u4e2d # \u5b9a\u4e49\u52a0\u8f7d\u51fd\u6570 def setup_datasets ( ngrams = 2 , vocab_train = None , vocab_test = None , include_unk = False ): train_csv_path = 'data/ag_news_csv/train.csv' test_csv_path = 'data/ag_news_csv/test.csv' if vocab_train is None : vocab_train = build_vocab_from_iterator ( _csv_iterator ( train_csv_path , ngrams )) else : if not isinstance ( vocab , Vocab ): raise TypeError ( \"Passed vocabulary is not of type Vocab\" ) if vocab_test is None : vocab_test = build_vocab_from_iterator ( _csv_iterator ( test_csv_path , ngrams )) else : if not isinstance ( vocab , Vocab ): raise TypeError ( \"Passed vocabulary is not of type Vocab\" ) train_data , train_labels = _create_data_from_iterator ( vocab_train , _csv_iterator ( train_csv_path , ngrams , yield_cls = True ), include_unk ) test_data , test_labels = _create_data_from_iterator ( vocab_test , _csv_iterator ( test_csv_path , ngrams , yield_cls = True ), include_unk ) if len ( train_labels ^ test_labels ) > 0 : raise ValueError ( \"Training and test labels don't match\" ) return ( TextClassificationDataset ( vocab_train , train_data , train_labels ), TextClassificationDataset ( vocab_test , test_data , test_labels )) # \u8c03\u7528\u51fd\u6570, \u52a0\u8f7d\u672c\u5730\u6570\u636e train_dataset , test_dataset = setup_datasets () print ( \"train_dataset\" , train_dataset ) 2 \u6848\u4f8b\u5b9e\u73b0 \u00b6 \u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4 \u7b2c\u4e00\u6b65: \u6784\u5efa\u5e26\u6709Embedding\u5c42\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b. \u7b2c\u4e8c\u6b65: \u5bf9\u6570\u636e\u8fdb\u884cbatch\u5904\u7406. \u7b2c\u4e09\u6b65: \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u51fd\u6570. \u7b2c\u56db\u6b65: \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1. \u7b2c\u4e94\u6b65: \u67e5\u770bembedding\u5c42\u5d4c\u5165\u7684\u8bcd\u5411\u91cf. 2.1 \u6784\u5efa\u5e26\u6709Embedding\u5c42\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b \u00b6 # \u5bfc\u5165\u5fc5\u5907\u7684torch\u6a21\u578b\u6784\u5efa\u5de5\u5177 import torch.nn as nn import torch.nn.functional as F # \u6307\u5b9aBATCH_SIZE\u7684\u5927\u5c0f BATCH_SIZE = 16 # \u8fdb\u884c\u53ef\u7528\u8bbe\u5907\u68c0\u6d4b, \u6709GPU\u7684\u8bdd\u5c06\u4f18\u5148\u4f7f\u7528GPU device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) class TextSentiment ( nn . Module ): \"\"\"\u6587\u672c\u5206\u7c7b\u6a21\u578b\"\"\" def __init__ ( self , vocab_size , embed_dim , num_class ): \"\"\" description: \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570 :param vocab_size: \u6574\u4e2a\u8bed\u6599\u5305\u542b\u7684\u4e0d\u540c\u8bcd\u6c47\u603b\u6570 :param embed_dim: \u6307\u5b9a\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6 :param num_class: \u6587\u672c\u5206\u7c7b\u7684\u7c7b\u522b\u603b\u6570 \"\"\" super () . __init__ () # \u5b9e\u4f8b\u5316embedding\u5c42, sparse=True\u4ee3\u8868\u6bcf\u6b21\u5bf9\u8be5\u5c42\u6c42\u89e3\u68af\u5ea6\u65f6, \u53ea\u66f4\u65b0\u90e8\u5206\u6743\u91cd. self . embedding = nn . Embedding ( vocab_size , embed_dim , sparse = True ) # \u5b9e\u4f8b\u5316\u7ebf\u6027\u5c42, \u53c2\u6570\u5206\u522b\u662fembed_dim\u548cnum_class. self . fc = nn . Linear ( embed_dim , num_class ) # \u4e3a\u5404\u5c42\u521d\u59cb\u5316\u6743\u91cd self . init_weights () def init_weights ( self ): \"\"\"\u521d\u59cb\u5316\u6743\u91cd\u51fd\u6570\"\"\" # \u6307\u5b9a\u521d\u59cb\u6743\u91cd\u7684\u53d6\u503c\u8303\u56f4\u6570 initrange = 0.5 # \u5404\u5c42\u7684\u6743\u91cd\u53c2\u6570\u90fd\u662f\u521d\u59cb\u5316\u4e3a\u5747\u5300\u5206\u5e03 self . embedding . weight . data . uniform_ ( - initrange , initrange ) self . fc . weight . data . uniform_ ( - initrange , initrange ) # \u504f\u7f6e\u521d\u59cb\u5316\u4e3a0 self . fc . bias . data . zero_ () def forward ( self , text ): \"\"\" :param text: \u6587\u672c\u6570\u503c\u6620\u5c04\u540e\u7684\u7ed3\u679c :return: \u4e0e\u7c7b\u522b\u6570\u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf, \u7528\u4ee5\u5224\u65ad\u6587\u672c\u7c7b\u522b \"\"\" # \u83b7\u5f97embedding\u7684\u7ed3\u679cembedded # >>> embedded.shape # (m, 32) \u5176\u4e2dm\u662fBATCH_SIZE\u5927\u5c0f\u7684\u6570\u636e\u4e2d\u8bcd\u6c47\u603b\u6570 embedded = self . embedding ( text ) # \u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u5c06(m, 32)\u8f6c\u5316\u6210(BATCH_SIZE, 32) # \u4ee5\u4fbf\u901a\u8fc7fc\u5c42\u540e\u80fd\u8ba1\u7b97\u76f8\u5e94\u7684\u635f\u5931 # \u9996\u5148, \u6211\u4eec\u5df2\u77e5m\u7684\u503c\u8fdc\u5927\u4e8eBATCH_SIZE=16, # \u7528m\u6574\u9664BATCH_SIZE, \u83b7\u5f97m\u4e2d\u5171\u5305\u542bc\u4e2aBATCH_SIZE c = embedded . size ( 0 ) // BATCH_SIZE # \u4e4b\u540e\u518d\u4eceembedded\u4e2d\u53d6c*BATCH_SIZE\u4e2a\u5411\u91cf\u5f97\u5230\u65b0\u7684embedded # \u8fd9\u4e2a\u65b0\u7684embedded\u4e2d\u7684\u5411\u91cf\u4e2a\u6570\u53ef\u4ee5\u6574\u9664BATCH_SIZE embedded = embedded [: BATCH_SIZE * c ] # \u56e0\u4e3a\u6211\u4eec\u60f3\u5229\u7528\u5e73\u5747\u6c60\u5316\u7684\u65b9\u6cd5\u6c42embedded\u4e2d\u6307\u5b9a\u884c\u6570\u7684\u5217\u7684\u5e73\u5747\u6570, # \u4f46\u5e73\u5747\u6c60\u5316\u65b9\u6cd5\u662f\u4f5c\u7528\u5728\u884c\u4e0a\u7684, \u5e76\u4e14\u9700\u89813\u7ef4\u8f93\u5165 # \u56e0\u6b64\u6211\u4eec\u5bf9\u65b0\u7684embedded\u8fdb\u884c\u8f6c\u7f6e\u5e76\u62d3\u5c55\u7ef4\u5ea6 embedded = embedded . transpose ( 1 , 0 ) . unsqueeze ( 0 ) # \u7136\u540e\u5c31\u662f\u8c03\u7528\u5e73\u5747\u6c60\u5316\u7684\u65b9\u6cd5, \u5e76\u4e14\u6838\u7684\u5927\u5c0f\u4e3ac # \u5373\u53d6\u6bcfc\u7684\u5143\u7d20\u8ba1\u7b97\u4e00\u6b21\u5747\u503c\u4f5c\u4e3a\u7ed3\u679c embedded = F . avg_pool1d ( embedded , kernel_size = c ) # \u6700\u540e\uff0c\u8fd8\u9700\u8981\u51cf\u53bb\u65b0\u589e\u7684\u7ef4\u5ea6, \u7136\u540e\u8f6c\u7f6e\u56de\u53bb\u8f93\u9001\u7ed9fc\u5c42 return self . fc ( embedded [ 0 ] . transpose ( 1 , 0 )) \u5b9e\u4f8b\u5316\u6a21\u578b: # \u83b7\u5f97\u6574\u4e2a\u8bed\u6599\u5305\u542b\u7684\u4e0d\u540c\u8bcd\u6c47\u603b\u6570 VOCAB_SIZE = len ( train_dataset . get_vocab ()) # \u6307\u5b9a\u8bcd\u5d4c\u5165\u7ef4\u5ea6 EMBED_DIM = 32 # \u83b7\u5f97\u7c7b\u522b\u603b\u6570 NUN_CLASS = len ( train_dataset . get_labels ()) # \u5b9e\u4f8b\u5316\u6a21\u578b model = TextSentiment ( VOCAB_SIZE , EMBED_DIM , NUN_CLASS ) . to ( device ) 2.2 \u5bf9\u6570\u636e\u8fdb\u884cbatch\u5904\u7406 \u00b6 def generate_batch ( batch ): \"\"\" description: \u751f\u6210batch\u6570\u636e\u51fd\u6570 :param batch: \u7531\u6837\u672c\u5f20\u91cf\u548c\u5bf9\u5e94\u6807\u7b7e\u7684\u5143\u7ec4\u7ec4\u6210\u7684batch_size\u5927\u5c0f\u7684\u5217\u8868 \u5f62\u5982: [(label1, sample1), (lable2, sample2), ..., (labelN, sampleN)] return: \u6837\u672c\u5f20\u91cf\u548c\u6807\u7b7e\u5404\u81ea\u7684\u5217\u8868\u5f62\u5f0f(\u5f20\u91cf) \u5f62\u5982: text = tensor([sample1, sample2, ..., sampleN]) label = tensor([label1, label2, ..., labelN]) \"\"\" # \u4ecebatch\u4e2d\u83b7\u5f97\u6807\u7b7e\u5f20\u91cf label = torch . tensor ([ entry [ 0 ] for entry in batch ]) # \u4ecebatch\u4e2d\u83b7\u5f97\u6837\u672c\u5f20\u91cf text = [ entry [ 1 ] for entry in batch ] text = torch . cat ( text ) # \u8fd4\u56de\u7ed3\u679c return text , label \u8c03\u7528: # \u5047\u8bbe\u4e00\u4e2a\u8f93\u5165: batch = [( 1 , torch . tensor ([ 3 , 23 , 2 , 8 ])), ( 0 , torch . tensor ([ 3 , 45 , 21 , 6 ]))] res = generate_batch ( batch ) print ( res ) \u8f93\u51fa\u6548\u679c: # \u5bf9\u5e94\u8f93\u5165\u7684\u4e24\u6761\u6570\u636e\u8fdb\u884c\u4e86\u76f8\u5e94\u7684\u62fc\u63a5 (tensor([ 3, 23, 2, 8, 3, 45, 21, 6]), tensor([1, 0])) 2.3 \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u51fd\u6570 \u00b6 # \u5bfc\u5165torch\u4e2d\u7684\u6570\u636e\u52a0\u8f7d\u5668\u65b9\u6cd5 from torch.utils.data import DataLoader def train ( train_data ): \"\"\"\u6a21\u578b\u8bad\u7ec3\u51fd\u6570\"\"\" # \u521d\u59cb\u5316\u8bad\u7ec3\u635f\u5931\u548c\u51c6\u786e\u7387\u4e3a0 train_loss = 0 train_acc = 0 # \u4f7f\u7528\u6570\u636e\u52a0\u8f7d\u5668\u751f\u6210BATCH_SIZE\u5927\u5c0f\u7684\u6570\u636e\u8fdb\u884c\u6279\u6b21\u8bad\u7ec3 # data\u5c31\u662fN\u591a\u4e2agenerate_batch\u51fd\u6570\u5904\u7406\u540e\u7684BATCH_SIZE\u5927\u5c0f\u7684\u6570\u636e\u751f\u6210\u5668 data = DataLoader ( train_data , batch_size = BATCH_SIZE , shuffle = True , collate_fn = generate_batch ) # \u5bf9data\u8fdb\u884c\u5faa\u73af\u904d\u5386, \u4f7f\u7528\u6bcf\u4e2abatch\u7684\u6570\u636e\u8fdb\u884c\u53c2\u6570\u66f4\u65b0 for i , ( text , cls ) in enumerate ( data ): # \u8bbe\u7f6e\u4f18\u5316\u5668\u521d\u59cb\u68af\u5ea6\u4e3a0 optimizer . zero_grad () # \u6a21\u578b\u8f93\u5165\u4e00\u4e2a\u6279\u6b21\u6570\u636e, \u83b7\u5f97\u8f93\u51fa output = model ( text ) # \u6839\u636e\u771f\u5b9e\u6807\u7b7e\u4e0e\u6a21\u578b\u8f93\u51fa\u8ba1\u7b97\u635f\u5931 loss = criterion ( output , cls ) # \u5c06\u8be5\u6279\u6b21\u7684\u635f\u5931\u52a0\u5230\u603b\u635f\u5931\u4e2d train_loss += loss . item () # \u8bef\u5dee\u53cd\u5411\u4f20\u64ad loss . backward () # \u53c2\u6570\u8fdb\u884c\u66f4\u65b0 optimizer . step () # \u5c06\u8be5\u6279\u6b21\u7684\u51c6\u786e\u7387\u52a0\u5230\u603b\u51c6\u786e\u7387\u4e2d train_acc += ( output . argmax ( 1 ) == cls ) . sum () . item () # \u8c03\u6574\u4f18\u5316\u5668\u5b66\u4e60\u7387 scheduler . step () # \u8fd4\u56de\u672c\u8f6e\u8bad\u7ec3\u7684\u5e73\u5747\u635f\u5931\u548c\u5e73\u5747\u51c6\u786e\u7387 return train_loss / len ( train_data ), train_acc / len ( train_data ) def valid ( valid_data ): \"\"\"\u6a21\u578b\u9a8c\u8bc1\u51fd\u6570\"\"\" # \u521d\u59cb\u5316\u9a8c\u8bc1\u635f\u5931\u548c\u51c6\u786e\u7387\u4e3a0 loss = 0 acc = 0 # \u548c\u8bad\u7ec3\u76f8\u540c, \u4f7f\u7528DataLoader\u83b7\u5f97\u8bad\u7ec3\u6570\u636e\u751f\u6210\u5668 data = DataLoader ( valid_data , batch_size = BATCH_SIZE , collate_fn = generate_batch ) # \u6309\u6279\u6b21\u53d6\u51fa\u6570\u636e\u9a8c\u8bc1 for text , cls in data : # \u9a8c\u8bc1\u9636\u6bb5, \u4e0d\u518d\u6c42\u89e3\u68af\u5ea6 with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa output = model ( text ) # \u8ba1\u7b97\u635f\u5931 loss = criterion ( output , cls ) # \u5c06\u635f\u5931\u548c\u51c6\u786e\u7387\u52a0\u5230\u603b\u635f\u5931\u548c\u51c6\u786e\u7387\u4e2d loss += loss . item () acc += ( output . argmax ( 1 ) == cls ) . sum () . item () # \u8fd4\u56de\u672c\u8f6e\u9a8c\u8bc1\u7684\u5e73\u5747\u635f\u5931\u548c\u5e73\u5747\u51c6\u786e\u7387 return loss / len ( valid_data ), acc / len ( valid_data ) 2.4 \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1 \u00b6 # \u5bfc\u5165\u65f6\u95f4\u5de5\u5177\u5305 import time # \u5bfc\u5165\u6570\u636e\u968f\u673a\u5212\u5206\u65b9\u6cd5\u5de5\u5177 from torch.utils.data.dataset import random_split # \u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570 N_EPOCHS = 10 # \u5b9a\u4e49\u521d\u59cb\u7684\u9a8c\u8bc1\u635f\u5931 min_valid_loss = float ( 'inf' ) # \u9009\u62e9\u635f\u5931\u51fd\u6570, \u8fd9\u91cc\u9009\u62e9\u9884\u5b9a\u4e49\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 criterion = torch . nn . CrossEntropyLoss () . to ( device ) # \u9009\u62e9\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668 optimizer = torch . optim . SGD ( model . parameters (), lr = 4.0 ) # \u9009\u62e9\u4f18\u5316\u5668\u6b65\u957f\u8c03\u8282\u65b9\u6cd5StepLR, \u7528\u6765\u8870\u51cf\u5b66\u4e60\u7387 scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , 1 , gamma = 0.9 ) # \u4ecetrain_dataset\u53d6\u51fa0.95\u4f5c\u4e3a\u8bad\u7ec3\u96c6, \u5148\u53d6\u5176\u957f\u5ea6 train_len = int ( len ( train_dataset ) * 0.95 ) # \u7136\u540e\u4f7f\u7528random_split\u8fdb\u884c\u4e71\u5e8f\u5212\u5206, \u5f97\u5230\u5bf9\u5e94\u7684\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6 sub_train_ , sub_valid_ = \\ random_split ( train_dataset , [ train_len , len ( train_dataset ) - train_len ]) # \u5f00\u59cb\u6bcf\u4e00\u8f6e\u8bad\u7ec3 for epoch in range ( N_EPOCHS ): # \u8bb0\u5f55\u6982\u8bba\u8bad\u7ec3\u7684\u5f00\u59cb\u65f6\u95f4 start_time = time . time () # \u8c03\u7528train\u548cvalid\u51fd\u6570\u5f97\u5230\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u5e73\u5747\u635f\u5931, \u5e73\u5747\u51c6\u786e\u7387 train_loss , train_acc = train ( sub_train_ ) valid_loss , valid_acc = valid ( sub_valid_ ) # \u8ba1\u7b97\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u603b\u8017\u65f6(\u79d2) secs = int ( time . time () - start_time ) # \u7528\u5206\u949f\u548c\u79d2\u8868\u793a mins = secs / 60 secs = secs % 60 # \u6253\u5370\u8bad\u7ec3\u548c\u9a8c\u8bc1\u8017\u65f6\uff0c\u5e73\u5747\u635f\u5931\uff0c\u5e73\u5747\u51c6\u786e\u7387 print ( 'Epoch: %d ' % ( epoch + 1 ), \" | time in %d minutes, %d seconds\" % ( mins , secs )) print ( f ' \\t Loss: { train_loss : .4f } (train) \\t | \\t Acc: { train_acc * 100 : .1f } %(train)' ) print ( f ' \\t Loss: { valid_loss : .4f } (valid) \\t | \\t Acc: { valid_acc * 100 : .1f } %(valid)' ) \u8f93\u51fa\u6548\u679c: 120000lines [00:06, 17834.17lines/s] 120000lines [00:11, 10071.77lines/s] 7600lines [00:00, 10432.95lines/s] Epoch: 1 | time in 0 minutes, 36 seconds Loss: 0.0592(train) | Acc: 63.9%(train) Loss: 0.0005(valid) | Acc: 69.2%(valid) Epoch: 2 | time in 0 minutes, 37 seconds Loss: 0.0507(train) | Acc: 71.3%(train) Loss: 0.0005(valid) | Acc: 70.7%(valid) Epoch: 3 | time in 0 minutes, 36 seconds Loss: 0.0484(train) | Acc: 72.8%(train) Loss: 0.0005(valid) | Acc: 71.4%(valid) Epoch: 4 | time in 0 minutes, 36 seconds Loss: 0.0474(train) | Acc: 73.4%(train) Loss: 0.0004(valid) | Acc: 72.0%(valid) Epoch: 5 | time in 0 minutes, 36 seconds Loss: 0.0455(train) | Acc: 74.8%(train) Loss: 0.0004(valid) | Acc: 72.5%(valid) Epoch: 6 | time in 0 minutes, 36 seconds Loss: 0.0451(train) | Acc: 74.9%(train) Loss: 0.0004(valid) | Acc: 72.3%(valid) Epoch: 7 | time in 0 minutes, 36 seconds Loss: 0.0446(train) | Acc: 75.3%(train) Loss: 0.0004(valid) | Acc: 72.0%(valid) Epoch: 8 | time in 0 minutes, 36 seconds Loss: 0.0437(train) | Acc: 75.9%(train) Loss: 0.0004(valid) | Acc: 71.4%(valid) Epoch: 9 | time in 0 minutes, 36 seconds Loss: 0.0431(train) | Acc: 76.2%(train) Loss: 0.0004(valid) | Acc: 72.7%(valid) Epoch: 10 | time in 0 minutes, 36 seconds Loss: 0.0426(train) | Acc: 76.6%(train) Loss: 0.0004(valid) | Acc: 72.6%(valid) 2.5 \u67e5\u770bembedding\u5c42\u5d4c\u5165\u7684\u8bcd\u5411\u91cf \u00b6 # \u6253\u5370\u4ece\u6a21\u578b\u7684\u72b6\u6001\u5b57\u5178\u4e2d\u83b7\u5f97\u7684Embedding\u77e9\u9635 print ( model . state_dict ()[ 'embedding.weight' ]) \u8f93\u51fa\u6548\u679c: tensor([[ 0.4401, -0.4177, -0.4161, ..., 0.2497, -0.4657, -0.1861], [-0.2574, -0.1952, 0.1443, ..., -0.4687, -0.0742, 0.2606], [-0.1926, -0.1153, -0.0167, ..., -0.0954, 0.0134, -0.0632], ..., [-0.0780, -0.2331, -0.3656, ..., -0.1899, 0.4083, 0.3002], [-0.0696, 0.4396, -0.1350, ..., 0.1019, 0.2792, -0.4749], [-0.2978, 0.1872, -0.1994, ..., 0.3435, 0.4729, -0.2608]]) 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u5173\u4e8e\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1: \u4ee5\u4e00\u6bb5\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u6587\u672c\u63cf\u8ff0\u5185\u5bb9\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u5c5e\u4e8e\u54ea\u4e00\u79cd\u7c7b\u578b\u7684\u65b0\u95fb, \u8fd9\u662f\u5178\u578b\u7684\u6587\u672c\u5206\u7c7b\u95ee\u9898, \u6211\u4eec\u8fd9\u91cc\u5047\u5b9a\u6bcf\u79cd\u7c7b\u578b\u662f\u4e92\u65a5\u7684, \u5373\u6587\u672c\u63cf\u8ff0\u6709\u4e14\u53ea\u6709\u4e00\u79cd\u7c7b\u578b. \u5b66\u4e60\u4e86\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u6570\u636e\u7684\u83b7\u53d6\u548c\u6837\u5f0f. \u5b66\u4e60\u4e86\u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u7684\u4e94\u4e2a\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u6784\u5efa\u5e26\u6709Embedding\u5c42\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b. \u7b2c\u4e8c\u6b65: \u5bf9\u6570\u636e\u8fdb\u884cbatch\u5904\u7406. \u7b2c\u4e09\u6b65: \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u51fd\u6570. \u7b2c\u56db\u6b65: \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1. \u7b2c\u4e94\u6b65: \u67e5\u770bembedding\u5c42\u5d4c\u5165\u7684\u8bcd\u5411\u91cf.","title":"8 \u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u6848\u4f8b"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#_1","text":"\u4e86\u89e3\u6709\u5173\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u548c\u6709\u5173\u6570\u636e. \u638c\u63e1\u4f7f\u7528\u6d45\u5c42\u7f51\u7edc\u6784\u5efa\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u5668\u7684\u5b9e\u73b0\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#1","text":"\u5173\u4e8e\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1: \u4ee5\u4e00\u6bb5\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u6587\u672c\u63cf\u8ff0\u5185\u5bb9\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u5c5e\u4e8e\u54ea\u4e00\u79cd\u7c7b\u578b\u7684\u65b0\u95fb, \u8fd9\u662f\u5178\u578b\u7684\u6587\u672c\u5206\u7c7b\u95ee\u9898, \u6211\u4eec\u8fd9\u91cc\u5047\u5b9a\u6bcf\u79cd\u7c7b\u578b\u662f\u4e92\u65a5\u7684, \u5373\u6587\u672c\u63cf\u8ff0\u6709\u4e14\u53ea\u6709\u4e00\u79cd\u7c7b\u578b. \u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u6570\u636e: \u6570\u636e\u6587\u4ef6\u9884\u89c8: # \u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/ag_news_csv\u4e0b - data/ - ag_news_csv/ classes.txt readme.txt test.csv train.csv \u6587\u4ef6\u8bf4\u660e: train.csv\u8868\u793a\u8bad\u7ec3\u6570\u636e, \u517112\u4e07\u6761\u6570\u636e; test.csv\u8868\u793a\u9a8c\u8bc1\u6570\u636e, \u51717600\u6761\u6570\u636e; classes.txt\u662f\u6807\u7b7e(\u65b0\u95fb\u4e3b\u9898)\u542b\u4e49\u6587\u4ef6, \u91cc\u9762\u6709\u56db\u4e2a\u5355\u8bcd'World', 'Sports', 'Business', 'Sci/Tech'\u4ee3\u8868\u65b0\u95fb\u7684\u56db\u4e2a\u4e3b\u9898, readme.txt\u662f\u8be5\u6570\u636e\u96c6\u7684\u82f1\u6587\u8bf4\u660e. train.csv\u9884\u89c8: \"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\" \"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\" \"3\",\"Oil and Economy Cloud Stocks' Outlook (Reuters)\",\"Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\" \"3\",\"Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)\",\"Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\" \"3\",\"Oil prices soar to all-time record, posing new menace to US economy (AFP)\",\"AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\" \"3\",\"Stocks End Up, But Near Year Lows (Reuters)\",\"Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\" \"3\",\"Money Funds Fell in Latest Week (AP)\",\"AP - Assets of the nation's retail money market mutual funds fell by #36;1.17 billion in the latest week to #36;849.98 trillion, the Investment Company Institute said Thursday.\" \"3\",\"Fed minutes show dissent over inflation (USATODAY.com)\",\"USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.\" \"3\",\"Safety Net (Forbes.com)\",\"Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"\"buying insurance was the furthest thing from my mind,\"\" says Riley.\" \"3\",\"Wall St. Bears Claw Back Into the Black\",\" NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again.\" \u6587\u4ef6\u5185\u5bb9\u8bf4\u660e: train.csv\u5171\u75313\u5217\u7ec4\u6210, \u4f7f\u7528','\u8fdb\u884c\u5206\u9694, \u5206\u522b\u4ee3\u8868: \u6807\u7b7e, \u65b0\u95fb\u6807\u9898, \u65b0\u95fb\u7b80\u8ff0; \u5176\u4e2d\u6807\u7b7e\u7528\"1\", \"2\", \"3\", \"4\"\u8868\u793a, \u4f9d\u6b21\u5bf9\u5e94classes\u4e2d\u7684\u5185\u5bb9. test.csv\u4e0etrain.csv\u5185\u5bb9\u683c\u5f0f\u4e0e\u542b\u4e49\u76f8\u540c. \u4ece\u672c\u5730\u8fdb\u884c\u6570\u636e\u7684\u52a0\u8f7d\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b from torchtext.legacy.datasets.text_classification import _csv_iterator , _create_data_from_iterator , TextClassificationDataset from torchtext.utils import extract_archive from torchtext.vocab import build_vocab_from_iterator , Vocab # \u4ece\u672c\u5730\u52a0\u8f7d\u6570\u636e\u7684\u65b9\u5f0f\uff0c\u672c\u5730\u6570\u636e\u5728\u865a\u62df\u673a/root/data/ag_news_csv\u4e2d # \u5b9a\u4e49\u52a0\u8f7d\u51fd\u6570 def setup_datasets ( ngrams = 2 , vocab_train = None , vocab_test = None , include_unk = False ): train_csv_path = 'data/ag_news_csv/train.csv' test_csv_path = 'data/ag_news_csv/test.csv' if vocab_train is None : vocab_train = build_vocab_from_iterator ( _csv_iterator ( train_csv_path , ngrams )) else : if not isinstance ( vocab , Vocab ): raise TypeError ( \"Passed vocabulary is not of type Vocab\" ) if vocab_test is None : vocab_test = build_vocab_from_iterator ( _csv_iterator ( test_csv_path , ngrams )) else : if not isinstance ( vocab , Vocab ): raise TypeError ( \"Passed vocabulary is not of type Vocab\" ) train_data , train_labels = _create_data_from_iterator ( vocab_train , _csv_iterator ( train_csv_path , ngrams , yield_cls = True ), include_unk ) test_data , test_labels = _create_data_from_iterator ( vocab_test , _csv_iterator ( test_csv_path , ngrams , yield_cls = True ), include_unk ) if len ( train_labels ^ test_labels ) > 0 : raise ValueError ( \"Training and test labels don't match\" ) return ( TextClassificationDataset ( vocab_train , train_data , train_labels ), TextClassificationDataset ( vocab_test , test_data , test_labels )) # \u8c03\u7528\u51fd\u6570, \u52a0\u8f7d\u672c\u5730\u6570\u636e train_dataset , test_dataset = setup_datasets () print ( \"train_dataset\" , train_dataset )","title":"1 \u6848\u4f8b\u8bf4\u660e"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#2","text":"\u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4 \u7b2c\u4e00\u6b65: \u6784\u5efa\u5e26\u6709Embedding\u5c42\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b. \u7b2c\u4e8c\u6b65: \u5bf9\u6570\u636e\u8fdb\u884cbatch\u5904\u7406. \u7b2c\u4e09\u6b65: \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u51fd\u6570. \u7b2c\u56db\u6b65: \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1. \u7b2c\u4e94\u6b65: \u67e5\u770bembedding\u5c42\u5d4c\u5165\u7684\u8bcd\u5411\u91cf.","title":"2 \u6848\u4f8b\u5b9e\u73b0"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#21-embedding","text":"# \u5bfc\u5165\u5fc5\u5907\u7684torch\u6a21\u578b\u6784\u5efa\u5de5\u5177 import torch.nn as nn import torch.nn.functional as F # \u6307\u5b9aBATCH_SIZE\u7684\u5927\u5c0f BATCH_SIZE = 16 # \u8fdb\u884c\u53ef\u7528\u8bbe\u5907\u68c0\u6d4b, \u6709GPU\u7684\u8bdd\u5c06\u4f18\u5148\u4f7f\u7528GPU device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) class TextSentiment ( nn . Module ): \"\"\"\u6587\u672c\u5206\u7c7b\u6a21\u578b\"\"\" def __init__ ( self , vocab_size , embed_dim , num_class ): \"\"\" description: \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570 :param vocab_size: \u6574\u4e2a\u8bed\u6599\u5305\u542b\u7684\u4e0d\u540c\u8bcd\u6c47\u603b\u6570 :param embed_dim: \u6307\u5b9a\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6 :param num_class: \u6587\u672c\u5206\u7c7b\u7684\u7c7b\u522b\u603b\u6570 \"\"\" super () . __init__ () # \u5b9e\u4f8b\u5316embedding\u5c42, sparse=True\u4ee3\u8868\u6bcf\u6b21\u5bf9\u8be5\u5c42\u6c42\u89e3\u68af\u5ea6\u65f6, \u53ea\u66f4\u65b0\u90e8\u5206\u6743\u91cd. self . embedding = nn . Embedding ( vocab_size , embed_dim , sparse = True ) # \u5b9e\u4f8b\u5316\u7ebf\u6027\u5c42, \u53c2\u6570\u5206\u522b\u662fembed_dim\u548cnum_class. self . fc = nn . Linear ( embed_dim , num_class ) # \u4e3a\u5404\u5c42\u521d\u59cb\u5316\u6743\u91cd self . init_weights () def init_weights ( self ): \"\"\"\u521d\u59cb\u5316\u6743\u91cd\u51fd\u6570\"\"\" # \u6307\u5b9a\u521d\u59cb\u6743\u91cd\u7684\u53d6\u503c\u8303\u56f4\u6570 initrange = 0.5 # \u5404\u5c42\u7684\u6743\u91cd\u53c2\u6570\u90fd\u662f\u521d\u59cb\u5316\u4e3a\u5747\u5300\u5206\u5e03 self . embedding . weight . data . uniform_ ( - initrange , initrange ) self . fc . weight . data . uniform_ ( - initrange , initrange ) # \u504f\u7f6e\u521d\u59cb\u5316\u4e3a0 self . fc . bias . data . zero_ () def forward ( self , text ): \"\"\" :param text: \u6587\u672c\u6570\u503c\u6620\u5c04\u540e\u7684\u7ed3\u679c :return: \u4e0e\u7c7b\u522b\u6570\u5c3a\u5bf8\u76f8\u540c\u7684\u5f20\u91cf, \u7528\u4ee5\u5224\u65ad\u6587\u672c\u7c7b\u522b \"\"\" # \u83b7\u5f97embedding\u7684\u7ed3\u679cembedded # >>> embedded.shape # (m, 32) \u5176\u4e2dm\u662fBATCH_SIZE\u5927\u5c0f\u7684\u6570\u636e\u4e2d\u8bcd\u6c47\u603b\u6570 embedded = self . embedding ( text ) # \u63a5\u4e0b\u6765\u6211\u4eec\u9700\u8981\u5c06(m, 32)\u8f6c\u5316\u6210(BATCH_SIZE, 32) # \u4ee5\u4fbf\u901a\u8fc7fc\u5c42\u540e\u80fd\u8ba1\u7b97\u76f8\u5e94\u7684\u635f\u5931 # \u9996\u5148, \u6211\u4eec\u5df2\u77e5m\u7684\u503c\u8fdc\u5927\u4e8eBATCH_SIZE=16, # \u7528m\u6574\u9664BATCH_SIZE, \u83b7\u5f97m\u4e2d\u5171\u5305\u542bc\u4e2aBATCH_SIZE c = embedded . size ( 0 ) // BATCH_SIZE # \u4e4b\u540e\u518d\u4eceembedded\u4e2d\u53d6c*BATCH_SIZE\u4e2a\u5411\u91cf\u5f97\u5230\u65b0\u7684embedded # \u8fd9\u4e2a\u65b0\u7684embedded\u4e2d\u7684\u5411\u91cf\u4e2a\u6570\u53ef\u4ee5\u6574\u9664BATCH_SIZE embedded = embedded [: BATCH_SIZE * c ] # \u56e0\u4e3a\u6211\u4eec\u60f3\u5229\u7528\u5e73\u5747\u6c60\u5316\u7684\u65b9\u6cd5\u6c42embedded\u4e2d\u6307\u5b9a\u884c\u6570\u7684\u5217\u7684\u5e73\u5747\u6570, # \u4f46\u5e73\u5747\u6c60\u5316\u65b9\u6cd5\u662f\u4f5c\u7528\u5728\u884c\u4e0a\u7684, \u5e76\u4e14\u9700\u89813\u7ef4\u8f93\u5165 # \u56e0\u6b64\u6211\u4eec\u5bf9\u65b0\u7684embedded\u8fdb\u884c\u8f6c\u7f6e\u5e76\u62d3\u5c55\u7ef4\u5ea6 embedded = embedded . transpose ( 1 , 0 ) . unsqueeze ( 0 ) # \u7136\u540e\u5c31\u662f\u8c03\u7528\u5e73\u5747\u6c60\u5316\u7684\u65b9\u6cd5, \u5e76\u4e14\u6838\u7684\u5927\u5c0f\u4e3ac # \u5373\u53d6\u6bcfc\u7684\u5143\u7d20\u8ba1\u7b97\u4e00\u6b21\u5747\u503c\u4f5c\u4e3a\u7ed3\u679c embedded = F . avg_pool1d ( embedded , kernel_size = c ) # \u6700\u540e\uff0c\u8fd8\u9700\u8981\u51cf\u53bb\u65b0\u589e\u7684\u7ef4\u5ea6, \u7136\u540e\u8f6c\u7f6e\u56de\u53bb\u8f93\u9001\u7ed9fc\u5c42 return self . fc ( embedded [ 0 ] . transpose ( 1 , 0 )) \u5b9e\u4f8b\u5316\u6a21\u578b: # \u83b7\u5f97\u6574\u4e2a\u8bed\u6599\u5305\u542b\u7684\u4e0d\u540c\u8bcd\u6c47\u603b\u6570 VOCAB_SIZE = len ( train_dataset . get_vocab ()) # \u6307\u5b9a\u8bcd\u5d4c\u5165\u7ef4\u5ea6 EMBED_DIM = 32 # \u83b7\u5f97\u7c7b\u522b\u603b\u6570 NUN_CLASS = len ( train_dataset . get_labels ()) # \u5b9e\u4f8b\u5316\u6a21\u578b model = TextSentiment ( VOCAB_SIZE , EMBED_DIM , NUN_CLASS ) . to ( device )","title":"2.1 \u6784\u5efa\u5e26\u6709Embedding\u5c42\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#22-batch","text":"def generate_batch ( batch ): \"\"\" description: \u751f\u6210batch\u6570\u636e\u51fd\u6570 :param batch: \u7531\u6837\u672c\u5f20\u91cf\u548c\u5bf9\u5e94\u6807\u7b7e\u7684\u5143\u7ec4\u7ec4\u6210\u7684batch_size\u5927\u5c0f\u7684\u5217\u8868 \u5f62\u5982: [(label1, sample1), (lable2, sample2), ..., (labelN, sampleN)] return: \u6837\u672c\u5f20\u91cf\u548c\u6807\u7b7e\u5404\u81ea\u7684\u5217\u8868\u5f62\u5f0f(\u5f20\u91cf) \u5f62\u5982: text = tensor([sample1, sample2, ..., sampleN]) label = tensor([label1, label2, ..., labelN]) \"\"\" # \u4ecebatch\u4e2d\u83b7\u5f97\u6807\u7b7e\u5f20\u91cf label = torch . tensor ([ entry [ 0 ] for entry in batch ]) # \u4ecebatch\u4e2d\u83b7\u5f97\u6837\u672c\u5f20\u91cf text = [ entry [ 1 ] for entry in batch ] text = torch . cat ( text ) # \u8fd4\u56de\u7ed3\u679c return text , label \u8c03\u7528: # \u5047\u8bbe\u4e00\u4e2a\u8f93\u5165: batch = [( 1 , torch . tensor ([ 3 , 23 , 2 , 8 ])), ( 0 , torch . tensor ([ 3 , 45 , 21 , 6 ]))] res = generate_batch ( batch ) print ( res ) \u8f93\u51fa\u6548\u679c: # \u5bf9\u5e94\u8f93\u5165\u7684\u4e24\u6761\u6570\u636e\u8fdb\u884c\u4e86\u76f8\u5e94\u7684\u62fc\u63a5 (tensor([ 3, 23, 2, 8, 3, 45, 21, 6]), tensor([1, 0]))","title":"2.2 \u5bf9\u6570\u636e\u8fdb\u884cbatch\u5904\u7406"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#23","text":"# \u5bfc\u5165torch\u4e2d\u7684\u6570\u636e\u52a0\u8f7d\u5668\u65b9\u6cd5 from torch.utils.data import DataLoader def train ( train_data ): \"\"\"\u6a21\u578b\u8bad\u7ec3\u51fd\u6570\"\"\" # \u521d\u59cb\u5316\u8bad\u7ec3\u635f\u5931\u548c\u51c6\u786e\u7387\u4e3a0 train_loss = 0 train_acc = 0 # \u4f7f\u7528\u6570\u636e\u52a0\u8f7d\u5668\u751f\u6210BATCH_SIZE\u5927\u5c0f\u7684\u6570\u636e\u8fdb\u884c\u6279\u6b21\u8bad\u7ec3 # data\u5c31\u662fN\u591a\u4e2agenerate_batch\u51fd\u6570\u5904\u7406\u540e\u7684BATCH_SIZE\u5927\u5c0f\u7684\u6570\u636e\u751f\u6210\u5668 data = DataLoader ( train_data , batch_size = BATCH_SIZE , shuffle = True , collate_fn = generate_batch ) # \u5bf9data\u8fdb\u884c\u5faa\u73af\u904d\u5386, \u4f7f\u7528\u6bcf\u4e2abatch\u7684\u6570\u636e\u8fdb\u884c\u53c2\u6570\u66f4\u65b0 for i , ( text , cls ) in enumerate ( data ): # \u8bbe\u7f6e\u4f18\u5316\u5668\u521d\u59cb\u68af\u5ea6\u4e3a0 optimizer . zero_grad () # \u6a21\u578b\u8f93\u5165\u4e00\u4e2a\u6279\u6b21\u6570\u636e, \u83b7\u5f97\u8f93\u51fa output = model ( text ) # \u6839\u636e\u771f\u5b9e\u6807\u7b7e\u4e0e\u6a21\u578b\u8f93\u51fa\u8ba1\u7b97\u635f\u5931 loss = criterion ( output , cls ) # \u5c06\u8be5\u6279\u6b21\u7684\u635f\u5931\u52a0\u5230\u603b\u635f\u5931\u4e2d train_loss += loss . item () # \u8bef\u5dee\u53cd\u5411\u4f20\u64ad loss . backward () # \u53c2\u6570\u8fdb\u884c\u66f4\u65b0 optimizer . step () # \u5c06\u8be5\u6279\u6b21\u7684\u51c6\u786e\u7387\u52a0\u5230\u603b\u51c6\u786e\u7387\u4e2d train_acc += ( output . argmax ( 1 ) == cls ) . sum () . item () # \u8c03\u6574\u4f18\u5316\u5668\u5b66\u4e60\u7387 scheduler . step () # \u8fd4\u56de\u672c\u8f6e\u8bad\u7ec3\u7684\u5e73\u5747\u635f\u5931\u548c\u5e73\u5747\u51c6\u786e\u7387 return train_loss / len ( train_data ), train_acc / len ( train_data ) def valid ( valid_data ): \"\"\"\u6a21\u578b\u9a8c\u8bc1\u51fd\u6570\"\"\" # \u521d\u59cb\u5316\u9a8c\u8bc1\u635f\u5931\u548c\u51c6\u786e\u7387\u4e3a0 loss = 0 acc = 0 # \u548c\u8bad\u7ec3\u76f8\u540c, \u4f7f\u7528DataLoader\u83b7\u5f97\u8bad\u7ec3\u6570\u636e\u751f\u6210\u5668 data = DataLoader ( valid_data , batch_size = BATCH_SIZE , collate_fn = generate_batch ) # \u6309\u6279\u6b21\u53d6\u51fa\u6570\u636e\u9a8c\u8bc1 for text , cls in data : # \u9a8c\u8bc1\u9636\u6bb5, \u4e0d\u518d\u6c42\u89e3\u68af\u5ea6 with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa output = model ( text ) # \u8ba1\u7b97\u635f\u5931 loss = criterion ( output , cls ) # \u5c06\u635f\u5931\u548c\u51c6\u786e\u7387\u52a0\u5230\u603b\u635f\u5931\u548c\u51c6\u786e\u7387\u4e2d loss += loss . item () acc += ( output . argmax ( 1 ) == cls ) . sum () . item () # \u8fd4\u56de\u672c\u8f6e\u9a8c\u8bc1\u7684\u5e73\u5747\u635f\u5931\u548c\u5e73\u5747\u51c6\u786e\u7387 return loss / len ( valid_data ), acc / len ( valid_data )","title":"2.3 \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u51fd\u6570"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#24","text":"# \u5bfc\u5165\u65f6\u95f4\u5de5\u5177\u5305 import time # \u5bfc\u5165\u6570\u636e\u968f\u673a\u5212\u5206\u65b9\u6cd5\u5de5\u5177 from torch.utils.data.dataset import random_split # \u6307\u5b9a\u8bad\u7ec3\u8f6e\u6570 N_EPOCHS = 10 # \u5b9a\u4e49\u521d\u59cb\u7684\u9a8c\u8bc1\u635f\u5931 min_valid_loss = float ( 'inf' ) # \u9009\u62e9\u635f\u5931\u51fd\u6570, \u8fd9\u91cc\u9009\u62e9\u9884\u5b9a\u4e49\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 criterion = torch . nn . CrossEntropyLoss () . to ( device ) # \u9009\u62e9\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668 optimizer = torch . optim . SGD ( model . parameters (), lr = 4.0 ) # \u9009\u62e9\u4f18\u5316\u5668\u6b65\u957f\u8c03\u8282\u65b9\u6cd5StepLR, \u7528\u6765\u8870\u51cf\u5b66\u4e60\u7387 scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , 1 , gamma = 0.9 ) # \u4ecetrain_dataset\u53d6\u51fa0.95\u4f5c\u4e3a\u8bad\u7ec3\u96c6, \u5148\u53d6\u5176\u957f\u5ea6 train_len = int ( len ( train_dataset ) * 0.95 ) # \u7136\u540e\u4f7f\u7528random_split\u8fdb\u884c\u4e71\u5e8f\u5212\u5206, \u5f97\u5230\u5bf9\u5e94\u7684\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6 sub_train_ , sub_valid_ = \\ random_split ( train_dataset , [ train_len , len ( train_dataset ) - train_len ]) # \u5f00\u59cb\u6bcf\u4e00\u8f6e\u8bad\u7ec3 for epoch in range ( N_EPOCHS ): # \u8bb0\u5f55\u6982\u8bba\u8bad\u7ec3\u7684\u5f00\u59cb\u65f6\u95f4 start_time = time . time () # \u8c03\u7528train\u548cvalid\u51fd\u6570\u5f97\u5230\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u5e73\u5747\u635f\u5931, \u5e73\u5747\u51c6\u786e\u7387 train_loss , train_acc = train ( sub_train_ ) valid_loss , valid_acc = valid ( sub_valid_ ) # \u8ba1\u7b97\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u603b\u8017\u65f6(\u79d2) secs = int ( time . time () - start_time ) # \u7528\u5206\u949f\u548c\u79d2\u8868\u793a mins = secs / 60 secs = secs % 60 # \u6253\u5370\u8bad\u7ec3\u548c\u9a8c\u8bc1\u8017\u65f6\uff0c\u5e73\u5747\u635f\u5931\uff0c\u5e73\u5747\u51c6\u786e\u7387 print ( 'Epoch: %d ' % ( epoch + 1 ), \" | time in %d minutes, %d seconds\" % ( mins , secs )) print ( f ' \\t Loss: { train_loss : .4f } (train) \\t | \\t Acc: { train_acc * 100 : .1f } %(train)' ) print ( f ' \\t Loss: { valid_loss : .4f } (valid) \\t | \\t Acc: { valid_acc * 100 : .1f } %(valid)' ) \u8f93\u51fa\u6548\u679c: 120000lines [00:06, 17834.17lines/s] 120000lines [00:11, 10071.77lines/s] 7600lines [00:00, 10432.95lines/s] Epoch: 1 | time in 0 minutes, 36 seconds Loss: 0.0592(train) | Acc: 63.9%(train) Loss: 0.0005(valid) | Acc: 69.2%(valid) Epoch: 2 | time in 0 minutes, 37 seconds Loss: 0.0507(train) | Acc: 71.3%(train) Loss: 0.0005(valid) | Acc: 70.7%(valid) Epoch: 3 | time in 0 minutes, 36 seconds Loss: 0.0484(train) | Acc: 72.8%(train) Loss: 0.0005(valid) | Acc: 71.4%(valid) Epoch: 4 | time in 0 minutes, 36 seconds Loss: 0.0474(train) | Acc: 73.4%(train) Loss: 0.0004(valid) | Acc: 72.0%(valid) Epoch: 5 | time in 0 minutes, 36 seconds Loss: 0.0455(train) | Acc: 74.8%(train) Loss: 0.0004(valid) | Acc: 72.5%(valid) Epoch: 6 | time in 0 minutes, 36 seconds Loss: 0.0451(train) | Acc: 74.9%(train) Loss: 0.0004(valid) | Acc: 72.3%(valid) Epoch: 7 | time in 0 minutes, 36 seconds Loss: 0.0446(train) | Acc: 75.3%(train) Loss: 0.0004(valid) | Acc: 72.0%(valid) Epoch: 8 | time in 0 minutes, 36 seconds Loss: 0.0437(train) | Acc: 75.9%(train) Loss: 0.0004(valid) | Acc: 71.4%(valid) Epoch: 9 | time in 0 minutes, 36 seconds Loss: 0.0431(train) | Acc: 76.2%(train) Loss: 0.0004(valid) | Acc: 72.7%(valid) Epoch: 10 | time in 0 minutes, 36 seconds Loss: 0.0426(train) | Acc: 76.6%(train) Loss: 0.0004(valid) | Acc: 72.6%(valid)","title":"2.4 \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#25-embedding","text":"# \u6253\u5370\u4ece\u6a21\u578b\u7684\u72b6\u6001\u5b57\u5178\u4e2d\u83b7\u5f97\u7684Embedding\u77e9\u9635 print ( model . state_dict ()[ 'embedding.weight' ]) \u8f93\u51fa\u6548\u679c: tensor([[ 0.4401, -0.4177, -0.4161, ..., 0.2497, -0.4657, -0.1861], [-0.2574, -0.1952, 0.1443, ..., -0.4687, -0.0742, 0.2606], [-0.1926, -0.1153, -0.0167, ..., -0.0954, 0.0134, -0.0632], ..., [-0.0780, -0.2331, -0.3656, ..., -0.1899, 0.4083, 0.3002], [-0.0696, 0.4396, -0.1350, ..., 0.1019, 0.2792, -0.4749], [-0.2978, 0.1872, -0.1994, ..., 0.3435, 0.4729, -0.2608]])","title":"2.5 \u67e5\u770bembedding\u5c42\u5d4c\u5165\u7684\u8bcd\u5411\u91cf"},{"location":"02_mkdocs_preprocess/8%20%E6%96%B0%E9%97%BB%E4%B8%BB%E9%A2%98%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B.html#3","text":"\u5b66\u4e60\u4e86\u5173\u4e8e\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1: \u4ee5\u4e00\u6bb5\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u6587\u672c\u63cf\u8ff0\u5185\u5bb9\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u5c5e\u4e8e\u54ea\u4e00\u79cd\u7c7b\u578b\u7684\u65b0\u95fb, \u8fd9\u662f\u5178\u578b\u7684\u6587\u672c\u5206\u7c7b\u95ee\u9898, \u6211\u4eec\u8fd9\u91cc\u5047\u5b9a\u6bcf\u79cd\u7c7b\u578b\u662f\u4e92\u65a5\u7684, \u5373\u6587\u672c\u63cf\u8ff0\u6709\u4e14\u53ea\u6709\u4e00\u79cd\u7c7b\u578b. \u5b66\u4e60\u4e86\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u6570\u636e\u7684\u83b7\u53d6\u548c\u6837\u5f0f. \u5b66\u4e60\u4e86\u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u7684\u4e94\u4e2a\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u6784\u5efa\u5e26\u6709Embedding\u5c42\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b. \u7b2c\u4e8c\u6b65: \u5bf9\u6570\u636e\u8fdb\u884cbatch\u5904\u7406. \u7b2c\u4e09\u6b65: \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u51fd\u6570. \u7b2c\u56db\u6b65: \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1. \u7b2c\u4e94\u6b65: \u67e5\u770bembedding\u5c42\u5d4c\u5165\u7684\u8bcd\u5411\u91cf.","title":"3 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/index.html","text":"\u00b6","title":"Index"},{"location":"03_mkdocs_RNN/index.html#_1","text":"","title":""},{"location":"03_mkdocs_RNN/1%20%E8%AE%A4%E8%AF%86RNN%E6%A8%A1%E5%9E%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662fRNN\u6a21\u578b. \u4e86\u89e3RNN\u6a21\u578b\u7684\u4f5c\u7528. \u4e86\u89e3RNN\u6a21\u578b\u7684\u5206\u7c7b. 1 \u4ec0\u4e48\u662fRNN\u6a21\u578b \u00b6 RNN(Recurrent Neural Network), \u4e2d\u6587\u79f0\u4f5c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc, \u5b83\u4e00\u822c\u4ee5\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u901a\u8fc7\u7f51\u7edc\u5185\u90e8\u7684\u7ed3\u6784\u8bbe\u8ba1\u6709\u6548\u6355\u6349\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\u7279\u5f81, \u4e00\u822c\u4e5f\u662f\u4ee5\u5e8f\u5217\u5f62\u5f0f\u8fdb\u884c\u8f93\u51fa. \u4e00\u822c\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784: RNN\u5355\u5c42\u7f51\u7edc\u7ed3\u6784: \u4ee5\u65f6\u95f4\u6b65\u5bf9RNN\u8fdb\u884c\u5c55\u5f00\u540e\u7684\u5355\u5c42\u7f51\u7edc\u7ed3\u6784: RNN\u7684\u5faa\u73af\u673a\u5236\u4f7f\u6a21\u578b\u9690\u5c42\u4e0a\u4e00\u65f6\u95f4\u6b65\u4ea7\u751f\u7684\u7ed3\u679c, \u80fd\u591f\u4f5c\u4e3a\u5f53\u4e0b\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206(\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u9664\u4e86\u6b63\u5e38\u7684\u8f93\u5165\u5916\u8fd8\u5305\u62ec\u4e0a\u4e00\u6b65\u7684\u9690\u5c42\u8f93\u51fa)\u5bf9\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u4ea7\u751f\u5f71\u54cd. 2 RNN\u6a21\u578b\u7684\u4f5c\u7528 \u00b6 \u56e0\u4e3aRNN\u7ed3\u6784\u80fd\u591f\u5f88\u597d\u5229\u7528\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb, \u56e0\u6b64\u9488\u5bf9\u81ea\u7136\u754c\u5177\u6709\u8fde\u7eed\u6027\u7684\u8f93\u5165\u5e8f\u5217, \u5982\u4eba\u7c7b\u7684\u8bed\u8a00, \u8bed\u97f3\u7b49\u8fdb\u884c\u5f88\u597d\u7684\u5904\u7406, \u5e7f\u6cdb\u5e94\u7528\u4e8eNLP\u9886\u57df\u7684\u5404\u9879\u4efb\u52a1, \u5982\u6587\u672c\u5206\u7c7b, \u60c5\u611f\u5206\u6790, \u610f\u56fe\u8bc6\u522b, \u673a\u5668\u7ffb\u8bd1\u7b49. \u4e0b\u9762\u6211\u4eec\u5c06\u4ee5\u4e00\u4e2a\u7528\u6237\u610f\u56fe\u8bc6\u522b\u7684\u4f8b\u5b50\u8fdb\u884c\u7b80\u5355\u7684\u5206\u6790: \u7b2c\u4e00\u6b65: \u7528\u6237\u8f93\u5165\u4e86\"What time is it ?\", \u6211\u4eec\u9996\u5148\u9700\u8981\u5bf9\u5b83\u8fdb\u884c\u57fa\u672c\u7684\u5206\u8bcd, \u56e0\u4e3aRNN\u662f\u6309\u7167\u987a\u5e8f\u5de5\u4f5c\u7684, \u6bcf\u6b21\u53ea\u63a5\u6536\u4e00\u4e2a\u5355\u8bcd\u8fdb\u884c\u5904\u7406. \u7b2c\u4e8c\u6b65: \u9996\u5148\u5c06\u5355\u8bcd\"What\"\u8f93\u9001\u7ed9RNN, \u5b83\u5c06\u4ea7\u751f\u4e00\u4e2a\u8f93\u51faO1. \u7b2c\u4e09\u6b65: \u7ee7\u7eed\u5c06\u5355\u8bcd\"time\"\u8f93\u9001\u7ed9RNN, \u4f46\u6b64\u65f6RNN\u4e0d\u4ec5\u4ec5\u5229\u7528\"time\"\u6765\u4ea7\u751f\u8f93\u51faO2, \u8fd8\u4f1a\u4f7f\u7528\u6765\u81ea\u4e0a\u4e00\u5c42\u9690\u5c42\u8f93\u51faO1\u4f5c\u4e3a\u8f93\u5165\u4fe1\u606f. \u7b2c\u56db\u6b65: \u91cd\u590d\u8fd9\u6837\u7684\u6b65\u9aa4, \u76f4\u5230\u5904\u7406\u5b8c\u6240\u6709\u7684\u5355\u8bcd. \u7b2c\u4e94\u6b65: \u6700\u540e\uff0c\u5c06\u6700\u7ec8\u7684\u9690\u5c42\u8f93\u51faO5\u8fdb\u884c\u5904\u7406\u6765\u89e3\u6790\u7528\u6237\u610f\u56fe. 3 RNN\u6a21\u578b\u7684\u5206\u7c7b \u00b6 \u8fd9\u91cc\u6211\u4eec\u5c06\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5bf9RNN\u6a21\u578b\u8fdb\u884c\u5206\u7c7b. \u7b2c\u4e00\u4e2a\u89d2\u5ea6\u662f\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784, \u7b2c\u4e8c\u4e2a\u89d2\u5ea6\u662fRNN\u7684\u5185\u90e8\u6784\u9020. \u6309\u7167\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784\u8fdb\u884c\u5206\u7c7b: N vs N - RNN N vs 1 - RNN 1 vs N - RNN N vs M - RNN \u6309\u7167RNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b: \u4f20\u7edfRNN LSTM Bi-LSTM GRU Bi-GRU N vs N - RNN: \u5b83\u662fRNN\u6700\u57fa\u7840\u7684\u7ed3\u6784\u5f62\u5f0f, \u6700\u5927\u7684\u7279\u70b9\u5c31\u662f: \u8f93\u5165\u548c\u8f93\u51fa\u5e8f\u5217\u662f\u7b49\u957f\u7684. \u7531\u4e8e\u8fd9\u4e2a\u9650\u5236\u7684\u5b58\u5728, \u4f7f\u5176\u9002\u7528\u8303\u56f4\u6bd4\u8f83\u5c0f, \u53ef\u7528\u4e8e\u751f\u6210\u7b49\u957f\u5ea6\u7684\u5408\u8f99\u8bd7\u53e5. N vs 1 - RNN: \u6709\u65f6\u5019\u6211\u4eec\u8981\u5904\u7406\u7684\u95ee\u9898\u8f93\u5165\u662f\u4e00\u4e2a\u5e8f\u5217\uff0c\u800c\u8981\u6c42\u8f93\u51fa\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u503c\u800c\u4e0d\u662f\u5e8f\u5217\uff0c\u5e94\u8be5\u600e\u6837\u5efa\u6a21\u5462\uff1f\u6211\u4eec\u53ea\u8981\u5728\u6700\u540e\u4e00\u4e2a\u9690\u5c42\u8f93\u51fah\u4e0a\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u5c31\u53ef\u4ee5\u4e86\uff0c\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u660e\u786e\u7ed3\u679c, \u8fd8\u8981\u4f7f\u7528sigmoid\u6216\u8005softmax\u8fdb\u884c\u5904\u7406. \u8fd9\u79cd\u7ed3\u6784\u7ecf\u5e38\u88ab\u5e94\u7528\u5728\u6587\u672c\u5206\u7c7b\u95ee\u9898\u4e0a. 1 vs N - RNN: \u5982\u679c\u8f93\u5165\u4e0d\u662f\u5e8f\u5217\u800c\u8f93\u51fa\u4e3a\u5e8f\u5217\u7684\u60c5\u51b5\u600e\u4e48\u5904\u7406\u5462\uff1f\u6211\u4eec\u6700\u5e38\u91c7\u7528\u7684\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u4f7f\u8be5\u8f93\u5165\u4f5c\u7528\u4e8e\u6bcf\u6b21\u7684\u8f93\u51fa\u4e4b\u4e0a. \u8fd9\u79cd\u7ed3\u6784\u53ef\u7528\u4e8e\u5c06\u56fe\u7247\u751f\u6210\u6587\u5b57\u4efb\u52a1\u7b49. N vs M - RNN: \u8fd9\u662f\u4e00\u79cd\u4e0d\u9650\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u7684RNN\u7ed3\u6784, \u5b83\u7531\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e24\u90e8\u5206\u7ec4\u6210, \u4e24\u8005\u7684\u5185\u90e8\u7ed3\u6784\u90fd\u662f\u67d0\u7c7bRNN, \u5b83\u4e5f\u88ab\u79f0\u4e3aseq2seq\u67b6\u6784. \u8f93\u5165\u6570\u636e\u9996\u5148\u901a\u8fc7\u7f16\u7801\u5668, \u6700\u7ec8\u8f93\u51fa\u4e00\u4e2a\u9690\u542b\u53d8\u91cfc, \u4e4b\u540e\u6700\u5e38\u7528\u7684\u505a\u6cd5\u662f\u4f7f\u7528\u8fd9\u4e2a\u9690\u542b\u53d8\u91cfc\u4f5c\u7528\u5728\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u4e0a, \u4ee5\u4fdd\u8bc1\u8f93\u5165\u4fe1\u606f\u88ab\u6709\u6548\u5229\u7528. seq2seq\u67b6\u6784\u6700\u65e9\u88ab\u63d0\u51fa\u5e94\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1, \u56e0\u4e3a\u5176\u8f93\u5165\u8f93\u51fa\u4e0d\u53d7\u9650\u5236\uff0c\u5982\u4eca\u4e5f\u662f\u5e94\u7528\u6700\u5e7f\u7684RNN\u6a21\u578b\u7ed3\u6784. \u5728\u673a\u5668\u7ffb\u8bd1, \u9605\u8bfb\u7406\u89e3, \u6587\u672c\u6458\u8981\u7b49\u4f17\u591a\u9886\u57df\u90fd\u8fdb\u884c\u4e86\u975e\u5e38\u591a\u7684\u5e94\u7528\u5b9e\u8df5. \u5173\u4e8eRNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b\u7684\u5185\u5bb9\u6211\u4eec\u5c06\u5728\u540e\u9762\u4f7f\u7528\u5355\u72ec\u7684\u5c0f\u8282\u8be6\u7ec6\u8bb2\u89e3. 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fRNN\u6a21\u578b: RNN(Recurrent Neural Network), \u4e2d\u6587\u79f0\u4f5c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc, \u5b83\u4e00\u822c\u4ee5\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u901a\u8fc7\u7f51\u7edc\u5185\u90e8\u7684\u7ed3\u6784\u8bbe\u8ba1\u6709\u6548\u6355\u6349\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\u7279\u5f81, \u4e00\u822c\u4e5f\u662f\u4ee5\u5e8f\u5217\u5f62\u5f0f\u8fdb\u884c\u8f93\u51fa. RNN\u7684\u5faa\u73af\u673a\u5236\u4f7f\u6a21\u578b\u9690\u5c42\u4e0a\u4e00\u65f6\u95f4\u6b65\u4ea7\u751f\u7684\u7ed3\u679c, \u80fd\u591f\u4f5c\u4e3a\u5f53\u4e0b\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206(\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u9664\u4e86\u6b63\u5e38\u7684\u8f93\u5165\u5916\u8fd8\u5305\u62ec\u4e0a\u4e00\u6b65\u7684\u9690\u5c42\u8f93\u51fa)\u5bf9\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u4ea7\u751f\u5f71\u54cd. \u5b66\u4e60\u4e86RNN\u6a21\u578b\u7684\u4f5c\u7528: \u56e0\u4e3aRNN\u7ed3\u6784\u80fd\u591f\u5f88\u597d\u5229\u7528\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb, \u56e0\u6b64\u9488\u5bf9\u81ea\u7136\u754c\u5177\u6709\u8fde\u7eed\u6027\u7684\u8f93\u5165\u5e8f\u5217, \u5982\u4eba\u7c7b\u7684\u8bed\u8a00, \u8bed\u97f3\u7b49\u8fdb\u884c\u5f88\u597d\u7684\u5904\u7406, \u5e7f\u6cdb\u5e94\u7528\u4e8eNLP\u9886\u57df\u7684\u5404\u9879\u4efb\u52a1, \u5982\u6587\u672c\u5206\u7c7b, \u60c5\u611f\u5206\u6790, \u610f\u56fe\u8bc6\u522b, \u673a\u5668\u7ffb\u8bd1\u7b49. \u4ee5\u4e00\u4e2a\u7528\u6237\u610f\u56fe\u8bc6\u522b\u7684\u4f8b\u5b50\u5bf9RNN\u7684\u8fd0\u884c\u8fc7\u7a0b\u8fdb\u884c\u7b80\u5355\u7684\u5206\u6790: \u7b2c\u4e00\u6b65: \u7528\u6237\u8f93\u5165\u4e86\"What time is it ?\", \u6211\u4eec\u9996\u5148\u9700\u8981\u5bf9\u5b83\u8fdb\u884c\u57fa\u672c\u7684\u5206\u8bcd, \u56e0\u4e3aRNN\u662f\u6309\u7167\u987a\u5e8f\u5de5\u4f5c\u7684, \u6bcf\u6b21\u53ea\u63a5\u6536\u4e00\u4e2a\u5355\u8bcd\u8fdb\u884c\u5904\u7406. \u7b2c\u4e8c\u6b65: \u9996\u5148\u5c06\u5355\u8bcd\"What\"\u8f93\u9001\u7ed9RNN, \u5b83\u5c06\u4ea7\u751f\u4e00\u4e2a\u8f93\u51faO1. \u7b2c\u4e09\u6b65: \u7ee7\u7eed\u5c06\u5355\u8bcd\"time\"\u8f93\u9001\u7ed9RNN, \u4f46\u6b64\u65f6RNN\u4e0d\u4ec5\u4ec5\u5229\u7528\"time\"\u6765\u4ea7\u751f\u8f93\u51faO2, \u8fd8\u4f1a\u4f7f\u7528\u6765\u81ea\u4e0a\u4e00\u5c42\u9690\u5c42\u8f93\u51faO1\u4f5c\u4e3a\u8f93\u5165\u4fe1\u606f. \u7b2c\u56db\u6b65: \u91cd\u590d\u8fd9\u6837\u7684\u6b65\u9aa4, \u76f4\u5230\u5904\u7406\u5b8c\u6240\u6709\u7684\u5355\u8bcd. \u7b2c\u4e94\u6b65: \u6700\u540e\uff0c\u5c06\u6700\u7ec8\u7684\u9690\u5c42\u8f93\u51faO5\u8fdb\u884c\u5904\u7406\u6765\u89e3\u6790\u7528\u6237\u610f\u56fe. \u5b66\u4e60\u4e86RNN\u6a21\u578b\u7684\u5206\u7c7b: \u8fd9\u91cc\u6211\u4eec\u5c06\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5bf9RNN\u6a21\u578b\u8fdb\u884c\u5206\u7c7b. \u7b2c\u4e00\u4e2a\u89d2\u5ea6\u662f\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784, \u7b2c\u4e8c\u4e2a\u89d2\u5ea6\u662fRNN\u7684\u5185\u90e8\u6784\u9020. \u6309\u7167\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784\u8fdb\u884c\u5206\u7c7b: N vs N - RNN N vs 1 - RNN 1 vs N - RNN N vs M - RNN N vs N - RNN: \u5b83\u662fRNN\u6700\u57fa\u7840\u7684\u7ed3\u6784\u5f62\u5f0f, \u6700\u5927\u7684\u7279\u70b9\u5c31\u662f: \u8f93\u5165\u548c\u8f93\u51fa\u5e8f\u5217\u662f\u7b49\u957f\u7684. \u7531\u4e8e\u8fd9\u4e2a\u9650\u5236\u7684\u5b58\u5728, \u4f7f\u5176\u9002\u7528\u8303\u56f4\u6bd4\u8f83\u5c0f, \u53ef\u7528\u4e8e\u751f\u6210\u7b49\u957f\u5ea6\u7684\u5408\u8f99\u8bd7\u53e5. N vs 1 - RNN: \u6709\u65f6\u5019\u6211\u4eec\u8981\u5904\u7406\u7684\u95ee\u9898\u8f93\u5165\u662f\u4e00\u4e2a\u5e8f\u5217\uff0c\u800c\u8981\u6c42\u8f93\u51fa\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u503c\u800c\u4e0d\u662f\u5e8f\u5217\uff0c\u5e94\u8be5\u600e\u6837\u5efa\u6a21\u5462\uff1f\u6211\u4eec\u53ea\u8981\u5728\u6700\u540e\u4e00\u4e2a\u9690\u5c42\u8f93\u51fah\u4e0a\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u5c31\u53ef\u4ee5\u4e86\uff0c\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u660e\u786e\u7ed3\u679c, \u8fd8\u8981\u4f7f\u7528sigmoid\u6216\u8005softmax\u8fdb\u884c\u5904\u7406. \u8fd9\u79cd\u7ed3\u6784\u7ecf\u5e38\u88ab\u5e94\u7528\u5728\u6587\u672c\u5206\u7c7b\u95ee\u9898\u4e0a. 1 vs N - RNN: \u5982\u679c\u8f93\u5165\u4e0d\u662f\u5e8f\u5217\u800c\u8f93\u51fa\u4e3a\u5e8f\u5217\u7684\u60c5\u51b5\u600e\u4e48\u5904\u7406\u5462\uff1f\u6211\u4eec\u6700\u5e38\u91c7\u7528\u7684\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u4f7f\u8be5\u8f93\u5165\u4f5c\u7528\u4e8e\u6bcf\u6b21\u7684\u8f93\u51fa\u4e4b\u4e0a. \u8fd9\u79cd\u7ed3\u6784\u53ef\u7528\u4e8e\u5c06\u56fe\u7247\u751f\u6210\u6587\u5b57\u4efb\u52a1\u7b49. N vs M - RNN: \u8fd9\u662f\u4e00\u79cd\u4e0d\u9650\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u7684RNN\u7ed3\u6784, \u5b83\u7531\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e24\u90e8\u5206\u7ec4\u6210, \u4e24\u8005\u7684\u5185\u90e8\u7ed3\u6784\u90fd\u662f\u67d0\u7c7bRNN, \u5b83\u4e5f\u88ab\u79f0\u4e3aseq2seq\u67b6\u6784. \u8f93\u5165\u6570\u636e\u9996\u5148\u901a\u8fc7\u7f16\u7801\u5668, \u6700\u7ec8\u8f93\u51fa\u4e00\u4e2a\u9690\u542b\u53d8\u91cfc, \u4e4b\u540e\u6700\u5e38\u7528\u7684\u505a\u6cd5\u662f\u4f7f\u7528\u8fd9\u4e2a\u9690\u542b\u53d8\u91cfc\u4f5c\u7528\u5728\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u4e0a, \u4ee5\u4fdd\u8bc1\u8f93\u5165\u4fe1\u606f\u88ab\u6709\u6548\u5229\u7528. seq2seq\u67b6\u6784\u6700\u65e9\u88ab\u63d0\u51fa\u5e94\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1, \u56e0\u4e3a\u5176\u8f93\u5165\u8f93\u51fa\u4e0d\u53d7\u9650\u5236\uff0c\u5982\u4eca\u4e5f\u662f\u5e94\u7528\u6700\u5e7f\u7684RNN\u6a21\u578b\u7ed3\u6784. \u5728\u673a\u5668\u7ffb\u8bd1, \u9605\u8bfb\u7406\u89e3, \u6587\u672c\u6458\u8981\u7b49\u4f17\u591a\u9886\u57df\u90fd\u8fdb\u884c\u4e86\u975e\u5e38\u591a\u7684\u5e94\u7528\u5b9e\u8df5. \u6309\u7167RNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b: \u4f20\u7edfRNN LSTM Bi-LSTM GRU Bi-GRU \u5173\u4e8eRNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b\u7684\u5185\u5bb9\u6211\u4eec\u5c06\u5728\u540e\u9762\u4f7f\u7528\u5355\u72ec\u7684\u5c0f\u8282\u8be6\u7ec6\u8bb2\u89e3.","title":"1 \u8ba4\u8bc6RNN\u6a21\u578b"},{"location":"03_mkdocs_RNN/1%20%E8%AE%A4%E8%AF%86RNN%E6%A8%A1%E5%9E%8B.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fRNN\u6a21\u578b. \u4e86\u89e3RNN\u6a21\u578b\u7684\u4f5c\u7528. \u4e86\u89e3RNN\u6a21\u578b\u7684\u5206\u7c7b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/1%20%E8%AE%A4%E8%AF%86RNN%E6%A8%A1%E5%9E%8B.html#1-rnn","text":"RNN(Recurrent Neural Network), \u4e2d\u6587\u79f0\u4f5c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc, \u5b83\u4e00\u822c\u4ee5\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u901a\u8fc7\u7f51\u7edc\u5185\u90e8\u7684\u7ed3\u6784\u8bbe\u8ba1\u6709\u6548\u6355\u6349\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\u7279\u5f81, \u4e00\u822c\u4e5f\u662f\u4ee5\u5e8f\u5217\u5f62\u5f0f\u8fdb\u884c\u8f93\u51fa. \u4e00\u822c\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784: RNN\u5355\u5c42\u7f51\u7edc\u7ed3\u6784: \u4ee5\u65f6\u95f4\u6b65\u5bf9RNN\u8fdb\u884c\u5c55\u5f00\u540e\u7684\u5355\u5c42\u7f51\u7edc\u7ed3\u6784: RNN\u7684\u5faa\u73af\u673a\u5236\u4f7f\u6a21\u578b\u9690\u5c42\u4e0a\u4e00\u65f6\u95f4\u6b65\u4ea7\u751f\u7684\u7ed3\u679c, \u80fd\u591f\u4f5c\u4e3a\u5f53\u4e0b\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206(\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u9664\u4e86\u6b63\u5e38\u7684\u8f93\u5165\u5916\u8fd8\u5305\u62ec\u4e0a\u4e00\u6b65\u7684\u9690\u5c42\u8f93\u51fa)\u5bf9\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u4ea7\u751f\u5f71\u54cd.","title":"1 \u4ec0\u4e48\u662fRNN\u6a21\u578b"},{"location":"03_mkdocs_RNN/1%20%E8%AE%A4%E8%AF%86RNN%E6%A8%A1%E5%9E%8B.html#2-rnn","text":"\u56e0\u4e3aRNN\u7ed3\u6784\u80fd\u591f\u5f88\u597d\u5229\u7528\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb, \u56e0\u6b64\u9488\u5bf9\u81ea\u7136\u754c\u5177\u6709\u8fde\u7eed\u6027\u7684\u8f93\u5165\u5e8f\u5217, \u5982\u4eba\u7c7b\u7684\u8bed\u8a00, \u8bed\u97f3\u7b49\u8fdb\u884c\u5f88\u597d\u7684\u5904\u7406, \u5e7f\u6cdb\u5e94\u7528\u4e8eNLP\u9886\u57df\u7684\u5404\u9879\u4efb\u52a1, \u5982\u6587\u672c\u5206\u7c7b, \u60c5\u611f\u5206\u6790, \u610f\u56fe\u8bc6\u522b, \u673a\u5668\u7ffb\u8bd1\u7b49. \u4e0b\u9762\u6211\u4eec\u5c06\u4ee5\u4e00\u4e2a\u7528\u6237\u610f\u56fe\u8bc6\u522b\u7684\u4f8b\u5b50\u8fdb\u884c\u7b80\u5355\u7684\u5206\u6790: \u7b2c\u4e00\u6b65: \u7528\u6237\u8f93\u5165\u4e86\"What time is it ?\", \u6211\u4eec\u9996\u5148\u9700\u8981\u5bf9\u5b83\u8fdb\u884c\u57fa\u672c\u7684\u5206\u8bcd, \u56e0\u4e3aRNN\u662f\u6309\u7167\u987a\u5e8f\u5de5\u4f5c\u7684, \u6bcf\u6b21\u53ea\u63a5\u6536\u4e00\u4e2a\u5355\u8bcd\u8fdb\u884c\u5904\u7406. \u7b2c\u4e8c\u6b65: \u9996\u5148\u5c06\u5355\u8bcd\"What\"\u8f93\u9001\u7ed9RNN, \u5b83\u5c06\u4ea7\u751f\u4e00\u4e2a\u8f93\u51faO1. \u7b2c\u4e09\u6b65: \u7ee7\u7eed\u5c06\u5355\u8bcd\"time\"\u8f93\u9001\u7ed9RNN, \u4f46\u6b64\u65f6RNN\u4e0d\u4ec5\u4ec5\u5229\u7528\"time\"\u6765\u4ea7\u751f\u8f93\u51faO2, \u8fd8\u4f1a\u4f7f\u7528\u6765\u81ea\u4e0a\u4e00\u5c42\u9690\u5c42\u8f93\u51faO1\u4f5c\u4e3a\u8f93\u5165\u4fe1\u606f. \u7b2c\u56db\u6b65: \u91cd\u590d\u8fd9\u6837\u7684\u6b65\u9aa4, \u76f4\u5230\u5904\u7406\u5b8c\u6240\u6709\u7684\u5355\u8bcd. \u7b2c\u4e94\u6b65: \u6700\u540e\uff0c\u5c06\u6700\u7ec8\u7684\u9690\u5c42\u8f93\u51faO5\u8fdb\u884c\u5904\u7406\u6765\u89e3\u6790\u7528\u6237\u610f\u56fe.","title":"2 RNN\u6a21\u578b\u7684\u4f5c\u7528"},{"location":"03_mkdocs_RNN/1%20%E8%AE%A4%E8%AF%86RNN%E6%A8%A1%E5%9E%8B.html#3-rnn","text":"\u8fd9\u91cc\u6211\u4eec\u5c06\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5bf9RNN\u6a21\u578b\u8fdb\u884c\u5206\u7c7b. \u7b2c\u4e00\u4e2a\u89d2\u5ea6\u662f\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784, \u7b2c\u4e8c\u4e2a\u89d2\u5ea6\u662fRNN\u7684\u5185\u90e8\u6784\u9020. \u6309\u7167\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784\u8fdb\u884c\u5206\u7c7b: N vs N - RNN N vs 1 - RNN 1 vs N - RNN N vs M - RNN \u6309\u7167RNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b: \u4f20\u7edfRNN LSTM Bi-LSTM GRU Bi-GRU N vs N - RNN: \u5b83\u662fRNN\u6700\u57fa\u7840\u7684\u7ed3\u6784\u5f62\u5f0f, \u6700\u5927\u7684\u7279\u70b9\u5c31\u662f: \u8f93\u5165\u548c\u8f93\u51fa\u5e8f\u5217\u662f\u7b49\u957f\u7684. \u7531\u4e8e\u8fd9\u4e2a\u9650\u5236\u7684\u5b58\u5728, \u4f7f\u5176\u9002\u7528\u8303\u56f4\u6bd4\u8f83\u5c0f, \u53ef\u7528\u4e8e\u751f\u6210\u7b49\u957f\u5ea6\u7684\u5408\u8f99\u8bd7\u53e5. N vs 1 - RNN: \u6709\u65f6\u5019\u6211\u4eec\u8981\u5904\u7406\u7684\u95ee\u9898\u8f93\u5165\u662f\u4e00\u4e2a\u5e8f\u5217\uff0c\u800c\u8981\u6c42\u8f93\u51fa\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u503c\u800c\u4e0d\u662f\u5e8f\u5217\uff0c\u5e94\u8be5\u600e\u6837\u5efa\u6a21\u5462\uff1f\u6211\u4eec\u53ea\u8981\u5728\u6700\u540e\u4e00\u4e2a\u9690\u5c42\u8f93\u51fah\u4e0a\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u5c31\u53ef\u4ee5\u4e86\uff0c\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u660e\u786e\u7ed3\u679c, \u8fd8\u8981\u4f7f\u7528sigmoid\u6216\u8005softmax\u8fdb\u884c\u5904\u7406. \u8fd9\u79cd\u7ed3\u6784\u7ecf\u5e38\u88ab\u5e94\u7528\u5728\u6587\u672c\u5206\u7c7b\u95ee\u9898\u4e0a. 1 vs N - RNN: \u5982\u679c\u8f93\u5165\u4e0d\u662f\u5e8f\u5217\u800c\u8f93\u51fa\u4e3a\u5e8f\u5217\u7684\u60c5\u51b5\u600e\u4e48\u5904\u7406\u5462\uff1f\u6211\u4eec\u6700\u5e38\u91c7\u7528\u7684\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u4f7f\u8be5\u8f93\u5165\u4f5c\u7528\u4e8e\u6bcf\u6b21\u7684\u8f93\u51fa\u4e4b\u4e0a. \u8fd9\u79cd\u7ed3\u6784\u53ef\u7528\u4e8e\u5c06\u56fe\u7247\u751f\u6210\u6587\u5b57\u4efb\u52a1\u7b49. N vs M - RNN: \u8fd9\u662f\u4e00\u79cd\u4e0d\u9650\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u7684RNN\u7ed3\u6784, \u5b83\u7531\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e24\u90e8\u5206\u7ec4\u6210, \u4e24\u8005\u7684\u5185\u90e8\u7ed3\u6784\u90fd\u662f\u67d0\u7c7bRNN, \u5b83\u4e5f\u88ab\u79f0\u4e3aseq2seq\u67b6\u6784. \u8f93\u5165\u6570\u636e\u9996\u5148\u901a\u8fc7\u7f16\u7801\u5668, \u6700\u7ec8\u8f93\u51fa\u4e00\u4e2a\u9690\u542b\u53d8\u91cfc, \u4e4b\u540e\u6700\u5e38\u7528\u7684\u505a\u6cd5\u662f\u4f7f\u7528\u8fd9\u4e2a\u9690\u542b\u53d8\u91cfc\u4f5c\u7528\u5728\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u4e0a, \u4ee5\u4fdd\u8bc1\u8f93\u5165\u4fe1\u606f\u88ab\u6709\u6548\u5229\u7528. seq2seq\u67b6\u6784\u6700\u65e9\u88ab\u63d0\u51fa\u5e94\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1, \u56e0\u4e3a\u5176\u8f93\u5165\u8f93\u51fa\u4e0d\u53d7\u9650\u5236\uff0c\u5982\u4eca\u4e5f\u662f\u5e94\u7528\u6700\u5e7f\u7684RNN\u6a21\u578b\u7ed3\u6784. \u5728\u673a\u5668\u7ffb\u8bd1, \u9605\u8bfb\u7406\u89e3, \u6587\u672c\u6458\u8981\u7b49\u4f17\u591a\u9886\u57df\u90fd\u8fdb\u884c\u4e86\u975e\u5e38\u591a\u7684\u5e94\u7528\u5b9e\u8df5. \u5173\u4e8eRNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b\u7684\u5185\u5bb9\u6211\u4eec\u5c06\u5728\u540e\u9762\u4f7f\u7528\u5355\u72ec\u7684\u5c0f\u8282\u8be6\u7ec6\u8bb2\u89e3.","title":"3 RNN\u6a21\u578b\u7684\u5206\u7c7b"},{"location":"03_mkdocs_RNN/1%20%E8%AE%A4%E8%AF%86RNN%E6%A8%A1%E5%9E%8B.html#4","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fRNN\u6a21\u578b: RNN(Recurrent Neural Network), \u4e2d\u6587\u79f0\u4f5c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc, \u5b83\u4e00\u822c\u4ee5\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u901a\u8fc7\u7f51\u7edc\u5185\u90e8\u7684\u7ed3\u6784\u8bbe\u8ba1\u6709\u6548\u6355\u6349\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\u7279\u5f81, \u4e00\u822c\u4e5f\u662f\u4ee5\u5e8f\u5217\u5f62\u5f0f\u8fdb\u884c\u8f93\u51fa. RNN\u7684\u5faa\u73af\u673a\u5236\u4f7f\u6a21\u578b\u9690\u5c42\u4e0a\u4e00\u65f6\u95f4\u6b65\u4ea7\u751f\u7684\u7ed3\u679c, \u80fd\u591f\u4f5c\u4e3a\u5f53\u4e0b\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206(\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u9664\u4e86\u6b63\u5e38\u7684\u8f93\u5165\u5916\u8fd8\u5305\u62ec\u4e0a\u4e00\u6b65\u7684\u9690\u5c42\u8f93\u51fa)\u5bf9\u5f53\u4e0b\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u4ea7\u751f\u5f71\u54cd. \u5b66\u4e60\u4e86RNN\u6a21\u578b\u7684\u4f5c\u7528: \u56e0\u4e3aRNN\u7ed3\u6784\u80fd\u591f\u5f88\u597d\u5229\u7528\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb, \u56e0\u6b64\u9488\u5bf9\u81ea\u7136\u754c\u5177\u6709\u8fde\u7eed\u6027\u7684\u8f93\u5165\u5e8f\u5217, \u5982\u4eba\u7c7b\u7684\u8bed\u8a00, \u8bed\u97f3\u7b49\u8fdb\u884c\u5f88\u597d\u7684\u5904\u7406, \u5e7f\u6cdb\u5e94\u7528\u4e8eNLP\u9886\u57df\u7684\u5404\u9879\u4efb\u52a1, \u5982\u6587\u672c\u5206\u7c7b, \u60c5\u611f\u5206\u6790, \u610f\u56fe\u8bc6\u522b, \u673a\u5668\u7ffb\u8bd1\u7b49. \u4ee5\u4e00\u4e2a\u7528\u6237\u610f\u56fe\u8bc6\u522b\u7684\u4f8b\u5b50\u5bf9RNN\u7684\u8fd0\u884c\u8fc7\u7a0b\u8fdb\u884c\u7b80\u5355\u7684\u5206\u6790: \u7b2c\u4e00\u6b65: \u7528\u6237\u8f93\u5165\u4e86\"What time is it ?\", \u6211\u4eec\u9996\u5148\u9700\u8981\u5bf9\u5b83\u8fdb\u884c\u57fa\u672c\u7684\u5206\u8bcd, \u56e0\u4e3aRNN\u662f\u6309\u7167\u987a\u5e8f\u5de5\u4f5c\u7684, \u6bcf\u6b21\u53ea\u63a5\u6536\u4e00\u4e2a\u5355\u8bcd\u8fdb\u884c\u5904\u7406. \u7b2c\u4e8c\u6b65: \u9996\u5148\u5c06\u5355\u8bcd\"What\"\u8f93\u9001\u7ed9RNN, \u5b83\u5c06\u4ea7\u751f\u4e00\u4e2a\u8f93\u51faO1. \u7b2c\u4e09\u6b65: \u7ee7\u7eed\u5c06\u5355\u8bcd\"time\"\u8f93\u9001\u7ed9RNN, \u4f46\u6b64\u65f6RNN\u4e0d\u4ec5\u4ec5\u5229\u7528\"time\"\u6765\u4ea7\u751f\u8f93\u51faO2, \u8fd8\u4f1a\u4f7f\u7528\u6765\u81ea\u4e0a\u4e00\u5c42\u9690\u5c42\u8f93\u51faO1\u4f5c\u4e3a\u8f93\u5165\u4fe1\u606f. \u7b2c\u56db\u6b65: \u91cd\u590d\u8fd9\u6837\u7684\u6b65\u9aa4, \u76f4\u5230\u5904\u7406\u5b8c\u6240\u6709\u7684\u5355\u8bcd. \u7b2c\u4e94\u6b65: \u6700\u540e\uff0c\u5c06\u6700\u7ec8\u7684\u9690\u5c42\u8f93\u51faO5\u8fdb\u884c\u5904\u7406\u6765\u89e3\u6790\u7528\u6237\u610f\u56fe. \u5b66\u4e60\u4e86RNN\u6a21\u578b\u7684\u5206\u7c7b: \u8fd9\u91cc\u6211\u4eec\u5c06\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5bf9RNN\u6a21\u578b\u8fdb\u884c\u5206\u7c7b. \u7b2c\u4e00\u4e2a\u89d2\u5ea6\u662f\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784, \u7b2c\u4e8c\u4e2a\u89d2\u5ea6\u662fRNN\u7684\u5185\u90e8\u6784\u9020. \u6309\u7167\u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ed3\u6784\u8fdb\u884c\u5206\u7c7b: N vs N - RNN N vs 1 - RNN 1 vs N - RNN N vs M - RNN N vs N - RNN: \u5b83\u662fRNN\u6700\u57fa\u7840\u7684\u7ed3\u6784\u5f62\u5f0f, \u6700\u5927\u7684\u7279\u70b9\u5c31\u662f: \u8f93\u5165\u548c\u8f93\u51fa\u5e8f\u5217\u662f\u7b49\u957f\u7684. \u7531\u4e8e\u8fd9\u4e2a\u9650\u5236\u7684\u5b58\u5728, \u4f7f\u5176\u9002\u7528\u8303\u56f4\u6bd4\u8f83\u5c0f, \u53ef\u7528\u4e8e\u751f\u6210\u7b49\u957f\u5ea6\u7684\u5408\u8f99\u8bd7\u53e5. N vs 1 - RNN: \u6709\u65f6\u5019\u6211\u4eec\u8981\u5904\u7406\u7684\u95ee\u9898\u8f93\u5165\u662f\u4e00\u4e2a\u5e8f\u5217\uff0c\u800c\u8981\u6c42\u8f93\u51fa\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u503c\u800c\u4e0d\u662f\u5e8f\u5217\uff0c\u5e94\u8be5\u600e\u6837\u5efa\u6a21\u5462\uff1f\u6211\u4eec\u53ea\u8981\u5728\u6700\u540e\u4e00\u4e2a\u9690\u5c42\u8f93\u51fah\u4e0a\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u5c31\u53ef\u4ee5\u4e86\uff0c\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u660e\u786e\u7ed3\u679c, \u8fd8\u8981\u4f7f\u7528sigmoid\u6216\u8005softmax\u8fdb\u884c\u5904\u7406. \u8fd9\u79cd\u7ed3\u6784\u7ecf\u5e38\u88ab\u5e94\u7528\u5728\u6587\u672c\u5206\u7c7b\u95ee\u9898\u4e0a. 1 vs N - RNN: \u5982\u679c\u8f93\u5165\u4e0d\u662f\u5e8f\u5217\u800c\u8f93\u51fa\u4e3a\u5e8f\u5217\u7684\u60c5\u51b5\u600e\u4e48\u5904\u7406\u5462\uff1f\u6211\u4eec\u6700\u5e38\u91c7\u7528\u7684\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u4f7f\u8be5\u8f93\u5165\u4f5c\u7528\u4e8e\u6bcf\u6b21\u7684\u8f93\u51fa\u4e4b\u4e0a. \u8fd9\u79cd\u7ed3\u6784\u53ef\u7528\u4e8e\u5c06\u56fe\u7247\u751f\u6210\u6587\u5b57\u4efb\u52a1\u7b49. N vs M - RNN: \u8fd9\u662f\u4e00\u79cd\u4e0d\u9650\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u7684RNN\u7ed3\u6784, \u5b83\u7531\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e24\u90e8\u5206\u7ec4\u6210, \u4e24\u8005\u7684\u5185\u90e8\u7ed3\u6784\u90fd\u662f\u67d0\u7c7bRNN, \u5b83\u4e5f\u88ab\u79f0\u4e3aseq2seq\u67b6\u6784. \u8f93\u5165\u6570\u636e\u9996\u5148\u901a\u8fc7\u7f16\u7801\u5668, \u6700\u7ec8\u8f93\u51fa\u4e00\u4e2a\u9690\u542b\u53d8\u91cfc, \u4e4b\u540e\u6700\u5e38\u7528\u7684\u505a\u6cd5\u662f\u4f7f\u7528\u8fd9\u4e2a\u9690\u542b\u53d8\u91cfc\u4f5c\u7528\u5728\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u4e0a, \u4ee5\u4fdd\u8bc1\u8f93\u5165\u4fe1\u606f\u88ab\u6709\u6548\u5229\u7528. seq2seq\u67b6\u6784\u6700\u65e9\u88ab\u63d0\u51fa\u5e94\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1, \u56e0\u4e3a\u5176\u8f93\u5165\u8f93\u51fa\u4e0d\u53d7\u9650\u5236\uff0c\u5982\u4eca\u4e5f\u662f\u5e94\u7528\u6700\u5e7f\u7684RNN\u6a21\u578b\u7ed3\u6784. \u5728\u673a\u5668\u7ffb\u8bd1, \u9605\u8bfb\u7406\u89e3, \u6587\u672c\u6458\u8981\u7b49\u4f17\u591a\u9886\u57df\u90fd\u8fdb\u884c\u4e86\u975e\u5e38\u591a\u7684\u5e94\u7528\u5b9e\u8df5. \u6309\u7167RNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b: \u4f20\u7edfRNN LSTM Bi-LSTM GRU Bi-GRU \u5173\u4e8eRNN\u7684\u5185\u90e8\u6784\u9020\u8fdb\u884c\u5206\u7c7b\u7684\u5185\u5bb9\u6211\u4eec\u5c06\u5728\u540e\u9762\u4f7f\u7528\u5355\u72ec\u7684\u5c0f\u8282\u8be6\u7ec6\u8bb2\u89e3.","title":"4 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u53ef\u4ee5\u4f7f\u7528pytorch\u5b9e\u73b0\u6ce8\u610f\u529b\u673a\u5236 \u77e5\u9053\u6ce8\u610f\u529b\u673a\u5236\u7684\u5b9e\u73b0\u6d41\u7a0b 1 Self-attetion\u5b9e\u73b0\u6b65\u9aa4 \u00b6 \u8fd9\u91cc\u6211\u4eec\u5b9e\u73b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u662f\u73b0\u5728\u6bd4\u8f83\u6d41\u884c\u7684\u70b9\u79ef\u76f8\u4e58\u7684\u6ce8\u610f\u529b\u673a\u5236 self-attention\u673a\u5236\u7684\u5b9e\u73b0\u6b65\u9aa4 \u7b2c\u4e00\u6b65: \u51c6\u5907\u8f93\u5165 \u7b2c\u4e8c\u6b65: \u521d\u59cb\u5316\u53c2\u6570 \u7b2c\u4e09\u6b65: \u83b7\u53d6key\uff0cquery\u548cvalue \u7b2c\u56db\u6b65: \u7ed9input1\u8ba1\u7b97attention score \u7b2c\u4e94\u6b65: \u8ba1\u7b97softmax \u7b2c\u516d\u6b65: \u7ed9value\u4e58\u4e0ascore \u7b2c\u4e03\u6b65: \u7ed9value\u52a0\u6743\u6c42\u548c\u83b7\u53d6output1 \u7b2c\u516b\u6b65: \u91cd\u590d\u6b65\u9aa44-7\uff0c\u83b7\u53d6output2\uff0coutput3 1.1 \u51c6\u5907\u8f93\u5165 \u00b6 # \u8fd9\u91cc\u6211\u4eec\u968f\u673a\u8bbe\u7f6e\u4e09\u4e2a\u8f93\u5165, \u6bcf\u4e2a\u8f93\u5165\u7684\u7ef4\u5ea6\u662f\u4e00\u4e2a4\u7ef4\u5411\u91cf import torch x = [ [ 1 , 0 , 1 , 0 ], # Input 1 [ 0 , 2 , 0 , 2 ], # Input 2 [ 1 , 1 , 1 , 1 ] # Input 3 ] x = torch . tensor ( x , dtype = torch . float32 ) 1.2 \u521d\u59cb\u5316\u53c2\u6570 \u00b6 # \u6bcf\u4e00\u4e2a\u8f93\u5165\u90fd\u6709\u4e09\u4e2a\u8868\u793a\uff0c\u5206\u522b\u4e3akey\uff08\u6a59\u9ec4\u8272\uff09query\uff08\u7ea2\u8272\uff09value\uff08\u7d2b\u8272\uff09\u3002\u6bd4\u5982\u8bf4\uff0c\u6bcf\u4e00\u4e2a\u8868\u793a\u6211\u4eec\u5e0c\u671b\u662f\u4e00\u4e2a3\u7ef4\u7684\u5411\u91cf\u3002\u7531\u4e8e\u8f93\u5165\u662f4\u7ef4\uff0c\u6240\u4ee5\u6211\u4eec\u7684\u53c2\u6570\u77e9\u9635\u4e3a 4*3 \u7ef4\u3002 # \u4e3a\u4e86\u80fd\u591f\u83b7\u53d6\u8fd9\u4e9b\u8868\u793a\uff0c\u6bcf\u4e00\u4e2a\u8f93\u5165\uff08\u7eff\u8272\uff09\u8981\u548ckey\uff0cquery\u548cvalue\u76f8\u4e58\uff0c\u5728\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u5982\u4e0b\u7684\u65b9\u5f0f\u521d\u59cb\u5316\u8fd9\u4e9b\u53c2\u6570\u3002 w_key = [ [ 0 , 0 , 1 ], [ 1 , 1 , 0 ], [ 0 , 1 , 0 ], [ 1 , 1 , 0 ] ] w_query = [ [ 1 , 0 , 1 ], [ 1 , 0 , 0 ], [ 0 , 0 , 1 ], [ 0 , 1 , 1 ] ] w_value = [ [ 0 , 2 , 0 ], [ 0 , 3 , 0 ], [ 1 , 0 , 3 ], [ 1 , 1 , 0 ] ] w_key = torch . tensor ( w_key , dtype = torch . float32 ) w_query = torch . tensor ( w_query , dtype = torch . float32 ) w_value = torch . tensor ( w_value , dtype = torch . float32 ) print ( \"w_key: \\n \" , w_key ) print ( \"w_query: \\n \" , w_query ) print ( \"w_value: \\n \" , w_value ) \u8f93\u51fa\u6548\u679c w_key : tensor ([[ 0. , 0. , 1. ], [ 1. , 1. , 0. ], [ 0. , 1. , 0. ], [ 1. , 1. , 0. ]]) w_query : tensor ([[ 1. , 0. , 1. ], [ 1. , 0. , 0. ], [ 0. , 0. , 1. ], [ 0. , 1. , 1. ]]) w_value : tensor ([[ 0. , 2. , 0. ], [ 0. , 3. , 0. ], [ 1. , 0. , 3. ], [ 1. , 1. , 0. ]]) 1.3 \u83b7\u53d6key\uff0cquery\u548cvalue \u00b6 * \u4f7f\u7528\u5411\u91cf\u5316\u83b7\u53d6keys\u7684\u503c [ 0 , 0 , 1 ] [ 1 , 0 , 1 , 0 ] [ 1 , 1 , 0 ] [ 0 , 1 , 1 ] [ 0 , 2 , 0 , 2 ] x [ 0 , 1 , 0 ] = [ 4 , 4 , 0 ] [ 1 , 1 , 1 , 1 ] [ 1 , 1 , 0 ] [ 2 , 3 , 1 ] \u4f7f\u7528\u5411\u91cf\u5316\u83b7\u53d6values\u7684\u503c [ 0 , 2 , 0 ] [ 1 , 0 , 1 , 0 ] [ 0 , 3 , 0 ] [ 1 , 2 , 3 ] [ 0 , 2 , 0 , 2 ] x [ 1 , 0 , 3 ] = [ 2 , 8 , 0 ] [ 1 , 1 , 1 , 1 ] [ 1 , 1 , 0 ] [ 2 , 6 , 3 ] \u4f7f\u7528\u5411\u91cf\u5316\u83b7\u53d6querys\u7684\u503c [ 1 , 0 , 1 ] [ 1 , 0 , 1 , 0 ] [ 1 , 0 , 0 ] [ 1 , 0 , 2 ] [ 0 , 2 , 0 , 2 ] x [ 0 , 0 , 1 ] = [ 2 , 2 , 2 ] [ 1 , 1 , 1 , 1 ] [ 0 , 1 , 1 ] [ 2 , 1 , 3 ] # \u5c06query key value\u5206\u522b\u8fdb\u884c\u8ba1\u7b97 keys = x @ w_key querys = x @ w_query values = x @ w_value print ( \"Keys: \\n \" , keys ) print ( \"Querys: \\n \" , querys ) print ( \"Values: \\n \" , values ) \u8f93\u51fa\u6548\u679c Keys : tensor ([[ 0. , 1. , 1. ], [ 4. , 4. , 0. ], [ 2. , 3. , 1. ]]) Querys : tensor ([[ 1. , 0. , 2. ], [ 2. , 2. , 2. ], [ 2. , 1. , 3. ]]) Values : tensor ([[ 1. , 2. , 3. ], [ 2. , 8. , 0. ], [ 2. , 6. , 3. ]]) 1.4 \u7ed9input1\u8ba1\u7b97attention score \u00b6 # \u4e3a\u4e86\u83b7\u53d6input1\u7684attention score\uff0c\u6211\u4eec\u4f7f\u7528\u70b9\u4e58\u6765\u5904\u7406\u6240\u6709\u7684key\u548cquery\uff0c\u5305\u62ec\u5b83\u81ea\u5df1\u7684key\u548cvalue\u3002\u8fd9\u6837\u6211\u4eec\u5c31\u80fd\u591f\u5f97\u52303\u4e2akey\u7684\u8868\u793a\uff08\u56e0\u4e3a\u6211\u4eec\u67093\u4e2a\u8f93\u5165\uff09\uff0c\u6211\u4eec\u5c31\u83b7\u5f97\u4e863\u4e2aattention score\uff08\u84dd\u8272\uff09 [ 0 , 4 , 2 ] [ 1 , 0 , 2 ] x [ 1 , 4 , 3 ] = [ 2 , 4 , 4 ] [ 1 , 0 , 1 ] # \u6ce8\u610f: \u8fd9\u91cc\u6211\u4eec\u53ea\u7528input1\u4e3e\u4f8b.\u5176\u4ed6\u7684\u8f93\u5165\u7684query\u548cinput1\u505a\u76f8\u540c\u7684\u64cd\u4f5c. attn_scores = querys @ keys . T print ( attn_scores ) \u8f93\u51fa\u6548\u679c tensor ([[ 2. , 4. , 4. ], # attention scores from Query 1 [ 4. , 16. , 12. ], # attention scores from Query 2 [ 4. , 12. , 10. ]]) # attention scores from Query 3 1.5 \u8ba1\u7b97softmax \u00b6 \u7ed9attention score\u5e94\u7528softmax\u3002 softmax([2, 4, 4]) = [0.0, 0.5, 0.5] from torch.nn.functional import softmax attn_scores_softmax = softmax ( attn_scores , dim =- 1 ) print ( attn_scores_softmax ) attn_scores_softmax = [ [ 0.0 , 0.5 , 0.5 ], [ 0.0 , 1.0 , 0.0 ], [ 0.0 , 0.9 , 0.1 ] ] attn_scores_softmax = torch . tensor ( attn_scores_softmax ) print ( attn_scores_softmax ) \u8f93\u51fa\u6548\u679c tensor ([[ 6.3379e-02 , 4.6831e-01 , 4.6831e-01 ], [ 6.0337e-06 , 9.8201e-01 , 1.7986e-02 ], [ 2.9539e-04 , 8.8054e-01 , 1.1917e-01 ]]) tensor ([[ 0.0000 , 0.5000 , 0.5000 ], [ 0.0000 , 1.0000 , 0.0000 ], [ 0.0000 , 0.9000 , 0.1000 ]]) 1.6 \u7ed9value\u4e58\u4e0ascore \u00b6 \u4f7f\u7528\u7ecf\u8fc7softmax\u540e\u7684attention score\u4e58\u4ee5\u5b83\u5bf9\u5e94\u7684value\u503c\uff08\u7d2b\u8272\uff09\uff0c\u8fd9\u6837\u5c31\u5f97\u5230\u4e863\u4e2aweighted values\uff08\u9ec4\u8272\uff09\u3002 1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0] 2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0] 3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5] weighted_values = values [:, None ] * attn_scores_softmax . T [:,:, None ] print ( weighted_values ) \u8f93\u51fa\u6548\u679c: tensor ([[[ 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 ]], [[ 1.0000 , 4.0000 , 0.0000 ], [ 2.0000 , 8.0000 , 0.0000 ], [ 1.8000 , 7.2000 , 0.0000 ]], [[ 1.0000 , 3.0000 , 1.5000 ], [ 0.0000 , 0.0000 , 0.0000 ], [ 0.2000 , 0.6000 , 0.3000 ]]]) 1.7 \u7ed9value\u52a0\u6743\u6c42\u548c\u83b7\u53d6output1 \u00b6 \u628a\u6240\u6709\u7684weighted values\uff08\u9ec4\u8272\uff09\u8fdb\u884celement-wise\u7684\u76f8\u52a0\u3002 [0.0, 0.0, 0.0] + [1.0, 4.0, 0.0] + [1.0, 3.0, 1.5] ------------------------ = [2.0, 7.0, 1.5] \u5f97\u5230\u7ed3\u679c\u5411\u91cf[2.0, 7.0, 1.5]\uff08\u6df1\u7eff\u8272\uff09\u5c31\u662fouput1\u7684\u548c\u5176\u4ed6key\u4ea4\u4e92\u7684query representation 1.8 \u91cd\u590d\u6b65\u9aa44-7\uff0c\u83b7\u53d6output2\uff0coutput3 \u00b6 outputs = weighted_values . sum ( dim = 0 ) print ( outputs ) \u8f93\u51fa\u6548\u679c tensor ([[ 2.0000 , 7.0000 , 1.5000 ], [ 2.0000 , 8.0000 , 0.0000 ], [ 2.0000 , 7.8000 , 0.3000 ]])","title":"10 Self attention\u673a\u5236\u7684\u4ee3\u7801\u5b9e\u73b0"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#_1","text":"\u53ef\u4ee5\u4f7f\u7528pytorch\u5b9e\u73b0\u6ce8\u610f\u529b\u673a\u5236 \u77e5\u9053\u6ce8\u610f\u529b\u673a\u5236\u7684\u5b9e\u73b0\u6d41\u7a0b","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#1-self-attetion","text":"\u8fd9\u91cc\u6211\u4eec\u5b9e\u73b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u662f\u73b0\u5728\u6bd4\u8f83\u6d41\u884c\u7684\u70b9\u79ef\u76f8\u4e58\u7684\u6ce8\u610f\u529b\u673a\u5236 self-attention\u673a\u5236\u7684\u5b9e\u73b0\u6b65\u9aa4 \u7b2c\u4e00\u6b65: \u51c6\u5907\u8f93\u5165 \u7b2c\u4e8c\u6b65: \u521d\u59cb\u5316\u53c2\u6570 \u7b2c\u4e09\u6b65: \u83b7\u53d6key\uff0cquery\u548cvalue \u7b2c\u56db\u6b65: \u7ed9input1\u8ba1\u7b97attention score \u7b2c\u4e94\u6b65: \u8ba1\u7b97softmax \u7b2c\u516d\u6b65: \u7ed9value\u4e58\u4e0ascore \u7b2c\u4e03\u6b65: \u7ed9value\u52a0\u6743\u6c42\u548c\u83b7\u53d6output1 \u7b2c\u516b\u6b65: \u91cd\u590d\u6b65\u9aa44-7\uff0c\u83b7\u53d6output2\uff0coutput3","title":"1 Self-attetion\u5b9e\u73b0\u6b65\u9aa4"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#11","text":"# \u8fd9\u91cc\u6211\u4eec\u968f\u673a\u8bbe\u7f6e\u4e09\u4e2a\u8f93\u5165, \u6bcf\u4e2a\u8f93\u5165\u7684\u7ef4\u5ea6\u662f\u4e00\u4e2a4\u7ef4\u5411\u91cf import torch x = [ [ 1 , 0 , 1 , 0 ], # Input 1 [ 0 , 2 , 0 , 2 ], # Input 2 [ 1 , 1 , 1 , 1 ] # Input 3 ] x = torch . tensor ( x , dtype = torch . float32 )","title":"1.1 \u51c6\u5907\u8f93\u5165"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#12","text":"# \u6bcf\u4e00\u4e2a\u8f93\u5165\u90fd\u6709\u4e09\u4e2a\u8868\u793a\uff0c\u5206\u522b\u4e3akey\uff08\u6a59\u9ec4\u8272\uff09query\uff08\u7ea2\u8272\uff09value\uff08\u7d2b\u8272\uff09\u3002\u6bd4\u5982\u8bf4\uff0c\u6bcf\u4e00\u4e2a\u8868\u793a\u6211\u4eec\u5e0c\u671b\u662f\u4e00\u4e2a3\u7ef4\u7684\u5411\u91cf\u3002\u7531\u4e8e\u8f93\u5165\u662f4\u7ef4\uff0c\u6240\u4ee5\u6211\u4eec\u7684\u53c2\u6570\u77e9\u9635\u4e3a 4*3 \u7ef4\u3002 # \u4e3a\u4e86\u80fd\u591f\u83b7\u53d6\u8fd9\u4e9b\u8868\u793a\uff0c\u6bcf\u4e00\u4e2a\u8f93\u5165\uff08\u7eff\u8272\uff09\u8981\u548ckey\uff0cquery\u548cvalue\u76f8\u4e58\uff0c\u5728\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u5982\u4e0b\u7684\u65b9\u5f0f\u521d\u59cb\u5316\u8fd9\u4e9b\u53c2\u6570\u3002 w_key = [ [ 0 , 0 , 1 ], [ 1 , 1 , 0 ], [ 0 , 1 , 0 ], [ 1 , 1 , 0 ] ] w_query = [ [ 1 , 0 , 1 ], [ 1 , 0 , 0 ], [ 0 , 0 , 1 ], [ 0 , 1 , 1 ] ] w_value = [ [ 0 , 2 , 0 ], [ 0 , 3 , 0 ], [ 1 , 0 , 3 ], [ 1 , 1 , 0 ] ] w_key = torch . tensor ( w_key , dtype = torch . float32 ) w_query = torch . tensor ( w_query , dtype = torch . float32 ) w_value = torch . tensor ( w_value , dtype = torch . float32 ) print ( \"w_key: \\n \" , w_key ) print ( \"w_query: \\n \" , w_query ) print ( \"w_value: \\n \" , w_value ) \u8f93\u51fa\u6548\u679c w_key : tensor ([[ 0. , 0. , 1. ], [ 1. , 1. , 0. ], [ 0. , 1. , 0. ], [ 1. , 1. , 0. ]]) w_query : tensor ([[ 1. , 0. , 1. ], [ 1. , 0. , 0. ], [ 0. , 0. , 1. ], [ 0. , 1. , 1. ]]) w_value : tensor ([[ 0. , 2. , 0. ], [ 0. , 3. , 0. ], [ 1. , 0. , 3. ], [ 1. , 1. , 0. ]])","title":"1.2 \u521d\u59cb\u5316\u53c2\u6570"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#13-keyqueryvalue","text":"* \u4f7f\u7528\u5411\u91cf\u5316\u83b7\u53d6keys\u7684\u503c [ 0 , 0 , 1 ] [ 1 , 0 , 1 , 0 ] [ 1 , 1 , 0 ] [ 0 , 1 , 1 ] [ 0 , 2 , 0 , 2 ] x [ 0 , 1 , 0 ] = [ 4 , 4 , 0 ] [ 1 , 1 , 1 , 1 ] [ 1 , 1 , 0 ] [ 2 , 3 , 1 ] \u4f7f\u7528\u5411\u91cf\u5316\u83b7\u53d6values\u7684\u503c [ 0 , 2 , 0 ] [ 1 , 0 , 1 , 0 ] [ 0 , 3 , 0 ] [ 1 , 2 , 3 ] [ 0 , 2 , 0 , 2 ] x [ 1 , 0 , 3 ] = [ 2 , 8 , 0 ] [ 1 , 1 , 1 , 1 ] [ 1 , 1 , 0 ] [ 2 , 6 , 3 ] \u4f7f\u7528\u5411\u91cf\u5316\u83b7\u53d6querys\u7684\u503c [ 1 , 0 , 1 ] [ 1 , 0 , 1 , 0 ] [ 1 , 0 , 0 ] [ 1 , 0 , 2 ] [ 0 , 2 , 0 , 2 ] x [ 0 , 0 , 1 ] = [ 2 , 2 , 2 ] [ 1 , 1 , 1 , 1 ] [ 0 , 1 , 1 ] [ 2 , 1 , 3 ] # \u5c06query key value\u5206\u522b\u8fdb\u884c\u8ba1\u7b97 keys = x @ w_key querys = x @ w_query values = x @ w_value print ( \"Keys: \\n \" , keys ) print ( \"Querys: \\n \" , querys ) print ( \"Values: \\n \" , values ) \u8f93\u51fa\u6548\u679c Keys : tensor ([[ 0. , 1. , 1. ], [ 4. , 4. , 0. ], [ 2. , 3. , 1. ]]) Querys : tensor ([[ 1. , 0. , 2. ], [ 2. , 2. , 2. ], [ 2. , 1. , 3. ]]) Values : tensor ([[ 1. , 2. , 3. ], [ 2. , 8. , 0. ], [ 2. , 6. , 3. ]])","title":"1.3 \u83b7\u53d6key\uff0cquery\u548cvalue"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#14-input1attention-score","text":"# \u4e3a\u4e86\u83b7\u53d6input1\u7684attention score\uff0c\u6211\u4eec\u4f7f\u7528\u70b9\u4e58\u6765\u5904\u7406\u6240\u6709\u7684key\u548cquery\uff0c\u5305\u62ec\u5b83\u81ea\u5df1\u7684key\u548cvalue\u3002\u8fd9\u6837\u6211\u4eec\u5c31\u80fd\u591f\u5f97\u52303\u4e2akey\u7684\u8868\u793a\uff08\u56e0\u4e3a\u6211\u4eec\u67093\u4e2a\u8f93\u5165\uff09\uff0c\u6211\u4eec\u5c31\u83b7\u5f97\u4e863\u4e2aattention score\uff08\u84dd\u8272\uff09 [ 0 , 4 , 2 ] [ 1 , 0 , 2 ] x [ 1 , 4 , 3 ] = [ 2 , 4 , 4 ] [ 1 , 0 , 1 ] # \u6ce8\u610f: \u8fd9\u91cc\u6211\u4eec\u53ea\u7528input1\u4e3e\u4f8b.\u5176\u4ed6\u7684\u8f93\u5165\u7684query\u548cinput1\u505a\u76f8\u540c\u7684\u64cd\u4f5c. attn_scores = querys @ keys . T print ( attn_scores ) \u8f93\u51fa\u6548\u679c tensor ([[ 2. , 4. , 4. ], # attention scores from Query 1 [ 4. , 16. , 12. ], # attention scores from Query 2 [ 4. , 12. , 10. ]]) # attention scores from Query 3","title":"1.4 \u7ed9input1\u8ba1\u7b97attention score"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#15-softmax","text":"\u7ed9attention score\u5e94\u7528softmax\u3002 softmax([2, 4, 4]) = [0.0, 0.5, 0.5] from torch.nn.functional import softmax attn_scores_softmax = softmax ( attn_scores , dim =- 1 ) print ( attn_scores_softmax ) attn_scores_softmax = [ [ 0.0 , 0.5 , 0.5 ], [ 0.0 , 1.0 , 0.0 ], [ 0.0 , 0.9 , 0.1 ] ] attn_scores_softmax = torch . tensor ( attn_scores_softmax ) print ( attn_scores_softmax ) \u8f93\u51fa\u6548\u679c tensor ([[ 6.3379e-02 , 4.6831e-01 , 4.6831e-01 ], [ 6.0337e-06 , 9.8201e-01 , 1.7986e-02 ], [ 2.9539e-04 , 8.8054e-01 , 1.1917e-01 ]]) tensor ([[ 0.0000 , 0.5000 , 0.5000 ], [ 0.0000 , 1.0000 , 0.0000 ], [ 0.0000 , 0.9000 , 0.1000 ]])","title":"1.5 \u8ba1\u7b97softmax"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#16-valuescore","text":"\u4f7f\u7528\u7ecf\u8fc7softmax\u540e\u7684attention score\u4e58\u4ee5\u5b83\u5bf9\u5e94\u7684value\u503c\uff08\u7d2b\u8272\uff09\uff0c\u8fd9\u6837\u5c31\u5f97\u5230\u4e863\u4e2aweighted values\uff08\u9ec4\u8272\uff09\u3002 1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0] 2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0] 3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5] weighted_values = values [:, None ] * attn_scores_softmax . T [:,:, None ] print ( weighted_values ) \u8f93\u51fa\u6548\u679c: tensor ([[[ 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 ], [ 0.0000 , 0.0000 , 0.0000 ]], [[ 1.0000 , 4.0000 , 0.0000 ], [ 2.0000 , 8.0000 , 0.0000 ], [ 1.8000 , 7.2000 , 0.0000 ]], [[ 1.0000 , 3.0000 , 1.5000 ], [ 0.0000 , 0.0000 , 0.0000 ], [ 0.2000 , 0.6000 , 0.3000 ]]])","title":"1.6 \u7ed9value\u4e58\u4e0ascore"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#17-valueoutput1","text":"\u628a\u6240\u6709\u7684weighted values\uff08\u9ec4\u8272\uff09\u8fdb\u884celement-wise\u7684\u76f8\u52a0\u3002 [0.0, 0.0, 0.0] + [1.0, 4.0, 0.0] + [1.0, 3.0, 1.5] ------------------------ = [2.0, 7.0, 1.5] \u5f97\u5230\u7ed3\u679c\u5411\u91cf[2.0, 7.0, 1.5]\uff08\u6df1\u7eff\u8272\uff09\u5c31\u662fouput1\u7684\u548c\u5176\u4ed6key\u4ea4\u4e92\u7684query representation","title":"1.7 \u7ed9value\u52a0\u6743\u6c42\u548c\u83b7\u53d6output1"},{"location":"03_mkdocs_RNN/10%20Self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html#18-4-7output2output3","text":"outputs = weighted_values . sum ( dim = 0 ) print ( outputs ) \u8f93\u51fa\u6548\u679c tensor ([[ 2.0000 , 7.0000 , 1.5000 ], [ 2.0000 , 8.0000 , 0.0000 ], [ 2.0000 , 7.8000 , 0.3000 ]])","title":"1.8 \u91cd\u590d\u6b65\u9aa44-7\uff0c\u83b7\u53d6output2\uff0coutput3"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u53ca\u8ba1\u7b97\u516c\u5f0f. \u638c\u63e1Pytorch\u4e2d\u4f20\u7edfRNN\u5de5\u5177\u7684\u4f7f\u7528. \u4e86\u89e3\u4f20\u7edfRNN\u7684\u4f18\u52bf\u4e0e\u7f3a\u70b9. 1 \u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u56fe \u00b6 1.1 RNN\u7ed3\u6784\u5206\u6790 \u00b6 \u7ed3\u6784\u89e3\u91ca\u56fe: \u5185\u90e8\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u628a\u76ee\u5149\u96c6\u4e2d\u5728\u4e2d\u95f4\u7684\u65b9\u5757\u90e8\u5206, \u5b83\u7684\u8f93\u5165\u6709\u4e24\u90e8\u5206, \u5206\u522b\u662fh(t-1)\u4ee5\u53cax(t), \u4ee3\u8868\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u8f93\u51fa, \u4ee5\u53ca\u6b64\u65f6\u95f4\u6b65\u7684\u8f93\u5165, \u5b83\u4eec\u8fdb\u5165RNN\u7ed3\u6784\u4f53\u540e, \u4f1a\"\u878d\u5408\"\u5230\u4e00\u8d77, \u8fd9\u79cd\u878d\u5408\u6211\u4eec\u6839\u636e\u7ed3\u6784\u89e3\u91ca\u53ef\u77e5, \u662f\u5c06\u4e8c\u8005\u8fdb\u884c\u62fc\u63a5, \u5f62\u6210\u65b0\u7684\u5f20\u91cf[x(t), h(t-1)], \u4e4b\u540e\u8fd9\u4e2a\u65b0\u7684\u5f20\u91cf\u5c06\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42(\u7ebf\u6027\u5c42), \u8be5\u5c42\u4f7f\u7528tanh\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570, \u6700\u7ec8\u5f97\u5230\u8be5\u65f6\u95f4\u6b65\u7684\u8f93\u51fah(t), \u5b83\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u548cx(t+1)\u4e00\u8d77\u8fdb\u5165\u7ed3\u6784\u4f53. \u4ee5\u6b64\u7c7b\u63a8. \u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a: \u6839\u636e\u7ed3\u6784\u5206\u6790\u5f97\u51fa\u5185\u90e8\u8ba1\u7b97\u516c\u5f0f: $$ h_t = tanh(W_t[X_t, h_{t-1}] + b_t) $$ \u6fc0\u6d3b\u51fd\u6570tanh\u7684\u4f5c\u7528: \u7528\u4e8e\u5e2e\u52a9\u8c03\u8282\u6d41\u7ecf\u7f51\u7edc\u7684\u503c, tanh\u51fd\u6570\u5c06\u503c\u538b\u7f29\u5728-1\u548c1\u4e4b\u95f4. 1.2 \u4f7f\u7528Pytorch\u6784\u5efaRNN\u6a21\u578b \u00b6 \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.RNN\u53ef\u8c03\u7528 nn.RNN\u4f7f\u7528\u793a\u4f8b1: import torch import torch.nn as nn def dm_rnn_for_base (): ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cfx\u7684\u7ef4\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1anum_layer(\u9690\u85cf\u5c42\u7684\u6570\u91cf) ''' rnn = nn . RNN ( 5 , 6 , 1 ) #A ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1asequence_length(\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570\u91cf) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6) ''' input = torch . randn ( 1 , 3 , 5 ) #B ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1anum_layer * num_directions(\u5c42\u6570*\u7f51\u7edc\u65b9\u5411) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u7684\u4e2a\u6570) ''' h0 = torch . randn ( 1 , 3 , 6 ) #C # [1,3,5],[1,3,6] ---> [1,3,6],[1,3,6] output , hn = rnn ( input , h0 ) print ( 'output--->' , output . shape , output ) print ( 'hn--->' , hn . shape , hn ) print ( 'rnn\u6a21\u578b--->' , rnn ) # \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a output ---> torch . Size ([ 1 , 3 , 6 ]) tensor ([[[ 0.8947 , - 0.6040 , 0.9878 , - 0.1070 , - 0.7071 , - 0.1434 ], [ 0.0955 , - 0.8216 , 0.9475 , - 0.7593 , - 0.8068 , - 0.5549 ], [ - 0.1524 , 0.7519 , - 0.1985 , 0.0937 , 0.2009 , - 0.0244 ]]], grad_fn =< StackBackward0 > ) hn ---> torch . Size ([ 1 , 3 , 6 ]) tensor ([[[ 0.8947 , - 0.6040 , 0.9878 , - 0.1070 , - 0.7071 , - 0.1434 ], [ 0.0955 , - 0.8216 , 0.9475 , - 0.7593 , - 0.8068 , - 0.5549 ], [ - 0.1524 , 0.7519 , - 0.1985 , 0.0937 , 0.2009 , - 0.0244 ]]], grad_fn =< StackBackward0 > ) rnn\u6a21\u578b ---> RNN ( 5 , 6 ) nn.RNN\u4f7f\u7528\u793a\u4f8b2 # \u8f93\u5165\u6570\u636e\u957f\u5ea6\u53d1\u751f\u53d8\u5316 def dm_rnn_for_sequencelen (): ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cfx\u7684\u7ef4\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1anum_layer(\u9690\u85cf\u5c42\u7684\u6570\u91cf) ''' rnn = nn . RNN ( 5 , 6 , 1 ) #A ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1asequence_length(\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570\u91cf) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6) ''' input = torch . randn ( 20 , 3 , 5 ) #B ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1anum_layer * num_directions(\u5c42\u6570*\u7f51\u7edc\u65b9\u5411) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u7684\u4e2a\u6570) ''' h0 = torch . randn ( 1 , 3 , 6 ) #C # [20,3,5],[1,3,6] --->[20,3,6],[1,3,6] output , hn = rnn ( input , h0 ) # print ( 'output--->' , output . shape ) print ( 'hn--->' , hn . shape ) print ( 'rnn\u6a21\u578b--->' , rnn ) # \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a output ---> torch . Size ([ 20 , 3 , 6 ]) hn ---> torch . Size ([ 1 , 3 , 6 ]) rnn\u6a21\u578b ---> RNN ( 5 , 6 ) nn.RNN\u4f7f\u7528\u793a\u4f8b3 def dm_run_for_hiddennum (): ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cfx\u7684\u7ef4\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1anum_layer(\u9690\u85cf\u5c42\u7684\u6570\u91cf) ''' rnn = nn . RNN ( 5 , 6 , 2 ) # A \u9690\u85cf\u5c42\u4e2a\u6570\u4ece1-->2 \u4e0b\u9762\u7a0b\u5e8f\u9700\u8981\u4fee\u6539\u7684\u5730\u65b9\uff1f ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1asequence_length(\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570\u91cf) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6) ''' input = torch . randn ( 1 , 3 , 5 ) # B ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1anum_layer * num_directions(\u5c42\u6570*\u7f51\u7edc\u65b9\u5411) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u7684\u4e2a\u6570) ''' h0 = torch . randn ( 2 , 3 , 6 ) # C output , hn = rnn ( input , h0 ) # print ( 'output-->' , output . shape , output ) print ( 'hn-->' , hn . shape , hn ) print ( 'rnn\u6a21\u578b--->' , rnn ) # nn\u6a21\u578b---> RNN(5, 6, num_layers=11) # \u7ed3\u8bba\uff1a\u82e5\u53ea\u6709\u4e00\u4e2a\u9690\u85cf\u6b21 output\u8f93\u51fa\u7ed3\u679c\u7b49\u4e8ehn # \u7ed3\u8bba\uff1a\u5982\u679c\u67092\u4e2a\u9690\u85cf\u5c42\uff0coutput\u7684\u8f93\u51fa\u7ed3\u679c\u67092\u4e2a\uff0chn\u7b49\u4e8e\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42 # \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a output --> torch . Size ([ 1 , 3 , 6 ]) tensor ([[[ 0.4987 , - 0.5756 , 0.1934 , 0.7284 , 0.4478 , - 0.1244 ], [ 0.6753 , 0.5011 , - 0.7141 , 0.4480 , 0.7186 , 0.5437 ], [ 0.6260 , 0.7600 , - 0.7384 , - 0.5080 , 0.9054 , 0.6011 ]]], grad_fn =< StackBackward0 > ) hn --> torch . Size ([ 2 , 3 , 6 ]) tensor ([[[ 0.4862 , 0.6872 , - 0.0437 , - 0.7826 , - 0.7136 , - 0.5715 ], [ 0.8942 , 0.4524 , - 0.1695 , - 0.5536 , - 0.4367 , - 0.3353 ], [ 0.5592 , 0.0444 , - 0.8384 , - 0.5193 , 0.7049 , - 0.0453 ]], [[ 0.4987 , - 0.5756 , 0.1934 , 0.7284 , 0.4478 , - 0.1244 ], [ 0.6753 , 0.5011 , - 0.7141 , 0.4480 , 0.7186 , 0.5437 ], [ 0.6260 , 0.7600 , - 0.7384 , - 0.5080 , 0.9054 , 0.6011 ]]], grad_fn =< StackBackward0 > ) rnn\u6a21\u578b ---> RNN ( 5 , 6 , num_layers = 2 ) 1.3 \u4f20\u7edfRNN\u4f18\u7f3a\u70b9 \u00b6 1 \u4f20\u7edfRNN\u7684\u4f18\u52bf \u00b6 \u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u7b80\u5355, \u5bf9\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u4f4e, \u76f8\u6bd4\u4e4b\u540e\u6211\u4eec\u8981\u5b66\u4e60\u7684RNN\u53d8\u4f53:LSTM\u548cGRU\u6a21\u578b\u53c2\u6570\u603b\u91cf\u5c11\u4e86\u5f88\u591a, \u5728\u77ed\u5e8f\u5217\u4efb\u52a1\u4e0a\u6027\u80fd\u548c\u6548\u679c\u90fd\u8868\u73b0\u4f18\u5f02. 2 \u4f20\u7edfRNN\u7684\u7f3a\u70b9 \u00b6 \u4f20\u7edfRNN\u5728\u89e3\u51b3\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u8054\u65f6, \u901a\u8fc7\u5b9e\u8df5\uff0c\u8bc1\u660e\u7ecf\u5178RNN\u8868\u73b0\u5f88\u5dee, \u539f\u56e0\u662f\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u8fc7\u957f\u7684\u5e8f\u5217\u5bfc\u81f4\u68af\u5ea6\u7684\u8ba1\u7b97\u5f02\u5e38, \u53d1\u751f\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8. 3 \u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u4ecb\u7ecd \u00b6 \u6839\u636e\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u548c\u94fe\u5f0f\u6cd5\u5219, \u68af\u5ea6\u7684\u8ba1\u7b97\u53ef\u4ee5\u7b80\u5316\u4e3a\u4ee5\u4e0b\u516c\u5f0f D_n=\\sigma^{\\prime}(z_1)w_1\\cdot\\sigma^{\\prime}(z_2)w_2\\cdot_{\\cdots}\\cdot\\sigma^{\\prime}(z_n)w_n D_n=\\sigma^{\\prime}(z_1)w_1\\cdot\\sigma^{\\prime}(z_2)w_2\\cdot_{\\cdots}\\cdot\\sigma^{\\prime}(z_n)w_n \u5176\u4e2dsigmoid\u7684\u5bfc\u6570\u503c\u57df\u662f\u56fa\u5b9a\u7684, \u5728[0, 0.25]\u4e4b\u95f4, \u800c\u4e00\u65e6\u516c\u5f0f\u4e2d\u7684w\u4e5f\u5c0f\u4e8e1, \u90a3\u4e48\u901a\u8fc7\u8fd9\u6837\u7684\u516c\u5f0f\u8fde\u4e58\u540e, \u6700\u7ec8\u7684\u68af\u5ea6\u5c31\u4f1a\u53d8\u5f97\u975e\u5e38\u975e\u5e38\u5c0f, \u8fd9\u79cd\u73b0\u8c61\u79f0\u4f5c\u68af\u5ea6\u6d88\u5931. \u53cd\u4e4b, \u5982\u679c\u6211\u4eec\u4eba\u4e3a\u7684\u589e\u5927w\u7684\u503c, \u4f7f\u5176\u5927\u4e8e1, \u90a3\u4e48\u8fde\u4e58\u591f\u5c31\u53ef\u80fd\u9020\u6210\u68af\u5ea6\u8fc7\u5927, \u79f0\u4f5c\u68af\u5ea6\u7206\u70b8. \u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u7684\u5371\u5bb3: \u5982\u679c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53d1\u751f\u4e86\u68af\u5ea6\u6d88\u5931\uff0c\u6743\u91cd\u65e0\u6cd5\u88ab\u66f4\u65b0\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u5931\u8d25; \u68af\u5ea6\u7206\u70b8\u6240\u5e26\u6765\u7684\u68af\u5ea6\u8fc7\u5927\uff0c\u5927\u5e45\u5ea6\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\uff0c\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u7ed3\u679c\u4f1a\u6ea2\u51fa\uff08NaN\u503c\uff09. 2 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4f20\u7edfRNN\u7684\u7ed3\u6784\u5e76\u8fdb\u884c\u4e86\u5206\u6790; \u5b83\u7684\u8f93\u5165\u6709\u4e24\u90e8\u5206, \u5206\u522b\u662fh(t-1)\u4ee5\u53cax(t), \u4ee3\u8868\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u8f93\u51fa, \u4ee5\u53ca\u6b64\u65f6\u95f4\u6b65\u7684\u8f93\u5165, \u5b83\u4eec\u8fdb\u5165RNN\u7ed3\u6784\u4f53\u540e, \u4f1a\"\u878d\u5408\"\u5230\u4e00\u8d77, \u8fd9\u79cd\u878d\u5408\u6211\u4eec\u6839\u636e\u7ed3\u6784\u89e3\u91ca\u53ef\u77e5, \u662f\u5c06\u4e8c\u8005\u8fdb\u884c\u62fc\u63a5, \u5f62\u6210\u65b0\u7684\u5f20\u91cf[x(t), h(t-1)], \u4e4b\u540e\u8fd9\u4e2a\u65b0\u7684\u5f20\u91cf\u5c06\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42(\u7ebf\u6027\u5c42), \u8be5\u5c42\u4f7f\u7528tanh\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570, \u6700\u7ec8\u5f97\u5230\u8be5\u65f6\u95f4\u6b65\u7684\u8f93\u51fah(t), \u5b83\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u548cx(t+1)\u4e00\u8d77\u8fdb\u5165\u7ed3\u6784\u4f53. \u4ee5\u6b64\u7c7b\u63a8. \u6839\u636e\u7ed3\u6784\u5206\u6790\u5f97\u51fa\u4e86\u4f20\u7edfRNN\u7684\u8ba1\u7b97\u516c\u5f0f. \u5b66\u4e60\u4e86\u6fc0\u6d3b\u51fd\u6570tanh\u7684\u4f5c\u7528: \u7528\u4e8e\u5e2e\u52a9\u8c03\u8282\u6d41\u7ecf\u7f51\u7edc\u7684\u503c, tanh\u51fd\u6570\u5c06\u503c\u538b\u7f29\u5728-1\u548c1\u4e4b\u95f4. \u5b66\u4e60\u4e86Pytorch\u4e2d\u4f20\u7edfRNN\u5de5\u5177\u7684\u4f7f\u7528: \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.RNN\u53ef\u8c03\u7528. nn.RNN\u7c7b\u521d\u59cb\u5316\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input_size: \u8f93\u5165\u5f20\u91cfx\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. hidden_size: \u9690\u5c42\u5f20\u91cfh\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. num_layers: \u9690\u542b\u5c42\u7684\u6570\u91cf. nonlinearity: \u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9, \u9ed8\u8ba4\u662ftanh. nn.RNN\u7c7b\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input: \u8f93\u5165\u5f20\u91cfx. h0: \u521d\u59cb\u5316\u7684\u9690\u5c42\u5f20\u91cfh. \u5b9e\u73b0\u4e86nn.RNN\u7684\u4f7f\u7528\u793a\u4f8b, \u83b7\u5f97RNN\u7684\u771f\u5b9e\u8fd4\u56de\u7ed3\u679c\u6837\u5f0f. \u5b66\u4e60\u4e86\u4f20\u7edfRNN\u7684\u4f18\u52bf: \u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u7b80\u5355, \u5bf9\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u4f4e, \u76f8\u6bd4\u4e4b\u540e\u6211\u4eec\u8981\u5b66\u4e60\u7684RNN\u53d8\u4f53:LSTM\u548cGRU\u6a21\u578b\u53c2\u6570\u603b\u91cf\u5c11\u4e86\u5f88\u591a, \u5728\u77ed\u5e8f\u5217\u4efb\u52a1\u4e0a\u6027\u80fd\u548c\u6548\u679c\u90fd\u8868\u73b0\u4f18\u5f02. \u5b66\u4e60\u4e86\u4f20\u7edfRNN\u7684\u7f3a\u70b9: \u4f20\u7edfRNN\u5728\u89e3\u51b3\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u8054\u65f6, \u901a\u8fc7\u5b9e\u8df5\uff0c\u8bc1\u660e\u7ecf\u5178RNN\u8868\u73b0\u5f88\u5dee, \u539f\u56e0\u662f\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u8fc7\u957f\u7684\u5e8f\u5217\u5bfc\u81f4\u68af\u5ea6\u7684\u8ba1\u7b97\u5f02\u5e38, \u53d1\u751f\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8: \u6839\u636e\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u548c\u94fe\u5f0f\u6cd5\u5219, \u5f97\u5230\u68af\u5ea6\u7684\u8ba1\u7b97\u7684\u7b80\u5316\u516c\u5f0f:\u5176\u4e2dsigmoid\u7684\u5bfc\u6570\u503c\u57df\u662f\u56fa\u5b9a\u7684, \u5728[0, 0.25]\u4e4b\u95f4, \u800c\u4e00\u65e6\u516c\u5f0f\u4e2d\u7684w\u4e5f\u5c0f\u4e8e1, \u90a3\u4e48\u901a\u8fc7\u8fd9\u6837\u7684\u516c\u5f0f\u8fde\u4e58\u540e, \u6700\u7ec8\u7684\u68af\u5ea6\u5c31\u4f1a\u53d8\u5f97\u975e\u5e38\u975e\u5e38\u5c0f, \u8fd9\u79cd\u73b0\u8c61\u79f0\u4f5c\u68af\u5ea6\u6d88\u5931. \u53cd\u4e4b, \u5982\u679c\u6211\u4eec\u4eba\u4e3a\u7684\u589e\u5927w\u7684\u503c, \u4f7f\u5176\u5927\u4e8e1, \u90a3\u4e48\u8fde\u4e58\u591f\u5c31\u53ef\u80fd\u9020\u6210\u68af\u5ea6\u8fc7\u5927, \u79f0\u4f5c\u68af\u5ea6\u7206\u70b8. \u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u7684\u5371\u5bb3: \u5982\u679c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53d1\u751f\u4e86\u68af\u5ea6\u6d88\u5931\uff0c\u6743\u91cd\u65e0\u6cd5\u88ab\u66f4\u65b0\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u5931\u8d25; \u68af\u5ea6\u7206\u70b8\u6240\u5e26\u6765\u7684\u68af\u5ea6\u8fc7\u5927\uff0c\u5927\u5e45\u5ea6\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\uff0c\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u7ed3\u679c\u4f1a\u6ea2\u51fa\uff08NaN\u503c\uff09.","title":"2 \u4f20\u7edfRNN\u6a21\u578b"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#_1","text":"\u4e86\u89e3\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u53ca\u8ba1\u7b97\u516c\u5f0f. \u638c\u63e1Pytorch\u4e2d\u4f20\u7edfRNN\u5de5\u5177\u7684\u4f7f\u7528. \u4e86\u89e3\u4f20\u7edfRNN\u7684\u4f18\u52bf\u4e0e\u7f3a\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#1-rnn","text":"","title":"1 \u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u56fe"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#11-rnn","text":"\u7ed3\u6784\u89e3\u91ca\u56fe: \u5185\u90e8\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u628a\u76ee\u5149\u96c6\u4e2d\u5728\u4e2d\u95f4\u7684\u65b9\u5757\u90e8\u5206, \u5b83\u7684\u8f93\u5165\u6709\u4e24\u90e8\u5206, \u5206\u522b\u662fh(t-1)\u4ee5\u53cax(t), \u4ee3\u8868\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u8f93\u51fa, \u4ee5\u53ca\u6b64\u65f6\u95f4\u6b65\u7684\u8f93\u5165, \u5b83\u4eec\u8fdb\u5165RNN\u7ed3\u6784\u4f53\u540e, \u4f1a\"\u878d\u5408\"\u5230\u4e00\u8d77, \u8fd9\u79cd\u878d\u5408\u6211\u4eec\u6839\u636e\u7ed3\u6784\u89e3\u91ca\u53ef\u77e5, \u662f\u5c06\u4e8c\u8005\u8fdb\u884c\u62fc\u63a5, \u5f62\u6210\u65b0\u7684\u5f20\u91cf[x(t), h(t-1)], \u4e4b\u540e\u8fd9\u4e2a\u65b0\u7684\u5f20\u91cf\u5c06\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42(\u7ebf\u6027\u5c42), \u8be5\u5c42\u4f7f\u7528tanh\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570, \u6700\u7ec8\u5f97\u5230\u8be5\u65f6\u95f4\u6b65\u7684\u8f93\u51fah(t), \u5b83\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u548cx(t+1)\u4e00\u8d77\u8fdb\u5165\u7ed3\u6784\u4f53. \u4ee5\u6b64\u7c7b\u63a8. \u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a: \u6839\u636e\u7ed3\u6784\u5206\u6790\u5f97\u51fa\u5185\u90e8\u8ba1\u7b97\u516c\u5f0f: $$ h_t = tanh(W_t[X_t, h_{t-1}] + b_t) $$ \u6fc0\u6d3b\u51fd\u6570tanh\u7684\u4f5c\u7528: \u7528\u4e8e\u5e2e\u52a9\u8c03\u8282\u6d41\u7ecf\u7f51\u7edc\u7684\u503c, tanh\u51fd\u6570\u5c06\u503c\u538b\u7f29\u5728-1\u548c1\u4e4b\u95f4.","title":"1.1 RNN\u7ed3\u6784\u5206\u6790"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#12-pytorchrnn","text":"\u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.RNN\u53ef\u8c03\u7528 nn.RNN\u4f7f\u7528\u793a\u4f8b1: import torch import torch.nn as nn def dm_rnn_for_base (): ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cfx\u7684\u7ef4\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1anum_layer(\u9690\u85cf\u5c42\u7684\u6570\u91cf) ''' rnn = nn . RNN ( 5 , 6 , 1 ) #A ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1asequence_length(\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570\u91cf) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6) ''' input = torch . randn ( 1 , 3 , 5 ) #B ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1anum_layer * num_directions(\u5c42\u6570*\u7f51\u7edc\u65b9\u5411) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u7684\u4e2a\u6570) ''' h0 = torch . randn ( 1 , 3 , 6 ) #C # [1,3,5],[1,3,6] ---> [1,3,6],[1,3,6] output , hn = rnn ( input , h0 ) print ( 'output--->' , output . shape , output ) print ( 'hn--->' , hn . shape , hn ) print ( 'rnn\u6a21\u578b--->' , rnn ) # \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a output ---> torch . Size ([ 1 , 3 , 6 ]) tensor ([[[ 0.8947 , - 0.6040 , 0.9878 , - 0.1070 , - 0.7071 , - 0.1434 ], [ 0.0955 , - 0.8216 , 0.9475 , - 0.7593 , - 0.8068 , - 0.5549 ], [ - 0.1524 , 0.7519 , - 0.1985 , 0.0937 , 0.2009 , - 0.0244 ]]], grad_fn =< StackBackward0 > ) hn ---> torch . Size ([ 1 , 3 , 6 ]) tensor ([[[ 0.8947 , - 0.6040 , 0.9878 , - 0.1070 , - 0.7071 , - 0.1434 ], [ 0.0955 , - 0.8216 , 0.9475 , - 0.7593 , - 0.8068 , - 0.5549 ], [ - 0.1524 , 0.7519 , - 0.1985 , 0.0937 , 0.2009 , - 0.0244 ]]], grad_fn =< StackBackward0 > ) rnn\u6a21\u578b ---> RNN ( 5 , 6 ) nn.RNN\u4f7f\u7528\u793a\u4f8b2 # \u8f93\u5165\u6570\u636e\u957f\u5ea6\u53d1\u751f\u53d8\u5316 def dm_rnn_for_sequencelen (): ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cfx\u7684\u7ef4\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1anum_layer(\u9690\u85cf\u5c42\u7684\u6570\u91cf) ''' rnn = nn . RNN ( 5 , 6 , 1 ) #A ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1asequence_length(\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570\u91cf) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6) ''' input = torch . randn ( 20 , 3 , 5 ) #B ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1anum_layer * num_directions(\u5c42\u6570*\u7f51\u7edc\u65b9\u5411) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u7684\u4e2a\u6570) ''' h0 = torch . randn ( 1 , 3 , 6 ) #C # [20,3,5],[1,3,6] --->[20,3,6],[1,3,6] output , hn = rnn ( input , h0 ) # print ( 'output--->' , output . shape ) print ( 'hn--->' , hn . shape ) print ( 'rnn\u6a21\u578b--->' , rnn ) # \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a output ---> torch . Size ([ 20 , 3 , 6 ]) hn ---> torch . Size ([ 1 , 3 , 6 ]) rnn\u6a21\u578b ---> RNN ( 5 , 6 ) nn.RNN\u4f7f\u7528\u793a\u4f8b3 def dm_run_for_hiddennum (): ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cfx\u7684\u7ef4\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1anum_layer(\u9690\u85cf\u5c42\u7684\u6570\u91cf) ''' rnn = nn . RNN ( 5 , 6 , 2 ) # A \u9690\u85cf\u5c42\u4e2a\u6570\u4ece1-->2 \u4e0b\u9762\u7a0b\u5e8f\u9700\u8981\u4fee\u6539\u7684\u5730\u65b9\uff1f ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1asequence_length(\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570\u91cf) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ainput_size(\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6) ''' input = torch . randn ( 1 , 3 , 5 ) # B ''' \u7b2c\u4e00\u4e2a\u53c2\u6570\uff1anum_layer * num_directions(\u5c42\u6570*\u7f51\u7edc\u65b9\u5411) \u7b2c\u4e8c\u4e2a\u53c2\u6570\uff1abatch_size(\u6279\u6b21\u7684\u6837\u672c\u6570) \u7b2c\u4e09\u4e2a\u53c2\u6570\uff1ahidden_size(\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\uff0c \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u7684\u4e2a\u6570) ''' h0 = torch . randn ( 2 , 3 , 6 ) # C output , hn = rnn ( input , h0 ) # print ( 'output-->' , output . shape , output ) print ( 'hn-->' , hn . shape , hn ) print ( 'rnn\u6a21\u578b--->' , rnn ) # nn\u6a21\u578b---> RNN(5, 6, num_layers=11) # \u7ed3\u8bba\uff1a\u82e5\u53ea\u6709\u4e00\u4e2a\u9690\u85cf\u6b21 output\u8f93\u51fa\u7ed3\u679c\u7b49\u4e8ehn # \u7ed3\u8bba\uff1a\u5982\u679c\u67092\u4e2a\u9690\u85cf\u5c42\uff0coutput\u7684\u8f93\u51fa\u7ed3\u679c\u67092\u4e2a\uff0chn\u7b49\u4e8e\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42 # \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c\u5982\u4e0b\uff1a output --> torch . Size ([ 1 , 3 , 6 ]) tensor ([[[ 0.4987 , - 0.5756 , 0.1934 , 0.7284 , 0.4478 , - 0.1244 ], [ 0.6753 , 0.5011 , - 0.7141 , 0.4480 , 0.7186 , 0.5437 ], [ 0.6260 , 0.7600 , - 0.7384 , - 0.5080 , 0.9054 , 0.6011 ]]], grad_fn =< StackBackward0 > ) hn --> torch . Size ([ 2 , 3 , 6 ]) tensor ([[[ 0.4862 , 0.6872 , - 0.0437 , - 0.7826 , - 0.7136 , - 0.5715 ], [ 0.8942 , 0.4524 , - 0.1695 , - 0.5536 , - 0.4367 , - 0.3353 ], [ 0.5592 , 0.0444 , - 0.8384 , - 0.5193 , 0.7049 , - 0.0453 ]], [[ 0.4987 , - 0.5756 , 0.1934 , 0.7284 , 0.4478 , - 0.1244 ], [ 0.6753 , 0.5011 , - 0.7141 , 0.4480 , 0.7186 , 0.5437 ], [ 0.6260 , 0.7600 , - 0.7384 , - 0.5080 , 0.9054 , 0.6011 ]]], grad_fn =< StackBackward0 > ) rnn\u6a21\u578b ---> RNN ( 5 , 6 , num_layers = 2 )","title":"1.2 \u4f7f\u7528Pytorch\u6784\u5efaRNN\u6a21\u578b"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#13-rnn","text":"","title":"1.3 \u4f20\u7edfRNN\u4f18\u7f3a\u70b9"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#1-rnn_1","text":"\u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u7b80\u5355, \u5bf9\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u4f4e, \u76f8\u6bd4\u4e4b\u540e\u6211\u4eec\u8981\u5b66\u4e60\u7684RNN\u53d8\u4f53:LSTM\u548cGRU\u6a21\u578b\u53c2\u6570\u603b\u91cf\u5c11\u4e86\u5f88\u591a, \u5728\u77ed\u5e8f\u5217\u4efb\u52a1\u4e0a\u6027\u80fd\u548c\u6548\u679c\u90fd\u8868\u73b0\u4f18\u5f02.","title":"1 \u4f20\u7edfRNN\u7684\u4f18\u52bf"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#2-rnn","text":"\u4f20\u7edfRNN\u5728\u89e3\u51b3\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u8054\u65f6, \u901a\u8fc7\u5b9e\u8df5\uff0c\u8bc1\u660e\u7ecf\u5178RNN\u8868\u73b0\u5f88\u5dee, \u539f\u56e0\u662f\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u8fc7\u957f\u7684\u5e8f\u5217\u5bfc\u81f4\u68af\u5ea6\u7684\u8ba1\u7b97\u5f02\u5e38, \u53d1\u751f\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8.","title":"2 \u4f20\u7edfRNN\u7684\u7f3a\u70b9"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#3","text":"\u6839\u636e\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u548c\u94fe\u5f0f\u6cd5\u5219, \u68af\u5ea6\u7684\u8ba1\u7b97\u53ef\u4ee5\u7b80\u5316\u4e3a\u4ee5\u4e0b\u516c\u5f0f D_n=\\sigma^{\\prime}(z_1)w_1\\cdot\\sigma^{\\prime}(z_2)w_2\\cdot_{\\cdots}\\cdot\\sigma^{\\prime}(z_n)w_n D_n=\\sigma^{\\prime}(z_1)w_1\\cdot\\sigma^{\\prime}(z_2)w_2\\cdot_{\\cdots}\\cdot\\sigma^{\\prime}(z_n)w_n \u5176\u4e2dsigmoid\u7684\u5bfc\u6570\u503c\u57df\u662f\u56fa\u5b9a\u7684, \u5728[0, 0.25]\u4e4b\u95f4, \u800c\u4e00\u65e6\u516c\u5f0f\u4e2d\u7684w\u4e5f\u5c0f\u4e8e1, \u90a3\u4e48\u901a\u8fc7\u8fd9\u6837\u7684\u516c\u5f0f\u8fde\u4e58\u540e, \u6700\u7ec8\u7684\u68af\u5ea6\u5c31\u4f1a\u53d8\u5f97\u975e\u5e38\u975e\u5e38\u5c0f, \u8fd9\u79cd\u73b0\u8c61\u79f0\u4f5c\u68af\u5ea6\u6d88\u5931. \u53cd\u4e4b, \u5982\u679c\u6211\u4eec\u4eba\u4e3a\u7684\u589e\u5927w\u7684\u503c, \u4f7f\u5176\u5927\u4e8e1, \u90a3\u4e48\u8fde\u4e58\u591f\u5c31\u53ef\u80fd\u9020\u6210\u68af\u5ea6\u8fc7\u5927, \u79f0\u4f5c\u68af\u5ea6\u7206\u70b8. \u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u7684\u5371\u5bb3: \u5982\u679c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53d1\u751f\u4e86\u68af\u5ea6\u6d88\u5931\uff0c\u6743\u91cd\u65e0\u6cd5\u88ab\u66f4\u65b0\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u5931\u8d25; \u68af\u5ea6\u7206\u70b8\u6240\u5e26\u6765\u7684\u68af\u5ea6\u8fc7\u5927\uff0c\u5927\u5e45\u5ea6\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\uff0c\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u7ed3\u679c\u4f1a\u6ea2\u51fa\uff08NaN\u503c\uff09.","title":"3 \u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html#2","text":"\u5b66\u4e60\u4e86\u4f20\u7edfRNN\u7684\u7ed3\u6784\u5e76\u8fdb\u884c\u4e86\u5206\u6790; \u5b83\u7684\u8f93\u5165\u6709\u4e24\u90e8\u5206, \u5206\u522b\u662fh(t-1)\u4ee5\u53cax(t), \u4ee3\u8868\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u8f93\u51fa, \u4ee5\u53ca\u6b64\u65f6\u95f4\u6b65\u7684\u8f93\u5165, \u5b83\u4eec\u8fdb\u5165RNN\u7ed3\u6784\u4f53\u540e, \u4f1a\"\u878d\u5408\"\u5230\u4e00\u8d77, \u8fd9\u79cd\u878d\u5408\u6211\u4eec\u6839\u636e\u7ed3\u6784\u89e3\u91ca\u53ef\u77e5, \u662f\u5c06\u4e8c\u8005\u8fdb\u884c\u62fc\u63a5, \u5f62\u6210\u65b0\u7684\u5f20\u91cf[x(t), h(t-1)], \u4e4b\u540e\u8fd9\u4e2a\u65b0\u7684\u5f20\u91cf\u5c06\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42(\u7ebf\u6027\u5c42), \u8be5\u5c42\u4f7f\u7528tanh\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570, \u6700\u7ec8\u5f97\u5230\u8be5\u65f6\u95f4\u6b65\u7684\u8f93\u51fah(t), \u5b83\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u548cx(t+1)\u4e00\u8d77\u8fdb\u5165\u7ed3\u6784\u4f53. \u4ee5\u6b64\u7c7b\u63a8. \u6839\u636e\u7ed3\u6784\u5206\u6790\u5f97\u51fa\u4e86\u4f20\u7edfRNN\u7684\u8ba1\u7b97\u516c\u5f0f. \u5b66\u4e60\u4e86\u6fc0\u6d3b\u51fd\u6570tanh\u7684\u4f5c\u7528: \u7528\u4e8e\u5e2e\u52a9\u8c03\u8282\u6d41\u7ecf\u7f51\u7edc\u7684\u503c, tanh\u51fd\u6570\u5c06\u503c\u538b\u7f29\u5728-1\u548c1\u4e4b\u95f4. \u5b66\u4e60\u4e86Pytorch\u4e2d\u4f20\u7edfRNN\u5de5\u5177\u7684\u4f7f\u7528: \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.RNN\u53ef\u8c03\u7528. nn.RNN\u7c7b\u521d\u59cb\u5316\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input_size: \u8f93\u5165\u5f20\u91cfx\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. hidden_size: \u9690\u5c42\u5f20\u91cfh\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. num_layers: \u9690\u542b\u5c42\u7684\u6570\u91cf. nonlinearity: \u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9, \u9ed8\u8ba4\u662ftanh. nn.RNN\u7c7b\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input: \u8f93\u5165\u5f20\u91cfx. h0: \u521d\u59cb\u5316\u7684\u9690\u5c42\u5f20\u91cfh. \u5b9e\u73b0\u4e86nn.RNN\u7684\u4f7f\u7528\u793a\u4f8b, \u83b7\u5f97RNN\u7684\u771f\u5b9e\u8fd4\u56de\u7ed3\u679c\u6837\u5f0f. \u5b66\u4e60\u4e86\u4f20\u7edfRNN\u7684\u4f18\u52bf: \u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u7b80\u5355, \u5bf9\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u4f4e, \u76f8\u6bd4\u4e4b\u540e\u6211\u4eec\u8981\u5b66\u4e60\u7684RNN\u53d8\u4f53:LSTM\u548cGRU\u6a21\u578b\u53c2\u6570\u603b\u91cf\u5c11\u4e86\u5f88\u591a, \u5728\u77ed\u5e8f\u5217\u4efb\u52a1\u4e0a\u6027\u80fd\u548c\u6548\u679c\u90fd\u8868\u73b0\u4f18\u5f02. \u5b66\u4e60\u4e86\u4f20\u7edfRNN\u7684\u7f3a\u70b9: \u4f20\u7edfRNN\u5728\u89e3\u51b3\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u8054\u65f6, \u901a\u8fc7\u5b9e\u8df5\uff0c\u8bc1\u660e\u7ecf\u5178RNN\u8868\u73b0\u5f88\u5dee, \u539f\u56e0\u662f\u5728\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u7684\u65f6\u5019, \u8fc7\u957f\u7684\u5e8f\u5217\u5bfc\u81f4\u68af\u5ea6\u7684\u8ba1\u7b97\u5f02\u5e38, \u53d1\u751f\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8: \u6839\u636e\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u548c\u94fe\u5f0f\u6cd5\u5219, \u5f97\u5230\u68af\u5ea6\u7684\u8ba1\u7b97\u7684\u7b80\u5316\u516c\u5f0f:\u5176\u4e2dsigmoid\u7684\u5bfc\u6570\u503c\u57df\u662f\u56fa\u5b9a\u7684, \u5728[0, 0.25]\u4e4b\u95f4, \u800c\u4e00\u65e6\u516c\u5f0f\u4e2d\u7684w\u4e5f\u5c0f\u4e8e1, \u90a3\u4e48\u901a\u8fc7\u8fd9\u6837\u7684\u516c\u5f0f\u8fde\u4e58\u540e, \u6700\u7ec8\u7684\u68af\u5ea6\u5c31\u4f1a\u53d8\u5f97\u975e\u5e38\u975e\u5e38\u5c0f, \u8fd9\u79cd\u73b0\u8c61\u79f0\u4f5c\u68af\u5ea6\u6d88\u5931. \u53cd\u4e4b, \u5982\u679c\u6211\u4eec\u4eba\u4e3a\u7684\u589e\u5927w\u7684\u503c, \u4f7f\u5176\u5927\u4e8e1, \u90a3\u4e48\u8fde\u4e58\u591f\u5c31\u53ef\u80fd\u9020\u6210\u68af\u5ea6\u8fc7\u5927, \u79f0\u4f5c\u68af\u5ea6\u7206\u70b8. \u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u7684\u5371\u5bb3: \u5982\u679c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53d1\u751f\u4e86\u68af\u5ea6\u6d88\u5931\uff0c\u6743\u91cd\u65e0\u6cd5\u88ab\u66f4\u65b0\uff0c\u6700\u7ec8\u5bfc\u81f4\u8bad\u7ec3\u5931\u8d25; \u68af\u5ea6\u7206\u70b8\u6240\u5e26\u6765\u7684\u68af\u5ea6\u8fc7\u5927\uff0c\u5927\u5e45\u5ea6\u66f4\u65b0\u7f51\u7edc\u53c2\u6570\uff0c\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u7ed3\u679c\u4f1a\u6ea2\u51fa\uff08NaN\u503c\uff09.","title":"2 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3LSTM\u5185\u90e8\u7ed3\u6784\u53ca\u8ba1\u7b97\u516c\u5f0f. \u638c\u63e1Pytorch\u4e2dLSTM\u5de5\u5177\u7684\u4f7f\u7528. \u4e86\u89e3LSTM\u7684\u4f18\u52bf\u4e0e\u7f3a\u70b9. 1 LSTM\u4ecb\u7ecd \u00b6 LSTM\uff08Long Short-Term Memory\uff09\u4e5f\u79f0\u957f\u77ed\u65f6\u8bb0\u5fc6\u7ed3\u6784, \u5b83\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u4e0e\u7ecf\u5178RNN\u76f8\u6bd4\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6LSTM\u7684\u7ed3\u6784\u66f4\u590d\u6742, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u56db\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u9057\u5fd8\u95e8 \u8f93\u5165\u95e8 \u7ec6\u80de\u72b6\u6001 \u8f93\u51fa\u95e8 2 LSTM\u7684\u5185\u90e8\u7ed3\u6784\u56fe \u00b6 2.1 LSTM\u7ed3\u6784\u5206\u6790 \u00b6 \u7ed3\u6784\u89e3\u91ca\u56fe: \u9057\u5fd8\u95e8\u90e8\u5206\u7ed3\u6784\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u9057\u5fd8\u95e8\u7ed3\u6784\u5206\u6790: \u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u975e\u5e38\u76f8\u4f3c, \u9996\u5148\u5c06\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165x(t)\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u62fc\u63a5, \u5f97\u5230[x(t), h(t-1)], \u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u505a\u53d8\u6362, \u6700\u540e\u901a\u8fc7sigmoid\u51fd\u6570\u8fdb\u884c\u6fc0\u6d3b\u5f97\u5230f(t), \u6211\u4eec\u53ef\u4ee5\u5c06f(t)\u770b\u4f5c\u662f\u95e8\u503c, \u597d\u6bd4\u4e00\u6247\u95e8\u5f00\u5408\u7684\u5927\u5c0f\u7a0b\u5ea6, \u95e8\u503c\u90fd\u5c06\u4f5c\u7528\u5728\u901a\u8fc7\u8be5\u6247\u95e8\u7684\u5f20\u91cf, \u9057\u5fd8\u95e8\u95e8\u503c\u5c06\u4f5c\u7528\u7684\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u4e0a, \u4ee3\u8868\u9057\u5fd8\u8fc7\u53bb\u7684\u591a\u5c11\u4fe1\u606f, \u53c8\u56e0\u4e3a\u9057\u5fd8\u95e8\u95e8\u503c\u662f\u7531x(t), h(t-1)\u8ba1\u7b97\u5f97\u6765\u7684, \u56e0\u6b64\u6574\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u6839\u636e\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165\u548c\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u6765\u51b3\u5b9a\u9057\u5fd8\u591a\u5c11\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u6240\u643a\u5e26\u7684\u8fc7\u5f80\u4fe1\u606f. \u9057\u5fd8\u95e8\u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a: \u6fc0\u6d3b\u51fd\u6570sigmiod\u7684\u4f5c\u7528: \u7528\u4e8e\u5e2e\u52a9\u8c03\u8282\u6d41\u7ecf\u7f51\u7edc\u7684\u503c, sigmoid\u51fd\u6570\u5c06\u503c\u538b\u7f29\u57280\u548c1\u4e4b\u95f4. \u8f93\u5165\u95e8\u90e8\u5206\u7ed3\u6784\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u8f93\u5165\u95e8\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u770b\u5230\u8f93\u5165\u95e8\u7684\u8ba1\u7b97\u516c\u5f0f\u6709\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5c31\u662f\u4ea7\u751f\u8f93\u5165\u95e8\u95e8\u503c\u7684\u516c\u5f0f, \u5b83\u548c\u9057\u5fd8\u95e8\u516c\u5f0f\u51e0\u4e4e\u76f8\u540c, \u533a\u522b\u53ea\u662f\u5728\u4e8e\u5b83\u4eec\u4e4b\u540e\u8981\u4f5c\u7528\u7684\u76ee\u6807\u4e0a. \u8fd9\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u8f93\u5165\u4fe1\u606f\u6709\u591a\u5c11\u9700\u8981\u8fdb\u884c\u8fc7\u6ee4. \u8f93\u5165\u95e8\u7684\u7b2c\u4e8c\u4e2a\u516c\u5f0f\u662f\u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u76f8\u540c. \u5bf9\u4e8eLSTM\u6765\u8bb2, \u5b83\u5f97\u5230\u7684\u662f\u5f53\u524d\u7684\u7ec6\u80de\u72b6\u6001, \u800c\u4e0d\u662f\u50cf\u7ecf\u5178RNN\u4e00\u6837\u5f97\u5230\u7684\u662f\u9690\u542b\u72b6\u6001. \u8f93\u5165\u95e8\u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a: \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u5206\u6790: \u7ec6\u80de\u66f4\u65b0\u7684\u7ed3\u6784\u4e0e\u8ba1\u7b97\u516c\u5f0f\u975e\u5e38\u5bb9\u6613\u7406\u89e3, \u8fd9\u91cc\u6ca1\u6709\u5168\u8fde\u63a5\u5c42, \u53ea\u662f\u5c06\u521a\u521a\u5f97\u5230\u7684\u9057\u5fd8\u95e8\u95e8\u503c\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u5f97\u5230\u7684C(t-1)\u76f8\u4e58, \u518d\u52a0\u4e0a\u8f93\u5165\u95e8\u95e8\u503c\u4e0e\u5f53\u524d\u65f6\u95f4\u6b65\u5f97\u5230\u7684\u672a\u66f4\u65b0C(t)\u76f8\u4e58\u7684\u7ed3\u679c. \u6700\u7ec8\u5f97\u5230\u66f4\u65b0\u540e\u7684C(t)\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u8fc7\u7a0b\u5c31\u662f\u5bf9\u9057\u5fd8\u95e8\u548c\u8f93\u5165\u95e8\u7684\u5e94\u7528. \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u8fc7\u7a0b\u6f14\u793a: \u8f93\u51fa\u95e8\u90e8\u5206\u7ed3\u6784\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u8f93\u51fa\u95e8\u7ed3\u6784\u5206\u6790: \u8f93\u51fa\u95e8\u90e8\u5206\u7684\u516c\u5f0f\u4e5f\u662f\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5373\u662f\u8ba1\u7b97\u8f93\u51fa\u95e8\u7684\u95e8\u503c, \u5b83\u548c\u9057\u5fd8\u95e8\uff0c\u8f93\u5165\u95e8\u8ba1\u7b97\u65b9\u5f0f\u76f8\u540c. \u7b2c\u4e8c\u4e2a\u5373\u662f\u4f7f\u7528\u8fd9\u4e2a\u95e8\u503c\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t), \u4ed6\u5c06\u4f5c\u7528\u5728\u66f4\u65b0\u540e\u7684\u7ec6\u80de\u72b6\u6001C(t)\u4e0a, \u5e76\u505atanh\u6fc0\u6d3b, \u6700\u7ec8\u5f97\u5230h(t)\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u8f93\u51fa\u95e8\u7684\u8fc7\u7a0b, \u5c31\u662f\u4e3a\u4e86\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t). \u8f93\u51fa\u95e8\u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a: 2.2 Bi-LSTM\u4ecb\u7ecd \u00b6 Bi-LSTM\u5373\u53cc\u5411LSTM, \u5b83\u6ca1\u6709\u6539\u53d8LSTM\u672c\u8eab\u4efb\u4f55\u7684\u5185\u90e8\u7ed3\u6784, \u53ea\u662f\u5c06LSTM\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. Bi-LSTM\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u770b\u5230\u56fe\u4e2d\u5bf9\"\u6211\u7231\u4e2d\u56fd\"\u8fd9\u53e5\u8bdd\u6216\u8005\u53eb\u8fd9\u4e2a\u8f93\u5165\u5e8f\u5217, \u8fdb\u884c\u4e86\u4ece\u5de6\u5230\u53f3\u548c\u4ece\u53f3\u5230\u5de6\u4e24\u6b21LSTM\u5904\u7406, \u5c06\u5f97\u5230\u7684\u7ed3\u679c\u5f20\u91cf\u8fdb\u884c\u4e86\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. \u8fd9\u79cd\u7ed3\u6784\u80fd\u591f\u6355\u6349\u8bed\u8a00\u8bed\u6cd5\u4e2d\u4e00\u4e9b\u7279\u5b9a\u7684\u524d\u7f6e\u6216\u540e\u7f6e\u7279\u5f81, \u589e\u5f3a\u8bed\u4e49\u5173\u8054,\u4f46\u662f\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e5f\u968f\u4e4b\u589e\u52a0\u4e86\u4e00\u500d, \u4e00\u822c\u9700\u8981\u5bf9\u8bed\u6599\u548c\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u8bc4\u4f30\u540e\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u8be5\u7ed3\u6784. 2.3 \u4f7f\u7528Pytorch\u6784\u5efaLSTM\u6a21\u578b \u00b6 \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.LSTM\u53ef\u8c03\u7528. nn.LSTM\u7c7b\u521d\u59cb\u5316\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input_size: \u8f93\u5165\u5f20\u91cfx\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. hidden_size: \u9690\u5c42\u5f20\u91cfh\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. num_layers: \u9690\u542b\u5c42\u7684\u6570\u91cf. bidirectional: \u662f\u5426\u9009\u62e9\u4f7f\u7528\u53cc\u5411LSTM, \u5982\u679c\u4e3aTrue, \u5219\u4f7f\u7528; \u9ed8\u8ba4\u4e0d\u4f7f\u7528. nn.LSTM\u7c7b\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input: \u8f93\u5165\u5f20\u91cfx. h0: \u521d\u59cb\u5316\u7684\u9690\u5c42\u5f20\u91cfh. c0: \u521d\u59cb\u5316\u7684\u7ec6\u80de\u72b6\u6001\u5f20\u91cfc. nn.LSTM\u4f7f\u7528\u793a\u4f8b: # \u5b9a\u4e49LSTM\u7684\u53c2\u6570\u542b\u4e49: (input_size, hidden_size, num_layers) # \u5b9a\u4e49\u8f93\u5165\u5f20\u91cf\u7684\u53c2\u6570\u542b\u4e49: (sequence_length, batch_size, input_size) # \u5b9a\u4e49\u9690\u85cf\u5c42\u521d\u59cb\u5f20\u91cf\u548c\u7ec6\u80de\u521d\u59cb\u72b6\u6001\u5f20\u91cf\u7684\u53c2\u6570\u542b\u4e49: # (num_layers * num_directions, batch_size, hidden_size) >>> import torch.nn as nn >>> import torch >>> rnn = nn . LSTM ( 5 , 6 , 2 ) >>> input = torch . randn ( 1 , 3 , 5 ) >>> h0 = torch . randn ( 2 , 3 , 6 ) >>> c0 = torch . randn ( 2 , 3 , 6 ) >>> output , ( hn , cn ) = rnn ( input , ( h0 , c0 )) >>> output tensor ([[[ 0.0447 , - 0.0335 , 0.1454 , 0.0438 , 0.0865 , 0.0416 ], [ 0.0105 , 0.1923 , 0.5507 , - 0.1742 , 0.1569 , - 0.0548 ], [ - 0.1186 , 0.1835 , - 0.0022 , - 0.1388 , - 0.0877 , - 0.4007 ]]], grad_fn =< StackBackward > ) >>> hn tensor ([[[ 0.4647 , - 0.2364 , 0.0645 , - 0.3996 , - 0.0500 , - 0.0152 ], [ 0.3852 , 0.0704 , 0.2103 , - 0.2524 , 0.0243 , 0.0477 ], [ 0.2571 , 0.0608 , 0.2322 , 0.1815 , - 0.0513 , - 0.0291 ]], [[ 0.0447 , - 0.0335 , 0.1454 , 0.0438 , 0.0865 , 0.0416 ], [ 0.0105 , 0.1923 , 0.5507 , - 0.1742 , 0.1569 , - 0.0548 ], [ - 0.1186 , 0.1835 , - 0.0022 , - 0.1388 , - 0.0877 , - 0.4007 ]]], grad_fn =< StackBackward > ) >>> cn tensor ([[[ 0.8083 , - 0.5500 , 0.1009 , - 0.5806 , - 0.0668 , - 0.1161 ], [ 0.7438 , 0.0957 , 0.5509 , - 0.7725 , 0.0824 , 0.0626 ], [ 0.3131 , 0.0920 , 0.8359 , 0.9187 , - 0.4826 , - 0.0717 ]], [[ 0.1240 , - 0.0526 , 0.3035 , 0.1099 , 0.5915 , 0.0828 ], [ 0.0203 , 0.8367 , 0.9832 , - 0.4454 , 0.3917 , - 0.1983 ], [ - 0.2976 , 0.7764 , - 0.0074 , - 0.1965 , - 0.1343 , - 0.6683 ]]], grad_fn =< StackBackward > ) 2.4 LSTM\u4f18\u7f3a\u70b9 \u00b6 LSTM\u4f18\u52bf: LSTM\u7684\u95e8\u7ed3\u6784\u80fd\u591f\u6709\u6548\u51cf\u7f13\u957f\u5e8f\u5217\u95ee\u9898\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u867d\u7136\u5e76\u4e0d\u80fd\u675c\u7edd\u8fd9\u79cd\u73b0\u8c61, \u4f46\u5728\u66f4\u957f\u7684\u5e8f\u5217\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRNN. LSTM\u7f3a\u70b9: \u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u76f8\u5bf9\u8f83\u590d\u6742, \u56e0\u6b64\u8bad\u7ec3\u6548\u7387\u5728\u540c\u7b49\u7b97\u529b\u4e0b\u8f83\u4f20\u7edfRNN\u4f4e\u5f88\u591a. 3 \u5c0f\u7ed3 \u00b6 LSTM\uff08Long Short-Term Memory\uff09\u4e5f\u79f0\u957f\u77ed\u65f6\u8bb0\u5fc6\u7ed3\u6784, \u5b83\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u4e0e\u7ecf\u5178RNN\u76f8\u6bd4\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6LSTM\u7684\u7ed3\u6784\u66f4\u590d\u6742, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u56db\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u9057\u5fd8\u95e8 \u8f93\u5165\u95e8 \u8f93\u51fa\u95e8 \u7ec6\u80de\u72b6\u6001 \u9057\u5fd8\u95e8\u7ed3\u6784\u5206\u6790: \u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u975e\u5e38\u76f8\u4f3c, \u9996\u5148\u5c06\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165x(t)\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u62fc\u63a5, \u5f97\u5230[x(t), h(t-1)], \u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u505a\u53d8\u6362, \u6700\u540e\u901a\u8fc7sigmoid\u51fd\u6570\u8fdb\u884c\u6fc0\u6d3b\u5f97\u5230f(t), \u6211\u4eec\u53ef\u4ee5\u5c06f(t)\u770b\u4f5c\u662f\u95e8\u503c, \u597d\u6bd4\u4e00\u6247\u95e8\u5f00\u5408\u7684\u5927\u5c0f\u7a0b\u5ea6, \u95e8\u503c\u90fd\u5c06\u4f5c\u7528\u5728\u901a\u8fc7\u8be5\u6247\u95e8\u7684\u5f20\u91cf, \u9057\u5fd8\u95e8\u95e8\u503c\u5c06\u4f5c\u7528\u7684\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u4e0a, \u4ee3\u8868\u9057\u5fd8\u8fc7\u53bb\u7684\u591a\u5c11\u4fe1\u606f, \u53c8\u56e0\u4e3a\u9057\u5fd8\u95e8\u95e8\u503c\u662f\u7531x(t), h(t-1)\u8ba1\u7b97\u5f97\u6765\u7684, \u56e0\u6b64\u6574\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u6839\u636e\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165\u548c\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u6765\u51b3\u5b9a\u9057\u5fd8\u591a\u5c11\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u6240\u643a\u5e26\u7684\u8fc7\u5f80\u4fe1\u606f. \u8f93\u5165\u95e8\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u770b\u5230\u8f93\u5165\u95e8\u7684\u8ba1\u7b97\u516c\u5f0f\u6709\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5c31\u662f\u4ea7\u751f\u8f93\u5165\u95e8\u95e8\u503c\u7684\u516c\u5f0f, \u5b83\u548c\u9057\u5fd8\u95e8\u516c\u5f0f\u51e0\u4e4e\u76f8\u540c, \u533a\u522b\u53ea\u662f\u5728\u4e8e\u5b83\u4eec\u4e4b\u540e\u8981\u4f5c\u7528\u7684\u76ee\u6807\u4e0a. \u8fd9\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u8f93\u5165\u4fe1\u606f\u6709\u591a\u5c11\u9700\u8981\u8fdb\u884c\u8fc7\u6ee4. \u8f93\u5165\u95e8\u7684\u7b2c\u4e8c\u4e2a\u516c\u5f0f\u662f\u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u76f8\u540c. \u5bf9\u4e8eLSTM\u6765\u8bb2, \u5b83\u5f97\u5230\u7684\u662f\u5f53\u524d\u7684\u7ec6\u80de\u72b6\u6001, \u800c\u4e0d\u662f\u50cf\u7ecf\u5178RNN\u4e00\u6837\u5f97\u5230\u7684\u662f\u9690\u542b\u72b6\u6001. \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u5206\u6790: \u7ec6\u80de\u66f4\u65b0\u7684\u7ed3\u6784\u4e0e\u8ba1\u7b97\u516c\u5f0f\u975e\u5e38\u5bb9\u6613\u7406\u89e3, \u8fd9\u91cc\u6ca1\u6709\u5168\u8fde\u63a5\u5c42, \u53ea\u662f\u5c06\u521a\u521a\u5f97\u5230\u7684\u9057\u5fd8\u95e8\u95e8\u503c\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u5f97\u5230\u7684C(t-1)\u76f8\u4e58, \u518d\u52a0\u4e0a\u8f93\u5165\u95e8\u95e8\u503c\u4e0e\u5f53\u524d\u65f6\u95f4\u6b65\u5f97\u5230\u7684\u672a\u66f4\u65b0C(t)\u76f8\u4e58\u7684\u7ed3\u679c. \u6700\u7ec8\u5f97\u5230\u66f4\u65b0\u540e\u7684C(t)\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u8fc7\u7a0b\u5c31\u662f\u5bf9\u9057\u5fd8\u95e8\u548c\u8f93\u5165\u95e8\u7684\u5e94\u7528. \u8f93\u51fa\u95e8\u7ed3\u6784\u5206\u6790: \u8f93\u51fa\u95e8\u90e8\u5206\u7684\u516c\u5f0f\u4e5f\u662f\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5373\u662f\u8ba1\u7b97\u8f93\u51fa\u95e8\u7684\u95e8\u503c, \u5b83\u548c\u9057\u5fd8\u95e8\uff0c\u8f93\u5165\u95e8\u8ba1\u7b97\u65b9\u5f0f\u76f8\u540c. \u7b2c\u4e8c\u4e2a\u5373\u662f\u4f7f\u7528\u8fd9\u4e2a\u95e8\u503c\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t), \u4ed6\u5c06\u4f5c\u7528\u5728\u66f4\u65b0\u540e\u7684\u7ec6\u80de\u72b6\u6001C(t)\u4e0a, \u5e76\u505atanh\u6fc0\u6d3b, \u6700\u7ec8\u5f97\u5230h(t)\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u8f93\u51fa\u95e8\u7684\u8fc7\u7a0b, \u5c31\u662f\u4e3a\u4e86\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t). \u4ec0\u4e48\u662fBi-LSTM ? Bi-LSTM\u5373\u53cc\u5411LSTM, \u5b83\u6ca1\u6709\u6539\u53d8LSTM\u672c\u8eab\u4efb\u4f55\u7684\u5185\u90e8\u7ed3\u6784, \u53ea\u662f\u5c06LSTM\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. Pytorch\u4e2dLSTM\u5de5\u5177\u7684\u4f7f\u7528: \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.LSTM\u53ef\u8c03\u7528. LSTM\u4f18\u52bf: LSTM\u7684\u95e8\u7ed3\u6784\u80fd\u591f\u6709\u6548\u51cf\u7f13\u957f\u5e8f\u5217\u95ee\u9898\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u867d\u7136\u5e76\u4e0d\u80fd\u675c\u7edd\u8fd9\u79cd\u73b0\u8c61, \u4f46\u5728\u66f4\u957f\u7684\u5e8f\u5217\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRNN. LSTM\u7f3a\u70b9: \u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u76f8\u5bf9\u8f83\u590d\u6742, \u56e0\u6b64\u8bad\u7ec3\u6548\u7387\u5728\u540c\u7b49\u7b97\u529b\u4e0b\u8f83\u4f20\u7edfRNN\u4f4e\u5f88\u591a.","title":"3 LSTM\u6a21\u578b"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#_1","text":"\u4e86\u89e3LSTM\u5185\u90e8\u7ed3\u6784\u53ca\u8ba1\u7b97\u516c\u5f0f. \u638c\u63e1Pytorch\u4e2dLSTM\u5de5\u5177\u7684\u4f7f\u7528. \u4e86\u89e3LSTM\u7684\u4f18\u52bf\u4e0e\u7f3a\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#1-lstm","text":"LSTM\uff08Long Short-Term Memory\uff09\u4e5f\u79f0\u957f\u77ed\u65f6\u8bb0\u5fc6\u7ed3\u6784, \u5b83\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u4e0e\u7ecf\u5178RNN\u76f8\u6bd4\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6LSTM\u7684\u7ed3\u6784\u66f4\u590d\u6742, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u56db\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u9057\u5fd8\u95e8 \u8f93\u5165\u95e8 \u7ec6\u80de\u72b6\u6001 \u8f93\u51fa\u95e8","title":"1 LSTM\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#2-lstm","text":"","title":"2 LSTM\u7684\u5185\u90e8\u7ed3\u6784\u56fe"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#21-lstm","text":"\u7ed3\u6784\u89e3\u91ca\u56fe: \u9057\u5fd8\u95e8\u90e8\u5206\u7ed3\u6784\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u9057\u5fd8\u95e8\u7ed3\u6784\u5206\u6790: \u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u975e\u5e38\u76f8\u4f3c, \u9996\u5148\u5c06\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165x(t)\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u62fc\u63a5, \u5f97\u5230[x(t), h(t-1)], \u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u505a\u53d8\u6362, \u6700\u540e\u901a\u8fc7sigmoid\u51fd\u6570\u8fdb\u884c\u6fc0\u6d3b\u5f97\u5230f(t), \u6211\u4eec\u53ef\u4ee5\u5c06f(t)\u770b\u4f5c\u662f\u95e8\u503c, \u597d\u6bd4\u4e00\u6247\u95e8\u5f00\u5408\u7684\u5927\u5c0f\u7a0b\u5ea6, \u95e8\u503c\u90fd\u5c06\u4f5c\u7528\u5728\u901a\u8fc7\u8be5\u6247\u95e8\u7684\u5f20\u91cf, \u9057\u5fd8\u95e8\u95e8\u503c\u5c06\u4f5c\u7528\u7684\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u4e0a, \u4ee3\u8868\u9057\u5fd8\u8fc7\u53bb\u7684\u591a\u5c11\u4fe1\u606f, \u53c8\u56e0\u4e3a\u9057\u5fd8\u95e8\u95e8\u503c\u662f\u7531x(t), h(t-1)\u8ba1\u7b97\u5f97\u6765\u7684, \u56e0\u6b64\u6574\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u6839\u636e\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165\u548c\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u6765\u51b3\u5b9a\u9057\u5fd8\u591a\u5c11\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u6240\u643a\u5e26\u7684\u8fc7\u5f80\u4fe1\u606f. \u9057\u5fd8\u95e8\u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a: \u6fc0\u6d3b\u51fd\u6570sigmiod\u7684\u4f5c\u7528: \u7528\u4e8e\u5e2e\u52a9\u8c03\u8282\u6d41\u7ecf\u7f51\u7edc\u7684\u503c, sigmoid\u51fd\u6570\u5c06\u503c\u538b\u7f29\u57280\u548c1\u4e4b\u95f4. \u8f93\u5165\u95e8\u90e8\u5206\u7ed3\u6784\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u8f93\u5165\u95e8\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u770b\u5230\u8f93\u5165\u95e8\u7684\u8ba1\u7b97\u516c\u5f0f\u6709\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5c31\u662f\u4ea7\u751f\u8f93\u5165\u95e8\u95e8\u503c\u7684\u516c\u5f0f, \u5b83\u548c\u9057\u5fd8\u95e8\u516c\u5f0f\u51e0\u4e4e\u76f8\u540c, \u533a\u522b\u53ea\u662f\u5728\u4e8e\u5b83\u4eec\u4e4b\u540e\u8981\u4f5c\u7528\u7684\u76ee\u6807\u4e0a. \u8fd9\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u8f93\u5165\u4fe1\u606f\u6709\u591a\u5c11\u9700\u8981\u8fdb\u884c\u8fc7\u6ee4. \u8f93\u5165\u95e8\u7684\u7b2c\u4e8c\u4e2a\u516c\u5f0f\u662f\u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u76f8\u540c. \u5bf9\u4e8eLSTM\u6765\u8bb2, \u5b83\u5f97\u5230\u7684\u662f\u5f53\u524d\u7684\u7ec6\u80de\u72b6\u6001, \u800c\u4e0d\u662f\u50cf\u7ecf\u5178RNN\u4e00\u6837\u5f97\u5230\u7684\u662f\u9690\u542b\u72b6\u6001. \u8f93\u5165\u95e8\u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a: \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u5206\u6790: \u7ec6\u80de\u66f4\u65b0\u7684\u7ed3\u6784\u4e0e\u8ba1\u7b97\u516c\u5f0f\u975e\u5e38\u5bb9\u6613\u7406\u89e3, \u8fd9\u91cc\u6ca1\u6709\u5168\u8fde\u63a5\u5c42, \u53ea\u662f\u5c06\u521a\u521a\u5f97\u5230\u7684\u9057\u5fd8\u95e8\u95e8\u503c\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u5f97\u5230\u7684C(t-1)\u76f8\u4e58, \u518d\u52a0\u4e0a\u8f93\u5165\u95e8\u95e8\u503c\u4e0e\u5f53\u524d\u65f6\u95f4\u6b65\u5f97\u5230\u7684\u672a\u66f4\u65b0C(t)\u76f8\u4e58\u7684\u7ed3\u679c. \u6700\u7ec8\u5f97\u5230\u66f4\u65b0\u540e\u7684C(t)\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u8fc7\u7a0b\u5c31\u662f\u5bf9\u9057\u5fd8\u95e8\u548c\u8f93\u5165\u95e8\u7684\u5e94\u7528. \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u8fc7\u7a0b\u6f14\u793a: \u8f93\u51fa\u95e8\u90e8\u5206\u7ed3\u6784\u56fe\u4e0e\u8ba1\u7b97\u516c\u5f0f: \u8f93\u51fa\u95e8\u7ed3\u6784\u5206\u6790: \u8f93\u51fa\u95e8\u90e8\u5206\u7684\u516c\u5f0f\u4e5f\u662f\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5373\u662f\u8ba1\u7b97\u8f93\u51fa\u95e8\u7684\u95e8\u503c, \u5b83\u548c\u9057\u5fd8\u95e8\uff0c\u8f93\u5165\u95e8\u8ba1\u7b97\u65b9\u5f0f\u76f8\u540c. \u7b2c\u4e8c\u4e2a\u5373\u662f\u4f7f\u7528\u8fd9\u4e2a\u95e8\u503c\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t), \u4ed6\u5c06\u4f5c\u7528\u5728\u66f4\u65b0\u540e\u7684\u7ec6\u80de\u72b6\u6001C(t)\u4e0a, \u5e76\u505atanh\u6fc0\u6d3b, \u6700\u7ec8\u5f97\u5230h(t)\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u8f93\u51fa\u95e8\u7684\u8fc7\u7a0b, \u5c31\u662f\u4e3a\u4e86\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t). \u8f93\u51fa\u95e8\u5185\u90e8\u7ed3\u6784\u8fc7\u7a0b\u6f14\u793a:","title":"2.1 LSTM\u7ed3\u6784\u5206\u6790"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#22-bi-lstm","text":"Bi-LSTM\u5373\u53cc\u5411LSTM, \u5b83\u6ca1\u6709\u6539\u53d8LSTM\u672c\u8eab\u4efb\u4f55\u7684\u5185\u90e8\u7ed3\u6784, \u53ea\u662f\u5c06LSTM\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. Bi-LSTM\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u770b\u5230\u56fe\u4e2d\u5bf9\"\u6211\u7231\u4e2d\u56fd\"\u8fd9\u53e5\u8bdd\u6216\u8005\u53eb\u8fd9\u4e2a\u8f93\u5165\u5e8f\u5217, \u8fdb\u884c\u4e86\u4ece\u5de6\u5230\u53f3\u548c\u4ece\u53f3\u5230\u5de6\u4e24\u6b21LSTM\u5904\u7406, \u5c06\u5f97\u5230\u7684\u7ed3\u679c\u5f20\u91cf\u8fdb\u884c\u4e86\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. \u8fd9\u79cd\u7ed3\u6784\u80fd\u591f\u6355\u6349\u8bed\u8a00\u8bed\u6cd5\u4e2d\u4e00\u4e9b\u7279\u5b9a\u7684\u524d\u7f6e\u6216\u540e\u7f6e\u7279\u5f81, \u589e\u5f3a\u8bed\u4e49\u5173\u8054,\u4f46\u662f\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e5f\u968f\u4e4b\u589e\u52a0\u4e86\u4e00\u500d, \u4e00\u822c\u9700\u8981\u5bf9\u8bed\u6599\u548c\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u8bc4\u4f30\u540e\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u8be5\u7ed3\u6784.","title":"2.2 Bi-LSTM\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#23-pytorchlstm","text":"\u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.LSTM\u53ef\u8c03\u7528. nn.LSTM\u7c7b\u521d\u59cb\u5316\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input_size: \u8f93\u5165\u5f20\u91cfx\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. hidden_size: \u9690\u5c42\u5f20\u91cfh\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. num_layers: \u9690\u542b\u5c42\u7684\u6570\u91cf. bidirectional: \u662f\u5426\u9009\u62e9\u4f7f\u7528\u53cc\u5411LSTM, \u5982\u679c\u4e3aTrue, \u5219\u4f7f\u7528; \u9ed8\u8ba4\u4e0d\u4f7f\u7528. nn.LSTM\u7c7b\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input: \u8f93\u5165\u5f20\u91cfx. h0: \u521d\u59cb\u5316\u7684\u9690\u5c42\u5f20\u91cfh. c0: \u521d\u59cb\u5316\u7684\u7ec6\u80de\u72b6\u6001\u5f20\u91cfc. nn.LSTM\u4f7f\u7528\u793a\u4f8b: # \u5b9a\u4e49LSTM\u7684\u53c2\u6570\u542b\u4e49: (input_size, hidden_size, num_layers) # \u5b9a\u4e49\u8f93\u5165\u5f20\u91cf\u7684\u53c2\u6570\u542b\u4e49: (sequence_length, batch_size, input_size) # \u5b9a\u4e49\u9690\u85cf\u5c42\u521d\u59cb\u5f20\u91cf\u548c\u7ec6\u80de\u521d\u59cb\u72b6\u6001\u5f20\u91cf\u7684\u53c2\u6570\u542b\u4e49: # (num_layers * num_directions, batch_size, hidden_size) >>> import torch.nn as nn >>> import torch >>> rnn = nn . LSTM ( 5 , 6 , 2 ) >>> input = torch . randn ( 1 , 3 , 5 ) >>> h0 = torch . randn ( 2 , 3 , 6 ) >>> c0 = torch . randn ( 2 , 3 , 6 ) >>> output , ( hn , cn ) = rnn ( input , ( h0 , c0 )) >>> output tensor ([[[ 0.0447 , - 0.0335 , 0.1454 , 0.0438 , 0.0865 , 0.0416 ], [ 0.0105 , 0.1923 , 0.5507 , - 0.1742 , 0.1569 , - 0.0548 ], [ - 0.1186 , 0.1835 , - 0.0022 , - 0.1388 , - 0.0877 , - 0.4007 ]]], grad_fn =< StackBackward > ) >>> hn tensor ([[[ 0.4647 , - 0.2364 , 0.0645 , - 0.3996 , - 0.0500 , - 0.0152 ], [ 0.3852 , 0.0704 , 0.2103 , - 0.2524 , 0.0243 , 0.0477 ], [ 0.2571 , 0.0608 , 0.2322 , 0.1815 , - 0.0513 , - 0.0291 ]], [[ 0.0447 , - 0.0335 , 0.1454 , 0.0438 , 0.0865 , 0.0416 ], [ 0.0105 , 0.1923 , 0.5507 , - 0.1742 , 0.1569 , - 0.0548 ], [ - 0.1186 , 0.1835 , - 0.0022 , - 0.1388 , - 0.0877 , - 0.4007 ]]], grad_fn =< StackBackward > ) >>> cn tensor ([[[ 0.8083 , - 0.5500 , 0.1009 , - 0.5806 , - 0.0668 , - 0.1161 ], [ 0.7438 , 0.0957 , 0.5509 , - 0.7725 , 0.0824 , 0.0626 ], [ 0.3131 , 0.0920 , 0.8359 , 0.9187 , - 0.4826 , - 0.0717 ]], [[ 0.1240 , - 0.0526 , 0.3035 , 0.1099 , 0.5915 , 0.0828 ], [ 0.0203 , 0.8367 , 0.9832 , - 0.4454 , 0.3917 , - 0.1983 ], [ - 0.2976 , 0.7764 , - 0.0074 , - 0.1965 , - 0.1343 , - 0.6683 ]]], grad_fn =< StackBackward > )","title":"2.3 \u4f7f\u7528Pytorch\u6784\u5efaLSTM\u6a21\u578b"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#24-lstm","text":"LSTM\u4f18\u52bf: LSTM\u7684\u95e8\u7ed3\u6784\u80fd\u591f\u6709\u6548\u51cf\u7f13\u957f\u5e8f\u5217\u95ee\u9898\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u867d\u7136\u5e76\u4e0d\u80fd\u675c\u7edd\u8fd9\u79cd\u73b0\u8c61, \u4f46\u5728\u66f4\u957f\u7684\u5e8f\u5217\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRNN. LSTM\u7f3a\u70b9: \u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u76f8\u5bf9\u8f83\u590d\u6742, \u56e0\u6b64\u8bad\u7ec3\u6548\u7387\u5728\u540c\u7b49\u7b97\u529b\u4e0b\u8f83\u4f20\u7edfRNN\u4f4e\u5f88\u591a.","title":"2.4 LSTM\u4f18\u7f3a\u70b9"},{"location":"03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html#3","text":"LSTM\uff08Long Short-Term Memory\uff09\u4e5f\u79f0\u957f\u77ed\u65f6\u8bb0\u5fc6\u7ed3\u6784, \u5b83\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u4e0e\u7ecf\u5178RNN\u76f8\u6bd4\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6LSTM\u7684\u7ed3\u6784\u66f4\u590d\u6742, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u56db\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u9057\u5fd8\u95e8 \u8f93\u5165\u95e8 \u8f93\u51fa\u95e8 \u7ec6\u80de\u72b6\u6001 \u9057\u5fd8\u95e8\u7ed3\u6784\u5206\u6790: \u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u975e\u5e38\u76f8\u4f3c, \u9996\u5148\u5c06\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165x(t)\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u62fc\u63a5, \u5f97\u5230[x(t), h(t-1)], \u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u505a\u53d8\u6362, \u6700\u540e\u901a\u8fc7sigmoid\u51fd\u6570\u8fdb\u884c\u6fc0\u6d3b\u5f97\u5230f(t), \u6211\u4eec\u53ef\u4ee5\u5c06f(t)\u770b\u4f5c\u662f\u95e8\u503c, \u597d\u6bd4\u4e00\u6247\u95e8\u5f00\u5408\u7684\u5927\u5c0f\u7a0b\u5ea6, \u95e8\u503c\u90fd\u5c06\u4f5c\u7528\u5728\u901a\u8fc7\u8be5\u6247\u95e8\u7684\u5f20\u91cf, \u9057\u5fd8\u95e8\u95e8\u503c\u5c06\u4f5c\u7528\u7684\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u4e0a, \u4ee3\u8868\u9057\u5fd8\u8fc7\u53bb\u7684\u591a\u5c11\u4fe1\u606f, \u53c8\u56e0\u4e3a\u9057\u5fd8\u95e8\u95e8\u503c\u662f\u7531x(t), h(t-1)\u8ba1\u7b97\u5f97\u6765\u7684, \u56e0\u6b64\u6574\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u6839\u636e\u5f53\u524d\u65f6\u95f4\u6b65\u8f93\u5165\u548c\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u9690\u542b\u72b6\u6001h(t-1)\u6765\u51b3\u5b9a\u9057\u5fd8\u591a\u5c11\u4e0a\u4e00\u5c42\u7684\u7ec6\u80de\u72b6\u6001\u6240\u643a\u5e26\u7684\u8fc7\u5f80\u4fe1\u606f. \u8f93\u5165\u95e8\u7ed3\u6784\u5206\u6790: \u6211\u4eec\u770b\u5230\u8f93\u5165\u95e8\u7684\u8ba1\u7b97\u516c\u5f0f\u6709\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5c31\u662f\u4ea7\u751f\u8f93\u5165\u95e8\u95e8\u503c\u7684\u516c\u5f0f, \u5b83\u548c\u9057\u5fd8\u95e8\u516c\u5f0f\u51e0\u4e4e\u76f8\u540c, \u533a\u522b\u53ea\u662f\u5728\u4e8e\u5b83\u4eec\u4e4b\u540e\u8981\u4f5c\u7528\u7684\u76ee\u6807\u4e0a. \u8fd9\u4e2a\u516c\u5f0f\u610f\u5473\u7740\u8f93\u5165\u4fe1\u606f\u6709\u591a\u5c11\u9700\u8981\u8fdb\u884c\u8fc7\u6ee4. \u8f93\u5165\u95e8\u7684\u7b2c\u4e8c\u4e2a\u516c\u5f0f\u662f\u4e0e\u4f20\u7edfRNN\u7684\u5185\u90e8\u7ed3\u6784\u8ba1\u7b97\u76f8\u540c. \u5bf9\u4e8eLSTM\u6765\u8bb2, \u5b83\u5f97\u5230\u7684\u662f\u5f53\u524d\u7684\u7ec6\u80de\u72b6\u6001, \u800c\u4e0d\u662f\u50cf\u7ecf\u5178RNN\u4e00\u6837\u5f97\u5230\u7684\u662f\u9690\u542b\u72b6\u6001. \u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u5206\u6790: \u7ec6\u80de\u66f4\u65b0\u7684\u7ed3\u6784\u4e0e\u8ba1\u7b97\u516c\u5f0f\u975e\u5e38\u5bb9\u6613\u7406\u89e3, \u8fd9\u91cc\u6ca1\u6709\u5168\u8fde\u63a5\u5c42, \u53ea\u662f\u5c06\u521a\u521a\u5f97\u5230\u7684\u9057\u5fd8\u95e8\u95e8\u503c\u4e0e\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u5f97\u5230\u7684C(t-1)\u76f8\u4e58, \u518d\u52a0\u4e0a\u8f93\u5165\u95e8\u95e8\u503c\u4e0e\u5f53\u524d\u65f6\u95f4\u6b65\u5f97\u5230\u7684\u672a\u66f4\u65b0C(t)\u76f8\u4e58\u7684\u7ed3\u679c. \u6700\u7ec8\u5f97\u5230\u66f4\u65b0\u540e\u7684C(t)\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u7ec6\u80de\u72b6\u6001\u66f4\u65b0\u8fc7\u7a0b\u5c31\u662f\u5bf9\u9057\u5fd8\u95e8\u548c\u8f93\u5165\u95e8\u7684\u5e94\u7528. \u8f93\u51fa\u95e8\u7ed3\u6784\u5206\u6790: \u8f93\u51fa\u95e8\u90e8\u5206\u7684\u516c\u5f0f\u4e5f\u662f\u4e24\u4e2a, \u7b2c\u4e00\u4e2a\u5373\u662f\u8ba1\u7b97\u8f93\u51fa\u95e8\u7684\u95e8\u503c, \u5b83\u548c\u9057\u5fd8\u95e8\uff0c\u8f93\u5165\u95e8\u8ba1\u7b97\u65b9\u5f0f\u76f8\u540c. \u7b2c\u4e8c\u4e2a\u5373\u662f\u4f7f\u7528\u8fd9\u4e2a\u95e8\u503c\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t), \u4ed6\u5c06\u4f5c\u7528\u5728\u66f4\u65b0\u540e\u7684\u7ec6\u80de\u72b6\u6001C(t)\u4e0a, \u5e76\u505atanh\u6fc0\u6d3b, \u6700\u7ec8\u5f97\u5230h(t)\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u95f4\u6b65\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u6574\u4e2a\u8f93\u51fa\u95e8\u7684\u8fc7\u7a0b, \u5c31\u662f\u4e3a\u4e86\u4ea7\u751f\u9690\u542b\u72b6\u6001h(t). \u4ec0\u4e48\u662fBi-LSTM ? Bi-LSTM\u5373\u53cc\u5411LSTM, \u5b83\u6ca1\u6709\u6539\u53d8LSTM\u672c\u8eab\u4efb\u4f55\u7684\u5185\u90e8\u7ed3\u6784, \u53ea\u662f\u5c06LSTM\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. Pytorch\u4e2dLSTM\u5de5\u5177\u7684\u4f7f\u7528: \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.LSTM\u53ef\u8c03\u7528. LSTM\u4f18\u52bf: LSTM\u7684\u95e8\u7ed3\u6784\u80fd\u591f\u6709\u6548\u51cf\u7f13\u957f\u5e8f\u5217\u95ee\u9898\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u867d\u7136\u5e76\u4e0d\u80fd\u675c\u7edd\u8fd9\u79cd\u73b0\u8c61, \u4f46\u5728\u66f4\u957f\u7684\u5e8f\u5217\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRNN. LSTM\u7f3a\u70b9: \u7531\u4e8e\u5185\u90e8\u7ed3\u6784\u76f8\u5bf9\u8f83\u590d\u6742, \u56e0\u6b64\u8bad\u7ec3\u6548\u7387\u5728\u540c\u7b49\u7b97\u529b\u4e0b\u8f83\u4f20\u7edfRNN\u4f4e\u5f88\u591a.","title":"3 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3GRU\u5185\u90e8\u7ed3\u6784\u53ca\u8ba1\u7b97\u516c\u5f0f. \u638c\u63e1Pytorch\u4e2dGRU\u5de5\u5177\u7684\u4f7f\u7528. \u4e86\u89e3GRU\u7684\u4f18\u52bf\u4e0e\u7f3a\u70b9. 1 GRU\u4ecb\u7ecd \u00b6 GRU\uff08Gated Recurrent Unit\uff09\u4e5f\u79f0\u95e8\u63a7\u5faa\u73af\u5355\u5143\u7ed3\u6784, \u5b83\u4e5f\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u540cLSTM\u4e00\u6837\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6\u5b83\u7684\u7ed3\u6784\u548c\u8ba1\u7b97\u8981\u6bd4LSTM\u66f4\u7b80\u5355, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u66f4\u65b0\u95e8 \u91cd\u7f6e\u95e8 2 GRU\u7684\u5185\u90e8\u7ed3\u6784\u56fe \u00b6 2.1 GRU\u7ed3\u6784\u5206\u6790 \u00b6 \u7ed3\u6784\u89e3\u91ca\u56fe: GRU\u7684\u66f4\u65b0\u95e8\u548c\u91cd\u7f6e\u95e8\u7ed3\u6784\u56fe: \u5185\u90e8\u7ed3\u6784\u5206\u6790: \u548c\u4e4b\u524d\u5206\u6790\u8fc7\u7684LSTM\u4e2d\u7684\u95e8\u63a7\u4e00\u6837, \u9996\u5148\u8ba1\u7b97\u66f4\u65b0\u95e8\u548c\u91cd\u7f6e\u95e8\u7684\u95e8\u503c, \u5206\u522b\u662fz(t)\u548cr(t), \u8ba1\u7b97\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528X(t)\u4e0eh(t-1)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u6362, \u518d\u7ecf\u8fc7sigmoid\u6fc0\u6d3b. \u4e4b\u540e\u91cd\u7f6e\u95e8\u95e8\u503c\u4f5c\u7528\u5728\u4e86h(t-1)\u4e0a, \u4ee3\u8868\u63a7\u5236\u4e0a\u4e00\u65f6\u95f4\u6b65\u4f20\u6765\u7684\u4fe1\u606f\u6709\u591a\u5c11\u53ef\u4ee5\u88ab\u5229\u7528. \u63a5\u7740\u5c31\u662f\u4f7f\u7528\u8fd9\u4e2a\u91cd\u7f6e\u540e\u7684h(t-1)\u8fdb\u884c\u57fa\u672c\u7684RNN\u8ba1\u7b97, \u5373\u4e0ex(t)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u5316, \u7ecf\u8fc7tanh\u6fc0\u6d3b, \u5f97\u5230\u65b0\u7684h(t). \u6700\u540e\u66f4\u65b0\u95e8\u7684\u95e8\u503c\u4f1a\u4f5c\u7528\u5728\u65b0\u7684h(t)\uff0c\u800c1-\u95e8\u503c\u4f1a\u4f5c\u7528\u5728h(t-1)\u4e0a, \u968f\u540e\u5c06\u4e24\u8005\u7684\u7ed3\u679c\u76f8\u52a0, \u5f97\u5230\u6700\u7ec8\u7684\u9690\u542b\u72b6\u6001\u8f93\u51fah(t), \u8fd9\u4e2a\u8fc7\u7a0b\u610f\u5473\u7740\u66f4\u65b0\u95e8\u6709\u80fd\u529b\u4fdd\u7559\u4e4b\u524d\u7684\u7ed3\u679c, \u5f53\u95e8\u503c\u8d8b\u4e8e1\u65f6, \u8f93\u51fa\u5c31\u662f\u65b0\u7684h(t), \u800c\u5f53\u95e8\u503c\u8d8b\u4e8e0\u65f6, \u8f93\u51fa\u5c31\u662f\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684h(t-1). 2.2 Bi-GRU\u4ecb\u7ecd \u00b6 Bi-GRU\u4e0eBi-LSTM\u7684\u903b\u8f91\u76f8\u540c, \u90fd\u662f\u4e0d\u6539\u53d8\u5176\u5185\u90e8\u7ed3\u6784, \u800c\u662f\u5c06\u6a21\u578b\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. \u5177\u4f53\u53c2\u89c1\u4e0a\u5c0f\u8282\u4e2d\u7684Bi-LSTM. 2.3 \u4f7f\u7528Pytorch\u6784\u5efaGRU\u6a21\u578b \u00b6 \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.GRU\u53ef\u8c03\u7528. nn.GRU\u7c7b\u521d\u59cb\u5316\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input_size: \u8f93\u5165\u5f20\u91cfx\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. hidden_size: \u9690\u5c42\u5f20\u91cfh\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. num_layers: \u9690\u542b\u5c42\u7684\u6570\u91cf. bidirectional: \u662f\u5426\u9009\u62e9\u4f7f\u7528\u53cc\u5411LSTM, \u5982\u679c\u4e3aTrue, \u5219\u4f7f\u7528; \u9ed8\u8ba4\u4e0d\u4f7f\u7528. nn.GRU\u7c7b\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input: \u8f93\u5165\u5f20\u91cfx. h0: \u521d\u59cb\u5316\u7684\u9690\u5c42\u5f20\u91cfh. nn.GRU\u4f7f\u7528\u793a\u4f8b: >>> import torch >>> import torch.nn as nn >>> rnn = nn . GRU ( 5 , 6 , 2 ) >>> input = torch . randn ( 1 , 3 , 5 ) >>> h0 = torch . randn ( 2 , 3 , 6 ) >>> output , hn = rnn ( input , h0 ) >>> output tensor ([[[ - 0.2097 , - 2.2225 , 0.6204 , - 0.1745 , - 0.1749 , - 0.0460 ], [ - 0.3820 , 0.0465 , - 0.4798 , 0.6837 , - 0.7894 , 0.5173 ], [ - 0.0184 , - 0.2758 , 1.2482 , 0.5514 , - 0.9165 , - 0.6667 ]]], grad_fn =< StackBackward > ) >>> hn tensor ([[[ 0.6578 , - 0.4226 , - 0.2129 , - 0.3785 , 0.5070 , 0.4338 ], [ - 0.5072 , 0.5948 , 0.8083 , 0.4618 , 0.1629 , - 0.1591 ], [ 0.2430 , - 0.4981 , 0.3846 , - 0.4252 , 0.7191 , 0.5420 ]], [[ - 0.2097 , - 2.2225 , 0.6204 , - 0.1745 , - 0.1749 , - 0.0460 ], [ - 0.3820 , 0.0465 , - 0.4798 , 0.6837 , - 0.7894 , 0.5173 ], [ - 0.0184 , - 0.2758 , 1.2482 , 0.5514 , - 0.9165 , - 0.6667 ]]], grad_fn =< StackBackward > ) 2.4 GRU\u4f18\u7f3a\u70b9 \u00b6 GRU\u7684\u4f18\u52bf: GRU\u548cLSTM\u4f5c\u7528\u76f8\u540c, \u5728\u6355\u6349\u957f\u5e8f\u5217\u8bed\u4e49\u5173\u8054\u65f6, \u80fd\u6709\u6548\u6291\u5236\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u6548\u679c\u90fd\u4f18\u4e8e\u4f20\u7edfRNN\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u6bd4LSTM\u8981\u5c0f. GRU\u7684\u7f3a\u70b9: GRU\u4ecd\u7136\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898, \u540c\u65f6\u5176\u4f5c\u7528RNN\u7684\u53d8\u4f53, \u6709\u7740RNN\u7ed3\u6784\u672c\u8eab\u7684\u4e00\u5927\u5f0a\u7aef, \u5373\u4e0d\u53ef\u5e76\u884c\u8ba1\u7b97, \u8fd9\u5728\u6570\u636e\u91cf\u548c\u6a21\u578b\u4f53\u91cf\u9010\u6b65\u589e\u5927\u7684\u672a\u6765, \u662fRNN\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888. 3 \u5c0f\u7ed3 \u00b6 GRU\uff08Gated Recurrent Unit\uff09\u4e5f\u79f0\u95e8\u63a7\u5faa\u73af\u5355\u5143\u7ed3\u6784, \u5b83\u4e5f\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u540cLSTM\u4e00\u6837\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6\u5b83\u7684\u7ed3\u6784\u548c\u8ba1\u7b97\u8981\u6bd4LSTM\u66f4\u7b80\u5355, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u66f4\u65b0\u95e8 \u91cd\u7f6e\u95e8 \u5185\u90e8\u7ed3\u6784\u5206\u6790: \u548c\u4e4b\u524d\u5206\u6790\u8fc7\u7684LSTM\u4e2d\u7684\u95e8\u63a7\u4e00\u6837, \u9996\u5148\u8ba1\u7b97\u66f4\u65b0\u95e8\u548c\u91cd\u7f6e\u95e8\u7684\u95e8\u503c, \u5206\u522b\u662fz(t)\u548cr(t), \u8ba1\u7b97\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528X(t)\u4e0eh(t-1)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u6362, \u518d\u7ecf\u8fc7sigmoid\u6fc0\u6d3b. \u4e4b\u540e\u91cd\u7f6e\u95e8\u95e8\u503c\u4f5c\u7528\u5728\u4e86h(t-1)\u4e0a, \u4ee3\u8868\u63a7\u5236\u4e0a\u4e00\u65f6\u95f4\u6b65\u4f20\u6765\u7684\u4fe1\u606f\u6709\u591a\u5c11\u53ef\u4ee5\u88ab\u5229\u7528. \u63a5\u7740\u5c31\u662f\u4f7f\u7528\u8fd9\u4e2a\u91cd\u7f6e\u540e\u7684h(t-1)\u8fdb\u884c\u57fa\u672c\u7684RNN\u8ba1\u7b97, \u5373\u4e0ex(t)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u5316, \u7ecf\u8fc7tanh\u6fc0\u6d3b, \u5f97\u5230\u65b0\u7684h(t). \u6700\u540e\u66f4\u65b0\u95e8\u7684\u95e8\u503c\u4f1a\u4f5c\u7528\u5728\u65b0\u7684h(t)\uff0c\u800c1-\u95e8\u503c\u4f1a\u4f5c\u7528\u5728h(t-1)\u4e0a, \u968f\u540e\u5c06\u4e24\u8005\u7684\u7ed3\u679c\u76f8\u52a0, \u5f97\u5230\u6700\u7ec8\u7684\u9690\u542b\u72b6\u6001\u8f93\u51fah(t), \u8fd9\u4e2a\u8fc7\u7a0b\u610f\u5473\u7740\u66f4\u65b0\u95e8\u6709\u80fd\u529b\u4fdd\u7559\u4e4b\u524d\u7684\u7ed3\u679c, \u5f53\u95e8\u503c\u8d8b\u4e8e1\u65f6, \u8f93\u51fa\u5c31\u662f\u65b0\u7684h(t), \u800c\u5f53\u95e8\u503c\u8d8b\u4e8e0\u65f6, \u8f93\u51fa\u5c31\u662f\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684h(t-1). Bi-GRU\u4e0eBi-LSTM\u7684\u903b\u8f91\u76f8\u540c, \u90fd\u662f\u4e0d\u6539\u53d8\u5176\u5185\u90e8\u7ed3\u6784, \u800c\u662f\u5c06\u6a21\u578b\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. \u5177\u4f53\u53c2\u89c1\u4e0a\u5c0f\u8282\u4e2d\u7684Bi-LSTM. Pytorch\u4e2dGRU\u5de5\u5177\u7684\u4f7f\u7528: \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.GRU\u53ef\u8c03\u7528. GRU\u7684\u4f18\u52bf: GRU\u548cLSTM\u4f5c\u7528\u76f8\u540c, \u5728\u6355\u6349\u957f\u5e8f\u5217\u8bed\u4e49\u5173\u8054\u65f6, \u80fd\u6709\u6548\u6291\u5236\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u6548\u679c\u90fd\u4f18\u4e8e\u4f20\u7edfRNN\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u6bd4LSTM\u8981\u5c0f. GRU\u7684\u7f3a\u70b9: GRU\u4ecd\u7136\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898, \u540c\u65f6\u5176\u4f5c\u7528RNN\u7684\u53d8\u4f53, \u6709\u7740RNN\u7ed3\u6784\u672c\u8eab\u7684\u4e00\u5927\u5f0a\u7aef, \u5373\u4e0d\u53ef\u5e76\u884c\u8ba1\u7b97, \u8fd9\u5728\u6570\u636e\u91cf\u548c\u6a21\u578b\u4f53\u91cf\u9010\u6b65\u589e\u5927\u7684\u672a\u6765, \u662fRNN\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888.","title":"4 GRU\u6a21\u578b"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#_1","text":"\u4e86\u89e3GRU\u5185\u90e8\u7ed3\u6784\u53ca\u8ba1\u7b97\u516c\u5f0f. \u638c\u63e1Pytorch\u4e2dGRU\u5de5\u5177\u7684\u4f7f\u7528. \u4e86\u89e3GRU\u7684\u4f18\u52bf\u4e0e\u7f3a\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#1-gru","text":"GRU\uff08Gated Recurrent Unit\uff09\u4e5f\u79f0\u95e8\u63a7\u5faa\u73af\u5355\u5143\u7ed3\u6784, \u5b83\u4e5f\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u540cLSTM\u4e00\u6837\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6\u5b83\u7684\u7ed3\u6784\u548c\u8ba1\u7b97\u8981\u6bd4LSTM\u66f4\u7b80\u5355, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u66f4\u65b0\u95e8 \u91cd\u7f6e\u95e8","title":"1 GRU\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#2-gru","text":"","title":"2 GRU\u7684\u5185\u90e8\u7ed3\u6784\u56fe"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#21-gru","text":"\u7ed3\u6784\u89e3\u91ca\u56fe: GRU\u7684\u66f4\u65b0\u95e8\u548c\u91cd\u7f6e\u95e8\u7ed3\u6784\u56fe: \u5185\u90e8\u7ed3\u6784\u5206\u6790: \u548c\u4e4b\u524d\u5206\u6790\u8fc7\u7684LSTM\u4e2d\u7684\u95e8\u63a7\u4e00\u6837, \u9996\u5148\u8ba1\u7b97\u66f4\u65b0\u95e8\u548c\u91cd\u7f6e\u95e8\u7684\u95e8\u503c, \u5206\u522b\u662fz(t)\u548cr(t), \u8ba1\u7b97\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528X(t)\u4e0eh(t-1)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u6362, \u518d\u7ecf\u8fc7sigmoid\u6fc0\u6d3b. \u4e4b\u540e\u91cd\u7f6e\u95e8\u95e8\u503c\u4f5c\u7528\u5728\u4e86h(t-1)\u4e0a, \u4ee3\u8868\u63a7\u5236\u4e0a\u4e00\u65f6\u95f4\u6b65\u4f20\u6765\u7684\u4fe1\u606f\u6709\u591a\u5c11\u53ef\u4ee5\u88ab\u5229\u7528. \u63a5\u7740\u5c31\u662f\u4f7f\u7528\u8fd9\u4e2a\u91cd\u7f6e\u540e\u7684h(t-1)\u8fdb\u884c\u57fa\u672c\u7684RNN\u8ba1\u7b97, \u5373\u4e0ex(t)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u5316, \u7ecf\u8fc7tanh\u6fc0\u6d3b, \u5f97\u5230\u65b0\u7684h(t). \u6700\u540e\u66f4\u65b0\u95e8\u7684\u95e8\u503c\u4f1a\u4f5c\u7528\u5728\u65b0\u7684h(t)\uff0c\u800c1-\u95e8\u503c\u4f1a\u4f5c\u7528\u5728h(t-1)\u4e0a, \u968f\u540e\u5c06\u4e24\u8005\u7684\u7ed3\u679c\u76f8\u52a0, \u5f97\u5230\u6700\u7ec8\u7684\u9690\u542b\u72b6\u6001\u8f93\u51fah(t), \u8fd9\u4e2a\u8fc7\u7a0b\u610f\u5473\u7740\u66f4\u65b0\u95e8\u6709\u80fd\u529b\u4fdd\u7559\u4e4b\u524d\u7684\u7ed3\u679c, \u5f53\u95e8\u503c\u8d8b\u4e8e1\u65f6, \u8f93\u51fa\u5c31\u662f\u65b0\u7684h(t), \u800c\u5f53\u95e8\u503c\u8d8b\u4e8e0\u65f6, \u8f93\u51fa\u5c31\u662f\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684h(t-1).","title":"2.1 GRU\u7ed3\u6784\u5206\u6790"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#22-bi-gru","text":"Bi-GRU\u4e0eBi-LSTM\u7684\u903b\u8f91\u76f8\u540c, \u90fd\u662f\u4e0d\u6539\u53d8\u5176\u5185\u90e8\u7ed3\u6784, \u800c\u662f\u5c06\u6a21\u578b\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. \u5177\u4f53\u53c2\u89c1\u4e0a\u5c0f\u8282\u4e2d\u7684Bi-LSTM.","title":"2.2 Bi-GRU\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#23-pytorchgru","text":"\u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.GRU\u53ef\u8c03\u7528. nn.GRU\u7c7b\u521d\u59cb\u5316\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input_size: \u8f93\u5165\u5f20\u91cfx\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. hidden_size: \u9690\u5c42\u5f20\u91cfh\u4e2d\u7279\u5f81\u7ef4\u5ea6\u7684\u5927\u5c0f. num_layers: \u9690\u542b\u5c42\u7684\u6570\u91cf. bidirectional: \u662f\u5426\u9009\u62e9\u4f7f\u7528\u53cc\u5411LSTM, \u5982\u679c\u4e3aTrue, \u5219\u4f7f\u7528; \u9ed8\u8ba4\u4e0d\u4f7f\u7528. nn.GRU\u7c7b\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e3b\u8981\u53c2\u6570\u89e3\u91ca: input: \u8f93\u5165\u5f20\u91cfx. h0: \u521d\u59cb\u5316\u7684\u9690\u5c42\u5f20\u91cfh. nn.GRU\u4f7f\u7528\u793a\u4f8b: >>> import torch >>> import torch.nn as nn >>> rnn = nn . GRU ( 5 , 6 , 2 ) >>> input = torch . randn ( 1 , 3 , 5 ) >>> h0 = torch . randn ( 2 , 3 , 6 ) >>> output , hn = rnn ( input , h0 ) >>> output tensor ([[[ - 0.2097 , - 2.2225 , 0.6204 , - 0.1745 , - 0.1749 , - 0.0460 ], [ - 0.3820 , 0.0465 , - 0.4798 , 0.6837 , - 0.7894 , 0.5173 ], [ - 0.0184 , - 0.2758 , 1.2482 , 0.5514 , - 0.9165 , - 0.6667 ]]], grad_fn =< StackBackward > ) >>> hn tensor ([[[ 0.6578 , - 0.4226 , - 0.2129 , - 0.3785 , 0.5070 , 0.4338 ], [ - 0.5072 , 0.5948 , 0.8083 , 0.4618 , 0.1629 , - 0.1591 ], [ 0.2430 , - 0.4981 , 0.3846 , - 0.4252 , 0.7191 , 0.5420 ]], [[ - 0.2097 , - 2.2225 , 0.6204 , - 0.1745 , - 0.1749 , - 0.0460 ], [ - 0.3820 , 0.0465 , - 0.4798 , 0.6837 , - 0.7894 , 0.5173 ], [ - 0.0184 , - 0.2758 , 1.2482 , 0.5514 , - 0.9165 , - 0.6667 ]]], grad_fn =< StackBackward > )","title":"2.3 \u4f7f\u7528Pytorch\u6784\u5efaGRU\u6a21\u578b"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#24-gru","text":"GRU\u7684\u4f18\u52bf: GRU\u548cLSTM\u4f5c\u7528\u76f8\u540c, \u5728\u6355\u6349\u957f\u5e8f\u5217\u8bed\u4e49\u5173\u8054\u65f6, \u80fd\u6709\u6548\u6291\u5236\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u6548\u679c\u90fd\u4f18\u4e8e\u4f20\u7edfRNN\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u6bd4LSTM\u8981\u5c0f. GRU\u7684\u7f3a\u70b9: GRU\u4ecd\u7136\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898, \u540c\u65f6\u5176\u4f5c\u7528RNN\u7684\u53d8\u4f53, \u6709\u7740RNN\u7ed3\u6784\u672c\u8eab\u7684\u4e00\u5927\u5f0a\u7aef, \u5373\u4e0d\u53ef\u5e76\u884c\u8ba1\u7b97, \u8fd9\u5728\u6570\u636e\u91cf\u548c\u6a21\u578b\u4f53\u91cf\u9010\u6b65\u589e\u5927\u7684\u672a\u6765, \u662fRNN\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888.","title":"2.4 GRU\u4f18\u7f3a\u70b9"},{"location":"03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html#3","text":"GRU\uff08Gated Recurrent Unit\uff09\u4e5f\u79f0\u95e8\u63a7\u5faa\u73af\u5355\u5143\u7ed3\u6784, \u5b83\u4e5f\u662f\u4f20\u7edfRNN\u7684\u53d8\u4f53, \u540cLSTM\u4e00\u6837\u80fd\u591f\u6709\u6548\u6355\u6349\u957f\u5e8f\u5217\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u8054, \u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u73b0\u8c61. \u540c\u65f6\u5b83\u7684\u7ed3\u6784\u548c\u8ba1\u7b97\u8981\u6bd4LSTM\u66f4\u7b80\u5355, \u5b83\u7684\u6838\u5fc3\u7ed3\u6784\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\u53bb\u89e3\u6790: \u66f4\u65b0\u95e8 \u91cd\u7f6e\u95e8 \u5185\u90e8\u7ed3\u6784\u5206\u6790: \u548c\u4e4b\u524d\u5206\u6790\u8fc7\u7684LSTM\u4e2d\u7684\u95e8\u63a7\u4e00\u6837, \u9996\u5148\u8ba1\u7b97\u66f4\u65b0\u95e8\u548c\u91cd\u7f6e\u95e8\u7684\u95e8\u503c, \u5206\u522b\u662fz(t)\u548cr(t), \u8ba1\u7b97\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528X(t)\u4e0eh(t-1)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u6362, \u518d\u7ecf\u8fc7sigmoid\u6fc0\u6d3b. \u4e4b\u540e\u91cd\u7f6e\u95e8\u95e8\u503c\u4f5c\u7528\u5728\u4e86h(t-1)\u4e0a, \u4ee3\u8868\u63a7\u5236\u4e0a\u4e00\u65f6\u95f4\u6b65\u4f20\u6765\u7684\u4fe1\u606f\u6709\u591a\u5c11\u53ef\u4ee5\u88ab\u5229\u7528. \u63a5\u7740\u5c31\u662f\u4f7f\u7528\u8fd9\u4e2a\u91cd\u7f6e\u540e\u7684h(t-1)\u8fdb\u884c\u57fa\u672c\u7684RNN\u8ba1\u7b97, \u5373\u4e0ex(t)\u62fc\u63a5\u8fdb\u884c\u7ebf\u6027\u53d8\u5316, \u7ecf\u8fc7tanh\u6fc0\u6d3b, \u5f97\u5230\u65b0\u7684h(t). \u6700\u540e\u66f4\u65b0\u95e8\u7684\u95e8\u503c\u4f1a\u4f5c\u7528\u5728\u65b0\u7684h(t)\uff0c\u800c1-\u95e8\u503c\u4f1a\u4f5c\u7528\u5728h(t-1)\u4e0a, \u968f\u540e\u5c06\u4e24\u8005\u7684\u7ed3\u679c\u76f8\u52a0, \u5f97\u5230\u6700\u7ec8\u7684\u9690\u542b\u72b6\u6001\u8f93\u51fah(t), \u8fd9\u4e2a\u8fc7\u7a0b\u610f\u5473\u7740\u66f4\u65b0\u95e8\u6709\u80fd\u529b\u4fdd\u7559\u4e4b\u524d\u7684\u7ed3\u679c, \u5f53\u95e8\u503c\u8d8b\u4e8e1\u65f6, \u8f93\u51fa\u5c31\u662f\u65b0\u7684h(t), \u800c\u5f53\u95e8\u503c\u8d8b\u4e8e0\u65f6, \u8f93\u51fa\u5c31\u662f\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684h(t-1). Bi-GRU\u4e0eBi-LSTM\u7684\u903b\u8f91\u76f8\u540c, \u90fd\u662f\u4e0d\u6539\u53d8\u5176\u5185\u90e8\u7ed3\u6784, \u800c\u662f\u5c06\u6a21\u578b\u5e94\u7528\u4e24\u6b21\u4e14\u65b9\u5411\u4e0d\u540c, \u518d\u5c06\u4e24\u6b21\u5f97\u5230\u7684LSTM\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa. \u5177\u4f53\u53c2\u89c1\u4e0a\u5c0f\u8282\u4e2d\u7684Bi-LSTM. Pytorch\u4e2dGRU\u5de5\u5177\u7684\u4f7f\u7528: \u4f4d\u7f6e: \u5728torch.nn\u5de5\u5177\u5305\u4e4b\u4e2d, \u901a\u8fc7torch.nn.GRU\u53ef\u8c03\u7528. GRU\u7684\u4f18\u52bf: GRU\u548cLSTM\u4f5c\u7528\u76f8\u540c, \u5728\u6355\u6349\u957f\u5e8f\u5217\u8bed\u4e49\u5173\u8054\u65f6, \u80fd\u6709\u6548\u6291\u5236\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8, \u6548\u679c\u90fd\u4f18\u4e8e\u4f20\u7edfRNN\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u6bd4LSTM\u8981\u5c0f. GRU\u7684\u7f3a\u70b9: GRU\u4ecd\u7136\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898, \u540c\u65f6\u5176\u4f5c\u7528RNN\u7684\u53d8\u4f53, \u6709\u7740RNN\u7ed3\u6784\u672c\u8eab\u7684\u4e00\u5927\u5f0a\u7aef, \u5373\u4e0d\u53ef\u5e76\u884c\u8ba1\u7b97, \u8fd9\u5728\u6570\u636e\u91cf\u548c\u6a21\u578b\u4f53\u91cf\u9010\u6b65\u589e\u5927\u7684\u672a\u6765, \u662fRNN\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888.","title":"3 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6709\u5173\u4eba\u540d\u5206\u7c7b\u95ee\u9898\u548c\u6709\u5173\u6570\u636e. \u638c\u63e1\u4f7f\u7528RNN\u6784\u5efa\u4eba\u540d\u5206\u7c7b\u5668\u5b9e\u73b0\u8fc7\u7a0b. 1 \u6848\u4f8b\u4ecb\u7ecd \u00b6 \u5173\u4e8e\u4eba\u540d\u5206\u7c7b\u95ee\u9898: \u4ee5\u4e00\u4e2a\u4eba\u540d\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u662f\u6765\u81ea\u54ea\u4e00\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d, \u8fd9\u5728\u67d0\u4e9b\u56fd\u9645\u5316\u516c\u53f8\u7684\u4e1a\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49, \u5728\u7528\u6237\u6ce8\u518c\u8fc7\u7a0b\u4e2d, \u4f1a\u6839\u636e\u7528\u6237\u586b\u5199\u7684\u540d\u5b57\u76f4\u63a5\u7ed9\u4ed6\u5206\u914d\u53ef\u80fd\u7684\u56fd\u5bb6\u6216\u5730\u533a\u9009\u9879, \u4ee5\u53ca\u8be5\u56fd\u5bb6\u6216\u5730\u533a\u7684\u56fd\u65d7, \u9650\u5236\u624b\u673a\u53f7\u7801\u4f4d\u6570\u7b49\u7b49. \u4eba\u540d\u5206\u7c7b\u6570\u636e\u9884\u89c8 \u6570\u636e\u5b58\u653e\u8def\u5f84\uff1a$(home)/data/name_classfication.txt \u6570\u636e\u683c\u5f0f\u8bf4\u660e \u6bcf\u4e00\u884c\u7b2c\u4e00\u4e2a\u5355\u8bcd\u4e3a\u4eba\u540d\uff0c\u7b2c\u4e8c\u4e2a\u5355\u8bcd\u4e3a\u56fd\u5bb6\u540d\u3002\u4e2d\u95f4\u7528\u5236\u8868\u7b26tab\u5206\u5272 Huffmann German Hummel German Hummel German Hutmacher German Ingersleben German Jaeger German Jager German Deng Chinese Ding Chinese Dong Chinese Dou Chinese Duan Chinese Eng Chinese Fan Chinese Fei Chinese Abaimov Russian Abakeliya Russian Abakovsky Russian Abakshin Russian Abakumoff Russian Abakumov Russian Abakumtsev Russian Abakushin Russian Abalakin Russian 2 \u6848\u4f8b\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4 \u7b2c\u4e00\u6b65\u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65\u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65\u6784\u5efaRNN\u6a21\u578b(\u5305\u62ec\u4f20\u7edfRNN, LSTM\u4ee5\u53caGRU) \u7b2c\u56db\u6b65\u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65\u6784\u5efa\u9884\u6d4b\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b 2.1 \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u00b6 # \u5bfc\u5165torch\u5de5\u5177 import torch # \u5bfc\u5165nn\u51c6\u5907\u6784\u5efa\u6a21\u578b import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # \u5bfc\u5165torch\u7684\u6570\u636e\u6e90 \u6570\u636e\u8fed\u4ee3\u5668\u5de5\u5177\u5305 from torch.utils.data import Dataset , DataLoader # \u7528\u4e8e\u83b7\u5f97\u5e38\u89c1\u5b57\u6bcd\u53ca\u5b57\u7b26\u89c4\u8303\u5316 import string # \u5bfc\u5165\u65f6\u95f4\u5de5\u5177\u5305 import time # \u5f15\u5165\u5236\u56fe\u5de5\u5177\u5305 import matplotlib.pyplot as plt # \u4eceio\u4e2d\u5bfc\u5165\u6587\u4ef6\u6253\u5f00\u65b9\u6cd5 from io import open 2.2 \u6570\u636e\u9884\u5904\u7406 \u00b6 \u8fd9\u91cc\u9700\u8981\u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42\u3002 1 \u83b7\u53d6\u5e38\u7528\u7684\u5b57\u7b26\u6570\u91cf \u00b6 # \u83b7\u53d6\u6240\u6709\u5e38\u7528\u5b57\u7b26\u5305\u62ec\u5b57\u6bcd\u548c\u5e38\u7528\u6807\u70b9 all_letters = string . ascii_letters + \" .,;'\" # \u83b7\u53d6\u5e38\u7528\u5b57\u7b26\u6570\u91cf n_letters = len ( all_letters ) print ( \"n_letter:\" , n_letters ) \u8f93\u51fa\u6548\u679c: n_letter: 57 2 \u56fd\u5bb6\u540d\u79cd\u7c7b\u6570\u548c\u4e2a\u6570 \u00b6 # \u56fd\u5bb6\u540d \u79cd\u7c7b\u6570 categorys = [ 'Italian' , 'English' , 'Arabic' , 'Spanish' , 'Scottish' , 'Irish' , 'Chinese' , 'Vietnamese' , 'Japanese' , 'French' , 'Greek' , 'Dutch' , 'Korean' , 'Polish' , 'Portuguese' , 'Russian' , 'Czech' , 'German' ] # \u56fd\u5bb6\u540d \u4e2a\u6570 categorynum = len ( categorys ) print ( 'categorys--->' , categorys ) \u8f93\u51fa\u6548\u679c: categorys ---> [ 'Italian' , 'English' , 'Arabic' , 'Spanish' , 'Scottish' , 'Irish' , 'Chinese' , 'Vietnamese' , 'Japanese' , 'French' , 'Greek' , 'Dutch' , 'Korean' , 'Polish' , 'Portuguese' , 'Russian' , 'Czech' , 'German' ] categorynum ---> 18 3 \u8bfb\u6570\u636e\u5230\u5185\u5b58 \u00b6 # \u601d\u8def\u5206\u6790 # 1 \u6253\u5f00\u6570\u636e\u6587\u4ef6 open(filename, mode='r', encoding='utf-8') # 2 \u6309\u884c\u8bfb\u6587\u4ef6\u3001\u63d0\u53d6\u6837\u672cx \u6837\u672cy line.strip().split('\\t') # 3 \u8fd4\u56de\u6837\u672cx\u7684\u5217\u8868\u3001\u6837\u672cy\u7684\u5217\u8868 my_list_x, my_list_y def read_data ( filename ): my_list_x , my_list_y = [], [] # \u6253\u5f00\u6587\u4ef6 with open ( filename , mode = 'r' , encoding = 'utf-8' ) as f : # \u6309\u7167\u884c\u8bfb\u6570\u636e for line in f . readlines (): if len ( line ) <= 5 : continue # \u6309\u7167\u884c\u63d0\u53d6\u6837\u672cx \u6837\u672cy ( x , y ) = line . strip () . split ( ' \\t ' ) my_list_x . append ( x ) my_list_y . append ( y ) # \u6253\u5370\u6837\u672c\u7684\u6570\u91cf print ( 'my_list_x->' , len ( my_list_x )) print ( 'my_list_y->' , len ( my_list_y )) # \u8fd4\u56de\u6837\u672cx\u7684\u5217\u8868\u3001\u6837\u672cy\u7684\u5217\u8868 return my_list_x , my_list_y 4 \u6784\u5efa\u6570\u636e\u6e90NameClassDataset \u00b6 # \u539f\u59cb\u6570\u636e -> \u6570\u636e\u6e90NameClassDataset --> \u6570\u636e\u8fed\u4ee3\u5668DataLoader # \u6784\u9020\u6570\u636e\u6e90 NameClassDataset\uff0c\u628a\u8bed\u6599\u8f6c\u6362\u6210x y # 1 init\u51fd\u6570 \u8bbe\u7f6e\u6837\u672cx\u548cy self.my_list_x self.my_list_y \u6761\u76ee\u6570self.sample_len # 2 __len__(self)\u51fd\u6570 \u83b7\u53d6\u6837\u672c\u6761\u6570 # 3 __getitem__(self, index)\u51fd\u6570 \u83b7\u53d6\u7b2c\u51e0\u6761\u6837\u672c\u6570\u636e # \u6309\u7d22\u5f15 \u83b7\u53d6\u6570\u636e\u6837\u672c x y # \u6837\u672cx one-hot\u5f20\u91cf\u5316 tensor_x[li][all_letters.find(letter)] = 1 # \u6837\u672cy \u5f20\u91cf\u5316 torch.tensor(categorys.index(y), dtype=torch.long) # \u8fd4\u56detensor_x, tensor_y class NameClassDataset ( Dataset ): def __init__ ( self , my_list_x , my_list_y ): # \u6837\u672cx self . my_list_x = my_list_x # \u6837\u672cy self . my_list_y = my_list_y # \u6837\u672c\u6761\u76ee\u6570 self . sample_len = len ( my_list_x ) # \u83b7\u53d6\u6837\u672c\u6761\u6570 def __len__ ( self ): return self . sample_len # \u83b7\u53d6\u7b2c\u51e0\u6761 \u6837\u672c\u6570\u636e def __getitem__ ( self , index ): # \u5bf9index\u5f02\u5e38\u503c\u8fdb\u884c\u4fee\u6b63 [0, self.sample_len-1] index = min ( max ( index , 0 ), self . sample_len - 1 ) # \u6309\u7d22\u5f15\u83b7\u53d6 \u6570\u636e\u6837\u672c x y x = self . my_list_x [ index ] y = self . my_list_y [ index ] # \u6837\u672cx one-hot\u5f20\u91cf\u5316 tensor_x = torch . zeros ( len ( x ), n_letters ) # \u904d\u5386\u4eba\u540d \u7684 \u6bcf\u4e2a\u5b57\u6bcd \u505a\u6210one-hot\u7f16\u7801 for li , letter in enumerate ( x ): # letter2indx \u4f7f\u7528all_letters.find(letter)\u67e5\u627e\u5b57\u6bcd\u5728all_letters\u8868\u4e2d\u7684\u4f4d\u7f6e \u7ed9one-hot\u8d4b\u503c tensor_x [ li ][ all_letters . find ( letter )] = 1 # \u6837\u672cy \u5f20\u91cf\u5316 tensor_y = torch . tensor ( categorys . index ( y ), dtype = torch . long ) # \u8fd4\u56de\u7ed3\u679c return tensor_x , tensor_y \u5206\u6790 \u6587\u672c\u5f20\u91cf\u5316\uff0c\u8fd9\u91cc\u4e5f\u5c31\u662f\u4eba\u540d\u5f20\u91cf\u5316\u662f\u901a\u8fc7one-hot\u7f16\u7801\u6765\u5b8c\u6210\u3002 # \u5c06\u5b57\u7b26\u4e32(\u5355\u8bcd\u7c92\u5ea6)\u8f6c\u5316\u4e3a\u5f20\u91cf\u8868\u793a\uff0c\u5982\uff1a\"ab\" ---> # tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0.]], # [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0.]]]) 5 \u6784\u5efa\u8fed\u4ee3\u5668\u904d\u5386\u6570\u636e \u00b6 def dm_test_NameClassDataset (): # 1 \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) print ( 'my_list_x length' , len ( my_list_x )) print ( 'my_list_y length' , len ( my_list_y )) # 2 \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # 3 \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) break \u8f93\u51fa\u6548\u679c: my_list_x length 20074 my_list_y length 20074 x.shape torch.Size([1, 5, 57]) tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]) y.shape torch.Size([1]) tensor([15]) 2.3 \u6784\u5efaRNN\u6a21\u578b \u00b6 1 \u6784\u5efaRNN\u6a21\u578b \u00b6 # RNN\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, input_size, hidden_size, output_size, num_layers=1) # 2 forward(input, hidden)\u51fd\u6570 # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u5f62\u72b6\u53d8\u5316 [seqlen,1,57],[1,1,128]) -> [seqlen,1,128],[1,1,128] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6[self.num_layers, 1, self.hidden_size] class RNN ( nn . Module ): def __init__ ( self , input_size , hidden_size , output_size , num_layers = 1 ): super ( RNN , self ) . __init__ () # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) self . input_size = input_size self . hidden_size = hidden_size self . output_size = output_size self . num_layers = num_layers # \u5b9a\u4e49rnn\u5c42 self . rnn = nn . RNN ( self . input_size , self . hidden_size , self . num_layers ) # \u5b9a\u4e49linear\u5c42 self . linear = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9a\u4e49softmax\u5c42 self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden ): '''input [6,57]-2\u7ef4\u77e9\u9635 hidden[1,1,57] - 3\u7ef4\u77e9\u9635''' # \u6570\u636e\u5f62\u72b6 [6,57] -> [6,1,57] input = input . unsqueeze ( 1 ) # 1 \u6570\u636e\u7ecf\u8fc7\u6a21\u578b \u63d0\u53d6\u4e8b\u7269\u7279\u5f81 # \u6570\u636e\u5f62\u72b6 [seqlen,1,57],[1,1,128]) -> [seqlen,1,18],[1,1,128] rr , hn = self . rnn ( input , hidden ) # \u6570\u636e\u5f62\u72b6 [seqlen,1,128] - [1, 128] eg:[6,1,128] --> [1,128] tmprr = rr [ - 1 ] # 2 \u6570\u636e\u7ecf\u8fc7\u5168\u8fde\u63a5\u5c42 [1,128] -->[1,18] tmprr = self . linear ( tmprr ) # 3 \u6570\u636e\u7ecf\u8fc7softmax\u5c42\u8fd4\u56de return self . softmax ( tmprr ), hn def inithidden ( self ): # \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() return torch . zeros ( self . num_layers , 1 , self . hidden_size ) torch.unsqueeze\u6f14\u793a: >>> x = torch.tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) tensor([[ 1, 2, 3, 4]]) >>> torch.unsqueeze(x, 1) tensor([[ 1], [ 2], [ 3], [ 4]]) \u8c03\u7528: def dm01_test_myrnn (): # 1 \u5b9e\u4f8b\u5316rnn\u5bf9\u8c61 myrnn = RNN ( 57 , 128 , 18 ) print ( 'myrnn--->' , myrnn ) # 2 \u51c6\u5907\u6570\u636e input = torch . randn ( 6 , 57 ) print ( input . shape ) hidden = myrnn . inithidden () # 3 \u7ed9\u6a21\u578b1\u6b21\u6027\u7684\u9001\u6570\u636e # [seqlen, 57], [1, 1, 128]) -> [1,18], [1,1,128] output , hidden = myrnn ( input , hidden ) print ( '\u4e00\u6b21\u6027\u7684\u9001\u6570\u636e\uff1aoutput->' , output . shape , output ) print ( 'hidden->' , hidden . shape ) # 4 \u7ed9\u6a21\u578b1\u4e2a\u5b57\u7b261\u4e2a\u5b57\u7b26\u7684\u5582\u6570\u636e hidden = myrnn . inithidden () for i in range ( input . shape [ 0 ]): tmpinput = input [ i ] . unsqueeze ( 0 ) output , hidden = myrnn ( tmpinput , hidden ) # \u6700\u540e\u4e00\u6b21ouput print ( '\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u9001\u6570\u636eoutput->' , output . shape , output ) \u8c03\u7528\u7ed3\u679c: myrnn ---> RNN ( ( rnn ): RNN ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) torch . Size ([ 6 , 57 ]) \u4e00\u6b21\u6027\u7684\u9001\u6570\u636e \uff1a output -> torch . Size ([ 1 , 18 ]) tensor ([[ - 2.8194 , - 3.1730 , - 3.3112 , - 2.9715 , - 3.0997 , - 2.8097 , - 2.8016 , - 2.8738 , - 2.7229 , - 2.8181 , - 2.7881 , - 3.0218 , - 2.9169 , - 2.6193 , - 2.8507 , - 2.9684 , - 2.8589 , - 2.8273 ]], grad_fn =< LogSoftmaxBackward0 > ) hidden -> torch . Size ([ 1 , 1 , 128 ]) \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u9001\u6570\u636eoutput -> torch . Size ([ 1 , 18 ]) tensor ([[ - 2.8194 , - 3.1730 , - 3.3112 , - 2.9715 , - 3.0997 , - 2.8097 , - 2.8016 , - 2.8738 , - 2.7229 , - 2.8181 , - 2.7881 , - 3.0218 , - 2.9169 , - 2.6193 , - 2.8507 , - 2.9684 , - 2.8589 , - 2.8273 ]], grad_fn =< LogSoftmaxBackward0 > ) 2 \u6784\u5efaLSTM\u6a21\u578b \u00b6 # LSTM\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, input_size, hidden_size, output_size, num_layers=1) # 2 forward(input, hidden)\u51fd\u6570 # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u5f62\u72b6\u53d8\u5316 [seqlen,1,57],[1,1,128]) -> [seqlen,1,128],[1,1,128] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6[self.num_layers, 1, self.hidden_size] class LSTM ( nn . Module ): def __init__ ( self , input_size , hidden_size , output_size , num_layers = 1 ): super ( LSTM , self ) . __init__ () # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) self . input_size = input_size self . hidden_size = hidden_size self . output_size = output_size self . num_layers = num_layers # \u5b9a\u4e49rnn\u5c42 self . rnn = nn . LSTM ( self . input_size , self . hidden_size , self . num_layers ) # \u5b9a\u4e49linear\u5c42 self . linear = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9a\u4e49softmax\u5c42 self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden , c ): # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548c hn c # \u6570\u636e\u5f62\u72b6 [6,57] -> [6,1,52] input = input . unsqueeze ( 1 ) # \u628a\u6570\u636e\u9001\u7ed9\u6a21\u578b \u63d0\u53d6\u4e8b\u7269\u7279\u5f81 # \u6570\u636e\u5f62\u72b6 [seqlen,1,57],[1,1,128], [1,1,128]) -> [seqlen,1,18],[1,1,128],[1,1,128] rr , ( hn , cn ) = self . rnn ( input , ( hidden , c )) # \u6570\u636e\u5f62\u72b6 [seqlen,1,128] - [1, 128] tmprr = rr [ - 1 ] tmprr = self . linear ( tmprr ) return self . softmax ( tmprr ), hn , cn def inithidden ( self ): # \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() hidden = c = torch . zeros ( self . num_layers , 1 , self . hidden_size ) return hidden , c 3 \u6784\u5efaGRU\u6a21\u578b \u00b6 # GRU\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, input_size, hidden_size, output_size, num_layers=1) # 2 forward(input, hidden)\u51fd\u6570 # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u5f62\u72b6\u53d8\u5316 [seqlen,1,57],[1,1,128]) -> [seqlen,1,128],[1,1,128] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6[self.num_layers, 1, self.hidden_size] class GRU ( nn . Module ): def __init__ ( self , input_size , hidden_size , output_size , num_layers = 1 ): super ( GRU , self ) . __init__ () # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) self . input_size = input_size self . hidden_size = hidden_size self . output_size = output_size self . num_layers = num_layers # \u5b9a\u4e49rnn\u5c42 self . rnn = nn . GRU ( self . input_size , self . hidden_size , self . num_layers ) # \u5b9a\u4e49linear\u5c42 self . linear = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9a\u4e49softmax\u5c42 self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden ): # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u6570\u636e\u5f62\u72b6 [6,57] -> [6,1,52] input = input . unsqueeze ( 1 ) # \u628a\u6570\u636e\u9001\u7ed9\u6a21\u578b \u63d0\u53d6\u4e8b\u7269\u7279\u5f81 # \u6570\u636e\u5f62\u72b6 [seqlen,1,57],[1,1,128]) -> [seqlen,1,18],[1,1,128] rr , hn = self . rnn ( input , hidden ) # \u6570\u636e\u5f62\u72b6 [seqlen,1,128] - [1, 128] tmprr = rr [ - 1 ] tmprr = self . linear ( tmprr ) return self . softmax ( tmprr ), hn def inithidden ( self ): # \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() return torch . zeros ( self . num_layers , 1 , self . hidden_size ) 4 \u6a21\u578bRNN_LSTM_GRU\u6d4b\u8bd5 \u00b6 def dm_test_rnn_lstm_gru (): # one-hot\u7f16\u7801\u7279\u5f8157\uff08n_letters\uff09\uff0c\u4e5f\u662fRNN\u7684\u8f93\u5165\u5c3a\u5bf8 input_size = 57 # \u5b9a\u4e49\u9690\u5c42\u7684\u6700\u540e\u4e00\u7ef4\u5c3a\u5bf8\u5927\u5c0f n_hidden = 128 # \u8f93\u51fa\u5c3a\u5bf8\u4e3a\u8bed\u8a00\u7c7b\u522b\u603b\u6570n_categories # 1\u4e2a\u5b57\u7b26\u9884\u6d4b\u621018\u4e2a\u7c7b\u522b output_size = 18 # 1 \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) print ( 'categorys--->' , categorys ) # 2 \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # 3 \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) my_rnn = RNN ( n_letters , n_hidden , categorynum ) my_lstm = LSTM ( n_letters , n_hidden , categorynum ) my_gru = GRU ( n_letters , n_hidden , categorynum ) print ( 'rnn \u6a21\u578b' , my_rnn ) print ( 'lstm \u6a21\u578b' , my_lstm ) print ( 'gru \u6a21\u578b' , my_gru ) for i , ( x , y ) in enumerate ( mydataloader ): # print('x.shape', x.shape, x) # print('y.shape', y.shape, y) # \u521d\u59cb\u5316\u4e00\u4e2a\u4e09\u7ef4\u7684\u9690\u5c420\u5f20\u91cf, \u4e5f\u662f\u521d\u59cb\u7684\u7ec6\u80de\u72b6\u6001\u5f20\u91cf output , hidden = my_rnn ( x [ 0 ], my_rnn . inithidden ()) print ( \"rnn output.shape--->:\" , output . shape , output ) if ( i == 0 ): break for i , ( x , y ) in enumerate ( mydataloader ): # print('x.shape', x.shape, x) # print('y.shape', y.shape, y) hidden , c = my_lstm . inithidden () output , hidden , c = my_lstm ( x [ 0 ], hidden , c ) print ( \"lstm output.shape--->:\" , output . shape , output ) if ( i == 0 ): break for i , ( x , y ) in enumerate ( mydataloader ): # print('x.shape', x.shape, x) # print('y.shape', y.shape, y) output , hidden = my_gru ( x [ 0 ], my_gru . inithidden ()) print ( \"gru output.shape--->:\" , output . shape , output ) if ( i == 0 ): break \u8f93\u51fa\u6548\u679c: rnn \u6a21\u578b RNN ( ( rnn ): RNN ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) lstm \u6a21\u578b LSTM ( ( rnn ): LSTM ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) gru \u6a21\u578b GRU ( ( rnn ): GRU ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) rnn output . shape ---> : torch . Size ([ 1 , 18 ]) tensor ([[ - 2.9552 , - 2.9024 , - 2.8828 , - 2.7737 , - 2.8387 , - 3.0154 , - 2.8587 , - 2.9567 , - 2.8406 , - 3.0098 , - 2.8152 , - 2.8472 , - 2.9561 , - 2.8780 , - 2.8332 , - 2.8117 , - 2.9560 , - 2.9384 ]], grad_fn =< LogSoftmaxBackward0 > ) lstm output . shape ---> : torch . Size ([ 1 , 18 ]) tensor ([[ - 2.9283 , - 3.0017 , - 2.8902 , - 2.8179 , - 2.8484 , - 2.8152 , - 2.9654 , - 2.8846 , - 2.8642 , - 2.8602 , - 2.8860 , - 2.9505 , - 2.8806 , - 2.9436 , - 2.8388 , - 2.9312 , - 2.9241 , - 2.8211 ]], grad_fn =< LogSoftmaxBackward0 > ) gru output . shape ---> : torch . Size ([ 1 , 18 ]) tensor ([[ - 2.8898 , - 3.0236 , - 2.7403 , - 2.8986 , - 2.8163 , - 2.9486 , - 2.8674 , - 2.9294 , - 2.8889 , - 3.0082 , - 2.8785 , - 2.8741 , - 2.8736 , - 2.7923 , - 2.9261 , - 2.8990 , - 2.9456 , - 2.8668 ]], grad_fn =< LogSoftmaxBackward0 > ) 2.4 \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3 \u00b6 1 \u6784\u5efaRNN\u8bad\u7ec3\u51fd\u6570 \u00b6 # \u601d\u8def\u5206\u6790 # \u4ece\u6587\u4ef6\u83b7\u53d6\u6570\u636e\u3001\u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61nameclassdataset \u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61mydataloader # \u5b9e\u4f8b\u5316\u6a21\u578b\u5bf9\u8c61my_rnn \u635f\u5931\u51fd\u6570\u5bf9\u8c61mycrossentropyloss=nn.NLLLoss() \u4f18\u5316\u5668\u5bf9\u8c61myadam # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 # starttime total_iter_num total_loss total_loss_list total_acc_num total_acc_list # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range(epochs) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i, (x, y) in enumerate(mydataloader) # \u7ed9\u6a21\u578b\u5582\u6570\u636e # \u8ba1\u7b97\u635f\u5931 # \u68af\u5ea6\u6e05\u96f6 # \u53cd\u5411\u4f20\u64ad # \u68af\u5ea6\u66f4\u65b0 # \u8ba1\u7b97\u8f85\u52a9\u4fe1\u606f # \u7d2f\u52a0\u603b\u635f\u5931\u548c\u51c6\u786e\u6570 \u6bcf100\u6b21\u8bad\u7ec3\u8ba1\u7b97\u4e00\u4e2a\u603b\u4f53\u5e73\u5747\u635f\u5931 \u603b\u4f53\u5e73\u5747\u51c6\u786e\u7387 \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 # \u5176\u4ed6 # \u9884\u6d4b\u5bf9\u9519 i_predit_tag = (1 if torch.argmax(output).item() == y.item() else 0) # \u6a21\u578b\u4fdd\u5b58 # torch.save(my_rnn.state_dict(), './my_rnn_model_%d.bin' % (epoch_idx + 1)) # \u8fd4\u56de \u5e73\u5747\u635f\u5931\u5217\u8868total_loss_list, \u65f6\u95f4total_time, \u5e73\u5747\u51c6\u786etotal_acc_list # \u6a21\u578b\u8bad\u7ec3\u53c2\u6570 mylr = 1e-3 epochs = 1 def my_train_rnn (): # \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # \u5b9e\u4f8b\u5316 \u6a21\u578b input_size = 57 n_hidden = 128 output_size = 18 my_rnn = RNN ( input_size , n_hidden , output_size ) print ( 'my_rnn\u6a21\u578b--->' , my_rnn ) # \u5b9e\u4f8b\u5316 \u635f\u5931\u51fd\u6570 adam\u4f18\u5316\u5668 mycrossentropyloss = nn . NLLLoss () myadam = optim . Adam ( my_rnn . parameters (), lr = mylr ) # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u53c2\u6570 starttime = time . time () total_iter_num = 0 # \u5df2\u8bad\u7ec3\u7684\u6837\u672c\u6570 total_loss = 0.0 # \u5df2\u8bad\u7ec3\u7684\u635f\u5931\u548c total_loss_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5f62\u6210\u635f\u5931\u5217\u8868 total_acc_num = 0 # \u5df2\u8bad\u7ec3\u6837\u672c\u9884\u6d4b\u51c6\u786e\u603b\u6570 total_acc_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u51c6\u786e\u7387 \u5f62\u6210\u5e73\u5747\u51c6\u786e\u7387\u5217\u8868 # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( x , y ) in enumerate ( mydataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e output , hidden = my_rnn ( x [ 0 ], my_rnn . inithidden ()) # \u8ba1\u7b97\u635f\u5931 myloss = mycrossentropyloss ( output , y ) # \u68af\u5ea6\u6e05\u96f6 myadam . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam . step () # \u8ba1\u7b97\u603b\u635f\u5931 total_iter_num = total_iter_num + 1 total_loss = total_loss + myloss . item () # \u8ba1\u7b97\u603b\u51c6\u786e\u7387 i_predit_tag = ( 1 if torch . argmax ( output ) . item () == y . item () else 0 ) total_acc_num = total_acc_num + i_predit_tag # \u6bcf100\u6b21\u8bad\u7ec3 \u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5e73\u5747\u51c6\u786e\u7387 if ( total_iter_num % 100 == 0 ): tmploss = total_loss / total_iter_num total_loss_list . append ( tmploss ) tmpacc = total_acc_num / total_iter_num total_acc_list . append ( tmpacc ) # \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 if ( total_iter_num % 2000 == 0 ): tmploss = total_loss / total_iter_num print ( '\u8f6e\u6b21: %d , \u635f\u5931: %.6f , \u65f6\u95f4: %d \uff0c\u51c6\u786e\u7387: %.3f ' % ( epoch_idx + 1 , tmploss , time . time () - starttime , tmpacc )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_rnn . state_dict (), './my_rnn_model_ %d .bin' % ( epoch_idx + 1 )) # \u8ba1\u7b97\u603b\u65f6\u95f4 total_time = int ( time . time () - starttime ) return total_loss_list , total_time , total_acc_list 2 \u6784\u5efaLSTM\u8bad\u7ec3\u51fd\u6570 \u00b6 # \u601d\u8def\u5206\u6790 # \u540cRNN\u5b9e\u73b0\u5206\u6790 def my_train_lstm (): # \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # \u5b9e\u4f8b\u5316 \u6a21\u578b input_size = 57 n_hidden = 128 output_size = 18 my_lstm = LSTM ( input_size , n_hidden , output_size ) print ( 'my_lstm\u6a21\u578b--->' , my_lstm ) # \u5b9e\u4f8b\u5316 \u635f\u5931\u51fd\u6570 adam\u4f18\u5316\u5668 mycrossentropyloss = nn . NLLLoss () myadam = optim . Adam ( my_lstm . parameters (), lr = mylr ) # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u53c2\u6570 starttime = time . time () total_iter_num = 0 # \u5df2\u8bad\u7ec3\u7684\u6837\u672c\u6570 total_loss = 0.0 # \u5df2\u8bad\u7ec3\u7684\u635f\u5931\u548c total_loss_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5f62\u6210\u635f\u5931\u5217\u8868 total_acc_num = 0 # \u5df2\u8bad\u7ec3\u6837\u672c\u9884\u6d4b\u51c6\u786e\u603b\u6570 total_acc_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u51c6\u786e\u7387 \u5f62\u6210\u5e73\u5747\u51c6\u786e\u7387\u5217\u8868 # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( x , y ) in enumerate ( mydataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e hidden , c = my_lstm . inithidden () output , hidden , c = my_lstm ( x [ 0 ], hidden , c ) # \u8ba1\u7b97\u635f\u5931 myloss = mycrossentropyloss ( output , y ) # \u68af\u5ea6\u6e05\u96f6 myadam . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam . step () # \u8ba1\u7b97\u603b\u635f\u5931 total_iter_num = total_iter_num + 1 total_loss = total_loss + myloss . item () # \u8ba1\u7b97\u603b\u51c6\u786e\u7387 i_predit_tag = ( 1 if torch . argmax ( output ) . item () == y . item () else 0 ) total_acc_num = total_acc_num + i_predit_tag # \u6bcf100\u6b21\u8bad\u7ec3 \u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5e73\u5747\u51c6\u786e\u7387 if ( total_iter_num % 100 == 0 ): tmploss = total_loss / total_iter_num total_loss_list . append ( tmploss ) tmpacc = total_acc_num / total_iter_num total_acc_list . append ( tmpacc ) # \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 if ( total_iter_num % 2000 == 0 ): tmploss = total_loss / total_iter_num print ( '\u8f6e\u6b21: %d , \u635f\u5931: %.6f , \u65f6\u95f4: %d \uff0c\u51c6\u786e\u7387: %.3f ' % ( epoch_idx + 1 , tmploss , time . time () - starttime , tmpacc )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_lstm . state_dict (), './my_lstm_model_ %d .bin' % ( epoch_idx + 1 )) # \u8ba1\u7b97\u603b\u65f6\u95f4 total_time = int ( time . time () - starttime ) return total_loss_list , total_time , total_acc_list 3 \u6784\u5efaGRU\u8bad\u7ec3\u51fd\u6570 \u00b6 # \u601d\u8def\u5206\u6790 # \u540cRNN\u5b9e\u73b0\u5206\u6790 def my_train_gru (): # \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # \u5b9e\u4f8b\u5316 \u6a21\u578b input_size = 57 n_hidden = 128 output_size = 18 my_gru = GRU ( input_size , n_hidden , output_size ) print ( 'my_gru\u6a21\u578b--->' , my_gru ) # \u5b9e\u4f8b\u5316 \u635f\u5931\u51fd\u6570 adam\u4f18\u5316\u5668 mycrossentropyloss = nn . NLLLoss () myadam = optim . Adam ( my_gru . parameters (), lr = mylr ) # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u53c2\u6570 starttime = time . time () total_iter_num = 0 # \u5df2\u8bad\u7ec3\u7684\u6837\u672c\u6570 total_loss = 0.0 # \u5df2\u8bad\u7ec3\u7684\u635f\u5931\u548c total_loss_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5f62\u6210\u635f\u5931\u5217\u8868 total_acc_num = 0 # \u5df2\u8bad\u7ec3\u6837\u672c\u9884\u6d4b\u51c6\u786e\u603b\u6570 total_acc_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u51c6\u786e\u7387 \u5f62\u6210\u5e73\u5747\u51c6\u786e\u7387\u5217\u8868 # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( x , y ) in enumerate ( mydataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e output , hidden = my_gru ( x [ 0 ], my_gru . inithidden ()) # \u8ba1\u7b97\u635f\u5931 myloss = mycrossentropyloss ( output , y ) # \u68af\u5ea6\u6e05\u96f6 myadam . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam . step () # \u8ba1\u7b97\u603b\u635f\u5931 total_iter_num = total_iter_num + 1 total_loss = total_loss + myloss . item () # \u8ba1\u7b97\u603b\u51c6\u786e\u7387 i_predit_tag = ( 1 if torch . argmax ( output ) . item () == y . item () else 0 ) total_acc_num = total_acc_num + i_predit_tag # \u6bcf100\u6b21\u8bad\u7ec3 \u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5e73\u5747\u51c6\u786e\u7387 if ( total_iter_num % 100 == 0 ): tmploss = total_loss / total_iter_num total_loss_list . append ( tmploss ) tmpacc = total_acc_num / total_iter_num total_acc_list . append ( tmpacc ) # \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 if ( total_iter_num % 2000 == 0 ): tmploss = total_loss / total_iter_num print ( '\u8f6e\u6b21: %d , \u635f\u5931: %.6f , \u65f6\u95f4: %d \uff0c\u51c6\u786e\u7387: %.3f ' % ( epoch_idx + 1 , tmploss , time . time () - starttime , tmpacc )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_gru . state_dict (), './my_gru_model_ %d .bin' % ( epoch_idx + 1 )) # \u8ba1\u7b97\u603b\u65f6\u95f4 total_time = int ( time . time () - starttime ) return total_loss_list , total_time , total_acc_list 4 \u6a21\u578b\u8bad\u7ec3\u5e76\u5236\u56fe \u00b6 def dm_test_train_rnn_lstm_gru (): total_loss_list_rnn , total_time_rnn , total_acc_list_rnn = my_train_rnn () total_loss_list_lstm , total_time_lstm , total_acc_list_lstm = my_train_lstm () total_loss_list_gru , total_time_gru , total_acc_list_gru = my_train_gru () # \u7ed8\u5236\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf # \u521b\u5efa\u753b\u5e030 plt . figure ( 0 ) # # \u7ed8\u5236\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf plt . plot ( total_loss_list_rnn , label = \"RNN\" ) plt . plot ( total_loss_list_lstm , color = \"red\" , label = \"LSTM\" ) plt . plot ( total_loss_list_gru , color = \"orange\" , label = \"GRU\" ) plt . legend ( loc = 'upper left' ) plt . savefig ( './img/RNN_LSTM_GRU_loss2.png' ) plt . show () # \u7ed8\u5236\u67f1\u72b6\u56fe # \u521b\u5efa\u753b\u5e031 plt . figure ( 1 ) x_data = [ \"RNN\" , \"LSTM\" , \"GRU\" ] y_data = [ total_time_rnn , total_time_lstm , total_time_gru ] # \u7ed8\u5236\u8bad\u7ec3\u8017\u65f6\u5bf9\u6bd4\u67f1\u72b6\u56fe plt . bar ( range ( len ( x_data )), y_data , tick_label = x_data ) plt . savefig ( './img/RNN_LSTM_GRU_period2.png' ) plt . show () # \u7ed8\u5236\u51c6\u786e\u7387\u5bf9\u6bd4\u66f2\u7ebf plt . figure ( 2 ) plt . plot ( total_acc_list_rnn , label = \"RNN\" ) plt . plot ( total_acc_list_lstm , color = \"red\" , label = \"LSTM\" ) plt . plot ( total_acc_list_gru , color = \"orange\" , label = \"GRU\" ) plt . legend ( loc = 'upper left' ) plt . savefig ( './img/RNN_LSTM_GRU_acc2.png' ) plt . show () RNN\u6a21\u578b\u8bad\u7ec3\u65e5\u5fd7\u8f93\u51fa: \u8f6e\u6b21 : 3 , \u635f\u5931 : 1.002102 , \u65f6\u95f4 : 54 \uff0c \u51c6\u786e\u7387 : 0.700 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.993880 , \u65f6\u95f4 : 56 \uff0c \u51c6\u786e\u7387 : 0.703 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.986200 , \u65f6\u95f4 : 58 \uff0c \u51c6\u786e\u7387 : 0.705 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.981136 , \u65f6\u95f4 : 61 \uff0c \u51c6\u786e\u7387 : 0.706 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.976931 , \u65f6\u95f4 : 63 \uff0c \u51c6\u786e\u7387 : 0.707 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.972190 , \u65f6\u95f4 : 65 \uff0c \u51c6\u786e\u7387 : 0.708 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.967081 , \u65f6\u95f4 : 68 \uff0c \u51c6\u786e\u7387 : 0.710 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.964384 , \u65f6\u95f4 : 70 \uff0c \u51c6\u786e\u7387 : 0.711 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.958782 , \u65f6\u95f4 : 72 \uff0c \u51c6\u786e\u7387 : 0.713 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.955343 , \u65f6\u95f4 : 75 \uff0c \u51c6\u786e\u7387 : 0.713 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.950741 , \u65f6\u95f4 : 77 \uff0c \u51c6\u786e\u7387 : 0.715 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.945756 , \u65f6\u95f4 : 80 \uff0c \u51c6\u786e\u7387 : 0.716 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.942663 , \u65f6\u95f4 : 82 \uff0c \u51c6\u786e\u7387 : 0.717 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.939319 , \u65f6\u95f4 : 84 \uff0c \u51c6\u786e\u7387 : 0.718 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.936169 , \u65f6\u95f4 : 87 \uff0c \u51c6\u786e\u7387 : 0.719 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.933440 , \u65f6\u95f4 : 89 \uff0c \u51c6\u786e\u7387 : 0.720 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.930918 , \u65f6\u95f4 : 91 \uff0c \u51c6\u786e\u7387 : 0.720 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.927330 , \u65f6\u95f4 : 94 \uff0c \u51c6\u786e\u7387 : 0.721 LSTM\u6a21\u578b\u8bad\u7ec3\u65e5\u5fd7\u8f93\u51fa: \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.805885 , \u65f6\u95f4 : 118 \uff0c \u51c6\u786e\u7387 : 0.759 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.794148 , \u65f6\u95f4 : 123 \uff0c \u51c6\u786e\u7387 : 0.762 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.783356 , \u65f6\u95f4 : 128 \uff0c \u51c6\u786e\u7387 : 0.765 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.774931 , \u65f6\u95f4 : 133 \uff0c \u51c6\u786e\u7387 : 0.767 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.765427 , \u65f6\u95f4 : 137 \uff0c \u51c6\u786e\u7387 : 0.769 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.757254 , \u65f6\u95f4 : 142 \uff0c \u51c6\u786e\u7387 : 0.771 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.750375 , \u65f6\u95f4 : 147 \uff0c \u51c6\u786e\u7387 : 0.773 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.743092 , \u65f6\u95f4 : 152 \uff0c \u51c6\u786e\u7387 : 0.775 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.732983 , \u65f6\u95f4 : 157 \uff0c \u51c6\u786e\u7387 : 0.778 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.723816 , \u65f6\u95f4 : 162 \uff0c \u51c6\u786e\u7387 : 0.780 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.716507 , \u65f6\u95f4 : 167 \uff0c \u51c6\u786e\u7387 : 0.782 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.708377 , \u65f6\u95f4 : 172 \uff0c \u51c6\u786e\u7387 : 0.785 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.700820 , \u65f6\u95f4 : 177 \uff0c \u51c6\u786e\u7387 : 0.787 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.694714 , \u65f6\u95f4 : 182 \uff0c \u51c6\u786e\u7387 : 0.788 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.688386 , \u65f6\u95f4 : 187 \uff0c \u51c6\u786e\u7387 : 0.790 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.683056 , \u65f6\u95f4 : 191 \uff0c \u51c6\u786e\u7387 : 0.791 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.677051 , \u65f6\u95f4 : 196 \uff0c \u51c6\u786e\u7387 : 0.793 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.671668 , \u65f6\u95f4 : 201 \uff0c \u51c6\u786e\u7387 : 0.794 GRU\u6a21\u578b\u8bad\u7ec3\u65e5\u5fd7\u8f93\u51fa: \u8f6e\u6b21:3, \u635f\u5931:0.743891, \u65f6\u95f4:106\uff0c\u51c6\u786e\u7387:0.772 \u8f6e\u6b21:3, \u635f\u5931:0.733144, \u65f6\u95f4:111\uff0c\u51c6\u786e\u7387:0.775 \u8f6e\u6b21:3, \u635f\u5931:0.723484, \u65f6\u95f4:116\uff0c\u51c6\u786e\u7387:0.777 \u8f6e\u6b21:3, \u635f\u5931:0.714760, \u65f6\u95f4:120\uff0c\u51c6\u786e\u7387:0.780 \u8f6e\u6b21:3, \u635f\u5931:0.706929, \u65f6\u95f4:125\uff0c\u51c6\u786e\u7387:0.782 \u8f6e\u6b21:3, \u635f\u5931:0.698657, \u65f6\u95f4:130\uff0c\u51c6\u786e\u7387:0.784 \u8f6e\u6b21:3, \u635f\u5931:0.690443, \u65f6\u95f4:134\uff0c\u51c6\u786e\u7387:0.787 \u8f6e\u6b21:3, \u635f\u5931:0.683878, \u65f6\u95f4:139\uff0c\u51c6\u786e\u7387:0.789 \u8f6e\u6b21:4, \u635f\u5931:0.674766, \u65f6\u95f4:144\uff0c\u51c6\u786e\u7387:0.791 \u8f6e\u6b21:4, \u635f\u5931:0.665543, \u65f6\u95f4:148\uff0c\u51c6\u786e\u7387:0.794 \u8f6e\u6b21:4, \u635f\u5931:0.657179, \u65f6\u95f4:153\uff0c\u51c6\u786e\u7387:0.796 \u8f6e\u6b21:4, \u635f\u5931:0.650314, \u65f6\u95f4:157\uff0c\u51c6\u786e\u7387:0.798 \u8f6e\u6b21:4, \u635f\u5931:0.643698, \u65f6\u95f4:162\uff0c\u51c6\u786e\u7387:0.800 \u8f6e\u6b21:4, \u635f\u5931:0.637341, \u65f6\u95f4:167\uff0c\u51c6\u786e\u7387:0.802 \u8f6e\u6b21:4, \u635f\u5931:0.632063, \u65f6\u95f4:171\uff0c\u51c6\u786e\u7387:0.803 \u8f6e\u6b21:4, \u635f\u5931:0.626060, \u65f6\u95f4:176\uff0c\u51c6\u786e\u7387:0.805 \u8f6e\u6b21:4, \u635f\u5931:0.621460, \u65f6\u95f4:180\uff0c\u51c6\u786e\u7387:0.806 \u8f6e\u6b21:4, \u635f\u5931:0.616704, \u65f6\u95f4:185\uff0c\u51c6\u786e\u7387:0.808 5 \u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u5206\u6790 \u00b6 1 \u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf\u5206\u6790 \u00b6 \u5de6\u56fe\uff1a1\u4e2a\u8f6e\u6b21\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf\uff0c\u53f3\u56fe4\u4e2a\u8f6e\u6b21\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf \u6a21\u578b\u8bad\u7ec3\u7684\u635f\u5931\u964d\u4f4e\u5feb\u6162\u4ee3\u8868\u6a21\u578b\u6536\u655b\u7a0b\u5ea6\u3002\u7531\u56fe\u53ef\u77e5, \u4f20\u7edfRNN\u7684\u6a21\u578b\u7b2c\u4e00\u4e2a\u8f6e\u6b21\u5f00\u59cb\u6536\u655b\u60c5\u51b5\u6700\u597d\uff0c\u7136\u540e\u662fGRU, \u6700\u540e\u662fLSTM, \u8fd9\u662f\u56e0\u4e3aRNN\u6a21\u578b\u7b80\u5355\u53c2\u6570\u5c11\uff0c\u89c1\u6548\u5feb\u3002\u968f\u7740\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\uff0cGRU\u6548\u679c\u6700\u597d\u3001LSTM\u6548\u679c\u6b21\u4e4b\u3001RNN\u6548\u679c\u6392\u6700\u540e\u3002 \u6240\u4ee5\u5728\u4ee5\u540e\u7684\u6a21\u578b\u9009\u7528\u65f6\uff0c \u8981\u901a\u8fc7\u5bf9\u4efb\u52a1\u7684\u5206\u6790\u4ee5\u53ca\u5b9e\u9a8c\u5bf9\u6bd4, \u9009\u62e9\u6700\u9002\u5408\u7684\u6a21\u578b\u3002 2 \u8bad\u7ec3\u8017\u65f6\u5206\u6790 \u00b6 \u8bad\u7ec3\u8017\u65f6\u5bf9\u6bd4\u56fe: \u6a21\u578b\u8bad\u7ec3\u7684\u8017\u65f6\u957f\u77ed\u4ee3\u8868\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7531\u56fe\u53ef\u77e5\uff0c \u4e5f\u6b63\u5982\u6211\u4eec\u4e4b\u524d\u7684\u7406\u8bba\u5206\u6790\uff0c\u4f20\u7edfRNN\u590d\u6742\u5ea6\u6700\u4f4e\uff0c \u8017\u65f6\u51e0\u4e4e\u53ea\u662f\u540e\u4e24\u8005\u7684\u4e00\u534a, \u7136\u540e\u662fGRU\uff0c\u6700\u540e\u662f\u590d\u6742\u5ea6\u6700\u9ad8\u7684LSTM\u3002 3 \u8bad\u7ec3\u51c6\u786e\u7387\u5206\u6790 \u00b6 \u8bad\u7ec3\u51c6\u786e\u7387\u5bf9\u6bd4\u56fe: \u7531\u56fe\u53ef\u77e5\uff0c GRU\u6548\u679c\u6700\u597d\u3001LSTM\u6548\u679c\u6b21\u4e4b\u3001RNN\u6548\u679c\u6392\u6700\u540e\u3002 4 \u7ed3\u8bba \u00b6 \u6a21\u578b\u9009\u7528\u4e00\u822c\u5e94\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u5e76\u975e\u8d8a\u590d\u6742\u6216\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u8868\u73b0\u8d8a\u597d\uff0c\u800c\u662f\u9700\u8981\u7ed3\u5408\u81ea\u5df1\u7684\u7279\u5b9a\u4efb\u52a1\uff0c\u4ece\u5bf9\u6570\u636e\u7684\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e2d\u83b7\u5f97\u6700\u4f73\u7b54\u6848\u3002 2.5 \u6784\u5efa\u9884\u6d4b\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b \u00b6 1 \u6784\u5efaRNN\u9884\u6d4b\u51fd\u6570 \u00b6 # 1 \u6784\u5efa\u4f20\u7edfRNN\u9884\u6d4b\u51fd\u6570 my_path_rnn = './model/my_rnn_model_1.bin' my_path_lstm = './model/my_lstm_model_1.bin' my_path_gru = './model/my_gru_model_1.bin' # \u5c06\u4eba\u540d\u8f6c\u5316\u4e3aonehot\u5f20\u91cf # eg 'bai' --> [3,57] def lineToTensor ( x ): # \u6587\u672c\u5f20\u91cf\u5316x tensor_x = torch . zeros ( len ( x ), n_letters ) # \u904d\u5386\u8fd9\u4e2a\u4eba\u540d\u4e2d\u7684\u6bcf\u4e2a\u5b57\u7b26\u7d22\u5f15\u548c\u5b57\u7b26 for li , letter in enumerate ( x ): # letter\u5728\u5b57\u7b26\u4e32all_letters\u4e2d\u7684\u4f4d\u7f6e \u5c31\u662fonehot\u5f20\u91cf1\u7d22\u5f15\u7684\u4f4d\u7f6e # letter\u5728\u5b57\u7b26\u4e32all_letters\u4e2d\u7684\u4f4d\u7f6e \u4f7f\u7528\u5b57\u7b26\u4e32find()\u65b9\u6cd5\u83b7\u53d6 tensor_x [ li ][ all_letters . find ( letter )] = 1 return tensor_x # \u601d\u8def\u5206\u6790 # 1 \u8f93\u5165\u6587\u672c\u6570\u636e \u5f20\u91cf\u5316one-hot # 2 \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 m.load_state_dict(torch.load(my_path_rnn)) # 3 \u6a21\u578b\u9884\u6d4b with torch.no_grad() # 4 \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d,\u663e\u793a\u6253\u5370\u7ed3\u679c output.topk(3, 1, True) # category_idx = topi[0][i] category = categorys[category_idx] # \u6784\u5efarnn\u9884\u6d4b\u51fd\u6570 def my_predict_rnn ( x ): n_letters = 57 n_hidden = 128 n_categories = 18 # \u8f93\u5165\u6587\u672c, \u5f20\u91cf\u5316one-hot x_tensor = lineToTensor ( x ) # \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 my_rnn = RNN ( n_letters , n_hidden , n_categories ) my_rnn . load_state_dict ( torch . load ( my_path_rnn )) with torch . no_grad (): # \u6a21\u578b\u9884\u6d4b output , hidden = my_rnn ( x_tensor , my_rnn . inithidden ()) # \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d # 3\u8868\u793a\u53d6\u524d3\u540d, 1\u8868\u793a\u8981\u6392\u5e8f\u7684\u7ef4\u5ea6, True\u8868\u793a\u662f\u5426\u8fd4\u56de\u6700\u5927\u6216\u662f\u6700\u4e0b\u7684\u5143\u7d20 topv , topi = output . topk ( 3 , 1 , True ) print ( 'rnn =>' , x ) for i in range ( 3 ): value = topv [ 0 ][ i ] category_idx = topi [ 0 ][ i ] category = categorys [ category_idx ] print ( ' \\t value: %d category: %s ' % ( value , category )) 2 \u6784\u5efaLSTM\u9884\u6d4b\u51fd\u6570 \u00b6 # \u6784\u5efaLSTM \u9884\u6d4b\u51fd\u6570 def my_predict_lstm ( x ): n_letters = 57 n_hidden = 128 n_categories = 18 # \u8f93\u5165\u6587\u672c, \u5f20\u91cf\u5316one-hot x_tensor = lineToTensor ( x ) # \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 my_lstm = LSTM ( n_letters , n_hidden , n_categories ) my_lstm . load_state_dict ( torch . load ( my_path_lstm )) with torch . no_grad (): # \u6a21\u578b\u9884\u6d4b hidden , c = my_lstm . inithidden () output , hidden , c = my_lstm ( x_tensor , hidden , c ) # \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d # 3\u8868\u793a\u53d6\u524d3\u540d, 1\u8868\u793a\u8981\u6392\u5e8f\u7684\u7ef4\u5ea6, True\u8868\u793a\u662f\u5426\u8fd4\u56de\u6700\u5927\u6216\u662f\u6700\u4e0b\u7684\u5143\u7d20 topv , topi = output . topk ( 3 , 1 , True ) print ( 'rnn =>' , x ) for i in range ( 3 ): value = topv [ 0 ][ i ] category_idx = topi [ 0 ][ i ] category = categorys [ category_idx ] print ( ' \\t value: %d category: %s ' % ( value , category )) print ( ' \\t value: %d category: %s ' % ( value , category )) 3 \u6784\u5efaGRU\u9884\u6d4b\u51fd\u6570 \u00b6 # \u6784\u5efaGRU \u9884\u6d4b\u51fd\u6570 def my_predict_gru ( x ): n_letters = 57 n_hidden = 128 n_categories = 18 # \u8f93\u5165\u6587\u672c, \u5f20\u91cf\u5316one-hot x_tensor = lineToTensor ( x ) # \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 my_gru = GRU ( n_letters , n_hidden , n_categories ) my_gru . load_state_dict ( torch . load ( my_path_gru )) with torch . no_grad (): # \u6a21\u578b\u9884\u6d4b output , hidden = my_gru ( x_tensor , my_gru . inithidden ()) # \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d # 3\u8868\u793a\u53d6\u524d3\u540d, 1\u8868\u793a\u8981\u6392\u5e8f\u7684\u7ef4\u5ea6, True\u8868\u793a\u662f\u5426\u8fd4\u56de\u6700\u5927\u6216\u662f\u6700\u4e0b\u7684\u5143\u7d20 topv , topi = output . topk ( 3 , 1 , True ) print ( 'rnn =>' , x ) for i in range ( 3 ): value = topv [ 0 ][ i ] category_idx = topi [ 0 ][ i ] category = categorys [ category_idx ] print ( ' \\t value: %d category: %s ' % ( value , category )) 4 \u6784\u5efaRNN_LSTM_GRU\u9884\u6d4b\u8c03\u7528\u51fd\u6570 \u00b6 def dm_test_predic_rnn_lstm_gru (): # \u628a\u4e09\u4e2a\u51fd\u6570\u7684\u5165\u53e3\u5730\u5740 \u7ec4\u6210\u5217\u8868\uff0c\u7edf\u4e00\u8f93\u5165\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5 for func in [ my_predict_rnn , my_predict_lstm , my_predict_gru ]: func ( 'zhang' ) \u8f93\u51fa\u6548\u679c rnn => zhang value:0 category:Russian value:0 category:Chinese value:-4 category:German rnn => zhang value:0 category:Chinese value:-1 category:Russian value:-1 category:German rnn => zhang value:0 category:Russian value:0 category:Chinese value:-2 category:Korean 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u5173\u4e8e\u4eba\u540d\u5206\u7c7b\u95ee\u9898: \u4ee5\u4e00\u4e2a\u4eba\u540d\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u662f\u6765\u81ea\u54ea\u4e00\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d, \u8fd9\u5728\u67d0\u4e9b\u56fd\u9645\u5316\u516c\u53f8\u7684\u4e1a\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49, \u5728\u7528\u6237\u6ce8\u518c\u8fc7\u7a0b\u4e2d, \u4f1a\u6839\u636e\u7528\u6237\u586b\u5199\u7684\u540d\u5b57\u76f4\u63a5\u7ed9\u4ed6\u5206\u914d\u53ef\u80fd\u7684\u56fd\u5bb6\u6216\u5730\u533a\u9009\u9879, \u4ee5\u53ca\u8be5\u56fd\u5bb6\u6216\u5730\u533a\u7684\u56fd\u65d7, \u9650\u5236\u624b\u673a\u53f7\u7801\u4f4d\u6570\u7b49\u7b49. \u4eba\u540d\u5206\u7c7b\u5668\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65: \u6784\u5efaRNN\u6a21\u578b(\u5305\u62ec\u4f20\u7edfRNN, LSTM\u4ee5\u53caGRU) \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65: \u6784\u5efa\u8bc4\u4f30\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 python\u7248\u672c\u4f7f\u75283.7.x, pytorch\u7248\u672c\u4f7f\u75281.6.1 \u7b2c\u4e8c\u6b65: \u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42 \u8bfb\u539f\u59cb\u6570\u636e\u5230\u5185\u5b58\uff0c\u6784\u5efa\u51fa\u6a21\u578b\u9700\u8981\u7684\u6570\u636ex\uff0c\u6807\u7b7ey\uff0c\u7136\u540e\u628a\u6570\u636e\u8f6c\u6210\u6570\u636e\u6e90\uff0c\u6700\u540e\u518d\u5c01\u88c5\u6210\u6570\u636e\u8fed\u4ee3\u5668 \u4ece\u7f16\u7a0b\u5b9e\u73b0\u6765\u770b\uff0c\u6587\u672c\u6570\u503c\u5316\uff0c\u6570\u503c\u5f20\u91cf\u5316\u662f\u901a\u8fc7one-hot\u7f16\u7801\u4e00\u6b65\u5b8c\u6210\u7684 \u7b2c\u4e09\u6b65: \u6784\u5efaRNN\u6a21\u578b \u6784\u5efa\u4f20\u7edf\u7684RNN\u6a21\u578b\u7684\u7c7bclass RNN. \u6784\u5efaLSTM\u6a21\u578b\u7684\u7c7bclass LSTM. \u6784\u5efaGRU\u6a21\u578b\u7684\u7c7bclass GRU. \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3 \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61 \u5b9e\u4f8b\u5316\u6a21\u578b\u5bf9\u8c61\u3001\u635f\u5931\u51fd\u6570\u5bf9\u8c61\u3001\u4f18\u5316\u5668\u5bf9\u8c61 \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 \u8bad\u7ec3\u6a21\u578b \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570\uff0c\u7ed9\u6a21\u578b\u5582\u6570\u636e\uff0c\u8ba1\u7b97\u635f\u5931 \uff0c\u68af\u5ea6\u6e05\u96f6 \uff0c\u53cd\u5411\u4f20\u64ad \uff0c \u68af\u5ea6\u66f4\u65b0\uff0c\u6253\u5370\u65e5\u5fd7 \u6a21\u578b\u4fdd\u5b58 \u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf\u5206\u6790: \u4f20\u7edfRNN\u7684\u6a21\u578b\u7b2c\u4e00\u4e2a\u8f6e\u6b21\u5f00\u59cb\u6536\u655b\u60c5\u51b5\u6700\u597d\uff0c\u7136\u540e\u662fGRU, \u6700\u540e\u662fLSTM, \u8fd9\u662f\u56e0\u4e3aRNN\u6a21\u578b\u7b80\u5355\u53c2\u6570\u5c11\uff0c\u89c1\u6548\u5feb\u3002 \u968f\u7740\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\uff0cGRU\u6548\u679c\u6700\u597d\u3001LSTM\u6548\u679c\u6b21\u4e4b\u3001RNN\u6548\u679c\u6392\u6700\u540e \u8bad\u7ec3\u8017\u65f6\u5bf9\u6bd4\u56fe\u5206\u6790: \u6a21\u578b\u8bad\u7ec3\u7684\u8017\u65f6\u957f\u77ed\u4ee3\u8868\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7531\u56fe\u53ef\u77e5\uff0c\u4e5f\u6b63\u5982\u6211\u4eec\u4e4b\u524d\u7684\u7406\u8bba\u5206\u6790\uff0c\u4f20\u7edfRNN\u590d\u6742\u5ea6\u6700\u4f4e\uff0c\u8017\u65f6\u51e0\u4e4e\u53ea\u662f\u540e\u4e24\u8005\u7684\u4e00\u534a\uff0c\u7136\u540e\u662fGRU\uff0c\u6700\u540e\u662f\u590d\u6742\u5ea6\u6700\u9ad8\u7684LSTM \u7ed3\u8bba: \u6a21\u578b\u9009\u7528\u4e00\u822c\u5e94\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\uff0c \u5e76\u975e\u8d8a\u590d\u6742\u6216\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u8868\u73b0\u8d8a\u597d\uff0c \u800c\u662f\u9700\u8981\u7ed3\u5408\u81ea\u5df1\u7684\u7279\u5b9a\u4efb\u52a1\uff0c\u4ece\u5bf9\u6570\u636e\u7684\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e2d\u83b7\u5f97\u6700\u4f73\u7b54\u6848 \u7b2c\u4e94\u6b65: \u6784\u5efa\u9884\u6d4b\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b \u6784\u5efa\u4f20\u7edfRNN\u9884\u6d4b\u51fd\u6570 \u6784\u5efaLSTM\u9884\u6d4b\u51fd\u6570 \u6784\u5efaGRU\u9884\u6d4b\u51fd\u6570 \u6784\u5efa\u9884\u6d4b\u51fd\u6570\u8c03\u7528\u51fd\u6570","title":"5 RNN\u6848\u4f8b \u4eba\u540d\u5206\u7c7b\u5668"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#_1","text":"\u4e86\u89e3\u6709\u5173\u4eba\u540d\u5206\u7c7b\u95ee\u9898\u548c\u6709\u5173\u6570\u636e. \u638c\u63e1\u4f7f\u7528RNN\u6784\u5efa\u4eba\u540d\u5206\u7c7b\u5668\u5b9e\u73b0\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#1","text":"\u5173\u4e8e\u4eba\u540d\u5206\u7c7b\u95ee\u9898: \u4ee5\u4e00\u4e2a\u4eba\u540d\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u662f\u6765\u81ea\u54ea\u4e00\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d, \u8fd9\u5728\u67d0\u4e9b\u56fd\u9645\u5316\u516c\u53f8\u7684\u4e1a\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49, \u5728\u7528\u6237\u6ce8\u518c\u8fc7\u7a0b\u4e2d, \u4f1a\u6839\u636e\u7528\u6237\u586b\u5199\u7684\u540d\u5b57\u76f4\u63a5\u7ed9\u4ed6\u5206\u914d\u53ef\u80fd\u7684\u56fd\u5bb6\u6216\u5730\u533a\u9009\u9879, \u4ee5\u53ca\u8be5\u56fd\u5bb6\u6216\u5730\u533a\u7684\u56fd\u65d7, \u9650\u5236\u624b\u673a\u53f7\u7801\u4f4d\u6570\u7b49\u7b49. \u4eba\u540d\u5206\u7c7b\u6570\u636e\u9884\u89c8 \u6570\u636e\u5b58\u653e\u8def\u5f84\uff1a$(home)/data/name_classfication.txt \u6570\u636e\u683c\u5f0f\u8bf4\u660e \u6bcf\u4e00\u884c\u7b2c\u4e00\u4e2a\u5355\u8bcd\u4e3a\u4eba\u540d\uff0c\u7b2c\u4e8c\u4e2a\u5355\u8bcd\u4e3a\u56fd\u5bb6\u540d\u3002\u4e2d\u95f4\u7528\u5236\u8868\u7b26tab\u5206\u5272 Huffmann German Hummel German Hummel German Hutmacher German Ingersleben German Jaeger German Jager German Deng Chinese Ding Chinese Dong Chinese Dou Chinese Duan Chinese Eng Chinese Fan Chinese Fei Chinese Abaimov Russian Abakeliya Russian Abakovsky Russian Abakshin Russian Abakumoff Russian Abakumov Russian Abakumtsev Russian Abakushin Russian Abalakin Russian","title":"1 \u6848\u4f8b\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#2","text":"\u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4 \u7b2c\u4e00\u6b65\u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65\u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65\u6784\u5efaRNN\u6a21\u578b(\u5305\u62ec\u4f20\u7edfRNN, LSTM\u4ee5\u53caGRU) \u7b2c\u56db\u6b65\u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65\u6784\u5efa\u9884\u6d4b\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b","title":"2  \u6848\u4f8b\u6b65\u9aa4"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#21","text":"# \u5bfc\u5165torch\u5de5\u5177 import torch # \u5bfc\u5165nn\u51c6\u5907\u6784\u5efa\u6a21\u578b import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # \u5bfc\u5165torch\u7684\u6570\u636e\u6e90 \u6570\u636e\u8fed\u4ee3\u5668\u5de5\u5177\u5305 from torch.utils.data import Dataset , DataLoader # \u7528\u4e8e\u83b7\u5f97\u5e38\u89c1\u5b57\u6bcd\u53ca\u5b57\u7b26\u89c4\u8303\u5316 import string # \u5bfc\u5165\u65f6\u95f4\u5de5\u5177\u5305 import time # \u5f15\u5165\u5236\u56fe\u5de5\u5177\u5305 import matplotlib.pyplot as plt # \u4eceio\u4e2d\u5bfc\u5165\u6587\u4ef6\u6253\u5f00\u65b9\u6cd5 from io import open","title":"2.1 \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#22","text":"\u8fd9\u91cc\u9700\u8981\u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42\u3002","title":"2.2 \u6570\u636e\u9884\u5904\u7406"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#1_1","text":"# \u83b7\u53d6\u6240\u6709\u5e38\u7528\u5b57\u7b26\u5305\u62ec\u5b57\u6bcd\u548c\u5e38\u7528\u6807\u70b9 all_letters = string . ascii_letters + \" .,;'\" # \u83b7\u53d6\u5e38\u7528\u5b57\u7b26\u6570\u91cf n_letters = len ( all_letters ) print ( \"n_letter:\" , n_letters ) \u8f93\u51fa\u6548\u679c: n_letter: 57","title":"1 \u83b7\u53d6\u5e38\u7528\u7684\u5b57\u7b26\u6570\u91cf"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#2_1","text":"# \u56fd\u5bb6\u540d \u79cd\u7c7b\u6570 categorys = [ 'Italian' , 'English' , 'Arabic' , 'Spanish' , 'Scottish' , 'Irish' , 'Chinese' , 'Vietnamese' , 'Japanese' , 'French' , 'Greek' , 'Dutch' , 'Korean' , 'Polish' , 'Portuguese' , 'Russian' , 'Czech' , 'German' ] # \u56fd\u5bb6\u540d \u4e2a\u6570 categorynum = len ( categorys ) print ( 'categorys--->' , categorys ) \u8f93\u51fa\u6548\u679c: categorys ---> [ 'Italian' , 'English' , 'Arabic' , 'Spanish' , 'Scottish' , 'Irish' , 'Chinese' , 'Vietnamese' , 'Japanese' , 'French' , 'Greek' , 'Dutch' , 'Korean' , 'Polish' , 'Portuguese' , 'Russian' , 'Czech' , 'German' ] categorynum ---> 18","title":"2 \u56fd\u5bb6\u540d\u79cd\u7c7b\u6570\u548c\u4e2a\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#3","text":"# \u601d\u8def\u5206\u6790 # 1 \u6253\u5f00\u6570\u636e\u6587\u4ef6 open(filename, mode='r', encoding='utf-8') # 2 \u6309\u884c\u8bfb\u6587\u4ef6\u3001\u63d0\u53d6\u6837\u672cx \u6837\u672cy line.strip().split('\\t') # 3 \u8fd4\u56de\u6837\u672cx\u7684\u5217\u8868\u3001\u6837\u672cy\u7684\u5217\u8868 my_list_x, my_list_y def read_data ( filename ): my_list_x , my_list_y = [], [] # \u6253\u5f00\u6587\u4ef6 with open ( filename , mode = 'r' , encoding = 'utf-8' ) as f : # \u6309\u7167\u884c\u8bfb\u6570\u636e for line in f . readlines (): if len ( line ) <= 5 : continue # \u6309\u7167\u884c\u63d0\u53d6\u6837\u672cx \u6837\u672cy ( x , y ) = line . strip () . split ( ' \\t ' ) my_list_x . append ( x ) my_list_y . append ( y ) # \u6253\u5370\u6837\u672c\u7684\u6570\u91cf print ( 'my_list_x->' , len ( my_list_x )) print ( 'my_list_y->' , len ( my_list_y )) # \u8fd4\u56de\u6837\u672cx\u7684\u5217\u8868\u3001\u6837\u672cy\u7684\u5217\u8868 return my_list_x , my_list_y","title":"3 \u8bfb\u6570\u636e\u5230\u5185\u5b58"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#4-nameclassdataset","text":"# \u539f\u59cb\u6570\u636e -> \u6570\u636e\u6e90NameClassDataset --> \u6570\u636e\u8fed\u4ee3\u5668DataLoader # \u6784\u9020\u6570\u636e\u6e90 NameClassDataset\uff0c\u628a\u8bed\u6599\u8f6c\u6362\u6210x y # 1 init\u51fd\u6570 \u8bbe\u7f6e\u6837\u672cx\u548cy self.my_list_x self.my_list_y \u6761\u76ee\u6570self.sample_len # 2 __len__(self)\u51fd\u6570 \u83b7\u53d6\u6837\u672c\u6761\u6570 # 3 __getitem__(self, index)\u51fd\u6570 \u83b7\u53d6\u7b2c\u51e0\u6761\u6837\u672c\u6570\u636e # \u6309\u7d22\u5f15 \u83b7\u53d6\u6570\u636e\u6837\u672c x y # \u6837\u672cx one-hot\u5f20\u91cf\u5316 tensor_x[li][all_letters.find(letter)] = 1 # \u6837\u672cy \u5f20\u91cf\u5316 torch.tensor(categorys.index(y), dtype=torch.long) # \u8fd4\u56detensor_x, tensor_y class NameClassDataset ( Dataset ): def __init__ ( self , my_list_x , my_list_y ): # \u6837\u672cx self . my_list_x = my_list_x # \u6837\u672cy self . my_list_y = my_list_y # \u6837\u672c\u6761\u76ee\u6570 self . sample_len = len ( my_list_x ) # \u83b7\u53d6\u6837\u672c\u6761\u6570 def __len__ ( self ): return self . sample_len # \u83b7\u53d6\u7b2c\u51e0\u6761 \u6837\u672c\u6570\u636e def __getitem__ ( self , index ): # \u5bf9index\u5f02\u5e38\u503c\u8fdb\u884c\u4fee\u6b63 [0, self.sample_len-1] index = min ( max ( index , 0 ), self . sample_len - 1 ) # \u6309\u7d22\u5f15\u83b7\u53d6 \u6570\u636e\u6837\u672c x y x = self . my_list_x [ index ] y = self . my_list_y [ index ] # \u6837\u672cx one-hot\u5f20\u91cf\u5316 tensor_x = torch . zeros ( len ( x ), n_letters ) # \u904d\u5386\u4eba\u540d \u7684 \u6bcf\u4e2a\u5b57\u6bcd \u505a\u6210one-hot\u7f16\u7801 for li , letter in enumerate ( x ): # letter2indx \u4f7f\u7528all_letters.find(letter)\u67e5\u627e\u5b57\u6bcd\u5728all_letters\u8868\u4e2d\u7684\u4f4d\u7f6e \u7ed9one-hot\u8d4b\u503c tensor_x [ li ][ all_letters . find ( letter )] = 1 # \u6837\u672cy \u5f20\u91cf\u5316 tensor_y = torch . tensor ( categorys . index ( y ), dtype = torch . long ) # \u8fd4\u56de\u7ed3\u679c return tensor_x , tensor_y \u5206\u6790 \u6587\u672c\u5f20\u91cf\u5316\uff0c\u8fd9\u91cc\u4e5f\u5c31\u662f\u4eba\u540d\u5f20\u91cf\u5316\u662f\u901a\u8fc7one-hot\u7f16\u7801\u6765\u5b8c\u6210\u3002 # \u5c06\u5b57\u7b26\u4e32(\u5355\u8bcd\u7c92\u5ea6)\u8f6c\u5316\u4e3a\u5f20\u91cf\u8868\u793a\uff0c\u5982\uff1a\"ab\" ---> # tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0.]], # [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., # 0., 0., 0., 0., 0., 0.]]])","title":"4 \u6784\u5efa\u6570\u636e\u6e90NameClassDataset"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#5","text":"def dm_test_NameClassDataset (): # 1 \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) print ( 'my_list_x length' , len ( my_list_x )) print ( 'my_list_y length' , len ( my_list_y )) # 2 \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # 3 \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) break \u8f93\u51fa\u6548\u679c: my_list_x length 20074 my_list_y length 20074 x.shape torch.Size([1, 5, 57]) tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]) y.shape torch.Size([1]) tensor([15])","title":"5 \u6784\u5efa\u8fed\u4ee3\u5668\u904d\u5386\u6570\u636e"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#23-rnn","text":"","title":"2.3 \u6784\u5efaRNN\u6a21\u578b"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#1-rnn","text":"# RNN\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, input_size, hidden_size, output_size, num_layers=1) # 2 forward(input, hidden)\u51fd\u6570 # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u5f62\u72b6\u53d8\u5316 [seqlen,1,57],[1,1,128]) -> [seqlen,1,128],[1,1,128] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6[self.num_layers, 1, self.hidden_size] class RNN ( nn . Module ): def __init__ ( self , input_size , hidden_size , output_size , num_layers = 1 ): super ( RNN , self ) . __init__ () # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) self . input_size = input_size self . hidden_size = hidden_size self . output_size = output_size self . num_layers = num_layers # \u5b9a\u4e49rnn\u5c42 self . rnn = nn . RNN ( self . input_size , self . hidden_size , self . num_layers ) # \u5b9a\u4e49linear\u5c42 self . linear = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9a\u4e49softmax\u5c42 self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden ): '''input [6,57]-2\u7ef4\u77e9\u9635 hidden[1,1,57] - 3\u7ef4\u77e9\u9635''' # \u6570\u636e\u5f62\u72b6 [6,57] -> [6,1,57] input = input . unsqueeze ( 1 ) # 1 \u6570\u636e\u7ecf\u8fc7\u6a21\u578b \u63d0\u53d6\u4e8b\u7269\u7279\u5f81 # \u6570\u636e\u5f62\u72b6 [seqlen,1,57],[1,1,128]) -> [seqlen,1,18],[1,1,128] rr , hn = self . rnn ( input , hidden ) # \u6570\u636e\u5f62\u72b6 [seqlen,1,128] - [1, 128] eg:[6,1,128] --> [1,128] tmprr = rr [ - 1 ] # 2 \u6570\u636e\u7ecf\u8fc7\u5168\u8fde\u63a5\u5c42 [1,128] -->[1,18] tmprr = self . linear ( tmprr ) # 3 \u6570\u636e\u7ecf\u8fc7softmax\u5c42\u8fd4\u56de return self . softmax ( tmprr ), hn def inithidden ( self ): # \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() return torch . zeros ( self . num_layers , 1 , self . hidden_size ) torch.unsqueeze\u6f14\u793a: >>> x = torch.tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) tensor([[ 1, 2, 3, 4]]) >>> torch.unsqueeze(x, 1) tensor([[ 1], [ 2], [ 3], [ 4]]) \u8c03\u7528: def dm01_test_myrnn (): # 1 \u5b9e\u4f8b\u5316rnn\u5bf9\u8c61 myrnn = RNN ( 57 , 128 , 18 ) print ( 'myrnn--->' , myrnn ) # 2 \u51c6\u5907\u6570\u636e input = torch . randn ( 6 , 57 ) print ( input . shape ) hidden = myrnn . inithidden () # 3 \u7ed9\u6a21\u578b1\u6b21\u6027\u7684\u9001\u6570\u636e # [seqlen, 57], [1, 1, 128]) -> [1,18], [1,1,128] output , hidden = myrnn ( input , hidden ) print ( '\u4e00\u6b21\u6027\u7684\u9001\u6570\u636e\uff1aoutput->' , output . shape , output ) print ( 'hidden->' , hidden . shape ) # 4 \u7ed9\u6a21\u578b1\u4e2a\u5b57\u7b261\u4e2a\u5b57\u7b26\u7684\u5582\u6570\u636e hidden = myrnn . inithidden () for i in range ( input . shape [ 0 ]): tmpinput = input [ i ] . unsqueeze ( 0 ) output , hidden = myrnn ( tmpinput , hidden ) # \u6700\u540e\u4e00\u6b21ouput print ( '\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u9001\u6570\u636eoutput->' , output . shape , output ) \u8c03\u7528\u7ed3\u679c: myrnn ---> RNN ( ( rnn ): RNN ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) torch . Size ([ 6 , 57 ]) \u4e00\u6b21\u6027\u7684\u9001\u6570\u636e \uff1a output -> torch . Size ([ 1 , 18 ]) tensor ([[ - 2.8194 , - 3.1730 , - 3.3112 , - 2.9715 , - 3.0997 , - 2.8097 , - 2.8016 , - 2.8738 , - 2.7229 , - 2.8181 , - 2.7881 , - 3.0218 , - 2.9169 , - 2.6193 , - 2.8507 , - 2.9684 , - 2.8589 , - 2.8273 ]], grad_fn =< LogSoftmaxBackward0 > ) hidden -> torch . Size ([ 1 , 1 , 128 ]) \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u9001\u6570\u636eoutput -> torch . Size ([ 1 , 18 ]) tensor ([[ - 2.8194 , - 3.1730 , - 3.3112 , - 2.9715 , - 3.0997 , - 2.8097 , - 2.8016 , - 2.8738 , - 2.7229 , - 2.8181 , - 2.7881 , - 3.0218 , - 2.9169 , - 2.6193 , - 2.8507 , - 2.9684 , - 2.8589 , - 2.8273 ]], grad_fn =< LogSoftmaxBackward0 > )","title":"1 \u6784\u5efaRNN\u6a21\u578b"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#2-lstm","text":"# LSTM\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, input_size, hidden_size, output_size, num_layers=1) # 2 forward(input, hidden)\u51fd\u6570 # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u5f62\u72b6\u53d8\u5316 [seqlen,1,57],[1,1,128]) -> [seqlen,1,128],[1,1,128] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6[self.num_layers, 1, self.hidden_size] class LSTM ( nn . Module ): def __init__ ( self , input_size , hidden_size , output_size , num_layers = 1 ): super ( LSTM , self ) . __init__ () # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) self . input_size = input_size self . hidden_size = hidden_size self . output_size = output_size self . num_layers = num_layers # \u5b9a\u4e49rnn\u5c42 self . rnn = nn . LSTM ( self . input_size , self . hidden_size , self . num_layers ) # \u5b9a\u4e49linear\u5c42 self . linear = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9a\u4e49softmax\u5c42 self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden , c ): # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548c hn c # \u6570\u636e\u5f62\u72b6 [6,57] -> [6,1,52] input = input . unsqueeze ( 1 ) # \u628a\u6570\u636e\u9001\u7ed9\u6a21\u578b \u63d0\u53d6\u4e8b\u7269\u7279\u5f81 # \u6570\u636e\u5f62\u72b6 [seqlen,1,57],[1,1,128], [1,1,128]) -> [seqlen,1,18],[1,1,128],[1,1,128] rr , ( hn , cn ) = self . rnn ( input , ( hidden , c )) # \u6570\u636e\u5f62\u72b6 [seqlen,1,128] - [1, 128] tmprr = rr [ - 1 ] tmprr = self . linear ( tmprr ) return self . softmax ( tmprr ), hn , cn def inithidden ( self ): # \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() hidden = c = torch . zeros ( self . num_layers , 1 , self . hidden_size ) return hidden , c","title":"2 \u6784\u5efaLSTM\u6a21\u578b"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#3-gru","text":"# GRU\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, input_size, hidden_size, output_size, num_layers=1) # 2 forward(input, hidden)\u51fd\u6570 # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u5f62\u72b6\u53d8\u5316 [seqlen,1,57],[1,1,128]) -> [seqlen,1,128],[1,1,128] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6[self.num_layers, 1, self.hidden_size] class GRU ( nn . Module ): def __init__ ( self , input_size , hidden_size , output_size , num_layers = 1 ): super ( GRU , self ) . __init__ () # 1 init\u51fd\u6570 \u51c6\u5907\u4e09\u4e2a\u5c42 self.rnn self.linear self.softmax=nn.LogSoftmax(dim=-1) self . input_size = input_size self . hidden_size = hidden_size self . output_size = output_size self . num_layers = num_layers # \u5b9a\u4e49rnn\u5c42 self . rnn = nn . GRU ( self . input_size , self . hidden_size , self . num_layers ) # \u5b9a\u4e49linear\u5c42 self . linear = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9a\u4e49softmax\u5c42 self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden ): # \u8ba9\u6570\u636e\u7ecf\u8fc7\u4e09\u4e2a\u5c42 \u8fd4\u56desoftmax\u7ed3\u679c\u548chn # \u6570\u636e\u5f62\u72b6 [6,57] -> [6,1,52] input = input . unsqueeze ( 1 ) # \u628a\u6570\u636e\u9001\u7ed9\u6a21\u578b \u63d0\u53d6\u4e8b\u7269\u7279\u5f81 # \u6570\u636e\u5f62\u72b6 [seqlen,1,57],[1,1,128]) -> [seqlen,1,18],[1,1,128] rr , hn = self . rnn ( input , hidden ) # \u6570\u636e\u5f62\u72b6 [seqlen,1,128] - [1, 128] tmprr = rr [ - 1 ] tmprr = self . linear ( tmprr ) return self . softmax ( tmprr ), hn def inithidden ( self ): # \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() return torch . zeros ( self . num_layers , 1 , self . hidden_size )","title":"3 \u6784\u5efaGRU\u6a21\u578b"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#4-rnn_lstm_gru","text":"def dm_test_rnn_lstm_gru (): # one-hot\u7f16\u7801\u7279\u5f8157\uff08n_letters\uff09\uff0c\u4e5f\u662fRNN\u7684\u8f93\u5165\u5c3a\u5bf8 input_size = 57 # \u5b9a\u4e49\u9690\u5c42\u7684\u6700\u540e\u4e00\u7ef4\u5c3a\u5bf8\u5927\u5c0f n_hidden = 128 # \u8f93\u51fa\u5c3a\u5bf8\u4e3a\u8bed\u8a00\u7c7b\u522b\u603b\u6570n_categories # 1\u4e2a\u5b57\u7b26\u9884\u6d4b\u621018\u4e2a\u7c7b\u522b output_size = 18 # 1 \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) print ( 'categorys--->' , categorys ) # 2 \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # 3 \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) my_rnn = RNN ( n_letters , n_hidden , categorynum ) my_lstm = LSTM ( n_letters , n_hidden , categorynum ) my_gru = GRU ( n_letters , n_hidden , categorynum ) print ( 'rnn \u6a21\u578b' , my_rnn ) print ( 'lstm \u6a21\u578b' , my_lstm ) print ( 'gru \u6a21\u578b' , my_gru ) for i , ( x , y ) in enumerate ( mydataloader ): # print('x.shape', x.shape, x) # print('y.shape', y.shape, y) # \u521d\u59cb\u5316\u4e00\u4e2a\u4e09\u7ef4\u7684\u9690\u5c420\u5f20\u91cf, \u4e5f\u662f\u521d\u59cb\u7684\u7ec6\u80de\u72b6\u6001\u5f20\u91cf output , hidden = my_rnn ( x [ 0 ], my_rnn . inithidden ()) print ( \"rnn output.shape--->:\" , output . shape , output ) if ( i == 0 ): break for i , ( x , y ) in enumerate ( mydataloader ): # print('x.shape', x.shape, x) # print('y.shape', y.shape, y) hidden , c = my_lstm . inithidden () output , hidden , c = my_lstm ( x [ 0 ], hidden , c ) print ( \"lstm output.shape--->:\" , output . shape , output ) if ( i == 0 ): break for i , ( x , y ) in enumerate ( mydataloader ): # print('x.shape', x.shape, x) # print('y.shape', y.shape, y) output , hidden = my_gru ( x [ 0 ], my_gru . inithidden ()) print ( \"gru output.shape--->:\" , output . shape , output ) if ( i == 0 ): break \u8f93\u51fa\u6548\u679c: rnn \u6a21\u578b RNN ( ( rnn ): RNN ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) lstm \u6a21\u578b LSTM ( ( rnn ): LSTM ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) gru \u6a21\u578b GRU ( ( rnn ): GRU ( 57 , 128 ) ( linear ): Linear ( in_features = 128 , out_features = 18 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) rnn output . shape ---> : torch . Size ([ 1 , 18 ]) tensor ([[ - 2.9552 , - 2.9024 , - 2.8828 , - 2.7737 , - 2.8387 , - 3.0154 , - 2.8587 , - 2.9567 , - 2.8406 , - 3.0098 , - 2.8152 , - 2.8472 , - 2.9561 , - 2.8780 , - 2.8332 , - 2.8117 , - 2.9560 , - 2.9384 ]], grad_fn =< LogSoftmaxBackward0 > ) lstm output . shape ---> : torch . Size ([ 1 , 18 ]) tensor ([[ - 2.9283 , - 3.0017 , - 2.8902 , - 2.8179 , - 2.8484 , - 2.8152 , - 2.9654 , - 2.8846 , - 2.8642 , - 2.8602 , - 2.8860 , - 2.9505 , - 2.8806 , - 2.9436 , - 2.8388 , - 2.9312 , - 2.9241 , - 2.8211 ]], grad_fn =< LogSoftmaxBackward0 > ) gru output . shape ---> : torch . Size ([ 1 , 18 ]) tensor ([[ - 2.8898 , - 3.0236 , - 2.7403 , - 2.8986 , - 2.8163 , - 2.9486 , - 2.8674 , - 2.9294 , - 2.8889 , - 3.0082 , - 2.8785 , - 2.8741 , - 2.8736 , - 2.7923 , - 2.9261 , - 2.8990 , - 2.9456 , - 2.8668 ]], grad_fn =< LogSoftmaxBackward0 > )","title":"4 \u6a21\u578bRNN_LSTM_GRU\u6d4b\u8bd5"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#24","text":"","title":"2.4 \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#1-rnn_1","text":"# \u601d\u8def\u5206\u6790 # \u4ece\u6587\u4ef6\u83b7\u53d6\u6570\u636e\u3001\u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61nameclassdataset \u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61mydataloader # \u5b9e\u4f8b\u5316\u6a21\u578b\u5bf9\u8c61my_rnn \u635f\u5931\u51fd\u6570\u5bf9\u8c61mycrossentropyloss=nn.NLLLoss() \u4f18\u5316\u5668\u5bf9\u8c61myadam # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 # starttime total_iter_num total_loss total_loss_list total_acc_num total_acc_list # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range(epochs) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i, (x, y) in enumerate(mydataloader) # \u7ed9\u6a21\u578b\u5582\u6570\u636e # \u8ba1\u7b97\u635f\u5931 # \u68af\u5ea6\u6e05\u96f6 # \u53cd\u5411\u4f20\u64ad # \u68af\u5ea6\u66f4\u65b0 # \u8ba1\u7b97\u8f85\u52a9\u4fe1\u606f # \u7d2f\u52a0\u603b\u635f\u5931\u548c\u51c6\u786e\u6570 \u6bcf100\u6b21\u8bad\u7ec3\u8ba1\u7b97\u4e00\u4e2a\u603b\u4f53\u5e73\u5747\u635f\u5931 \u603b\u4f53\u5e73\u5747\u51c6\u786e\u7387 \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 # \u5176\u4ed6 # \u9884\u6d4b\u5bf9\u9519 i_predit_tag = (1 if torch.argmax(output).item() == y.item() else 0) # \u6a21\u578b\u4fdd\u5b58 # torch.save(my_rnn.state_dict(), './my_rnn_model_%d.bin' % (epoch_idx + 1)) # \u8fd4\u56de \u5e73\u5747\u635f\u5931\u5217\u8868total_loss_list, \u65f6\u95f4total_time, \u5e73\u5747\u51c6\u786etotal_acc_list # \u6a21\u578b\u8bad\u7ec3\u53c2\u6570 mylr = 1e-3 epochs = 1 def my_train_rnn (): # \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # \u5b9e\u4f8b\u5316 \u6a21\u578b input_size = 57 n_hidden = 128 output_size = 18 my_rnn = RNN ( input_size , n_hidden , output_size ) print ( 'my_rnn\u6a21\u578b--->' , my_rnn ) # \u5b9e\u4f8b\u5316 \u635f\u5931\u51fd\u6570 adam\u4f18\u5316\u5668 mycrossentropyloss = nn . NLLLoss () myadam = optim . Adam ( my_rnn . parameters (), lr = mylr ) # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u53c2\u6570 starttime = time . time () total_iter_num = 0 # \u5df2\u8bad\u7ec3\u7684\u6837\u672c\u6570 total_loss = 0.0 # \u5df2\u8bad\u7ec3\u7684\u635f\u5931\u548c total_loss_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5f62\u6210\u635f\u5931\u5217\u8868 total_acc_num = 0 # \u5df2\u8bad\u7ec3\u6837\u672c\u9884\u6d4b\u51c6\u786e\u603b\u6570 total_acc_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u51c6\u786e\u7387 \u5f62\u6210\u5e73\u5747\u51c6\u786e\u7387\u5217\u8868 # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( x , y ) in enumerate ( mydataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e output , hidden = my_rnn ( x [ 0 ], my_rnn . inithidden ()) # \u8ba1\u7b97\u635f\u5931 myloss = mycrossentropyloss ( output , y ) # \u68af\u5ea6\u6e05\u96f6 myadam . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam . step () # \u8ba1\u7b97\u603b\u635f\u5931 total_iter_num = total_iter_num + 1 total_loss = total_loss + myloss . item () # \u8ba1\u7b97\u603b\u51c6\u786e\u7387 i_predit_tag = ( 1 if torch . argmax ( output ) . item () == y . item () else 0 ) total_acc_num = total_acc_num + i_predit_tag # \u6bcf100\u6b21\u8bad\u7ec3 \u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5e73\u5747\u51c6\u786e\u7387 if ( total_iter_num % 100 == 0 ): tmploss = total_loss / total_iter_num total_loss_list . append ( tmploss ) tmpacc = total_acc_num / total_iter_num total_acc_list . append ( tmpacc ) # \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 if ( total_iter_num % 2000 == 0 ): tmploss = total_loss / total_iter_num print ( '\u8f6e\u6b21: %d , \u635f\u5931: %.6f , \u65f6\u95f4: %d \uff0c\u51c6\u786e\u7387: %.3f ' % ( epoch_idx + 1 , tmploss , time . time () - starttime , tmpacc )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_rnn . state_dict (), './my_rnn_model_ %d .bin' % ( epoch_idx + 1 )) # \u8ba1\u7b97\u603b\u65f6\u95f4 total_time = int ( time . time () - starttime ) return total_loss_list , total_time , total_acc_list","title":"1 \u6784\u5efaRNN\u8bad\u7ec3\u51fd\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#2-lstm_1","text":"# \u601d\u8def\u5206\u6790 # \u540cRNN\u5b9e\u73b0\u5206\u6790 def my_train_lstm (): # \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # \u5b9e\u4f8b\u5316 \u6a21\u578b input_size = 57 n_hidden = 128 output_size = 18 my_lstm = LSTM ( input_size , n_hidden , output_size ) print ( 'my_lstm\u6a21\u578b--->' , my_lstm ) # \u5b9e\u4f8b\u5316 \u635f\u5931\u51fd\u6570 adam\u4f18\u5316\u5668 mycrossentropyloss = nn . NLLLoss () myadam = optim . Adam ( my_lstm . parameters (), lr = mylr ) # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u53c2\u6570 starttime = time . time () total_iter_num = 0 # \u5df2\u8bad\u7ec3\u7684\u6837\u672c\u6570 total_loss = 0.0 # \u5df2\u8bad\u7ec3\u7684\u635f\u5931\u548c total_loss_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5f62\u6210\u635f\u5931\u5217\u8868 total_acc_num = 0 # \u5df2\u8bad\u7ec3\u6837\u672c\u9884\u6d4b\u51c6\u786e\u603b\u6570 total_acc_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u51c6\u786e\u7387 \u5f62\u6210\u5e73\u5747\u51c6\u786e\u7387\u5217\u8868 # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( x , y ) in enumerate ( mydataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e hidden , c = my_lstm . inithidden () output , hidden , c = my_lstm ( x [ 0 ], hidden , c ) # \u8ba1\u7b97\u635f\u5931 myloss = mycrossentropyloss ( output , y ) # \u68af\u5ea6\u6e05\u96f6 myadam . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam . step () # \u8ba1\u7b97\u603b\u635f\u5931 total_iter_num = total_iter_num + 1 total_loss = total_loss + myloss . item () # \u8ba1\u7b97\u603b\u51c6\u786e\u7387 i_predit_tag = ( 1 if torch . argmax ( output ) . item () == y . item () else 0 ) total_acc_num = total_acc_num + i_predit_tag # \u6bcf100\u6b21\u8bad\u7ec3 \u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5e73\u5747\u51c6\u786e\u7387 if ( total_iter_num % 100 == 0 ): tmploss = total_loss / total_iter_num total_loss_list . append ( tmploss ) tmpacc = total_acc_num / total_iter_num total_acc_list . append ( tmpacc ) # \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 if ( total_iter_num % 2000 == 0 ): tmploss = total_loss / total_iter_num print ( '\u8f6e\u6b21: %d , \u635f\u5931: %.6f , \u65f6\u95f4: %d \uff0c\u51c6\u786e\u7387: %.3f ' % ( epoch_idx + 1 , tmploss , time . time () - starttime , tmpacc )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_lstm . state_dict (), './my_lstm_model_ %d .bin' % ( epoch_idx + 1 )) # \u8ba1\u7b97\u603b\u65f6\u95f4 total_time = int ( time . time () - starttime ) return total_loss_list , total_time , total_acc_list","title":"2 \u6784\u5efaLSTM\u8bad\u7ec3\u51fd\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#3-gru_1","text":"# \u601d\u8def\u5206\u6790 # \u540cRNN\u5b9e\u73b0\u5206\u6790 def my_train_gru (): # \u83b7\u53d6\u6570\u636e myfilename = './data/name_classfication.txt' my_list_x , my_list_y = read_data ( myfilename ) # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 nameclassdataset = NameClassDataset ( my_list_x , my_list_y ) # \u5b9e\u4f8b\u5316 \u6a21\u578b input_size = 57 n_hidden = 128 output_size = 18 my_gru = GRU ( input_size , n_hidden , output_size ) print ( 'my_gru\u6a21\u578b--->' , my_gru ) # \u5b9e\u4f8b\u5316 \u635f\u5931\u51fd\u6570 adam\u4f18\u5316\u5668 mycrossentropyloss = nn . NLLLoss () myadam = optim . Adam ( my_gru . parameters (), lr = mylr ) # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u53c2\u6570 starttime = time . time () total_iter_num = 0 # \u5df2\u8bad\u7ec3\u7684\u6837\u672c\u6570 total_loss = 0.0 # \u5df2\u8bad\u7ec3\u7684\u635f\u5931\u548c total_loss_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5f62\u6210\u635f\u5931\u5217\u8868 total_acc_num = 0 # \u5df2\u8bad\u7ec3\u6837\u672c\u9884\u6d4b\u51c6\u786e\u603b\u6570 total_acc_list = [] # \u6bcf100\u4e2a\u6837\u672c\u6c42\u4e00\u6b21\u5e73\u5747\u51c6\u786e\u7387 \u5f62\u6210\u5e73\u5747\u51c6\u786e\u7387\u5217\u8868 # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = nameclassdataset , batch_size = 1 , shuffle = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( x , y ) in enumerate ( mydataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e output , hidden = my_gru ( x [ 0 ], my_gru . inithidden ()) # \u8ba1\u7b97\u635f\u5931 myloss = mycrossentropyloss ( output , y ) # \u68af\u5ea6\u6e05\u96f6 myadam . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam . step () # \u8ba1\u7b97\u603b\u635f\u5931 total_iter_num = total_iter_num + 1 total_loss = total_loss + myloss . item () # \u8ba1\u7b97\u603b\u51c6\u786e\u7387 i_predit_tag = ( 1 if torch . argmax ( output ) . item () == y . item () else 0 ) total_acc_num = total_acc_num + i_predit_tag # \u6bcf100\u6b21\u8bad\u7ec3 \u6c42\u4e00\u6b21\u5e73\u5747\u635f\u5931 \u5e73\u5747\u51c6\u786e\u7387 if ( total_iter_num % 100 == 0 ): tmploss = total_loss / total_iter_num total_loss_list . append ( tmploss ) tmpacc = total_acc_num / total_iter_num total_acc_list . append ( tmpacc ) # \u6bcf2000\u6b21\u8bad\u7ec3 \u6253\u5370\u65e5\u5fd7 if ( total_iter_num % 2000 == 0 ): tmploss = total_loss / total_iter_num print ( '\u8f6e\u6b21: %d , \u635f\u5931: %.6f , \u65f6\u95f4: %d \uff0c\u51c6\u786e\u7387: %.3f ' % ( epoch_idx + 1 , tmploss , time . time () - starttime , tmpacc )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_gru . state_dict (), './my_gru_model_ %d .bin' % ( epoch_idx + 1 )) # \u8ba1\u7b97\u603b\u65f6\u95f4 total_time = int ( time . time () - starttime ) return total_loss_list , total_time , total_acc_list","title":"3 \u6784\u5efaGRU\u8bad\u7ec3\u51fd\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#4","text":"def dm_test_train_rnn_lstm_gru (): total_loss_list_rnn , total_time_rnn , total_acc_list_rnn = my_train_rnn () total_loss_list_lstm , total_time_lstm , total_acc_list_lstm = my_train_lstm () total_loss_list_gru , total_time_gru , total_acc_list_gru = my_train_gru () # \u7ed8\u5236\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf # \u521b\u5efa\u753b\u5e030 plt . figure ( 0 ) # # \u7ed8\u5236\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf plt . plot ( total_loss_list_rnn , label = \"RNN\" ) plt . plot ( total_loss_list_lstm , color = \"red\" , label = \"LSTM\" ) plt . plot ( total_loss_list_gru , color = \"orange\" , label = \"GRU\" ) plt . legend ( loc = 'upper left' ) plt . savefig ( './img/RNN_LSTM_GRU_loss2.png' ) plt . show () # \u7ed8\u5236\u67f1\u72b6\u56fe # \u521b\u5efa\u753b\u5e031 plt . figure ( 1 ) x_data = [ \"RNN\" , \"LSTM\" , \"GRU\" ] y_data = [ total_time_rnn , total_time_lstm , total_time_gru ] # \u7ed8\u5236\u8bad\u7ec3\u8017\u65f6\u5bf9\u6bd4\u67f1\u72b6\u56fe plt . bar ( range ( len ( x_data )), y_data , tick_label = x_data ) plt . savefig ( './img/RNN_LSTM_GRU_period2.png' ) plt . show () # \u7ed8\u5236\u51c6\u786e\u7387\u5bf9\u6bd4\u66f2\u7ebf plt . figure ( 2 ) plt . plot ( total_acc_list_rnn , label = \"RNN\" ) plt . plot ( total_acc_list_lstm , color = \"red\" , label = \"LSTM\" ) plt . plot ( total_acc_list_gru , color = \"orange\" , label = \"GRU\" ) plt . legend ( loc = 'upper left' ) plt . savefig ( './img/RNN_LSTM_GRU_acc2.png' ) plt . show () RNN\u6a21\u578b\u8bad\u7ec3\u65e5\u5fd7\u8f93\u51fa: \u8f6e\u6b21 : 3 , \u635f\u5931 : 1.002102 , \u65f6\u95f4 : 54 \uff0c \u51c6\u786e\u7387 : 0.700 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.993880 , \u65f6\u95f4 : 56 \uff0c \u51c6\u786e\u7387 : 0.703 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.986200 , \u65f6\u95f4 : 58 \uff0c \u51c6\u786e\u7387 : 0.705 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.981136 , \u65f6\u95f4 : 61 \uff0c \u51c6\u786e\u7387 : 0.706 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.976931 , \u65f6\u95f4 : 63 \uff0c \u51c6\u786e\u7387 : 0.707 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.972190 , \u65f6\u95f4 : 65 \uff0c \u51c6\u786e\u7387 : 0.708 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.967081 , \u65f6\u95f4 : 68 \uff0c \u51c6\u786e\u7387 : 0.710 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.964384 , \u65f6\u95f4 : 70 \uff0c \u51c6\u786e\u7387 : 0.711 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.958782 , \u65f6\u95f4 : 72 \uff0c \u51c6\u786e\u7387 : 0.713 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.955343 , \u65f6\u95f4 : 75 \uff0c \u51c6\u786e\u7387 : 0.713 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.950741 , \u65f6\u95f4 : 77 \uff0c \u51c6\u786e\u7387 : 0.715 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.945756 , \u65f6\u95f4 : 80 \uff0c \u51c6\u786e\u7387 : 0.716 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.942663 , \u65f6\u95f4 : 82 \uff0c \u51c6\u786e\u7387 : 0.717 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.939319 , \u65f6\u95f4 : 84 \uff0c \u51c6\u786e\u7387 : 0.718 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.936169 , \u65f6\u95f4 : 87 \uff0c \u51c6\u786e\u7387 : 0.719 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.933440 , \u65f6\u95f4 : 89 \uff0c \u51c6\u786e\u7387 : 0.720 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.930918 , \u65f6\u95f4 : 91 \uff0c \u51c6\u786e\u7387 : 0.720 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.927330 , \u65f6\u95f4 : 94 \uff0c \u51c6\u786e\u7387 : 0.721 LSTM\u6a21\u578b\u8bad\u7ec3\u65e5\u5fd7\u8f93\u51fa: \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.805885 , \u65f6\u95f4 : 118 \uff0c \u51c6\u786e\u7387 : 0.759 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.794148 , \u65f6\u95f4 : 123 \uff0c \u51c6\u786e\u7387 : 0.762 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.783356 , \u65f6\u95f4 : 128 \uff0c \u51c6\u786e\u7387 : 0.765 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.774931 , \u65f6\u95f4 : 133 \uff0c \u51c6\u786e\u7387 : 0.767 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.765427 , \u65f6\u95f4 : 137 \uff0c \u51c6\u786e\u7387 : 0.769 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.757254 , \u65f6\u95f4 : 142 \uff0c \u51c6\u786e\u7387 : 0.771 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.750375 , \u65f6\u95f4 : 147 \uff0c \u51c6\u786e\u7387 : 0.773 \u8f6e\u6b21 : 3 , \u635f\u5931 : 0.743092 , \u65f6\u95f4 : 152 \uff0c \u51c6\u786e\u7387 : 0.775 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.732983 , \u65f6\u95f4 : 157 \uff0c \u51c6\u786e\u7387 : 0.778 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.723816 , \u65f6\u95f4 : 162 \uff0c \u51c6\u786e\u7387 : 0.780 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.716507 , \u65f6\u95f4 : 167 \uff0c \u51c6\u786e\u7387 : 0.782 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.708377 , \u65f6\u95f4 : 172 \uff0c \u51c6\u786e\u7387 : 0.785 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.700820 , \u65f6\u95f4 : 177 \uff0c \u51c6\u786e\u7387 : 0.787 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.694714 , \u65f6\u95f4 : 182 \uff0c \u51c6\u786e\u7387 : 0.788 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.688386 , \u65f6\u95f4 : 187 \uff0c \u51c6\u786e\u7387 : 0.790 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.683056 , \u65f6\u95f4 : 191 \uff0c \u51c6\u786e\u7387 : 0.791 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.677051 , \u65f6\u95f4 : 196 \uff0c \u51c6\u786e\u7387 : 0.793 \u8f6e\u6b21 : 4 , \u635f\u5931 : 0.671668 , \u65f6\u95f4 : 201 \uff0c \u51c6\u786e\u7387 : 0.794 GRU\u6a21\u578b\u8bad\u7ec3\u65e5\u5fd7\u8f93\u51fa: \u8f6e\u6b21:3, \u635f\u5931:0.743891, \u65f6\u95f4:106\uff0c\u51c6\u786e\u7387:0.772 \u8f6e\u6b21:3, \u635f\u5931:0.733144, \u65f6\u95f4:111\uff0c\u51c6\u786e\u7387:0.775 \u8f6e\u6b21:3, \u635f\u5931:0.723484, \u65f6\u95f4:116\uff0c\u51c6\u786e\u7387:0.777 \u8f6e\u6b21:3, \u635f\u5931:0.714760, \u65f6\u95f4:120\uff0c\u51c6\u786e\u7387:0.780 \u8f6e\u6b21:3, \u635f\u5931:0.706929, \u65f6\u95f4:125\uff0c\u51c6\u786e\u7387:0.782 \u8f6e\u6b21:3, \u635f\u5931:0.698657, \u65f6\u95f4:130\uff0c\u51c6\u786e\u7387:0.784 \u8f6e\u6b21:3, \u635f\u5931:0.690443, \u65f6\u95f4:134\uff0c\u51c6\u786e\u7387:0.787 \u8f6e\u6b21:3, \u635f\u5931:0.683878, \u65f6\u95f4:139\uff0c\u51c6\u786e\u7387:0.789 \u8f6e\u6b21:4, \u635f\u5931:0.674766, \u65f6\u95f4:144\uff0c\u51c6\u786e\u7387:0.791 \u8f6e\u6b21:4, \u635f\u5931:0.665543, \u65f6\u95f4:148\uff0c\u51c6\u786e\u7387:0.794 \u8f6e\u6b21:4, \u635f\u5931:0.657179, \u65f6\u95f4:153\uff0c\u51c6\u786e\u7387:0.796 \u8f6e\u6b21:4, \u635f\u5931:0.650314, \u65f6\u95f4:157\uff0c\u51c6\u786e\u7387:0.798 \u8f6e\u6b21:4, \u635f\u5931:0.643698, \u65f6\u95f4:162\uff0c\u51c6\u786e\u7387:0.800 \u8f6e\u6b21:4, \u635f\u5931:0.637341, \u65f6\u95f4:167\uff0c\u51c6\u786e\u7387:0.802 \u8f6e\u6b21:4, \u635f\u5931:0.632063, \u65f6\u95f4:171\uff0c\u51c6\u786e\u7387:0.803 \u8f6e\u6b21:4, \u635f\u5931:0.626060, \u65f6\u95f4:176\uff0c\u51c6\u786e\u7387:0.805 \u8f6e\u6b21:4, \u635f\u5931:0.621460, \u65f6\u95f4:180\uff0c\u51c6\u786e\u7387:0.806 \u8f6e\u6b21:4, \u635f\u5931:0.616704, \u65f6\u95f4:185\uff0c\u51c6\u786e\u7387:0.808","title":"4 \u6a21\u578b\u8bad\u7ec3\u5e76\u5236\u56fe"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#5_1","text":"","title":"5 \u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u5206\u6790"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#1_2","text":"\u5de6\u56fe\uff1a1\u4e2a\u8f6e\u6b21\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf\uff0c\u53f3\u56fe4\u4e2a\u8f6e\u6b21\u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf \u6a21\u578b\u8bad\u7ec3\u7684\u635f\u5931\u964d\u4f4e\u5feb\u6162\u4ee3\u8868\u6a21\u578b\u6536\u655b\u7a0b\u5ea6\u3002\u7531\u56fe\u53ef\u77e5, \u4f20\u7edfRNN\u7684\u6a21\u578b\u7b2c\u4e00\u4e2a\u8f6e\u6b21\u5f00\u59cb\u6536\u655b\u60c5\u51b5\u6700\u597d\uff0c\u7136\u540e\u662fGRU, \u6700\u540e\u662fLSTM, \u8fd9\u662f\u56e0\u4e3aRNN\u6a21\u578b\u7b80\u5355\u53c2\u6570\u5c11\uff0c\u89c1\u6548\u5feb\u3002\u968f\u7740\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\uff0cGRU\u6548\u679c\u6700\u597d\u3001LSTM\u6548\u679c\u6b21\u4e4b\u3001RNN\u6548\u679c\u6392\u6700\u540e\u3002 \u6240\u4ee5\u5728\u4ee5\u540e\u7684\u6a21\u578b\u9009\u7528\u65f6\uff0c \u8981\u901a\u8fc7\u5bf9\u4efb\u52a1\u7684\u5206\u6790\u4ee5\u53ca\u5b9e\u9a8c\u5bf9\u6bd4, \u9009\u62e9\u6700\u9002\u5408\u7684\u6a21\u578b\u3002","title":"1 \u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf\u5206\u6790"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#2_2","text":"\u8bad\u7ec3\u8017\u65f6\u5bf9\u6bd4\u56fe: \u6a21\u578b\u8bad\u7ec3\u7684\u8017\u65f6\u957f\u77ed\u4ee3\u8868\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7531\u56fe\u53ef\u77e5\uff0c \u4e5f\u6b63\u5982\u6211\u4eec\u4e4b\u524d\u7684\u7406\u8bba\u5206\u6790\uff0c\u4f20\u7edfRNN\u590d\u6742\u5ea6\u6700\u4f4e\uff0c \u8017\u65f6\u51e0\u4e4e\u53ea\u662f\u540e\u4e24\u8005\u7684\u4e00\u534a, \u7136\u540e\u662fGRU\uff0c\u6700\u540e\u662f\u590d\u6742\u5ea6\u6700\u9ad8\u7684LSTM\u3002","title":"2 \u8bad\u7ec3\u8017\u65f6\u5206\u6790"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#3_1","text":"\u8bad\u7ec3\u51c6\u786e\u7387\u5bf9\u6bd4\u56fe: \u7531\u56fe\u53ef\u77e5\uff0c GRU\u6548\u679c\u6700\u597d\u3001LSTM\u6548\u679c\u6b21\u4e4b\u3001RNN\u6548\u679c\u6392\u6700\u540e\u3002","title":"3 \u8bad\u7ec3\u51c6\u786e\u7387\u5206\u6790"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#4_1","text":"\u6a21\u578b\u9009\u7528\u4e00\u822c\u5e94\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u5e76\u975e\u8d8a\u590d\u6742\u6216\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u8868\u73b0\u8d8a\u597d\uff0c\u800c\u662f\u9700\u8981\u7ed3\u5408\u81ea\u5df1\u7684\u7279\u5b9a\u4efb\u52a1\uff0c\u4ece\u5bf9\u6570\u636e\u7684\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e2d\u83b7\u5f97\u6700\u4f73\u7b54\u6848\u3002","title":"4 \u7ed3\u8bba"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#25","text":"","title":"2.5 \u6784\u5efa\u9884\u6d4b\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#1-rnn_2","text":"# 1 \u6784\u5efa\u4f20\u7edfRNN\u9884\u6d4b\u51fd\u6570 my_path_rnn = './model/my_rnn_model_1.bin' my_path_lstm = './model/my_lstm_model_1.bin' my_path_gru = './model/my_gru_model_1.bin' # \u5c06\u4eba\u540d\u8f6c\u5316\u4e3aonehot\u5f20\u91cf # eg 'bai' --> [3,57] def lineToTensor ( x ): # \u6587\u672c\u5f20\u91cf\u5316x tensor_x = torch . zeros ( len ( x ), n_letters ) # \u904d\u5386\u8fd9\u4e2a\u4eba\u540d\u4e2d\u7684\u6bcf\u4e2a\u5b57\u7b26\u7d22\u5f15\u548c\u5b57\u7b26 for li , letter in enumerate ( x ): # letter\u5728\u5b57\u7b26\u4e32all_letters\u4e2d\u7684\u4f4d\u7f6e \u5c31\u662fonehot\u5f20\u91cf1\u7d22\u5f15\u7684\u4f4d\u7f6e # letter\u5728\u5b57\u7b26\u4e32all_letters\u4e2d\u7684\u4f4d\u7f6e \u4f7f\u7528\u5b57\u7b26\u4e32find()\u65b9\u6cd5\u83b7\u53d6 tensor_x [ li ][ all_letters . find ( letter )] = 1 return tensor_x # \u601d\u8def\u5206\u6790 # 1 \u8f93\u5165\u6587\u672c\u6570\u636e \u5f20\u91cf\u5316one-hot # 2 \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 m.load_state_dict(torch.load(my_path_rnn)) # 3 \u6a21\u578b\u9884\u6d4b with torch.no_grad() # 4 \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d,\u663e\u793a\u6253\u5370\u7ed3\u679c output.topk(3, 1, True) # category_idx = topi[0][i] category = categorys[category_idx] # \u6784\u5efarnn\u9884\u6d4b\u51fd\u6570 def my_predict_rnn ( x ): n_letters = 57 n_hidden = 128 n_categories = 18 # \u8f93\u5165\u6587\u672c, \u5f20\u91cf\u5316one-hot x_tensor = lineToTensor ( x ) # \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 my_rnn = RNN ( n_letters , n_hidden , n_categories ) my_rnn . load_state_dict ( torch . load ( my_path_rnn )) with torch . no_grad (): # \u6a21\u578b\u9884\u6d4b output , hidden = my_rnn ( x_tensor , my_rnn . inithidden ()) # \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d # 3\u8868\u793a\u53d6\u524d3\u540d, 1\u8868\u793a\u8981\u6392\u5e8f\u7684\u7ef4\u5ea6, True\u8868\u793a\u662f\u5426\u8fd4\u56de\u6700\u5927\u6216\u662f\u6700\u4e0b\u7684\u5143\u7d20 topv , topi = output . topk ( 3 , 1 , True ) print ( 'rnn =>' , x ) for i in range ( 3 ): value = topv [ 0 ][ i ] category_idx = topi [ 0 ][ i ] category = categorys [ category_idx ] print ( ' \\t value: %d category: %s ' % ( value , category ))","title":"1 \u6784\u5efaRNN\u9884\u6d4b\u51fd\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#2-lstm_2","text":"# \u6784\u5efaLSTM \u9884\u6d4b\u51fd\u6570 def my_predict_lstm ( x ): n_letters = 57 n_hidden = 128 n_categories = 18 # \u8f93\u5165\u6587\u672c, \u5f20\u91cf\u5316one-hot x_tensor = lineToTensor ( x ) # \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 my_lstm = LSTM ( n_letters , n_hidden , n_categories ) my_lstm . load_state_dict ( torch . load ( my_path_lstm )) with torch . no_grad (): # \u6a21\u578b\u9884\u6d4b hidden , c = my_lstm . inithidden () output , hidden , c = my_lstm ( x_tensor , hidden , c ) # \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d # 3\u8868\u793a\u53d6\u524d3\u540d, 1\u8868\u793a\u8981\u6392\u5e8f\u7684\u7ef4\u5ea6, True\u8868\u793a\u662f\u5426\u8fd4\u56de\u6700\u5927\u6216\u662f\u6700\u4e0b\u7684\u5143\u7d20 topv , topi = output . topk ( 3 , 1 , True ) print ( 'rnn =>' , x ) for i in range ( 3 ): value = topv [ 0 ][ i ] category_idx = topi [ 0 ][ i ] category = categorys [ category_idx ] print ( ' \\t value: %d category: %s ' % ( value , category )) print ( ' \\t value: %d category: %s ' % ( value , category ))","title":"2 \u6784\u5efaLSTM\u9884\u6d4b\u51fd\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#3-gru_2","text":"# \u6784\u5efaGRU \u9884\u6d4b\u51fd\u6570 def my_predict_gru ( x ): n_letters = 57 n_hidden = 128 n_categories = 18 # \u8f93\u5165\u6587\u672c, \u5f20\u91cf\u5316one-hot x_tensor = lineToTensor ( x ) # \u5b9e\u4f8b\u5316\u6a21\u578b \u52a0\u8f7d\u5df2\u8bad\u7ec3\u6a21\u578b\u53c2\u6570 my_gru = GRU ( n_letters , n_hidden , n_categories ) my_gru . load_state_dict ( torch . load ( my_path_gru )) with torch . no_grad (): # \u6a21\u578b\u9884\u6d4b output , hidden = my_gru ( x_tensor , my_gru . inithidden ()) # \u4ece\u9884\u6d4b\u7ed3\u679c\u4e2d\u53d6\u51fa\u524d3\u540d # 3\u8868\u793a\u53d6\u524d3\u540d, 1\u8868\u793a\u8981\u6392\u5e8f\u7684\u7ef4\u5ea6, True\u8868\u793a\u662f\u5426\u8fd4\u56de\u6700\u5927\u6216\u662f\u6700\u4e0b\u7684\u5143\u7d20 topv , topi = output . topk ( 3 , 1 , True ) print ( 'rnn =>' , x ) for i in range ( 3 ): value = topv [ 0 ][ i ] category_idx = topi [ 0 ][ i ] category = categorys [ category_idx ] print ( ' \\t value: %d category: %s ' % ( value , category ))","title":"3 \u6784\u5efaGRU\u9884\u6d4b\u51fd\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#4-rnn_lstm_gru_1","text":"def dm_test_predic_rnn_lstm_gru (): # \u628a\u4e09\u4e2a\u51fd\u6570\u7684\u5165\u53e3\u5730\u5740 \u7ec4\u6210\u5217\u8868\uff0c\u7edf\u4e00\u8f93\u5165\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5 for func in [ my_predict_rnn , my_predict_lstm , my_predict_gru ]: func ( 'zhang' ) \u8f93\u51fa\u6548\u679c rnn => zhang value:0 category:Russian value:0 category:Chinese value:-4 category:German rnn => zhang value:0 category:Chinese value:-1 category:Russian value:-1 category:German rnn => zhang value:0 category:Russian value:0 category:Chinese value:-2 category:Korean","title":"4 \u6784\u5efaRNN_LSTM_GRU\u9884\u6d4b\u8c03\u7528\u51fd\u6570"},{"location":"03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html#3_2","text":"\u5b66\u4e60\u4e86\u5173\u4e8e\u4eba\u540d\u5206\u7c7b\u95ee\u9898: \u4ee5\u4e00\u4e2a\u4eba\u540d\u4e3a\u8f93\u5165, \u4f7f\u7528\u6a21\u578b\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u5b83\u6700\u6709\u53ef\u80fd\u662f\u6765\u81ea\u54ea\u4e00\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d, \u8fd9\u5728\u67d0\u4e9b\u56fd\u9645\u5316\u516c\u53f8\u7684\u4e1a\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49, \u5728\u7528\u6237\u6ce8\u518c\u8fc7\u7a0b\u4e2d, \u4f1a\u6839\u636e\u7528\u6237\u586b\u5199\u7684\u540d\u5b57\u76f4\u63a5\u7ed9\u4ed6\u5206\u914d\u53ef\u80fd\u7684\u56fd\u5bb6\u6216\u5730\u533a\u9009\u9879, \u4ee5\u53ca\u8be5\u56fd\u5bb6\u6216\u5730\u533a\u7684\u56fd\u65d7, \u9650\u5236\u624b\u673a\u53f7\u7801\u4f4d\u6570\u7b49\u7b49. \u4eba\u540d\u5206\u7c7b\u5668\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65: \u6784\u5efaRNN\u6a21\u578b(\u5305\u62ec\u4f20\u7edfRNN, LSTM\u4ee5\u53caGRU) \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65: \u6784\u5efa\u8bc4\u4f30\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 python\u7248\u672c\u4f7f\u75283.7.x, pytorch\u7248\u672c\u4f7f\u75281.6.1 \u7b2c\u4e8c\u6b65: \u5bf9data\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u6ee1\u8db3\u8bad\u7ec3\u8981\u6c42 \u8bfb\u539f\u59cb\u6570\u636e\u5230\u5185\u5b58\uff0c\u6784\u5efa\u51fa\u6a21\u578b\u9700\u8981\u7684\u6570\u636ex\uff0c\u6807\u7b7ey\uff0c\u7136\u540e\u628a\u6570\u636e\u8f6c\u6210\u6570\u636e\u6e90\uff0c\u6700\u540e\u518d\u5c01\u88c5\u6210\u6570\u636e\u8fed\u4ee3\u5668 \u4ece\u7f16\u7a0b\u5b9e\u73b0\u6765\u770b\uff0c\u6587\u672c\u6570\u503c\u5316\uff0c\u6570\u503c\u5f20\u91cf\u5316\u662f\u901a\u8fc7one-hot\u7f16\u7801\u4e00\u6b65\u5b8c\u6210\u7684 \u7b2c\u4e09\u6b65: \u6784\u5efaRNN\u6a21\u578b \u6784\u5efa\u4f20\u7edf\u7684RNN\u6a21\u578b\u7684\u7c7bclass RNN. \u6784\u5efaLSTM\u6a21\u578b\u7684\u7c7bclass LSTM. \u6784\u5efaGRU\u6a21\u578b\u7684\u7c7bclass GRU. \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u5e76\u8fdb\u884c\u8bad\u7ec3 \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61 \u5b9e\u4f8b\u5316\u6a21\u578b\u5bf9\u8c61\u3001\u635f\u5931\u51fd\u6570\u5bf9\u8c61\u3001\u4f18\u5316\u5668\u5bf9\u8c61 \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 \u8bad\u7ec3\u6a21\u578b \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570\uff0c\u7ed9\u6a21\u578b\u5582\u6570\u636e\uff0c\u8ba1\u7b97\u635f\u5931 \uff0c\u68af\u5ea6\u6e05\u96f6 \uff0c\u53cd\u5411\u4f20\u64ad \uff0c \u68af\u5ea6\u66f4\u65b0\uff0c\u6253\u5370\u65e5\u5fd7 \u6a21\u578b\u4fdd\u5b58 \u635f\u5931\u5bf9\u6bd4\u66f2\u7ebf\u5206\u6790: \u4f20\u7edfRNN\u7684\u6a21\u578b\u7b2c\u4e00\u4e2a\u8f6e\u6b21\u5f00\u59cb\u6536\u655b\u60c5\u51b5\u6700\u597d\uff0c\u7136\u540e\u662fGRU, \u6700\u540e\u662fLSTM, \u8fd9\u662f\u56e0\u4e3aRNN\u6a21\u578b\u7b80\u5355\u53c2\u6570\u5c11\uff0c\u89c1\u6548\u5feb\u3002 \u968f\u7740\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\uff0cGRU\u6548\u679c\u6700\u597d\u3001LSTM\u6548\u679c\u6b21\u4e4b\u3001RNN\u6548\u679c\u6392\u6700\u540e \u8bad\u7ec3\u8017\u65f6\u5bf9\u6bd4\u56fe\u5206\u6790: \u6a21\u578b\u8bad\u7ec3\u7684\u8017\u65f6\u957f\u77ed\u4ee3\u8868\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7531\u56fe\u53ef\u77e5\uff0c\u4e5f\u6b63\u5982\u6211\u4eec\u4e4b\u524d\u7684\u7406\u8bba\u5206\u6790\uff0c\u4f20\u7edfRNN\u590d\u6742\u5ea6\u6700\u4f4e\uff0c\u8017\u65f6\u51e0\u4e4e\u53ea\u662f\u540e\u4e24\u8005\u7684\u4e00\u534a\uff0c\u7136\u540e\u662fGRU\uff0c\u6700\u540e\u662f\u590d\u6742\u5ea6\u6700\u9ad8\u7684LSTM \u7ed3\u8bba: \u6a21\u578b\u9009\u7528\u4e00\u822c\u5e94\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\uff0c \u5e76\u975e\u8d8a\u590d\u6742\u6216\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u8868\u73b0\u8d8a\u597d\uff0c \u800c\u662f\u9700\u8981\u7ed3\u5408\u81ea\u5df1\u7684\u7279\u5b9a\u4efb\u52a1\uff0c\u4ece\u5bf9\u6570\u636e\u7684\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e2d\u83b7\u5f97\u6700\u4f73\u7b54\u6848 \u7b2c\u4e94\u6b65: \u6784\u5efa\u9884\u6d4b\u51fd\u6570\u5e76\u8fdb\u884c\u9884\u6d4b \u6784\u5efa\u4f20\u7edfRNN\u9884\u6d4b\u51fd\u6570 \u6784\u5efaLSTM\u9884\u6d4b\u51fd\u6570 \u6784\u5efaGRU\u9884\u6d4b\u51fd\u6570 \u6784\u5efa\u9884\u6d4b\u51fd\u6570\u8c03\u7528\u51fd\u6570","title":"3 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236\u7684\u7531\u6765 \u7406\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236 \u4e86\u89e3\u5e38\u89c1\u7684\u6ce8\u610f\u529b\u7c7b\u578b\u4ee5\u53ca\u4f5c\u4e1a 1. \u6ce8\u610f\u529b\u673a\u5236\u7684\u7531\u6765\uff0c\u89e3\u51b3\u4e86\u4ec0\u4e48\u95ee\u9898\uff1f \u00b6 \u5728\u8ba4\u8bc6\u6ce8\u610f\u529b\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u7b80\u5355\u4e86\u89e3\u4e0b\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff1a \u4f8b\u5b50\uff1aseq2seq(Sequence to Sequence))\u67b6\u6784\u7ffb\u8bd1\u4efb\u52a1 seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002 \u56fe\u4e2d\u8868\u793a\u7684\u662f\u4e00\u4e2a\u4e2d\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1\uff1a\u6b22\u8fce \u6765 \u5317\u4eac \u2192 welcome to BeiJing\u3002\u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"\u6b22\u8fce \u6765 \u5317\u4eac\"\uff0c\u901a\u8fc7GRU\u6a21\u578b\u83b7\u5f97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6700\u540e\u5c06\u5b83\u4eec\u62fc\u63a5\u6210\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\uff1b\u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00 \u65e9\u671f\u5728\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u8fd9\u4e00\u7c7bseq2seq\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u7684\u505a\u6cd5\u662f\u5229\u7528\u4e00\u4e2a\u7f16\u7801\u5668(Encoder)\u548c\u4e00\u4e2a\u89e3\u7801\u5668(Decoder)\u6784\u5efa\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f46\u662f\u57fa\u4e8e\u7f16\u7801\u89e3\u7801\u7684\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a \u95ee\u98981\uff1a\u5982\u679c\u7ffb\u8bd1\u7684\u53e5\u5b50\u5f88\u957f\u5f88\u590d\u6742\uff0c\u6bd4\u5982\u76f4\u63a5\u4e00\u7bc7\u6587\u7ae0\u8f93\u8fdb\u53bb\uff0c\u6a21\u578b\u7684\u8ba1\u7b97\u91cf\u5f88\u5927\uff0c\u5e76\u4e14\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0b\u964d\u4e25\u91cd\u3002 \u95ee\u98982\uff1a\u5728\u7ffb\u8bd1\u65f6\uff0c\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b\uff0c\u540c\u4e00\u4e2a\u8bcd\u5177\u6709\u4e0d\u540c\u7684\u542b\u4e49\uff0c\u4f46\u662f\u7f51\u7edc\u5bf9\u8fd9\u4e9b\u8bcd\u5411\u91cf\u5e76\u6ca1\u6709\u533a\u5206\u5ea6\uff0c\u6ca1\u6709\u8003\u8651\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u6548\u679c\u6bd4\u8f83\u5dee\u3002 \u9488\u5bf9\u8fd9\u6837\u7684\u95ee\u9898\uff0c\u6ce8\u610f\u529b\u673a\u5236\u88ab\u63d0\u51fa\u3002 2. \u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236 \u00b6 \u6ce8\u610f\u529b\u673a\u5236\u65e9\u5728\u4e0a\u4e16\u7eaa\u4e5d\u5341\u5e74\u4ee3\u5c31\u6709\u7814\u7a76\uff0c\u6700\u65e9\u6ce8\u610f\u529b\u673a\u5236\u5e94\u7528\u5728\u89c6\u89c9\u9886\u57df\uff0c\u540e\u6765\u4f34\u968f\u77402017\u5e74Transformer\u6a21\u578b\u7ed3\u6784\u7684\u63d0\u51fa\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5728NLP,CV\u76f8\u5173\u95ee\u9898\u7684\u6a21\u578b\u7f51\u7edc\u8bbe\u8ba1\u4e0a\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u201c\u6ce8\u610f\u529b\u673a\u5236\u201d\u5b9e\u9645\u4e0a\u5c31\u662f\u60f3\u5c06\u4eba\u7684\u611f\u77e5\u65b9\u5f0f\u3001\u6ce8\u610f\u529b\u7684\u884c\u4e3a\u5e94\u7528\u5728\u673a\u5668\u4e0a\uff0c\u8ba9\u673a\u5668\u5b66\u4f1a\u53bb\u611f\u77e5\u6570\u636e\u4e2d\u7684\u91cd\u8981\u548c\u4e0d\u91cd\u8981\u7684\u90e8\u5206\u3002 \u4e3e\u4f8b\u8bf4\u660e\uff1a\u5f53\u6211\u4eec\u770b\u5230\u4e0b\u9762\u8fd9\u5f20\u56fe\u65f6\uff0c\u77ed\u65f6\u95f4\u5185\u5927\u8111\u53ef\u80fd\u53ea\u5bf9\u56fe\u7247\u4e2d\u7684\u201c\u9526\u6c5f\u996d\u5e97\u201d\u6709\u5370\u8c61\uff0c\u5373\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u4e86\u201c\u9526\u6c5f\u996d\u5e97\u201d\u5904\u3002\u77ed\u65f6\u95f4\u5185\uff0c\u5927\u8111\u53ef\u80fd\u5e76\u6ca1\u6709\u6ce8\u610f\u5230\u9526\u6c5f\u996d\u5e97\u4e0a\u9762\u6709\u4e00\u4e32\u7535\u8bdd\u53f7\u7801\uff0c\u4e0b\u9762\u6709\u51e0\u4e2a\u884c\u4eba\uff0c\u540e\u9762\u8fd8\u6709\u201c\u559c\u8fd0\u6765\u5927\u9152\u5bb6\u201d\u7b49\u4fe1\u606f\u3002 \u6240\u4ee5\uff0c\u5927\u8111\u5728\u77ed\u65f6\u95f4\u5185\u5904\u7406\u4fe1\u606f\u65f6\uff0c\u4e3b\u8981\u5c06\u56fe\u7247\u4e2d\u6700\u5438\u5f15\u4eba\u6ce8\u610f\u529b\u7684\u90e8\u5206\u8bfb\u51fa\u6765\u4e86\uff0c\u5927\u8111\u6ce8\u610f\u529b\u53ea\u5173\u6ce8\u5438\u5f15\u4eba\u7684\u90e8\u5206, \u7c7b\u4f3c\u4e0b\u56fe\u6240\u793a. \u540c\u6837\u7684\u5982\u679c\u6211\u4eec\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u6211\u4eec\u8981\u8ba9\u673a\u5668\u6ce8\u610f\u5230\u6bcf\u4e2a\u8bcd\u5411\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u6709\u4fa7\u91cd\u5730\u8fdb\u884c\u7ffb\u8bd1\uff0c\u6a21\u62df\u4eba\u7c7b\u7406\u89e3\u7684\u8fc7\u7a0b\u3002 3. \u6ce8\u610f\u529b\u673a\u5236\u5206\u7c7b\u4ee5\u53ca\u5982\u4f55\u5b9e\u73b0 \u00b6 \u901a\u4fd7\u6765\u8bb2\u5c31\u662f\u5bf9\u4e8e\u6a21\u578b\u7684\u6bcf\u4e00\u4e2a\u8f93\u5165\u9879\uff0c\u53ef\u80fd\u662f\u56fe\u7247\u4e2d\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u6216\u8005\u662f\u8bed\u53e5\u4e2d\u7684\u67d0\u4e2a\u5355\u8bcd\u5206\u914d\u4e00\u4e2a\u6743\u91cd\uff0c\u8fd9\u4e2a\u6743\u91cd\u7684\u5927\u5c0f\u5c31\u4ee3\u8868\u4e86\u6211\u4eec\u5e0c\u671b\u6a21\u578b\u5bf9\u8be5\u90e8\u5206\u4e00\u4e2a\u5173\u6ce8\u7a0b\u5ea6\u3002\u8fd9\u6837\u4e00\u6765\uff0c\u901a\u8fc7\u6743\u91cd\u5927\u5c0f\u6765\u6a21\u62df\u4eba\u5728\u5904\u7406\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u7684\u4fa7\u91cd\uff0c\u6709\u6548\u7684\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u964d\u4f4e\u4e86\u8ba1\u7b97\u91cf\u3002 \u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u901a\u5e38\u53ef\u5206\u4e3a\u4e09\u7c7b: \u8f6f\u6ce8\u610f\uff08\u5168\u5c40\u6ce8\u610f\uff09\u3001\u786c\u6ce8\u610f\uff08\u5c40\u90e8\u6ce8\u610f\uff09\u548c\u81ea\u6ce8\u610f\uff08\u5185\u6ce8\u610f\uff09 \u8f6f\u6ce8\u610f\u673a\u5236(Soft/Global Attention: \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u7684\u5206\u914d\u7684\u6743\u91cd\u4e3a0-1\u4e4b\u95f4\uff0c\u4e5f\u5c31\u662f\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u591a\u4e00\u70b9\uff0c\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u5c11\u4e00\u70b9\uff0c\u56e0\u4e3a\u5bf9\u5927\u90e8\u5206\u4fe1\u606f\u90fd\u6709\u8003\u8651\uff0c\u4f46\u8003\u8651\u7a0b\u5ea6\u4e0d\u4e00\u6837\uff0c\u6240\u4ee5\u76f8\u5bf9\u6765\u8bf4\u8ba1\u7b97\u91cf\u6bd4\u8f83\u5927\u3002 \u786c\u6ce8\u610f\u673a\u5236(Hard/Local Attention,[\u4e86\u89e3\u5373\u53ef]): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u975e0\u53731\uff0c\u548c\u8f6f\u6ce8\u610f\u4e0d\u540c\uff0c\u786c\u6ce8\u610f\u673a\u5236\u53ea\u8003\u8651\u90a3\u90e8\u5206\u9700\u8981\u5173\u6ce8\uff0c\u54ea\u90e8\u5206\u4e0d\u5173\u6ce8\uff0c\u4e5f\u5c31\u662f\u76f4\u63a5\u820d\u5f03\u6389\u4e00\u4e9b\u4e0d\u76f8\u5173\u9879\u3002\u4f18\u52bf\u5728\u4e8e\u53ef\u4ee5\u51cf\u5c11\u4e00\u5b9a\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u6709\u53ef\u80fd\u4e22\u5931\u6389\u4e00\u4e9b\u672c\u5e94\u8be5\u6ce8\u610f\u7684\u4fe1\u606f\u3002 \u81ea\u6ce8\u610f\u529b\u673a\u5236( Self/Intra Attention): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u53d6\u51b3\u4e8e\u8f93\u5165\u9879\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5373\u901a\u8fc7\u8f93\u5165\u9879\u5185\u90e8\u7684\"\u8868\u51b3\"\u6765\u51b3\u5b9a\u5e94\u8be5\u5173\u6ce8\u54ea\u4e9b\u8f93\u5165\u9879\u3002\u548c\u524d\u4e24\u79cd\u76f8\u6bd4\uff0c\u5728\u5904\u7406\u5f88\u957f\u7684\u8f93\u5165\u65f6\uff0c\u5177\u6709\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf\u3002 3.1 Soft Attention (\u6700\u5e38\u89c1) \u00b6 \u9700\u8981\u6ce8\u610f\uff1a\u6ce8\u610f\u529b\u673a\u5236\u662f\u4e00\u79cd\u901a\u7528\u7684\u601d\u60f3\u548c\u6280\u672f\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u6a21\u578b\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u7528\u4e8e\u4efb\u4f55\u6a21\u578b\u3002\u6211\u4eec\u8fd9\u91cc\u53ea\u662f\u4ee5\u6587\u672c\u5904\u7406\u9886\u57df\u7684Encoder-Decoder\u6846\u67b6\u4e3a\u4f8b\u8fdb\u884c\u7406\u89e3\u3002\u8fd9\u91cc\u6211\u4eec\u5206\u522b\u4ee5\u666e\u901aEncoder-Decoder\u6846\u67b6\u4ee5\u53ca\u52a0Attention\u7684Encoder-Decoder\u6846\u67b6\u5206\u522b\u505a\u5bf9\u6bd4\u3002 3.1.1 \u666e\u901aEncoder-Decoder\u6846\u67b6 \u00b6 \u4e0b\u56fe1\u662fEncoder-Decoder\u6846\u67b6\u7684\u4e00\u79cd\u62bd\u8c61\u8868\u793a\u65b9\u5f0f\uff1a \u4e0a\u56fe\u56fe\u4f8b\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u7531\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u751f\u6210\u53e6\u5916\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u7684\u901a\u7528\u5904\u7406\u6a21\u578b\u3002\u5bf9\u4e8e\u53e5\u5b50\u5bf9 \uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u7ed9\u5b9a\u8f93\u5165\u53e5\u5b50Source\uff0c\u671f\u5f85\u901a\u8fc7Encoder-Decoder\u6846\u67b6\u6765\u751f\u6210\u76ee\u6807\u53e5\u5b50Target\u3002Source\u548cTarget\u53ef\u4ee5\u662f\u540c\u4e00\u79cd\u8bed\u8a00\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e24\u79cd\u4e0d\u540c\u7684\u8bed\u8a00\u3002\u800cSource\u548cTarget\u5206\u522b\u7531\u5404\u81ea\u7684\u5355\u8bcd\u5e8f\u5217\u6784\u6210\uff1a $$ Source = \\langle X_1,X_2 \\cdots X_m \\rangle \\\\ Target = \\langle y_1,y_2 \\cdots y_n \\rangle $$ encoder\u987e\u540d\u601d\u4e49\u5c31\u662f\u5bf9\u8f93\u5165\u53e5\u5b50Source\u8fdb\u884c\u7f16\u7801\uff0c\u5c06\u8f93\u5165\u53e5\u5b50\u901a\u8fc7\u975e\u7ebf\u6027\u53d8\u6362\u8f6c\u5316\u4e3a\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\uff1a $$ C = F(X_1,X_2 \\cdots X_m) $$ \u5bf9\u4e8e\u89e3\u7801\u5668Decoder\u6765\u8bf4\uff0c\u5176\u4efb\u52a1\u662f\u6839\u636e\u53e5\u5b50Source\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u548c\u4e4b\u524d\u5df2\u7ecf\u751f\u6210\u7684\u5386\u53f2\u4fe1\u606f,y_1, y_2\u2026y_i-1\u6765\u751f\u6210i\u65f6\u523b\u8981\u751f\u6210\u7684\u5355\u8bcdy_i $$ y_i = G(C,y_1,y_2 \\cdots y_{i-1}) $$ \u4e0a\u8ff0\u56fe\u4e2d\u5c55\u793a\u7684Encoder-Decoder\u6846\u67b6\u662f\u6ca1\u6709\u4f53\u73b0\u51fa\u201c\u6ce8\u610f\u529b\u6a21\u578b\u201d\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u662f\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u7684\u5206\u5fc3\u6a21\u578b\u3002\u4e3a\u4ec0\u4e48\u8bf4\u5b83\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u5462\uff1f\u8bf7\u89c2\u5bdf\u4e0b\u76ee\u6807\u53e5\u5b50Target\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u751f\u6210\u8fc7\u7a0b\u5982\u4e0b\uff1a $$ y_1 = f(C) \\\\ y_2 = f(C, y_1) \\\\ y_3 = f(C, y_1, y_2) $$ \u5176\u4e2df\u662fDecoder\u7684\u975e\u7ebf\u6027\u53d8\u6362\u51fd\u6570\u3002\u4ece\u8fd9\u91cc\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u751f\u6210\u76ee\u6807\u53e5\u5b50\u7684\u5355\u8bcd\u65f6\uff0c\u4e0d\u8bba\u751f\u6210\u54ea\u4e2a\u5355\u8bcd\uff0c\u5b83\u4eec\u4f7f\u7528\u7684\u8f93\u5165\u53e5\u5b50Source\u7684\u8bed\u4e49\u7f16\u7801C\u90fd\u662f\u4e00\u6837\u7684\uff0c\u6ca1\u6709\u4efb\u4f55\u533a\u522b\u3002\u800c\u8bed\u4e49\u7f16\u7801C\u53c8\u662f\u901a\u8fc7\u5bf9source\u7ecf\u8fc7Encoder\u7f16\u7801\u4ea7\u751f\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8etarget\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u5355\u8bcd\uff0csource\u4e2d\u4efb\u610f\u5355\u8bcd\u5bf9\u67d0\u4e2a\u76ee\u6807\u5355\u8bcdy_i\u6765\u8bf4\u5f71\u54cd\u529b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u8bf4\u56fe1\u4e2d\u7684\u6a21\u578b\u6ca1\u6709\u4f53\u73b0\u6ce8\u610f\u529b\u7684\u539f\u56e0\u3002 3.1.2 \u52a0Attention\u7684Encoder-Decoder\u6846\u67b6 \u00b6 \u4e3e\u4f8b\u8bf4\u660e\uff0c\u4e3a\u4f55\u6dfb\u52a0Attention: \u6bd4\u5982\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff0c\u8f93\u5165source\u4e3a\uff1aTom chase Jerry\uff0c\u8f93\u51fatarget\u4e3a\uff1a\u201c\u6c64\u59c6\u201d\uff0c\u201c\u8ffd\u9010\u201d\uff0c\u201c\u6770\u745e\u201d\u3002\u5728\u7ffb\u8bd1\u201cJerry\u201d\u8fd9\u4e2a\u4e2d\u6587\u5355\u8bcd\u7684\u65f6\u5019\uff0c\u666e\u901aEncoder-Decoder\u6846\u67b6\u4e2d\uff0csource\u91cc\u7684\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u7ffb\u8bd1\u76ee\u6807\u5355\u8bcd\u201c\u6770\u745e\u201d\u8d21\u732e\u662f\u76f8\u540c\u7684\uff0c\u5f88\u660e\u663e\u8fd9\u91cc\u4e0d\u592a\u5408\u7406\uff0c\u663e\u7136\u201cJerry\u201d\u5bf9\u4e8e\u7ffb\u8bd1\u6210\u201c\u6770\u745e\u201d\u66f4\u91cd\u8981\u3002 \u5982\u679c\u5f15\u5165Attention\u6a21\u578b\uff0c\u5728\u751f\u6210\u201c\u6770\u745e\u201d\u7684\u65f6\u5019\uff0c\u5e94\u8be5\u4f53\u73b0\u51fa\u82f1\u6587\u5355\u8bcd\u5bf9\u4e8e\u7ffb\u8bd1\u5f53\u524d\u4e2d\u6587\u5355\u8bcd\u4e0d\u540c\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u6bd4\u5982\u7ed9\u51fa\u7c7b\u4f3c\u4e0b\u9762\u4e00\u4e2a\u6982\u7387\u5206\u5e03\u503c\uff1a\uff08Tom,0.3\uff09(Chase,0.2) (Jerry,0.5).\u6bcf\u4e2a\u82f1\u6587\u5355\u8bcd\u7684\u6982\u7387\u4ee3\u8868\u4e86\u7ffb\u8bd1\u5f53\u524d\u5355\u8bcd\u201c\u6770\u745e\u201d\u65f6\uff0c\u6ce8\u610f\u529b\u5206\u914d\u6a21\u578b\u5206\u914d\u7ed9\u4e0d\u540c\u82f1\u6587\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5927\u5c0f\u3002 \u56e0\u6b64\uff0c\u57fa\u4e8e\u4e0a\u8ff0\u4f8b\u5b50\u6240\u793a, \u5bf9\u4e8etarget\u4e2d\u4efb\u610f\u4e00\u4e2a\u5355\u8bcd\u90fd\u5e94\u8be5\u6709\u5bf9\u5e94\u7684source\u4e2d\u7684\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387.\u800c\u4e14\uff0c\u7531\u4e8e\u6ce8\u610f\u529b\u6a21\u578b\u7684\u52a0\u5165\uff0c\u539f\u6765\u5728\u751f\u6210target\u5355\u8bcd\u65f6\u5019\u7684\u4e2d\u95f4\u8bed\u4e49C\u5c31\u4e0d\u518d\u662f\u56fa\u5b9a\u7684\uff0c\u800c\u662f\u4f1a\u6839\u636e\u6ce8\u610f\u529b\u6982\u7387\u53d8\u5316\u7684C\uff0c\u52a0\u5165\u4e86\u6ce8\u610f\u529b\u6a21\u578b\u7684Encoder-Decoder\u6846\u67b6\u5c31\u53d8\u6210\u4e86\u4e0b\u56fe2\u6240\u793a\uff1a \u5373\u751f\u6210\u76ee\u6807\u53e5\u5b50\u5355\u8bcd\u7684\u8fc7\u7a0b\u6210\u4e86\u4e0b\u9762\u7684\u5f62\u5f0f\uff1a $$ y_1 = f1(C_1) \\\\ y_2 = f1(C_2, y_1) \\\\ y_3 = f1(C_3, y_1, y_2) $$ \u800c\u6bcf\u4e2aCi\u53ef\u80fd\u5bf9\u5e94\u7740\u4e0d\u540c\u7684\u6e90\u8bed\u53e5\u5b50\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\uff0c\u6bd4\u5982\u5bf9\u4e8e\u4e0a\u9762\u7684\u82f1\u6c49\u7ffb\u8bd1\u6765\u8bf4\uff0c\u5176\u5bf9\u5e94\u7684\u4fe1\u606f\u53ef\u80fd\u5982\u4e0b: $$ C_{Tom}=g(0.6*f2(Tom), 0.2*f2(Chase), 0.2*f2(Jerry)) \\\\ C_{Chase}=g(0.2*f2(Tom), 0.7*f2(Chase), 0.1*f2(Jerry)) \\\\ C_{Jerry}=g(0.3*f2(Tom), 0.2*f2(Chase), 0.5*f2(Jerry)) $$ f2\u51fd\u6570\u4ee3\u8868Encoder\u5bf9\u8f93\u5165\u82f1\u6587\u5355\u8bcd\u7684\u67d0\u79cd\u53d8\u6362\u51fd\u6570\uff0c\u6bd4\u5982\u5982\u679cEncoder\u662f\u7528\u7684RNN\u6a21\u578b\u7684\u8bdd\uff0c\u8fd9\u4e2af2\u51fd\u6570\u7684\u7ed3\u679c\u5f80\u5f80\u662f\u67d0\u4e2a\u65f6\u523b\u8f93\u5165\u540e\u9690\u5c42\u8282\u70b9\u7684\u72b6\u6001\u503c\uff1bg\u4ee3\u8868Encoder\u6839\u636e\u5355\u8bcd\u7684\u4e2d\u95f4\u8868\u793a\u5408\u6210\u6574\u4e2a\u53e5\u5b50\u4e2d\u95f4\u8bed\u4e49\u8868\u793a\u7684\u53d8\u6362\u51fd\u6570\uff0c\u4e00\u822c\u7684\u505a\u6cd5\u4e2d\uff0cg\u51fd\u6570\u5c31\u662f\u5bf9\u6784\u6210\u5143\u7d20\u52a0\u6743\u6c42\u548c\uff0c\u5373\u4e0b\u5217\u516c\u5f0f C_i = \\sum_{j=1}^{L_x}a_{ij}h_j C_i = \\sum_{j=1}^{L_x}a_{ij}h_j Lx\u4ee3\u8868\u8f93\u5165\u53e5\u5b50source\u7684\u957f\u5ea6, a_ij\u4ee3\u8868\u5728Target\u8f93\u51fa\u7b2ci\u4e2a\u5355\u8bcd\u65f6source\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u7b2cj\u4e2a\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u7cfb\u6570, \u800chj\u5219\u662fsource\u8f93\u5165\u53e5\u5b50\u4e2d\u7b2cj\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5047\u8bbeCi\u4e0b\u6807i\u5c31\u662f\u4e0a\u9762\u4f8b\u5b50\u6240\u8bf4\u7684'\u6c64\u59c6', \u90a3\u4e48Lx\u5c31\u662f3, h1=f('Tom'), h2=f('Chase'),h3=f('jerry')\u5206\u522b\u8f93\u5165\u53e5\u5b50\u6bcf\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u6a21\u578b\u6743\u503c\u5219\u5206\u522b\u662f0.6, 0.2, 0.2, \u6240\u4ee5g\u51fd\u6570\u672c\u8d28\u4e0a\u5c31\u662f\u52a0\u6743\u6c42\u548c\u51fd\u6570, \u5982\u679c\u5f62\u8c61\u8868\u793a\u7684\u8bdd, \u7ffb\u8bd1\u4e2d\u6587\u5355\u8bcd'\u6c64\u59c6'\u7684\u65f6\u5019, \u6570\u5b66\u516c\u5f0f\u5bf9\u5e94\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aCi\u7684\u5f62\u6210\u8fc7\u7a0b\u7c7b\u4f3c\u4e0b\u56fe3: 3.1.3 \u5982\u4f55\u5f97\u5230\u6ce8\u610f\u529b\u6982\u7387\u5206\u5e03 \u00b6 \u4e3a\u4e86\u4fbf\u4e8e\u8bf4\u660e\uff0c\u6211\u4eec\u5047\u8bbeEncoder-Decoder\u6846\u67b6\u4e2d\uff0cEncoder\u548cDecoder\u90fd\u91c7\u7528RNN\u6a21\u578b\uff0c\u5982\u4e0b\u56fe4\u6240\u793a\uff1a \u90a3\u4e48\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\u503c\u7684\u901a\u7528\u8ba1\u7b97\u8fc7\u7a0b\u5982\u4e0b\uff1a \u4e0a\u56fe\u4e2dh_i\u8868\u793aSource\u4e2d\u5355\u8bcdj\u5bf9\u5e94\u7684\u9690\u5c42\u8282\u70b9\u72b6\u6001h_j\uff0cH_i\u8868\u793aTarget\u4e2d\u5355\u8bcdi\u7684\u9690\u5c42\u8282\u70b9\u72b6\u6001\uff0c\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u662fTarget\u4e2d\u5355\u8bcdi\u5bf9Source\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u9f50\u53ef\u80fd\u6027\uff0c\u5373F(h_j,H_i-1)\uff0c\u800c\u51fd\u6570F\u53ef\u4ee5\u7528\u4e0d\u540c\u7684\u65b9\u6cd5\uff0c\u7136\u540e\u51fd\u6570F\u7684\u8f93\u51fa\u7ecf\u8fc7softmax\u8fdb\u884c\u5f52\u4e00\u5316\u5c31\u5f97\u5230\u4e86\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\u3002 \u4e0a\u9762\u5c31\u662f\u7ecf\u5178\u7684Soft Attention\u6a21\u578b\u7684\u57fa\u672c\u601d\u60f3\uff0c\u533a\u522b\u53ea\u662f\u51fd\u6570F\u4f1a\u6709\u6240\u4e0d\u540c\u3002 3.1.4 Attention\u673a\u5236\u7684\u672c\u8d28\u601d\u60f3 \u00b6 \u5176\u5b9eAttention\u673a\u5236\u53ef\u4ee5\u770b\u4f5c\uff0cTarget\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u662f\u5bf9Source\u6bcf\u4e2a\u5355\u8bcd\u7684\u52a0\u6743\u6c42\u548c\uff0c\u800c\u6743\u91cd\u662fSource\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9Target\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u91cd\u8981\u7a0b\u5ea6\u3002\u56e0\u6b64\uff0cAttention\u7684\u672c\u8d28\u601d\u60f3\u4f1a\u8868\u793a\u6210\u4e0b\u56fe\uff1a \u5c06Source\u4e2d\u7684\u6784\u6210\u5143\u7d20\u770b\u4f5c\u662f\u4e00\u7cfb\u5217\u7684 \u6570\u636e\u5bf9\uff0c\u7ed9\u5b9aTarget\u4e2d\u7684\u67d0\u4e2a\u5143\u7d20Query\uff0c\u901a\u8fc7\u8ba1\u7b97Query\u548c\u5404\u4e2aKey\u7684\u76f8\u4f3c\u6027\u6216\u8005\u76f8\u5173\u6027\uff0c\u5373\u6743\u91cd\u7cfb\u6570\uff1b\u7136\u540e\u5bf9Value\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u5e76\u5f97\u5230\u6700\u7ec8\u7684Attention\u6570\u503c\u3002\u5c06\u672c\u8d28\u601d\u60f3\u8868\u793a\u6210\u516c\u5f0f\u5982\u4e0b\uff1a \u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u63d0\u5230\uff1aSource \u4e2d\u7684 Key \u548c Value \u5408\u4e8c\u4e3a\u4e00\uff0c\u6307\u5411\u7684\u662f\u540c\u4e00\u4e2a\u4e1c\u897f\uff0c\u4e5f\u5373\u8f93\u5165\u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u5e94\u7684\u8bed\u4e49\u7f16\u7801\uff0c\u6240\u4ee5\u53ef\u80fd\u4e0d\u5bb9\u6613\u770b\u51fa\u8fd9\u79cd\u80fd\u591f\u4f53\u73b0\u672c\u8d28\u601d\u60f3\u7684\u7ed3\u6784\u3002\u56e0\u6b64\uff0cAttention\u8ba1\u7b97\u8f6c\u6362\u4e3a\u4e0b\u97623\u4e2a\u9636\u6bb5\u3002 \u8f93\u5165\u7531\u4e09\u90e8\u5206\u6784\u6210\uff1aQuery\u3001Key\u548cValue\u3002\u5176\u4e2d\uff0c(Key, Value)\u662f\u5177\u6709\u76f8\u4e92\u5173\u8054\u7684KV\u5bf9\uff0cQuery\u662f\u8f93\u5165\u7684\u201c\u95ee\u9898\u201d\uff0cAttention\u53ef\u4ee5\u5c06Query\u8f6c\u5316\u4e3a\u4e0eQuery\u6700\u76f8\u5173\u7684\u5411\u91cf\u8868\u793a\u3002 Attention\u7684\u8ba1\u7b97\u4e3b\u8981\u52063\u6b65\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002 Attention 3\u6b65\u8ba1\u7b97\u8fc7\u7a0bAttention3\u6b65\u8ba1\u7b97\u8fc7\u7a0b \u7b2c\u4e00\u6b65\uff1aQuery\u548cKey\u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5f97\u5230Attention Score\uff1b \u7b2c\u4e8c\u6b65\uff1a\u5bf9Attention Score\u8fdb\u884cSoftmax\u5f52\u4e00\u5316\uff0c\u5f97\u5230\u6743\u503c\u77e9\u9635\uff1b \u7b2c\u4e09\u6b65\uff1a\u6743\u91cd\u77e9\u9635\u4e0eValue\u8fdb\u884c\u52a0\u6743\u6c42\u548c\u8ba1\u7b97\u3002 Query\u3001Key\u548cValue\u7684\u542b\u4e49\u662f\u4ec0\u4e48\u5462\uff1f\u6211\u4eec\u4ee5\u521a\u624d\u5927\u8111\u8bfb\u56fe\u4e3a\u4f8b\u3002Value\u53ef\u4ee5\u7406\u89e3\u4e3a\u4eba\u773c\u89c6\u7f51\u819c\u5bf9\u6574\u5f20\u56fe\u7247\u4fe1\u606f\u7684\u539f\u59cb\u6355\u6349\uff0c\u4e0d\u53d7\u201c\u6ce8\u610f\u529b\u201d\u6240\u5f71\u54cd\u3002\u6211\u4eec\u53ef\u4ee5\u5c06Value\u7406\u89e3\u4e3a\u50cf\u7d20\u7ea7\u522b\u7684\u4fe1\u606f\uff0c\u90a3\u4e48\u5047\u8bbe\u53ea\u8981\u4e00\u5f20\u56fe\u7247\u5448\u73b0\u5728\u4eba\u773c\u9762\u524d\uff0c\u56fe\u7247\u4e2d\u7684\u50cf\u7d20\u90fd\u4f1a\u88ab\u89c6\u7f51\u819c\u6355\u6349\u5230\u3002Key\u4e0eValue\u76f8\u5173\u8054\uff0cKey\u662f\u56fe\u7247\u539f\u59cb\u4fe1\u606f\u6240\u5bf9\u5e94\u7684\u5173\u952e\u6027\u63d0\u793a\u4fe1\u606f\uff0c\u6bd4\u5982\u201c\u9526\u6c5f\u996d\u5e97\u201d\u90e8\u5206\u662f\u5c06\u56fe\u7247\u4e2d\u7684\u539f\u59cb\u50cf\u7d20\u4fe1\u606f\u62bd\u8c61\u4e3a\u4e2d\u6587\u6587\u5b57\u548c\u724c\u533e\u7684\u63d0\u793a\u4fe1\u606f\u3002\u4e00\u4e2a\u4e2d\u6587\u8bfb\u8005\u770b\u5230\u8fd9\u5f20\u56fe\u7247\u65f6\uff0c\u8bfb\u8005\u5927\u8111\u6709\u610f\u8bc6\u5730\u5411\u56fe\u7247\u83b7\u53d6\u4fe1\u606f\uff0c\u5373\u53d1\u8d77\u4e86\u4e00\u6b21Query\uff0cQuery\u4e2d\u5305\u542b\u4e86\u8bfb\u8005\u7684\u610f\u56fe\u7b49\u4fe1\u606f\u3002\u5728\u4e00\u6b21\u8bfb\u56fe\u8fc7\u7a0b\u4e2d\uff0cQuery\u4e0eKey\u4e4b\u95f4\u8ba1\u7b97\u51faAttention Score\uff0c\u5f97\u5230\u6700\u5177\u6709\u5438\u5f15\u529b\u7684\u90e8\u5206\uff0c\u5e76\u53ea\u5bf9\u5177\u6709\u5438\u5f15\u529b\u7684Value\u4fe1\u606f\u8fdb\u884c\u63d0\u53d6\uff0c\u53cd\u9988\u5230\u5927\u8111\u4e2d\u3002\u5c31\u50cf\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u7ecf\u8fc7\u5927\u8111\u7684\u6ce8\u610f\u529b\u673a\u5236\u7684\u7b5b\u9009\uff0c\u4e00\u6b21Query\u540e\uff0c\u5927\u8111\u53ea\u5173\u6ce8\u201c\u9526\u6c5f\u996d\u5e97\u201d\u7684\u724c\u533e\u90e8\u5206\u3002 \u518d\u4ee5\u4e00\u4e2a\u641c\u7d22\u5f15\u64ce\u7684\u68c0\u7d22\u4e3a\u4f8b\u3002\u4f7f\u7528\u67d0\u4e2aQuery\u53bb\u641c\u7d22\u5f15\u64ce\u91cc\u641c\u7d22\uff0c\u641c\u7d22\u5f15\u64ce\u91cc\u9762\u6709\u597d\u591a\u6587\u7ae0\uff0c\u6bcf\u4e2a\u6587\u7ae0\u7684\u5168\u6587\u53ef\u4ee5\u88ab\u7406\u89e3\u6210Value\uff1b\u6587\u7ae0\u7684\u5173\u952e\u6027\u4fe1\u606f\u662f\u6807\u9898\uff0c\u53ef\u4ee5\u5c06\u6807\u9898\u8ba4\u4e3a\u662fKey\u3002\u641c\u7d22\u5f15\u64ce\u7528Query\u548c\u90a3\u4e9b\u6587\u7ae0\u4eec\u7684\u6807\u9898\uff08Key\uff09\u8fdb\u884c\u5339\u914d\uff0c\u770b\u770b\u76f8\u4f3c\u5ea6\uff08\u8ba1\u7b97Attention Score)\u3002\u6211\u4eec\u60f3\u5f97\u5230\u8ddfQuery\u76f8\u5173\u7684\u77e5\u8bc6\uff0c\u4e8e\u662f\u7528\u8fd9\u4e9b\u76f8\u4f3c\u5ea6\u5c06\u68c0\u7d22\u7684\u6587\u7ae0Value\u505a\u4e00\u4e2a\u52a0\u6743\u548c\uff0c\u90a3\u4e48\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u4fe1\u606f\uff0c\u65b0\u7684\u4fe1\u606f\u878d\u5408\u4e86\u76f8\u5173\u6027\u5f3a\u7684\u6587\u7ae0\u4eec\uff0c\u800c\u76f8\u5173\u6027\u5f31\u7684\u6587\u7ae0\u53ef\u80fd\u88ab\u8fc7\u6ee4\u6389\u3002 3.2 Hard Attention \u00b6 \u57283.1\u7ae0\u8282\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u79cd\u8f6f\u6027\u6ce8\u610f\u529b\u7684\u65b9\u5f0f\u8fdb\u884cAttention\u673a\u5236\uff0c\u5b83\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u5e03\u6765\u52a0\u6743\u6c42\u548c\u878d\u5408\u5404\u4e2a\u8f93\u5165\u5411\u91cf\u3002\u800c\u786c\u6027\u6ce8\u610f\u529b\uff08Hard Attention\uff09\u673a\u5236\u5219\u4e0d\u662f\u91c7\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5b83\u662f\u6839\u636e\u6ce8\u610f\u529b\u5206\u5e03\u9009\u62e9\u8f93\u5165\u5411\u91cf\u4e2d\u7684\u4e00\u4e2a\u4f5c\u4e3a\u8f93\u51fa\u3002\u8fd9\u91cc\u6709\u4e24\u79cd\u9009\u62e9\u65b9\u5f0f\uff1a \u9009\u62e9\u6ce8\u610f\u529b\u5206\u5e03\u4e2d\uff0c\u5206\u6570\u6700\u5927\u7684\u90a3\u4e00\u9879\u5bf9\u5e94\u7684\u8f93\u5165\u5411\u91cf\u4f5c\u4e3aAttention\u673a\u5236\u7684\u8f93\u51fa\u3002 \u6839\u636e\u6ce8\u610f\u529b\u5206\u5e03\u8fdb\u884c\u968f\u673a\u91c7\u6837\uff0c\u91c7\u6837\u7ed3\u679c\u4f5c\u4e3aAttention\u673a\u5236\u7684\u8f93\u51fa\u3002 \u786c\u6027\u6ce8\u610f\u529b\u901a\u8fc7\u4ee5\u4e0a\u4e24\u79cd\u65b9\u5f0f\u9009\u62e9Attention\u7684\u8f93\u51fa\uff0c\u8fd9\u4f1a\u4f7f\u5f97\u6700\u7ec8\u7684\u635f\u5931\u51fd\u6570\u4e0e\u6ce8\u610f\u529b\u5206\u5e03\u4e4b\u95f4\u7684\u51fd\u6570\u5173\u7cfb\u4e0d\u53ef\u5bfc\uff0c\u5bfc\u81f4\u65e0\u6cd5\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u8bad\u7ec3\u6a21\u578b\uff0c\u786c\u6027\u6ce8\u610f\u529b\u901a\u5e38\u9700\u8981\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u8fdb\u884c\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u4e00\u822c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u4f1a\u4f7f\u7528\u8f6f\u6027\u6ce8\u610f\u529b\u7684\u65b9\u5f0f\u8fdb\u884c\u8ba1\u7b97\uff0c 3.3 Self Attention \u00b6 Self Attention\u662fGoogle\u5728transformer\u6a21\u578b\u4e2d\u63d0\u51fa\u7684\uff0c\u4e0a\u9762\u4ecb\u7ecd\u7684\u90fd\u662f\u4e00\u822c\u60c5\u51b5\u4e0bAttention\u53d1\u751f\u5728Target\u5143\u7d20Query\u548cSource\u4e2d\u6240\u6709\u5143\u7d20\u4e4b\u95f4\u3002\u800cSelf Attention\uff0c\u6307\u7684\u662fSource\u5185\u90e8\u5143\u7d20\u4e4b\u95f4\u6216\u8005Target\u5185\u90e8\u5143\u7d20\u4e4b\u95f4\u53d1\u751f\u7684Attention\u673a\u5236\uff0c\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3aTarget=Source\u8fd9\u79cd\u7279\u6b8a\u60c5\u51b5\u4e0b\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002\u5f53\u7136\uff0c\u5177\u4f53\u7684\u8ba1\u7b97\u8fc7\u7a0b\u4ecd\u7136\u662f\u4e00\u6837\u7684\uff0c\u53ea\u662f\u8ba1\u7b97\u5bf9\u8c61\u53d1\u751f\u4e86\u53d8\u5316\u800c\u5df2\u3002 \u4e0a\u9762\u5185\u5bb9\u4e5f\u6709\u8bf4\u5230\uff0c\u4e00\u822c\u60c5\u51b5\u4e0bAttention\u672c\u8d28\u4e0a\u662fTarget\u548cSource\u4e4b\u95f4\u7684\u4e00\u79cd\u5355\u8bcd\u5bf9\u9f50\u673a\u5236\u3002\u90a3\u4e48\u5982\u679c\u662fSelf Attention\u673a\u5236\uff0c\u5230\u5e95\u5b66\u7684\u662f\u54ea\u4e9b\u89c4\u5f8b\u6216\u8005\u62bd\u53d6\u4e86\u54ea\u4e9b\u7279\u5f81\u5462\uff1f\u6216\u8005\u8bf4\u5f15\u5165Self Attention\u6709\u4ec0\u4e48\u589e\u76ca\u6216\u8005\u597d\u5904\u5462\uff1f\u4ecd\u7136\u4ee5\u673a\u5668\u7ffb\u8bd1\u4e3a\u4f8b\u6765\u8bf4\u660e, \u5982\u4e0b\u56fe\u6240\u793a\uff1a Attention\u7684\u53d1\u5c55\u4e3b\u8981\u7ecf\u5386\u4e86\u4e24\u4e2a\u9636\u6bb5\uff1a \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, self Attention\u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u6355\u6349\u5230\u8bed\u4e49\u5c42\u9762\u7684\u7279\u5f81(its\u7684\u6307\u4ee3\u5bf9\u8c61\u662fLaw). \u5e94\u7528\u4f20\u7edf\u7684RNN, LSTM, \u5728\u83b7\u53d6\u957f\u8ddd\u79bb\u8bed\u4e49\u7279\u5f81\u548c\u7ed3\u6784\u7279\u5f81\u7684\u65f6\u5019, \u9700\u8981\u6309\u7167\u5e8f\u5217\u987a\u5e8f\u4f9d\u6b21\u8ba1\u7b97, \u8ddd\u79bb\u8d8a\u8fdc\u7684\u8054\u7cfb\u4fe1\u606f\u7684\u635f\u8017\u8d8a\u5927, \u6709\u6548\u63d0\u53d6\u548c\u6355\u83b7\u7684\u53ef\u80fd\u6027\u8d8a\u5c0f. \u4f46\u662f\u5e94\u7528self-attention\u65f6, \u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4f1a\u76f4\u63a5\u5c06\u53e5\u5b50\u4e2d\u4efb\u610f\u4e24\u4e2atoken\u7684\u8054\u7cfb\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u6b65\u9aa4\u76f4\u63a5\u8054\u7cfb\u8d77\u6765 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u7531\u6765\u4ee5\u53ca\u89e3\u51b3\u7684\u95ee\u9898: \u65e9\u671f\u5728\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u8fd9\u4e00\u7c7bseq2seq\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u7684\u505a\u6cd5\u662f\u5229\u7528\u4e00\u4e2a\u7f16\u7801\u5668(Encoder)\u548c\u4e00\u4e2a\u89e3\u7801\u5668(Decoder)\u6784\u5efa\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f46\u662f\u57fa\u4e8e\u7f16\u7801\u89e3\u7801\u7684\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a \u95ee\u98981\uff1a\u5982\u679c\u7ffb\u8bd1\u7684\u53e5\u5b50\u5f88\u957f\u5f88\u590d\u6742\uff0c\u6bd4\u5982\u76f4\u63a5\u4e00\u7bc7\u6587\u7ae0\u8f93\u8fdb\u53bb\uff0c\u6a21\u578b\u7684\u8ba1\u7b97\u91cf\u5f88\u5927\uff0c\u5e76\u4e14\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0b\u964d\u4e25\u91cd\u3002 \u95ee\u98982\uff1a\u5728\u7ffb\u8bd1\u65f6\uff0c\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b\uff0c\u540c\u4e00\u4e2a\u8bcd\u5177\u6709\u4e0d\u540c\u7684\u542b\u4e49\uff0c\u4f46\u662f\u7f51\u7edc\u5bf9\u8fd9\u4e9b\u8bcd\u5411\u91cf\u5e76\u6ca1\u6709\u533a\u5206\u5ea6\uff0c\u6ca1\u6709\u8003\u8651\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u6548\u679c\u6bd4\u8f83\u5dee\u3002 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236\uff1a \u201c\u6ce8\u610f\u529b\u673a\u5236\u201d\u5b9e\u9645\u4e0a\u5c31\u662f\u60f3\u5c06\u4eba\u7684\u611f\u77e5\u65b9\u5f0f\u3001\u6ce8\u610f\u529b\u7684\u884c\u4e3a\u5e94\u7528\u5728\u673a\u5668\u4e0a\uff0c\u8ba9\u673a\u5668\u5b66\u4f1a\u53bb\u611f\u77e5\u6570\u636e\u4e2d\u7684\u91cd\u8981\u548c\u4e0d\u91cd\u8981\u7684\u90e8\u5206\u3002 \u5b66\u4e60\u4e86\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u7c7b\u522b: \u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u901a\u5e38\u53ef\u5206\u4e3a\u4e09\u7c7b: \u8f6f\u6ce8\u610f\uff08\u5168\u5c40\u6ce8\u610f\uff09\u3001\u786c\u6ce8\u610f\uff08\u5c40\u90e8\u6ce8\u610f\uff09\u548c\u81ea\u6ce8\u610f\uff08\u5185\u6ce8\u610f\uff09 \u8f6f\u6ce8\u610f\u673a\u5236(Soft/Global Attention: \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u7684\u5206\u914d\u7684\u6743\u91cd\u4e3a0-1\u4e4b\u95f4\uff0c\u4e5f\u5c31\u662f\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u591a\u4e00\u70b9\uff0c\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u5c11\u4e00\u70b9\uff0c\u56e0\u4e3a\u5bf9\u5927\u90e8\u5206\u4fe1\u606f\u90fd\u6709\u8003\u8651\uff0c\u4f46\u8003\u8651\u7a0b\u5ea6\u4e0d\u4e00\u6837\uff0c\u6240\u4ee5\u76f8\u5bf9\u6765\u8bf4\u8ba1\u7b97\u91cf\u6bd4\u8f83\u5927\u3002 \u786c\u6ce8\u610f\u673a\u5236(Hard/Local Attention,[\u4e86\u89e3\u5373\u53ef]): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u975e0\u53731\uff0c\u548c\u8f6f\u6ce8\u610f\u4e0d\u540c\uff0c\u786c\u6ce8\u610f\u673a\u5236\u53ea\u8003\u8651\u90a3\u90e8\u5206\u9700\u8981\u5173\u6ce8\uff0c\u54ea\u90e8\u5206\u4e0d\u5173\u6ce8\uff0c\u4e5f\u5c31\u662f\u76f4\u63a5\u820d\u5f03\u6389\u4e00\u4e9b\u4e0d\u76f8\u5173\u9879\u3002\u4f18\u52bf\u5728\u4e8e\u53ef\u4ee5\u51cf\u5c11\u4e00\u5b9a\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u6709\u53ef\u80fd\u4e22\u5931\u6389\u4e00\u4e9b\u672c\u5e94\u8be5\u6ce8\u610f\u7684\u4fe1\u606f\u3002 \u81ea\u6ce8\u610f\u529b\u673a\u5236( Self/Intra Attention): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u53d6\u51b3\u4e8e\u8f93\u5165\u9879\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5373\u901a\u8fc7\u8f93\u5165\u9879\u5185\u90e8\u7684\"\u8868\u51b3\"\u6765\u51b3\u5b9a\u5e94\u8be5\u5173\u6ce8\u54ea\u4e9b\u8f93\u5165\u9879\u3002\u548c\u524d\u4e24\u79cd\u76f8\u6bd4\uff0c\u5728\u5904\u7406\u5f88\u957f\u7684\u8f93\u5165\u65f6\uff0c\u5177\u6709\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf\u3002","title":"6 \u6ce8\u610f\u529b\u673a\u5236\u4ecb\u7ecd1"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236\u7684\u7531\u6765 \u7406\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236 \u4e86\u89e3\u5e38\u89c1\u7684\u6ce8\u610f\u529b\u7c7b\u578b\u4ee5\u53ca\u4f5c\u4e1a","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#1","text":"\u5728\u8ba4\u8bc6\u6ce8\u610f\u529b\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u7b80\u5355\u4e86\u89e3\u4e0b\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff1a \u4f8b\u5b50\uff1aseq2seq(Sequence to Sequence))\u67b6\u6784\u7ffb\u8bd1\u4efb\u52a1 seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002 \u56fe\u4e2d\u8868\u793a\u7684\u662f\u4e00\u4e2a\u4e2d\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1\uff1a\u6b22\u8fce \u6765 \u5317\u4eac \u2192 welcome to BeiJing\u3002\u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"\u6b22\u8fce \u6765 \u5317\u4eac\"\uff0c\u901a\u8fc7GRU\u6a21\u578b\u83b7\u5f97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6700\u540e\u5c06\u5b83\u4eec\u62fc\u63a5\u6210\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\uff1b\u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00 \u65e9\u671f\u5728\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u8fd9\u4e00\u7c7bseq2seq\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u7684\u505a\u6cd5\u662f\u5229\u7528\u4e00\u4e2a\u7f16\u7801\u5668(Encoder)\u548c\u4e00\u4e2a\u89e3\u7801\u5668(Decoder)\u6784\u5efa\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f46\u662f\u57fa\u4e8e\u7f16\u7801\u89e3\u7801\u7684\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a \u95ee\u98981\uff1a\u5982\u679c\u7ffb\u8bd1\u7684\u53e5\u5b50\u5f88\u957f\u5f88\u590d\u6742\uff0c\u6bd4\u5982\u76f4\u63a5\u4e00\u7bc7\u6587\u7ae0\u8f93\u8fdb\u53bb\uff0c\u6a21\u578b\u7684\u8ba1\u7b97\u91cf\u5f88\u5927\uff0c\u5e76\u4e14\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0b\u964d\u4e25\u91cd\u3002 \u95ee\u98982\uff1a\u5728\u7ffb\u8bd1\u65f6\uff0c\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b\uff0c\u540c\u4e00\u4e2a\u8bcd\u5177\u6709\u4e0d\u540c\u7684\u542b\u4e49\uff0c\u4f46\u662f\u7f51\u7edc\u5bf9\u8fd9\u4e9b\u8bcd\u5411\u91cf\u5e76\u6ca1\u6709\u533a\u5206\u5ea6\uff0c\u6ca1\u6709\u8003\u8651\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u6548\u679c\u6bd4\u8f83\u5dee\u3002 \u9488\u5bf9\u8fd9\u6837\u7684\u95ee\u9898\uff0c\u6ce8\u610f\u529b\u673a\u5236\u88ab\u63d0\u51fa\u3002","title":"1.  \u6ce8\u610f\u529b\u673a\u5236\u7684\u7531\u6765\uff0c\u89e3\u51b3\u4e86\u4ec0\u4e48\u95ee\u9898\uff1f"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#2","text":"\u6ce8\u610f\u529b\u673a\u5236\u65e9\u5728\u4e0a\u4e16\u7eaa\u4e5d\u5341\u5e74\u4ee3\u5c31\u6709\u7814\u7a76\uff0c\u6700\u65e9\u6ce8\u610f\u529b\u673a\u5236\u5e94\u7528\u5728\u89c6\u89c9\u9886\u57df\uff0c\u540e\u6765\u4f34\u968f\u77402017\u5e74Transformer\u6a21\u578b\u7ed3\u6784\u7684\u63d0\u51fa\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5728NLP,CV\u76f8\u5173\u95ee\u9898\u7684\u6a21\u578b\u7f51\u7edc\u8bbe\u8ba1\u4e0a\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002\u201c\u6ce8\u610f\u529b\u673a\u5236\u201d\u5b9e\u9645\u4e0a\u5c31\u662f\u60f3\u5c06\u4eba\u7684\u611f\u77e5\u65b9\u5f0f\u3001\u6ce8\u610f\u529b\u7684\u884c\u4e3a\u5e94\u7528\u5728\u673a\u5668\u4e0a\uff0c\u8ba9\u673a\u5668\u5b66\u4f1a\u53bb\u611f\u77e5\u6570\u636e\u4e2d\u7684\u91cd\u8981\u548c\u4e0d\u91cd\u8981\u7684\u90e8\u5206\u3002 \u4e3e\u4f8b\u8bf4\u660e\uff1a\u5f53\u6211\u4eec\u770b\u5230\u4e0b\u9762\u8fd9\u5f20\u56fe\u65f6\uff0c\u77ed\u65f6\u95f4\u5185\u5927\u8111\u53ef\u80fd\u53ea\u5bf9\u56fe\u7247\u4e2d\u7684\u201c\u9526\u6c5f\u996d\u5e97\u201d\u6709\u5370\u8c61\uff0c\u5373\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u4e86\u201c\u9526\u6c5f\u996d\u5e97\u201d\u5904\u3002\u77ed\u65f6\u95f4\u5185\uff0c\u5927\u8111\u53ef\u80fd\u5e76\u6ca1\u6709\u6ce8\u610f\u5230\u9526\u6c5f\u996d\u5e97\u4e0a\u9762\u6709\u4e00\u4e32\u7535\u8bdd\u53f7\u7801\uff0c\u4e0b\u9762\u6709\u51e0\u4e2a\u884c\u4eba\uff0c\u540e\u9762\u8fd8\u6709\u201c\u559c\u8fd0\u6765\u5927\u9152\u5bb6\u201d\u7b49\u4fe1\u606f\u3002 \u6240\u4ee5\uff0c\u5927\u8111\u5728\u77ed\u65f6\u95f4\u5185\u5904\u7406\u4fe1\u606f\u65f6\uff0c\u4e3b\u8981\u5c06\u56fe\u7247\u4e2d\u6700\u5438\u5f15\u4eba\u6ce8\u610f\u529b\u7684\u90e8\u5206\u8bfb\u51fa\u6765\u4e86\uff0c\u5927\u8111\u6ce8\u610f\u529b\u53ea\u5173\u6ce8\u5438\u5f15\u4eba\u7684\u90e8\u5206, \u7c7b\u4f3c\u4e0b\u56fe\u6240\u793a. \u540c\u6837\u7684\u5982\u679c\u6211\u4eec\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u6211\u4eec\u8981\u8ba9\u673a\u5668\u6ce8\u610f\u5230\u6bcf\u4e2a\u8bcd\u5411\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u6709\u4fa7\u91cd\u5730\u8fdb\u884c\u7ffb\u8bd1\uff0c\u6a21\u62df\u4eba\u7c7b\u7406\u89e3\u7684\u8fc7\u7a0b\u3002","title":"2. \u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#3","text":"\u901a\u4fd7\u6765\u8bb2\u5c31\u662f\u5bf9\u4e8e\u6a21\u578b\u7684\u6bcf\u4e00\u4e2a\u8f93\u5165\u9879\uff0c\u53ef\u80fd\u662f\u56fe\u7247\u4e2d\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u6216\u8005\u662f\u8bed\u53e5\u4e2d\u7684\u67d0\u4e2a\u5355\u8bcd\u5206\u914d\u4e00\u4e2a\u6743\u91cd\uff0c\u8fd9\u4e2a\u6743\u91cd\u7684\u5927\u5c0f\u5c31\u4ee3\u8868\u4e86\u6211\u4eec\u5e0c\u671b\u6a21\u578b\u5bf9\u8be5\u90e8\u5206\u4e00\u4e2a\u5173\u6ce8\u7a0b\u5ea6\u3002\u8fd9\u6837\u4e00\u6765\uff0c\u901a\u8fc7\u6743\u91cd\u5927\u5c0f\u6765\u6a21\u62df\u4eba\u5728\u5904\u7406\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u7684\u4fa7\u91cd\uff0c\u6709\u6548\u7684\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u964d\u4f4e\u4e86\u8ba1\u7b97\u91cf\u3002 \u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u901a\u5e38\u53ef\u5206\u4e3a\u4e09\u7c7b: \u8f6f\u6ce8\u610f\uff08\u5168\u5c40\u6ce8\u610f\uff09\u3001\u786c\u6ce8\u610f\uff08\u5c40\u90e8\u6ce8\u610f\uff09\u548c\u81ea\u6ce8\u610f\uff08\u5185\u6ce8\u610f\uff09 \u8f6f\u6ce8\u610f\u673a\u5236(Soft/Global Attention: \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u7684\u5206\u914d\u7684\u6743\u91cd\u4e3a0-1\u4e4b\u95f4\uff0c\u4e5f\u5c31\u662f\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u591a\u4e00\u70b9\uff0c\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u5c11\u4e00\u70b9\uff0c\u56e0\u4e3a\u5bf9\u5927\u90e8\u5206\u4fe1\u606f\u90fd\u6709\u8003\u8651\uff0c\u4f46\u8003\u8651\u7a0b\u5ea6\u4e0d\u4e00\u6837\uff0c\u6240\u4ee5\u76f8\u5bf9\u6765\u8bf4\u8ba1\u7b97\u91cf\u6bd4\u8f83\u5927\u3002 \u786c\u6ce8\u610f\u673a\u5236(Hard/Local Attention,[\u4e86\u89e3\u5373\u53ef]): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u975e0\u53731\uff0c\u548c\u8f6f\u6ce8\u610f\u4e0d\u540c\uff0c\u786c\u6ce8\u610f\u673a\u5236\u53ea\u8003\u8651\u90a3\u90e8\u5206\u9700\u8981\u5173\u6ce8\uff0c\u54ea\u90e8\u5206\u4e0d\u5173\u6ce8\uff0c\u4e5f\u5c31\u662f\u76f4\u63a5\u820d\u5f03\u6389\u4e00\u4e9b\u4e0d\u76f8\u5173\u9879\u3002\u4f18\u52bf\u5728\u4e8e\u53ef\u4ee5\u51cf\u5c11\u4e00\u5b9a\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u6709\u53ef\u80fd\u4e22\u5931\u6389\u4e00\u4e9b\u672c\u5e94\u8be5\u6ce8\u610f\u7684\u4fe1\u606f\u3002 \u81ea\u6ce8\u610f\u529b\u673a\u5236( Self/Intra Attention): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u53d6\u51b3\u4e8e\u8f93\u5165\u9879\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5373\u901a\u8fc7\u8f93\u5165\u9879\u5185\u90e8\u7684\"\u8868\u51b3\"\u6765\u51b3\u5b9a\u5e94\u8be5\u5173\u6ce8\u54ea\u4e9b\u8f93\u5165\u9879\u3002\u548c\u524d\u4e24\u79cd\u76f8\u6bd4\uff0c\u5728\u5904\u7406\u5f88\u957f\u7684\u8f93\u5165\u65f6\uff0c\u5177\u6709\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf\u3002","title":"3. \u6ce8\u610f\u529b\u673a\u5236\u5206\u7c7b\u4ee5\u53ca\u5982\u4f55\u5b9e\u73b0"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#31-soft-attention","text":"\u9700\u8981\u6ce8\u610f\uff1a\u6ce8\u610f\u529b\u673a\u5236\u662f\u4e00\u79cd\u901a\u7528\u7684\u601d\u60f3\u548c\u6280\u672f\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u6a21\u578b\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u7528\u4e8e\u4efb\u4f55\u6a21\u578b\u3002\u6211\u4eec\u8fd9\u91cc\u53ea\u662f\u4ee5\u6587\u672c\u5904\u7406\u9886\u57df\u7684Encoder-Decoder\u6846\u67b6\u4e3a\u4f8b\u8fdb\u884c\u7406\u89e3\u3002\u8fd9\u91cc\u6211\u4eec\u5206\u522b\u4ee5\u666e\u901aEncoder-Decoder\u6846\u67b6\u4ee5\u53ca\u52a0Attention\u7684Encoder-Decoder\u6846\u67b6\u5206\u522b\u505a\u5bf9\u6bd4\u3002","title":"3.1 Soft Attention (\u6700\u5e38\u89c1)"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#311-encoder-decoder","text":"\u4e0b\u56fe1\u662fEncoder-Decoder\u6846\u67b6\u7684\u4e00\u79cd\u62bd\u8c61\u8868\u793a\u65b9\u5f0f\uff1a \u4e0a\u56fe\u56fe\u4f8b\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u7531\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u751f\u6210\u53e6\u5916\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u7684\u901a\u7528\u5904\u7406\u6a21\u578b\u3002\u5bf9\u4e8e\u53e5\u5b50\u5bf9 \uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u7ed9\u5b9a\u8f93\u5165\u53e5\u5b50Source\uff0c\u671f\u5f85\u901a\u8fc7Encoder-Decoder\u6846\u67b6\u6765\u751f\u6210\u76ee\u6807\u53e5\u5b50Target\u3002Source\u548cTarget\u53ef\u4ee5\u662f\u540c\u4e00\u79cd\u8bed\u8a00\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e24\u79cd\u4e0d\u540c\u7684\u8bed\u8a00\u3002\u800cSource\u548cTarget\u5206\u522b\u7531\u5404\u81ea\u7684\u5355\u8bcd\u5e8f\u5217\u6784\u6210\uff1a $$ Source = \\langle X_1,X_2 \\cdots X_m \\rangle \\\\ Target = \\langle y_1,y_2 \\cdots y_n \\rangle $$ encoder\u987e\u540d\u601d\u4e49\u5c31\u662f\u5bf9\u8f93\u5165\u53e5\u5b50Source\u8fdb\u884c\u7f16\u7801\uff0c\u5c06\u8f93\u5165\u53e5\u5b50\u901a\u8fc7\u975e\u7ebf\u6027\u53d8\u6362\u8f6c\u5316\u4e3a\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\uff1a $$ C = F(X_1,X_2 \\cdots X_m) $$ \u5bf9\u4e8e\u89e3\u7801\u5668Decoder\u6765\u8bf4\uff0c\u5176\u4efb\u52a1\u662f\u6839\u636e\u53e5\u5b50Source\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u548c\u4e4b\u524d\u5df2\u7ecf\u751f\u6210\u7684\u5386\u53f2\u4fe1\u606f,y_1, y_2\u2026y_i-1\u6765\u751f\u6210i\u65f6\u523b\u8981\u751f\u6210\u7684\u5355\u8bcdy_i $$ y_i = G(C,y_1,y_2 \\cdots y_{i-1}) $$ \u4e0a\u8ff0\u56fe\u4e2d\u5c55\u793a\u7684Encoder-Decoder\u6846\u67b6\u662f\u6ca1\u6709\u4f53\u73b0\u51fa\u201c\u6ce8\u610f\u529b\u6a21\u578b\u201d\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u662f\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u7684\u5206\u5fc3\u6a21\u578b\u3002\u4e3a\u4ec0\u4e48\u8bf4\u5b83\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u5462\uff1f\u8bf7\u89c2\u5bdf\u4e0b\u76ee\u6807\u53e5\u5b50Target\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u751f\u6210\u8fc7\u7a0b\u5982\u4e0b\uff1a $$ y_1 = f(C) \\\\ y_2 = f(C, y_1) \\\\ y_3 = f(C, y_1, y_2) $$ \u5176\u4e2df\u662fDecoder\u7684\u975e\u7ebf\u6027\u53d8\u6362\u51fd\u6570\u3002\u4ece\u8fd9\u91cc\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u751f\u6210\u76ee\u6807\u53e5\u5b50\u7684\u5355\u8bcd\u65f6\uff0c\u4e0d\u8bba\u751f\u6210\u54ea\u4e2a\u5355\u8bcd\uff0c\u5b83\u4eec\u4f7f\u7528\u7684\u8f93\u5165\u53e5\u5b50Source\u7684\u8bed\u4e49\u7f16\u7801C\u90fd\u662f\u4e00\u6837\u7684\uff0c\u6ca1\u6709\u4efb\u4f55\u533a\u522b\u3002\u800c\u8bed\u4e49\u7f16\u7801C\u53c8\u662f\u901a\u8fc7\u5bf9source\u7ecf\u8fc7Encoder\u7f16\u7801\u4ea7\u751f\u7684\uff0c\u56e0\u6b64\u5bf9\u4e8etarget\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u5355\u8bcd\uff0csource\u4e2d\u4efb\u610f\u5355\u8bcd\u5bf9\u67d0\u4e2a\u76ee\u6807\u5355\u8bcdy_i\u6765\u8bf4\u5f71\u54cd\u529b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u8bf4\u56fe1\u4e2d\u7684\u6a21\u578b\u6ca1\u6709\u4f53\u73b0\u6ce8\u610f\u529b\u7684\u539f\u56e0\u3002","title":"3.1.1 \u666e\u901aEncoder-Decoder\u6846\u67b6"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#312-attentionencoder-decoder","text":"\u4e3e\u4f8b\u8bf4\u660e\uff0c\u4e3a\u4f55\u6dfb\u52a0Attention: \u6bd4\u5982\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff0c\u8f93\u5165source\u4e3a\uff1aTom chase Jerry\uff0c\u8f93\u51fatarget\u4e3a\uff1a\u201c\u6c64\u59c6\u201d\uff0c\u201c\u8ffd\u9010\u201d\uff0c\u201c\u6770\u745e\u201d\u3002\u5728\u7ffb\u8bd1\u201cJerry\u201d\u8fd9\u4e2a\u4e2d\u6587\u5355\u8bcd\u7684\u65f6\u5019\uff0c\u666e\u901aEncoder-Decoder\u6846\u67b6\u4e2d\uff0csource\u91cc\u7684\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u7ffb\u8bd1\u76ee\u6807\u5355\u8bcd\u201c\u6770\u745e\u201d\u8d21\u732e\u662f\u76f8\u540c\u7684\uff0c\u5f88\u660e\u663e\u8fd9\u91cc\u4e0d\u592a\u5408\u7406\uff0c\u663e\u7136\u201cJerry\u201d\u5bf9\u4e8e\u7ffb\u8bd1\u6210\u201c\u6770\u745e\u201d\u66f4\u91cd\u8981\u3002 \u5982\u679c\u5f15\u5165Attention\u6a21\u578b\uff0c\u5728\u751f\u6210\u201c\u6770\u745e\u201d\u7684\u65f6\u5019\uff0c\u5e94\u8be5\u4f53\u73b0\u51fa\u82f1\u6587\u5355\u8bcd\u5bf9\u4e8e\u7ffb\u8bd1\u5f53\u524d\u4e2d\u6587\u5355\u8bcd\u4e0d\u540c\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u6bd4\u5982\u7ed9\u51fa\u7c7b\u4f3c\u4e0b\u9762\u4e00\u4e2a\u6982\u7387\u5206\u5e03\u503c\uff1a\uff08Tom,0.3\uff09(Chase,0.2) (Jerry,0.5).\u6bcf\u4e2a\u82f1\u6587\u5355\u8bcd\u7684\u6982\u7387\u4ee3\u8868\u4e86\u7ffb\u8bd1\u5f53\u524d\u5355\u8bcd\u201c\u6770\u745e\u201d\u65f6\uff0c\u6ce8\u610f\u529b\u5206\u914d\u6a21\u578b\u5206\u914d\u7ed9\u4e0d\u540c\u82f1\u6587\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5927\u5c0f\u3002 \u56e0\u6b64\uff0c\u57fa\u4e8e\u4e0a\u8ff0\u4f8b\u5b50\u6240\u793a, \u5bf9\u4e8etarget\u4e2d\u4efb\u610f\u4e00\u4e2a\u5355\u8bcd\u90fd\u5e94\u8be5\u6709\u5bf9\u5e94\u7684source\u4e2d\u7684\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387.\u800c\u4e14\uff0c\u7531\u4e8e\u6ce8\u610f\u529b\u6a21\u578b\u7684\u52a0\u5165\uff0c\u539f\u6765\u5728\u751f\u6210target\u5355\u8bcd\u65f6\u5019\u7684\u4e2d\u95f4\u8bed\u4e49C\u5c31\u4e0d\u518d\u662f\u56fa\u5b9a\u7684\uff0c\u800c\u662f\u4f1a\u6839\u636e\u6ce8\u610f\u529b\u6982\u7387\u53d8\u5316\u7684C\uff0c\u52a0\u5165\u4e86\u6ce8\u610f\u529b\u6a21\u578b\u7684Encoder-Decoder\u6846\u67b6\u5c31\u53d8\u6210\u4e86\u4e0b\u56fe2\u6240\u793a\uff1a \u5373\u751f\u6210\u76ee\u6807\u53e5\u5b50\u5355\u8bcd\u7684\u8fc7\u7a0b\u6210\u4e86\u4e0b\u9762\u7684\u5f62\u5f0f\uff1a $$ y_1 = f1(C_1) \\\\ y_2 = f1(C_2, y_1) \\\\ y_3 = f1(C_3, y_1, y_2) $$ \u800c\u6bcf\u4e2aCi\u53ef\u80fd\u5bf9\u5e94\u7740\u4e0d\u540c\u7684\u6e90\u8bed\u53e5\u5b50\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\uff0c\u6bd4\u5982\u5bf9\u4e8e\u4e0a\u9762\u7684\u82f1\u6c49\u7ffb\u8bd1\u6765\u8bf4\uff0c\u5176\u5bf9\u5e94\u7684\u4fe1\u606f\u53ef\u80fd\u5982\u4e0b: $$ C_{Tom}=g(0.6*f2(Tom), 0.2*f2(Chase), 0.2*f2(Jerry)) \\\\ C_{Chase}=g(0.2*f2(Tom), 0.7*f2(Chase), 0.1*f2(Jerry)) \\\\ C_{Jerry}=g(0.3*f2(Tom), 0.2*f2(Chase), 0.5*f2(Jerry)) $$ f2\u51fd\u6570\u4ee3\u8868Encoder\u5bf9\u8f93\u5165\u82f1\u6587\u5355\u8bcd\u7684\u67d0\u79cd\u53d8\u6362\u51fd\u6570\uff0c\u6bd4\u5982\u5982\u679cEncoder\u662f\u7528\u7684RNN\u6a21\u578b\u7684\u8bdd\uff0c\u8fd9\u4e2af2\u51fd\u6570\u7684\u7ed3\u679c\u5f80\u5f80\u662f\u67d0\u4e2a\u65f6\u523b\u8f93\u5165\u540e\u9690\u5c42\u8282\u70b9\u7684\u72b6\u6001\u503c\uff1bg\u4ee3\u8868Encoder\u6839\u636e\u5355\u8bcd\u7684\u4e2d\u95f4\u8868\u793a\u5408\u6210\u6574\u4e2a\u53e5\u5b50\u4e2d\u95f4\u8bed\u4e49\u8868\u793a\u7684\u53d8\u6362\u51fd\u6570\uff0c\u4e00\u822c\u7684\u505a\u6cd5\u4e2d\uff0cg\u51fd\u6570\u5c31\u662f\u5bf9\u6784\u6210\u5143\u7d20\u52a0\u6743\u6c42\u548c\uff0c\u5373\u4e0b\u5217\u516c\u5f0f C_i = \\sum_{j=1}^{L_x}a_{ij}h_j C_i = \\sum_{j=1}^{L_x}a_{ij}h_j Lx\u4ee3\u8868\u8f93\u5165\u53e5\u5b50source\u7684\u957f\u5ea6, a_ij\u4ee3\u8868\u5728Target\u8f93\u51fa\u7b2ci\u4e2a\u5355\u8bcd\u65f6source\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u7b2cj\u4e2a\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u7cfb\u6570, \u800chj\u5219\u662fsource\u8f93\u5165\u53e5\u5b50\u4e2d\u7b2cj\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5047\u8bbeCi\u4e0b\u6807i\u5c31\u662f\u4e0a\u9762\u4f8b\u5b50\u6240\u8bf4\u7684'\u6c64\u59c6', \u90a3\u4e48Lx\u5c31\u662f3, h1=f('Tom'), h2=f('Chase'),h3=f('jerry')\u5206\u522b\u8f93\u5165\u53e5\u5b50\u6bcf\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u6a21\u578b\u6743\u503c\u5219\u5206\u522b\u662f0.6, 0.2, 0.2, \u6240\u4ee5g\u51fd\u6570\u672c\u8d28\u4e0a\u5c31\u662f\u52a0\u6743\u6c42\u548c\u51fd\u6570, \u5982\u679c\u5f62\u8c61\u8868\u793a\u7684\u8bdd, \u7ffb\u8bd1\u4e2d\u6587\u5355\u8bcd'\u6c64\u59c6'\u7684\u65f6\u5019, \u6570\u5b66\u516c\u5f0f\u5bf9\u5e94\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aCi\u7684\u5f62\u6210\u8fc7\u7a0b\u7c7b\u4f3c\u4e0b\u56fe3:","title":"3.1.2 \u52a0Attention\u7684Encoder-Decoder\u6846\u67b6"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#313","text":"\u4e3a\u4e86\u4fbf\u4e8e\u8bf4\u660e\uff0c\u6211\u4eec\u5047\u8bbeEncoder-Decoder\u6846\u67b6\u4e2d\uff0cEncoder\u548cDecoder\u90fd\u91c7\u7528RNN\u6a21\u578b\uff0c\u5982\u4e0b\u56fe4\u6240\u793a\uff1a \u90a3\u4e48\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\u503c\u7684\u901a\u7528\u8ba1\u7b97\u8fc7\u7a0b\u5982\u4e0b\uff1a \u4e0a\u56fe\u4e2dh_i\u8868\u793aSource\u4e2d\u5355\u8bcdj\u5bf9\u5e94\u7684\u9690\u5c42\u8282\u70b9\u72b6\u6001h_j\uff0cH_i\u8868\u793aTarget\u4e2d\u5355\u8bcdi\u7684\u9690\u5c42\u8282\u70b9\u72b6\u6001\uff0c\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u662fTarget\u4e2d\u5355\u8bcdi\u5bf9Source\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u9f50\u53ef\u80fd\u6027\uff0c\u5373F(h_j,H_i-1)\uff0c\u800c\u51fd\u6570F\u53ef\u4ee5\u7528\u4e0d\u540c\u7684\u65b9\u6cd5\uff0c\u7136\u540e\u51fd\u6570F\u7684\u8f93\u51fa\u7ecf\u8fc7softmax\u8fdb\u884c\u5f52\u4e00\u5316\u5c31\u5f97\u5230\u4e86\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\u3002 \u4e0a\u9762\u5c31\u662f\u7ecf\u5178\u7684Soft Attention\u6a21\u578b\u7684\u57fa\u672c\u601d\u60f3\uff0c\u533a\u522b\u53ea\u662f\u51fd\u6570F\u4f1a\u6709\u6240\u4e0d\u540c\u3002","title":"3.1.3 \u5982\u4f55\u5f97\u5230\u6ce8\u610f\u529b\u6982\u7387\u5206\u5e03"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#314-attention","text":"\u5176\u5b9eAttention\u673a\u5236\u53ef\u4ee5\u770b\u4f5c\uff0cTarget\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u662f\u5bf9Source\u6bcf\u4e2a\u5355\u8bcd\u7684\u52a0\u6743\u6c42\u548c\uff0c\u800c\u6743\u91cd\u662fSource\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9Target\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u91cd\u8981\u7a0b\u5ea6\u3002\u56e0\u6b64\uff0cAttention\u7684\u672c\u8d28\u601d\u60f3\u4f1a\u8868\u793a\u6210\u4e0b\u56fe\uff1a \u5c06Source\u4e2d\u7684\u6784\u6210\u5143\u7d20\u770b\u4f5c\u662f\u4e00\u7cfb\u5217\u7684 \u6570\u636e\u5bf9\uff0c\u7ed9\u5b9aTarget\u4e2d\u7684\u67d0\u4e2a\u5143\u7d20Query\uff0c\u901a\u8fc7\u8ba1\u7b97Query\u548c\u5404\u4e2aKey\u7684\u76f8\u4f3c\u6027\u6216\u8005\u76f8\u5173\u6027\uff0c\u5373\u6743\u91cd\u7cfb\u6570\uff1b\u7136\u540e\u5bf9Value\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u5e76\u5f97\u5230\u6700\u7ec8\u7684Attention\u6570\u503c\u3002\u5c06\u672c\u8d28\u601d\u60f3\u8868\u793a\u6210\u516c\u5f0f\u5982\u4e0b\uff1a \u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u63d0\u5230\uff1aSource \u4e2d\u7684 Key \u548c Value \u5408\u4e8c\u4e3a\u4e00\uff0c\u6307\u5411\u7684\u662f\u540c\u4e00\u4e2a\u4e1c\u897f\uff0c\u4e5f\u5373\u8f93\u5165\u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u5e94\u7684\u8bed\u4e49\u7f16\u7801\uff0c\u6240\u4ee5\u53ef\u80fd\u4e0d\u5bb9\u6613\u770b\u51fa\u8fd9\u79cd\u80fd\u591f\u4f53\u73b0\u672c\u8d28\u601d\u60f3\u7684\u7ed3\u6784\u3002\u56e0\u6b64\uff0cAttention\u8ba1\u7b97\u8f6c\u6362\u4e3a\u4e0b\u97623\u4e2a\u9636\u6bb5\u3002 \u8f93\u5165\u7531\u4e09\u90e8\u5206\u6784\u6210\uff1aQuery\u3001Key\u548cValue\u3002\u5176\u4e2d\uff0c(Key, Value)\u662f\u5177\u6709\u76f8\u4e92\u5173\u8054\u7684KV\u5bf9\uff0cQuery\u662f\u8f93\u5165\u7684\u201c\u95ee\u9898\u201d\uff0cAttention\u53ef\u4ee5\u5c06Query\u8f6c\u5316\u4e3a\u4e0eQuery\u6700\u76f8\u5173\u7684\u5411\u91cf\u8868\u793a\u3002 Attention\u7684\u8ba1\u7b97\u4e3b\u8981\u52063\u6b65\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002 Attention 3\u6b65\u8ba1\u7b97\u8fc7\u7a0bAttention3\u6b65\u8ba1\u7b97\u8fc7\u7a0b \u7b2c\u4e00\u6b65\uff1aQuery\u548cKey\u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5f97\u5230Attention Score\uff1b \u7b2c\u4e8c\u6b65\uff1a\u5bf9Attention Score\u8fdb\u884cSoftmax\u5f52\u4e00\u5316\uff0c\u5f97\u5230\u6743\u503c\u77e9\u9635\uff1b \u7b2c\u4e09\u6b65\uff1a\u6743\u91cd\u77e9\u9635\u4e0eValue\u8fdb\u884c\u52a0\u6743\u6c42\u548c\u8ba1\u7b97\u3002 Query\u3001Key\u548cValue\u7684\u542b\u4e49\u662f\u4ec0\u4e48\u5462\uff1f\u6211\u4eec\u4ee5\u521a\u624d\u5927\u8111\u8bfb\u56fe\u4e3a\u4f8b\u3002Value\u53ef\u4ee5\u7406\u89e3\u4e3a\u4eba\u773c\u89c6\u7f51\u819c\u5bf9\u6574\u5f20\u56fe\u7247\u4fe1\u606f\u7684\u539f\u59cb\u6355\u6349\uff0c\u4e0d\u53d7\u201c\u6ce8\u610f\u529b\u201d\u6240\u5f71\u54cd\u3002\u6211\u4eec\u53ef\u4ee5\u5c06Value\u7406\u89e3\u4e3a\u50cf\u7d20\u7ea7\u522b\u7684\u4fe1\u606f\uff0c\u90a3\u4e48\u5047\u8bbe\u53ea\u8981\u4e00\u5f20\u56fe\u7247\u5448\u73b0\u5728\u4eba\u773c\u9762\u524d\uff0c\u56fe\u7247\u4e2d\u7684\u50cf\u7d20\u90fd\u4f1a\u88ab\u89c6\u7f51\u819c\u6355\u6349\u5230\u3002Key\u4e0eValue\u76f8\u5173\u8054\uff0cKey\u662f\u56fe\u7247\u539f\u59cb\u4fe1\u606f\u6240\u5bf9\u5e94\u7684\u5173\u952e\u6027\u63d0\u793a\u4fe1\u606f\uff0c\u6bd4\u5982\u201c\u9526\u6c5f\u996d\u5e97\u201d\u90e8\u5206\u662f\u5c06\u56fe\u7247\u4e2d\u7684\u539f\u59cb\u50cf\u7d20\u4fe1\u606f\u62bd\u8c61\u4e3a\u4e2d\u6587\u6587\u5b57\u548c\u724c\u533e\u7684\u63d0\u793a\u4fe1\u606f\u3002\u4e00\u4e2a\u4e2d\u6587\u8bfb\u8005\u770b\u5230\u8fd9\u5f20\u56fe\u7247\u65f6\uff0c\u8bfb\u8005\u5927\u8111\u6709\u610f\u8bc6\u5730\u5411\u56fe\u7247\u83b7\u53d6\u4fe1\u606f\uff0c\u5373\u53d1\u8d77\u4e86\u4e00\u6b21Query\uff0cQuery\u4e2d\u5305\u542b\u4e86\u8bfb\u8005\u7684\u610f\u56fe\u7b49\u4fe1\u606f\u3002\u5728\u4e00\u6b21\u8bfb\u56fe\u8fc7\u7a0b\u4e2d\uff0cQuery\u4e0eKey\u4e4b\u95f4\u8ba1\u7b97\u51faAttention Score\uff0c\u5f97\u5230\u6700\u5177\u6709\u5438\u5f15\u529b\u7684\u90e8\u5206\uff0c\u5e76\u53ea\u5bf9\u5177\u6709\u5438\u5f15\u529b\u7684Value\u4fe1\u606f\u8fdb\u884c\u63d0\u53d6\uff0c\u53cd\u9988\u5230\u5927\u8111\u4e2d\u3002\u5c31\u50cf\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u7ecf\u8fc7\u5927\u8111\u7684\u6ce8\u610f\u529b\u673a\u5236\u7684\u7b5b\u9009\uff0c\u4e00\u6b21Query\u540e\uff0c\u5927\u8111\u53ea\u5173\u6ce8\u201c\u9526\u6c5f\u996d\u5e97\u201d\u7684\u724c\u533e\u90e8\u5206\u3002 \u518d\u4ee5\u4e00\u4e2a\u641c\u7d22\u5f15\u64ce\u7684\u68c0\u7d22\u4e3a\u4f8b\u3002\u4f7f\u7528\u67d0\u4e2aQuery\u53bb\u641c\u7d22\u5f15\u64ce\u91cc\u641c\u7d22\uff0c\u641c\u7d22\u5f15\u64ce\u91cc\u9762\u6709\u597d\u591a\u6587\u7ae0\uff0c\u6bcf\u4e2a\u6587\u7ae0\u7684\u5168\u6587\u53ef\u4ee5\u88ab\u7406\u89e3\u6210Value\uff1b\u6587\u7ae0\u7684\u5173\u952e\u6027\u4fe1\u606f\u662f\u6807\u9898\uff0c\u53ef\u4ee5\u5c06\u6807\u9898\u8ba4\u4e3a\u662fKey\u3002\u641c\u7d22\u5f15\u64ce\u7528Query\u548c\u90a3\u4e9b\u6587\u7ae0\u4eec\u7684\u6807\u9898\uff08Key\uff09\u8fdb\u884c\u5339\u914d\uff0c\u770b\u770b\u76f8\u4f3c\u5ea6\uff08\u8ba1\u7b97Attention Score)\u3002\u6211\u4eec\u60f3\u5f97\u5230\u8ddfQuery\u76f8\u5173\u7684\u77e5\u8bc6\uff0c\u4e8e\u662f\u7528\u8fd9\u4e9b\u76f8\u4f3c\u5ea6\u5c06\u68c0\u7d22\u7684\u6587\u7ae0Value\u505a\u4e00\u4e2a\u52a0\u6743\u548c\uff0c\u90a3\u4e48\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u4fe1\u606f\uff0c\u65b0\u7684\u4fe1\u606f\u878d\u5408\u4e86\u76f8\u5173\u6027\u5f3a\u7684\u6587\u7ae0\u4eec\uff0c\u800c\u76f8\u5173\u6027\u5f31\u7684\u6587\u7ae0\u53ef\u80fd\u88ab\u8fc7\u6ee4\u6389\u3002","title":"3.1.4 Attention\u673a\u5236\u7684\u672c\u8d28\u601d\u60f3"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#32-hard-attention","text":"\u57283.1\u7ae0\u8282\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u79cd\u8f6f\u6027\u6ce8\u610f\u529b\u7684\u65b9\u5f0f\u8fdb\u884cAttention\u673a\u5236\uff0c\u5b83\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u5e03\u6765\u52a0\u6743\u6c42\u548c\u878d\u5408\u5404\u4e2a\u8f93\u5165\u5411\u91cf\u3002\u800c\u786c\u6027\u6ce8\u610f\u529b\uff08Hard Attention\uff09\u673a\u5236\u5219\u4e0d\u662f\u91c7\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u5b83\u662f\u6839\u636e\u6ce8\u610f\u529b\u5206\u5e03\u9009\u62e9\u8f93\u5165\u5411\u91cf\u4e2d\u7684\u4e00\u4e2a\u4f5c\u4e3a\u8f93\u51fa\u3002\u8fd9\u91cc\u6709\u4e24\u79cd\u9009\u62e9\u65b9\u5f0f\uff1a \u9009\u62e9\u6ce8\u610f\u529b\u5206\u5e03\u4e2d\uff0c\u5206\u6570\u6700\u5927\u7684\u90a3\u4e00\u9879\u5bf9\u5e94\u7684\u8f93\u5165\u5411\u91cf\u4f5c\u4e3aAttention\u673a\u5236\u7684\u8f93\u51fa\u3002 \u6839\u636e\u6ce8\u610f\u529b\u5206\u5e03\u8fdb\u884c\u968f\u673a\u91c7\u6837\uff0c\u91c7\u6837\u7ed3\u679c\u4f5c\u4e3aAttention\u673a\u5236\u7684\u8f93\u51fa\u3002 \u786c\u6027\u6ce8\u610f\u529b\u901a\u8fc7\u4ee5\u4e0a\u4e24\u79cd\u65b9\u5f0f\u9009\u62e9Attention\u7684\u8f93\u51fa\uff0c\u8fd9\u4f1a\u4f7f\u5f97\u6700\u7ec8\u7684\u635f\u5931\u51fd\u6570\u4e0e\u6ce8\u610f\u529b\u5206\u5e03\u4e4b\u95f4\u7684\u51fd\u6570\u5173\u7cfb\u4e0d\u53ef\u5bfc\uff0c\u5bfc\u81f4\u65e0\u6cd5\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u8bad\u7ec3\u6a21\u578b\uff0c\u786c\u6027\u6ce8\u610f\u529b\u901a\u5e38\u9700\u8981\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u8fdb\u884c\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u4e00\u822c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u4f1a\u4f7f\u7528\u8f6f\u6027\u6ce8\u610f\u529b\u7684\u65b9\u5f0f\u8fdb\u884c\u8ba1\u7b97\uff0c","title":"3.2 Hard Attention"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#33-self-attention","text":"Self Attention\u662fGoogle\u5728transformer\u6a21\u578b\u4e2d\u63d0\u51fa\u7684\uff0c\u4e0a\u9762\u4ecb\u7ecd\u7684\u90fd\u662f\u4e00\u822c\u60c5\u51b5\u4e0bAttention\u53d1\u751f\u5728Target\u5143\u7d20Query\u548cSource\u4e2d\u6240\u6709\u5143\u7d20\u4e4b\u95f4\u3002\u800cSelf Attention\uff0c\u6307\u7684\u662fSource\u5185\u90e8\u5143\u7d20\u4e4b\u95f4\u6216\u8005Target\u5185\u90e8\u5143\u7d20\u4e4b\u95f4\u53d1\u751f\u7684Attention\u673a\u5236\uff0c\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3aTarget=Source\u8fd9\u79cd\u7279\u6b8a\u60c5\u51b5\u4e0b\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002\u5f53\u7136\uff0c\u5177\u4f53\u7684\u8ba1\u7b97\u8fc7\u7a0b\u4ecd\u7136\u662f\u4e00\u6837\u7684\uff0c\u53ea\u662f\u8ba1\u7b97\u5bf9\u8c61\u53d1\u751f\u4e86\u53d8\u5316\u800c\u5df2\u3002 \u4e0a\u9762\u5185\u5bb9\u4e5f\u6709\u8bf4\u5230\uff0c\u4e00\u822c\u60c5\u51b5\u4e0bAttention\u672c\u8d28\u4e0a\u662fTarget\u548cSource\u4e4b\u95f4\u7684\u4e00\u79cd\u5355\u8bcd\u5bf9\u9f50\u673a\u5236\u3002\u90a3\u4e48\u5982\u679c\u662fSelf Attention\u673a\u5236\uff0c\u5230\u5e95\u5b66\u7684\u662f\u54ea\u4e9b\u89c4\u5f8b\u6216\u8005\u62bd\u53d6\u4e86\u54ea\u4e9b\u7279\u5f81\u5462\uff1f\u6216\u8005\u8bf4\u5f15\u5165Self Attention\u6709\u4ec0\u4e48\u589e\u76ca\u6216\u8005\u597d\u5904\u5462\uff1f\u4ecd\u7136\u4ee5\u673a\u5668\u7ffb\u8bd1\u4e3a\u4f8b\u6765\u8bf4\u660e, \u5982\u4e0b\u56fe\u6240\u793a\uff1a Attention\u7684\u53d1\u5c55\u4e3b\u8981\u7ecf\u5386\u4e86\u4e24\u4e2a\u9636\u6bb5\uff1a \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, self Attention\u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u6355\u6349\u5230\u8bed\u4e49\u5c42\u9762\u7684\u7279\u5f81(its\u7684\u6307\u4ee3\u5bf9\u8c61\u662fLaw). \u5e94\u7528\u4f20\u7edf\u7684RNN, LSTM, \u5728\u83b7\u53d6\u957f\u8ddd\u79bb\u8bed\u4e49\u7279\u5f81\u548c\u7ed3\u6784\u7279\u5f81\u7684\u65f6\u5019, \u9700\u8981\u6309\u7167\u5e8f\u5217\u987a\u5e8f\u4f9d\u6b21\u8ba1\u7b97, \u8ddd\u79bb\u8d8a\u8fdc\u7684\u8054\u7cfb\u4fe1\u606f\u7684\u635f\u8017\u8d8a\u5927, \u6709\u6548\u63d0\u53d6\u548c\u6355\u83b7\u7684\u53ef\u80fd\u6027\u8d8a\u5c0f. \u4f46\u662f\u5e94\u7528self-attention\u65f6, \u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4f1a\u76f4\u63a5\u5c06\u53e5\u5b50\u4e2d\u4efb\u610f\u4e24\u4e2atoken\u7684\u8054\u7cfb\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u6b65\u9aa4\u76f4\u63a5\u8054\u7cfb\u8d77\u6765","title":"3.3 Self Attention"},{"location":"03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html#4","text":"\u5b66\u4e60\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u7531\u6765\u4ee5\u53ca\u89e3\u51b3\u7684\u95ee\u9898: \u65e9\u671f\u5728\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u8fd9\u4e00\u7c7bseq2seq\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u7684\u505a\u6cd5\u662f\u5229\u7528\u4e00\u4e2a\u7f16\u7801\u5668(Encoder)\u548c\u4e00\u4e2a\u89e3\u7801\u5668(Decoder)\u6784\u5efa\u7aef\u5230\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f46\u662f\u57fa\u4e8e\u7f16\u7801\u89e3\u7801\u7684\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a \u95ee\u98981\uff1a\u5982\u679c\u7ffb\u8bd1\u7684\u53e5\u5b50\u5f88\u957f\u5f88\u590d\u6742\uff0c\u6bd4\u5982\u76f4\u63a5\u4e00\u7bc7\u6587\u7ae0\u8f93\u8fdb\u53bb\uff0c\u6a21\u578b\u7684\u8ba1\u7b97\u91cf\u5f88\u5927\uff0c\u5e76\u4e14\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0b\u964d\u4e25\u91cd\u3002 \u95ee\u98982\uff1a\u5728\u7ffb\u8bd1\u65f6\uff0c\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b\uff0c\u540c\u4e00\u4e2a\u8bcd\u5177\u6709\u4e0d\u540c\u7684\u542b\u4e49\uff0c\u4f46\u662f\u7f51\u7edc\u5bf9\u8fd9\u4e9b\u8bcd\u5411\u91cf\u5e76\u6ca1\u6709\u533a\u5206\u5ea6\uff0c\u6ca1\u6709\u8003\u8651\u8bcd\u4e0e\u8bcd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u6548\u679c\u6bd4\u8f83\u5dee\u3002 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236\uff1a \u201c\u6ce8\u610f\u529b\u673a\u5236\u201d\u5b9e\u9645\u4e0a\u5c31\u662f\u60f3\u5c06\u4eba\u7684\u611f\u77e5\u65b9\u5f0f\u3001\u6ce8\u610f\u529b\u7684\u884c\u4e3a\u5e94\u7528\u5728\u673a\u5668\u4e0a\uff0c\u8ba9\u673a\u5668\u5b66\u4f1a\u53bb\u611f\u77e5\u6570\u636e\u4e2d\u7684\u91cd\u8981\u548c\u4e0d\u91cd\u8981\u7684\u90e8\u5206\u3002 \u5b66\u4e60\u4e86\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u7c7b\u522b: \u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u901a\u5e38\u53ef\u5206\u4e3a\u4e09\u7c7b: \u8f6f\u6ce8\u610f\uff08\u5168\u5c40\u6ce8\u610f\uff09\u3001\u786c\u6ce8\u610f\uff08\u5c40\u90e8\u6ce8\u610f\uff09\u548c\u81ea\u6ce8\u610f\uff08\u5185\u6ce8\u610f\uff09 \u8f6f\u6ce8\u610f\u673a\u5236(Soft/Global Attention: \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u7684\u5206\u914d\u7684\u6743\u91cd\u4e3a0-1\u4e4b\u95f4\uff0c\u4e5f\u5c31\u662f\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u591a\u4e00\u70b9\uff0c\u67d0\u4e9b\u90e8\u5206\u5173\u6ce8\u7684\u5c11\u4e00\u70b9\uff0c\u56e0\u4e3a\u5bf9\u5927\u90e8\u5206\u4fe1\u606f\u90fd\u6709\u8003\u8651\uff0c\u4f46\u8003\u8651\u7a0b\u5ea6\u4e0d\u4e00\u6837\uff0c\u6240\u4ee5\u76f8\u5bf9\u6765\u8bf4\u8ba1\u7b97\u91cf\u6bd4\u8f83\u5927\u3002 \u786c\u6ce8\u610f\u673a\u5236(Hard/Local Attention,[\u4e86\u89e3\u5373\u53ef]): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u975e0\u53731\uff0c\u548c\u8f6f\u6ce8\u610f\u4e0d\u540c\uff0c\u786c\u6ce8\u610f\u673a\u5236\u53ea\u8003\u8651\u90a3\u90e8\u5206\u9700\u8981\u5173\u6ce8\uff0c\u54ea\u90e8\u5206\u4e0d\u5173\u6ce8\uff0c\u4e5f\u5c31\u662f\u76f4\u63a5\u820d\u5f03\u6389\u4e00\u4e9b\u4e0d\u76f8\u5173\u9879\u3002\u4f18\u52bf\u5728\u4e8e\u53ef\u4ee5\u51cf\u5c11\u4e00\u5b9a\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u6709\u53ef\u80fd\u4e22\u5931\u6389\u4e00\u4e9b\u672c\u5e94\u8be5\u6ce8\u610f\u7684\u4fe1\u606f\u3002 \u81ea\u6ce8\u610f\u529b\u673a\u5236( Self/Intra Attention): \u5bf9\u6bcf\u4e2a\u8f93\u5165\u9879\u5206\u914d\u7684\u6743\u91cd\u53d6\u51b3\u4e8e\u8f93\u5165\u9879\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5373\u901a\u8fc7\u8f93\u5165\u9879\u5185\u90e8\u7684\"\u8868\u51b3\"\u6765\u51b3\u5b9a\u5e94\u8be5\u5173\u6ce8\u54ea\u4e9b\u8f93\u5165\u9879\u3002\u548c\u524d\u4e24\u79cd\u76f8\u6bd4\uff0c\u5728\u5904\u7406\u5f88\u957f\u7684\u8f93\u5165\u65f6\uff0c\u5177\u6709\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf\u3002","title":"4 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u4ee5\u53ca\u5e38\u89c1\u7684\u8ba1\u7b97\u89c4\u5219 \u4e86\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236\u53ca\u5176\u4f5c\u7528 \u638c\u63e1\u6ce8\u610f\u529b\u673a\u5236\u7684\u5b9e\u73b0\u6b65\u9aa4 1 \u6ce8\u610f\u529b\u673a\u5236\u89c4\u5219 \u00b6 \u5b83\u9700\u8981\u4e09\u4e2a\u6307\u5b9a\u7684\u8f93\u5165Q(query), K(key), V(value), \u7136\u540e\u901a\u8fc7\u8ba1\u7b97\u516c\u5f0f\u5f97\u5230\u6ce8\u610f\u529b\u7684\u7ed3\u679c, \u8fd9\u4e2a\u7ed3\u679c\u4ee3\u8868query\u5728key\u548cvalue\u4f5c\u7528\u4e0b\u7684\u6ce8\u610f\u529b\u8868\u793a. \u5f53\u8f93\u5165\u7684Q=K=V\u65f6, \u79f0\u4f5c\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\uff1b\u5f53Q\u3001K\u3001V\u4e0d\u76f8\u7b49\u65f6\u79f0\u4e3a\u4e00\u822c\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219 \u4f8b\u5b50\uff1aseq2seq\u67b6\u6784\u7ffb\u8bd1\u5e94\u7528\u4e2d\u7684Q\u3001K\u3001V\u89e3\u91ca seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002 \u56fe\u4e2d\u8868\u793a\u7684\u662f\u4e00\u4e2a\u4e2d\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1\uff1a\u6b22\u8fce \u6765 \u5317\u4eac \u2192 welcome to BeiJing\u3002\u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"\u6b22\u8fce \u6765 \u5317\u4eac\"\uff0c\u901a\u8fc7GRU\u6a21\u578b\u83b7\u5f97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6700\u540e\u5c06\u5b83\u4eec\u62fc\u63a5\u6210\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\uff1b\u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00. \u5728\u4e0a\u8ff0\u673a\u5668\u7ffb\u8bd1\u67b6\u6784\u4e2d\u52a0\u5165Attention\u7684\u65b9\u5f0f\u6709\u4e24\u79cd\uff1a \u7b2c\u4e00\u79cdtensorflow\u7248\u672c(\u4f20\u7edf\u65b9\u5f0f)\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u4e0a\u56fe\u7ffb\u8bd1\u5e94\u7528\u4e2d\u7684Q\u3001K\u3001V\u89e3\u91ca \u67e5\u8be2\u5f20\u91cfQ: \u89e3\u7801\u5668\u6bcf\u4e00\u6b65\u8f93\u51fa\u6216\u8005\u662f\u5f53\u524d\u8f93\u5165\u7684x \u952e\u5f20\u91cfK: \u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \u503c\u5f20\u91cfV:\u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \u7b2c\u4e8c\u79cdPytorch\u7248\u672c(\u6539\u8fdb\u7248)\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u4e0a\u56fe\u7ffb\u8bd1\u5e94\u7528\u4e2d\u7684Q\u3001K\u3001V\u89e3\u91ca \u67e5\u8be2\u5f20\u91cfQ: \u89e3\u7801\u5668\u6bcf\u4e00\u6b65\u7684\u8f93\u51fa\u6216\u8005\u662f\u5f53\u524d\u8f93\u5165\u7684x \u952e\u5f20\u91cfK: \u89e3\u7801\u5668\u4e0a\u4e00\u6b65\u7684\u9690\u85cf\u5c42\u8f93\u51fa \u503c\u5f20\u91cfV:\u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u8f93\u51fa\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \u4e24\u4e2a\u7248\u672c\u5bf9\u6bd4\uff1a pytorch\u7248\u672c\u7684\u662f\u4e58\u578battention\uff0ctensorflow\u7248\u672c\u7684\u662f\u52a0\u578battention\u3002pytorch\u8fd9\u91cc\u76f4\u63a5\u5c06\u4e0e\u4e0a\u4e00\u4e2aunit\u9690\u72b6\u6001prev_hidden\u62fc\u63a5\u8d77\u6765\u2716W\u5f97\u5230score\uff0c\u4e4b\u540e\u5c06score\u8fc7softmax\u5f97\u5230attenion_weights. \u89e3\u7801\u8fc7\u7a0b\u5982\u4e0b\uff1a \uff081\uff09\u91c7\u7528\u81ea\u56de\u5f52\u673a\u5236\uff0c\u6bd4\u5982\uff1a\u8f93\u5165\u201cgo\u201d\u6765\u9884\u6d4b\u201cwelcome\u201d\uff0c\u8f93\u5165\u201cwelcome\u201d\u6765\u9884\u6d4b\"to\",\u8f93\u5165\u201cto\u201d\u6765\u9884\u6d4b\u201cBeijing\u201d\u3002\u5728\u8f93\u5165\u201cwelcome\u201d\u6765\u9884\u6d4b\"to\"\u89e3\u7801\u4e2d\uff0c\u53ef\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236 \uff082\uff09\u67e5\u8be2\u5f20\u91cfQ\uff1a\u4e00\u822c\u53ef\u4ee5\u662f\u201cwelcome\u201d\u8bcd\u5d4c\u5165\u5c42\u4ee5\u540e\u7684\u7ed3\u679c\uff0c\u67e5\u8be2\u5f20\u91cfQ\u4e3a\u751f\u6210\u8c01\u5c31\u662f\u8c01\u7684\u67e5\u8be2\u5f20\u91cf\uff08\u6bd4\u5982\u8fd9\u91cc\u4e3a\u4e86\u751f\u6210\u201cto\u201d\uff0c\u5219\u67e5\u8be2\u5f20\u91cf\u5c31\u662f\u201cto\u201d\u7684\u67e5\u8be2\u5f20\u91cf\uff0c\u8bf7\u4ed4\u7ec6\u4f53\u4f1a\u8fd9\u4e00\u70b9\uff09 \uff083\uff09 \u952e\u5411\u91cfK\uff1a\u4e00\u822c\u53ef\u4ee5\u662f\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u85cf\u5c42\u8f93\u51fa \uff084\uff09\u503c\u5411\u91cfV\uff1a\u4e00\u822c\u53ef\u4ee5\u662f\u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \uff085\uff09\u67e5\u8be2\u5f20\u91cfQ\u6765\u751f\u6210\u201cto\u201d\uff0c\u53bb\u68c0\u7d22\u201cto\u201d\u5355\u8bcd\u548c\u201c\u6b22\u8fce\u201d\u3001\u201c\u6765\u201d\u3001\u201c\u5317\u4eac\u201d\u4e09\u4e2a\u5355\u8bcd\u7684\u6743\u91cd\u5206\u5e03\uff0c\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\uff08\u7528\u6743\u91cd\u5206\u5e03 \u4e58\u4ee5\u5185\u5bb9V\uff09 1.3 \u5e38\u89c1\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219 \u00b6 \u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. $$ Attention(Q,K,V)=Softmax(Linear([Q,K]))\\cdot V $$ \u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316\u540e\u518d\u4f7f\u7528tanh\u51fd\u6570\u6fc0\u6d3b, \u7136\u540e\u518d\u8fdb\u884c\u5185\u90e8\u6c42\u548c, \u6700\u540e\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u518d\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. $$ Attention(Q,K,V)=Softmax(sum(tanh(Linear([Q,K]))))\\cdot V $$ \u5c06Q\u4e0eK\u7684\u8f6c\u7f6e\u505a\u70b9\u79ef\u8fd0\u7b97, \u7136\u540e\u9664\u4ee5\u4e00\u4e2a\u7f29\u653e\u7cfb\u6570, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. $$ Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V $$ \u8bf4\u660e\uff1a\u5f53\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635\u548cV\u90fd\u662f\u4e09\u7ef4\u5f20\u91cf\u4e14\u7b2c\u4e00\u7ef4\u4ee3\u8868\u4e3abatch\u6761\u6570\u65f6, \u5219\u505abmm\u8fd0\u7b97.bmm\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u5f20\u91cf\u4e58\u6cd5\u8fd0\u7b97. bmm\u8fd0\u7b97\u6f14\u793a: # \u5982\u679c\u53c2\u65701\u5f62\u72b6\u662f(b \u00d7 n \u00d7 m), \u53c2\u65702\u5f62\u72b6\u662f(b \u00d7 m \u00d7 p), \u5219\u8f93\u51fa\u4e3a(b \u00d7 n \u00d7 p) >>> input = torch . randn ( 10 , 3 , 4 ) >>> mat2 = torch . randn ( 10 , 4 , 5 ) >>> res = torch . bmm ( input , mat2 ) >>> res . size () torch . Size ([ 10 , 3 , 5 ]) 2 \u4ec0\u4e48\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6ce8\u610f\u529b\u673a\u5236 \u00b6 \u6ce8\u610f\u529b\u673a\u5236\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u80fd\u591f\u5e94\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u7684\u8f7d\u4f53, \u540c\u65f6\u5305\u62ec\u4e00\u4e9b\u5fc5\u8981\u7684\u5168\u8fde\u63a5\u5c42\u4ee5\u53ca\u76f8\u5173\u5f20\u91cf\u5904\u7406, \u4f7f\u5176\u4e0e\u5e94\u7528\u7f51\u7edc\u878d\u4e3a\u4e00\u4f53. \u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u6ce8\u610f\u529b\u673a\u5236\u79f0\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236. \u8bf4\u660e: NLP\u9886\u57df\u4e2d, \u5f53\u524d\u7684\u6ce8\u610f\u529b\u673a\u5236\u5927\u591a\u6570\u5e94\u7528\u4e8eseq2seq\u67b6\u6784, \u5373\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b. \u8bf7\u601d\u8003\uff1a\u4e3a\u4ec0\u4e48\u8981\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff1f 1\u3001rnn\u7b49\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u968f\u7740\u65f6\u95f4\u6b65\u7684\u589e\u957f\uff0c\u524d\u9762\u5355\u8bcd\u7684\u7279\u5f81\u4f1a\u9057\u5fd8\uff0c\u9020\u6210\u5bf9\u53e5\u5b50\u7279\u5f81\u63d0\u53d6\u4e0d\u5145\u5206 2\u3001rnn\u7b49\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u662f\u4e00\u4e2a\u65f6\u95f4\u6b65\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u63d0\u53d6\u5e8f\u5217\u7279\u5f81\uff0c\u6548\u7387\u4f4e\u4e0b 3\u3001\u7814\u7a76\u8005\u5f00\u59cb\u601d\u8003\uff0c\u80fd\u4e0d\u80fd\u5bf932\u4e2a\u5355\u8bcd\uff08\u5e8f\u5217\uff09\u540c\u65f6\u63d0\u53d6\u4e8b\u7269\u7279\u5f81\uff0c\u800c\u4e14\u8fd8\u662f\u5e76\u884c\u7684\uff0c\u6240\u4ee5\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff01 3 \u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528 \u00b6 \u5728\u89e3\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u80fd\u591f\u6839\u636e\u6a21\u578b\u76ee\u6807\u6709\u6548\u7684\u805a\u7126\u7f16\u7801\u5668\u7684\u8f93\u51fa\u7ed3\u679c, \u5f53\u5176\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u65f6\u63d0\u5347\u6548\u679c. \u6539\u5584\u4ee5\u5f80\u7f16\u7801\u5668\u8f93\u51fa\u662f\u5355\u4e00\u5b9a\u957f\u5f20\u91cf, \u65e0\u6cd5\u5b58\u50a8\u8fc7\u591a\u4fe1\u606f\u7684\u60c5\u51b5. \u5728\u7f16\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u4e3b\u8981\u89e3\u51b3\u8868\u5f81\u95ee\u9898, \u76f8\u5f53\u4e8e\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5f97\u5230\u8f93\u5165\u7684\u6ce8\u610f\u529b\u8868\u793a. \u4e00\u822c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b(self-attention). \u6ce8\u610f\u529b\u673a\u5236\u5728\u7f51\u7edc\u4e2d\u5b9e\u73b0\u7684\u56fe\u5f62\u8868\u793a: 4 \u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6b65\u9aa4 \u00b6 4.1 \u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65: \u6839\u636e\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219, \u5bf9Q\uff0cK\uff0cV\u8fdb\u884c\u76f8\u5e94\u7684\u8ba1\u7b97. \u7b2c\u4e8c\u6b65: \u6839\u636e\u7b2c\u4e00\u6b65\u91c7\u7528\u7684\u8ba1\u7b97\u65b9\u6cd5, \u5982\u679c\u662f\u62fc\u63a5\u65b9\u6cd5\uff0c\u5219\u9700\u8981\u5c06Q\u4e0e\u7b2c\u4e8c\u6b65\u7684\u8ba1\u7b97\u7ed3\u679c\u518d\u8fdb\u884c\u62fc\u63a5, \u5982\u679c\u662f\u8f6c\u7f6e\u70b9\u79ef, \u4e00\u822c\u662f\u81ea\u6ce8\u610f\u529b, Q\u4e0eV\u76f8\u540c, \u5219\u4e0d\u9700\u8981\u8fdb\u884c\u4e0eQ\u7684\u62fc\u63a5. \u7b2c\u4e09\u6b65: \u6700\u540e\u4e3a\u4e86\u4f7f\u6574\u4e2aattention\u673a\u5236\u6309\u7167\u6307\u5b9a\u5c3a\u5bf8\u8f93\u51fa, \u4f7f\u7528\u7ebf\u6027\u5c42\u4f5c\u7528\u5728\u7b2c\u4e8c\u6b65\u7684\u7ed3\u679c\u4e0a\u505a\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362, \u5f97\u5230\u6700\u7ec8\u5bf9Q\u7684\u6ce8\u610f\u529b\u8868\u793a. 4.2 \u4ee3\u7801\u5b9e\u73b0 \u00b6 \u5e38\u89c1\u6ce8\u610f\u529b\u673a\u5236\u7684\u4ee3\u7801\u5206\u6790: # \u4efb\u52a1\u63cf\u8ff0\uff1a # \u6709QKV\uff1av\u662f\u5185\u5bb9\u6bd4\u598232\u4e2a\u5355\u8bcd\uff0c\u6bcf\u4e2a\u5355\u8bcd64\u4e2a\u7279\u5f81\uff0ck\u662f32\u4e2a\u5355\u8bcd\u7684\u7d22\u5f15\uff0cq\u662f\u67e5\u8be2\u5f20\u91cf # \u6211\u4eec\u7684\u4efb\u52a1\uff1a\u8f93\u5165\u67e5\u8be2\u5f20\u91cfq\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6765\u8ba1\u7b97\u5982\u4e0b\u4fe1\u606f\uff1a # 1\u3001\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03\uff1a\u67e5\u8be2\u5f20\u91cfq\u548c\u5176\u4ed632\u4e2a\u5355\u8bcd\u76f8\u5173\u6027\uff08\u76f8\u8bc6\u5ea6\uff09 # 2\u3001\u67e5\u8be2\u5f20\u91cfq\u7684\u7ed3\u679c\u8868\u793a\uff1a\u6709\u4e00\u4e2a\u666e\u901a\u7684q\u5347\u7ea7\u6210\u4e00\u4e2a\u66f4\u5f3a\u5927q\uff1b\u7528q\u548cv\u505abmm\u8fd0\u7b97 # 3 \u6ce8\u610f\uff1a\u67e5\u8be2\u5f20\u91cfq\u67e5\u8be2\u7684\u76ee\u6807\u662f\u8c01\uff0c\u5c31\u662f\u8c01\u7684\u67e5\u8be2\u5f20\u91cf\u3002 # eg\uff1a\u6bd4\u5982\u67e5\u8be2\u5f20\u91cfq\u662f\u6765\u67e5\u8be2\u5355\u8bcd\"\u6211\"\uff0c\u5219q\u5c31\u662f\u6211\u7684\u67e5\u8be2\u5f20\u91cf import torch import torch.nn as nn import torch.nn.functional as F # MyAtt\u7c7b\u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, query_size, key_size, value_size1, value_size2, output_size) # \u51c6\u59072\u4e2a\u7ebf\u6027\u5c42 \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03self.attn \u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8fdb\u884c\u8f93\u51fa\u5c42 self.attn_combine # 2 forward(self, Q, K, V): # \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,32] # \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,64] # q \u4e0e attn_applied \u878d\u5408\uff0c\u518d\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,32] # \u8fd4\u56de\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput:[1,1,32], \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights:[1,32] class MyAtt ( nn . Module ): # 32 32 32 64 32 def __init__ ( self , query_size , key_size , value_size1 , value_size2 , output_size ): super ( MyAtt , self ) . __init__ () self . query_size = query_size self . key_size = key_size self . value_size1 = value_size1 self . value_size2 = value_size2 self . output_size = output_size # \u7ebf\u6027\u5c421 \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 self . attn = nn . Linear ( self . query_size + self . key_size , self . value_size1 ) # \u7ebf\u6027\u5c422 \u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa\u5c42 self.attn_combine self . attn_combine = nn . Linear ( self . query_size + self . value_size2 , output_size ) def forward ( self , Q , K , V ): # 1 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,32] # [1,1,32],[1,1,32]--> [1,32],[1,32]->[1,64] # [1,64] --> [1,32] # tmp1 = torch.cat( (Q[0], K[0]), dim=1) # tmp2 = self.attn(tmp1) # tmp3 = F.softmax(tmp2, dim=1) attn_weights = F . softmax ( self . attn ( torch . cat ( ( Q [ 0 ], K [ 0 ]), dim =- 1 )), dim =- 1 ) # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,64] # [1,1,32] * [1,32,64] ---> [1,1,64] attn_applied = torch . bmm ( attn_weights . unsqueeze ( 0 ), V ) # 3 q \u4e0e attn_applied \u878d\u5408\uff0c\u518d\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,64] # 3-1 q\u4e0e\u7ed3\u679c\u8868\u793a\u62fc\u63a5 [1,32],[1,64] ---> [1,96] output = torch . cat (( Q [ 0 ], attn_applied [ 0 ]), dim =- 1 ) # 3-2 shape [1,96] ---> [1,32] output = self . attn_combine ( output ) . unsqueeze ( 0 ) # 4 \u8fd4\u56de\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput:[1,1,32], \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights:[1,32] return output , attn_weights \u8c03\u7528: if __name__ == '__main__' : query_size = 32 key_size = 32 value_size1 = 32 # 32\u4e2a\u5355\u8bcd value_size2 = 64 # 64\u4e2a\u7279\u5f81 output_size = 32 Q = torch . randn ( 1 , 1 , 32 ) K = torch . randn ( 1 , 1 , 32 ) V = torch . randn ( 1 , 32 , 64 ) # V = torch.randn(1, value_size1, value_size2) # 1 \u5b9e\u4f8b\u5316\u6ce8\u610f\u529b\u7c7b \u5bf9\u8c61 myattobj = MyAtt ( query_size , key_size , value_size1 , value_size2 , output_size ) # 2 \u628aQKV\u6570\u636e\u6254\u7ed9\u6ce8\u610f\u673a\u5236\uff0c\u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u3001\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 output , attn_weights = myattobj ( Q , K , V ) print ( '\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput--->' , output . shape , output ) print ( '\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights--->' , attn_weights . shape , attn_weights ) \u8f93\u51fa\u6548\u679c: \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput ---> torch . Size ([ 1 , 1 , 32 ]) tensor ([[[ 0.3135 , - 0.0539 , 0.0597 , - 0.0046 , - 0.3389 , - 0.1238 , 1.0385 , 0.8896 , - 0.0268 , - 0.0705 , - 0.8409 , 0.6547 , 0.5909 , - 0.6048 , 0.6303 , - 0.2233 , 0.7678 , - 0.3140 , 0.3635 , - 0.3234 , - 0.1053 , 0.5845 , 0.1163 , - 0.2203 , - 0.0812 , - 0.0868 , 0.0218 , - 0.0597 , 0.6923 , - 0.1848 , - 0.8266 , - 0.0614 ]]], grad_fn =< UnsqueezeBackward0 > ) \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights ---> torch . Size ([ 1 , 32 ]) tensor ([[ 0.0843 , 0.0174 , 0.0138 , 0.0431 , 0.0110 , 0.0308 , 0.0608 , 0.0216 , 0.0101 , 0.0406 , 0.0462 , 0.0111 , 0.0349 , 0.0065 , 0.0383 , 0.0526 , 0.0151 , 0.0193 , 0.0294 , 0.0632 , 0.0322 , 0.0072 , 0.0294 , 0.0388 , 0.0135 , 0.0443 , 0.0594 , 0.0332 , 0.0117 , 0.0168 , 0.0293 , 0.0344 ]], grad_fn =< SoftmaxBackward0 > ) \u66f4\u591a\u6709\u5173\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e94\u7528\u6211\u4eec\u5c06\u5728\u6848\u4f8b\u4e2d\u8fdb\u884c\u8be6\u5c3d\u7684\u7406\u89e3\u5206\u6790. 5 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219: \u5b83\u9700\u8981\u4e09\u4e2a\u6307\u5b9a\u7684\u8f93\u5165Q(query), K(key), V(value), \u7136\u540e\u901a\u8fc7\u8ba1\u7b97\u516c\u5f0f\u5f97\u5230\u6ce8\u610f\u529b\u7684\u7ed3\u679c, \u8fd9\u4e2a\u7ed3\u679c\u4ee3\u8868query\u5728key\u548cvalue\u4f5c\u7528\u4e0b\u7684\u6ce8\u610f\u529b\u8868\u793a. \u5f53\u8f93\u5165\u7684Q=K=V\u65f6, \u79f0\u4f5c\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219. \u5e38\u89c1\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219: \u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. \u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316\u540e\u518d\u4f7f\u7528tanh\u51fd\u6570\u6fc0\u6d3b, \u7136\u540e\u518d\u8fdb\u884c\u5185\u90e8\u6c42\u548c, \u6700\u540e\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u518d\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. \u5c06Q\u4e0eK\u7684\u8f6c\u7f6e\u505a\u70b9\u79ef\u8fd0\u7b97, \u7136\u540e\u9664\u4ee5\u4e00\u4e2a\u7f29\u653e\u7cfb\u6570, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6df1\u5ea6\u5b66\u4e60\u6ce8\u610f\u529b\u673a\u5236: \u6ce8\u610f\u529b\u673a\u5236\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u80fd\u591f\u5e94\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u7684\u8f7d\u4f53, \u540c\u65f6\u5305\u62ec\u4e00\u4e9b\u5fc5\u8981\u7684\u5168\u8fde\u63a5\u5c42\u4ee5\u53ca\u76f8\u5173\u5f20\u91cf\u5904\u7406, \u4f7f\u5176\u4e0e\u5e94\u7528\u7f51\u7edc\u878d\u4e3a\u4e00\u4f53. \u4f7f\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u6ce8\u610f\u529b\u673a\u5236\u79f0\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236. \u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528: \u5728\u89e3\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u80fd\u591f\u6839\u636e\u6a21\u578b\u76ee\u6807\u6709\u6548\u7684\u805a\u7126\u7f16\u7801\u5668\u7684\u8f93\u51fa\u7ed3\u679c, \u5f53\u5176\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u65f6\u63d0\u5347\u6548\u679c. \u6539\u5584\u4ee5\u5f80\u7f16\u7801\u5668\u8f93\u51fa\u662f\u5355\u4e00\u5b9a\u957f\u5f20\u91cf, \u65e0\u6cd5\u5b58\u50a8\u8fc7\u591a\u4fe1\u606f\u7684\u60c5\u51b5. \u5728\u7f16\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u4e3b\u8981\u89e3\u51b3\u8868\u5f81\u95ee\u9898, \u76f8\u5f53\u4e8e\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5f97\u5230\u8f93\u5165\u7684\u6ce8\u610f\u529b\u8868\u793a. \u4e00\u822c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b(self-attention). \u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u6839\u636e\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219, \u5bf9Q\uff0cK\uff0cV\u8fdb\u884c\u76f8\u5e94\u7684\u8ba1\u7b97. \u7b2c\u4e8c\u6b65: \u6839\u636e\u7b2c\u4e00\u6b65\u91c7\u7528\u7684\u8ba1\u7b97\u65b9\u6cd5, \u5982\u679c\u662f\u62fc\u63a5\u65b9\u6cd5\uff0c\u5219\u9700\u8981\u5c06Q\u4e0e\u7b2c\u4e8c\u6b65\u7684\u8ba1\u7b97\u7ed3\u679c\u518d\u8fdb\u884c\u62fc\u63a5, \u5982\u679c\u662f\u8f6c\u7f6e\u70b9\u79ef, \u4e00\u822c\u662f\u81ea\u6ce8\u610f\u529b, Q\u4e0eV\u76f8\u540c, \u5219\u4e0d\u9700\u8981\u8fdb\u884c\u4e0eQ\u7684\u62fc\u63a5. \u7b2c\u4e09\u6b65: \u6700\u540e\u4e3a\u4e86\u4f7f\u6574\u4e2aattention\u673a\u5236\u6309\u7167\u6307\u5b9a\u5c3a\u5bf8\u8f93\u51fa, \u4f7f\u7528\u7ebf\u6027\u5c42\u4f5c\u7528\u5728\u7b2c\u4e8c\u6b65\u7684\u7ed3\u679c\u4e0a\u505a\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362, \u5f97\u5230\u6700\u7ec8\u5bf9Q\u7684\u6ce8\u610f\u529b\u8868\u793a. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u5e38\u89c1\u7684\u6ce8\u610f\u529b\u673a\u5236\u7684\u7c7bAttn.","title":"7 \u6ce8\u610f\u529b\u673a\u5236\u4ecb\u7ecd2"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u4ee5\u53ca\u5e38\u89c1\u7684\u8ba1\u7b97\u89c4\u5219 \u4e86\u89e3\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u673a\u5236\u53ca\u5176\u4f5c\u7528 \u638c\u63e1\u6ce8\u610f\u529b\u673a\u5236\u7684\u5b9e\u73b0\u6b65\u9aa4","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#1","text":"\u5b83\u9700\u8981\u4e09\u4e2a\u6307\u5b9a\u7684\u8f93\u5165Q(query), K(key), V(value), \u7136\u540e\u901a\u8fc7\u8ba1\u7b97\u516c\u5f0f\u5f97\u5230\u6ce8\u610f\u529b\u7684\u7ed3\u679c, \u8fd9\u4e2a\u7ed3\u679c\u4ee3\u8868query\u5728key\u548cvalue\u4f5c\u7528\u4e0b\u7684\u6ce8\u610f\u529b\u8868\u793a. \u5f53\u8f93\u5165\u7684Q=K=V\u65f6, \u79f0\u4f5c\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\uff1b\u5f53Q\u3001K\u3001V\u4e0d\u76f8\u7b49\u65f6\u79f0\u4e3a\u4e00\u822c\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219 \u4f8b\u5b50\uff1aseq2seq\u67b6\u6784\u7ffb\u8bd1\u5e94\u7528\u4e2d\u7684Q\u3001K\u3001V\u89e3\u91ca seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002 \u56fe\u4e2d\u8868\u793a\u7684\u662f\u4e00\u4e2a\u4e2d\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1\uff1a\u6b22\u8fce \u6765 \u5317\u4eac \u2192 welcome to BeiJing\u3002\u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"\u6b22\u8fce \u6765 \u5317\u4eac\"\uff0c\u901a\u8fc7GRU\u6a21\u578b\u83b7\u5f97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6700\u540e\u5c06\u5b83\u4eec\u62fc\u63a5\u6210\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\uff1b\u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00. \u5728\u4e0a\u8ff0\u673a\u5668\u7ffb\u8bd1\u67b6\u6784\u4e2d\u52a0\u5165Attention\u7684\u65b9\u5f0f\u6709\u4e24\u79cd\uff1a \u7b2c\u4e00\u79cdtensorflow\u7248\u672c(\u4f20\u7edf\u65b9\u5f0f)\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u4e0a\u56fe\u7ffb\u8bd1\u5e94\u7528\u4e2d\u7684Q\u3001K\u3001V\u89e3\u91ca \u67e5\u8be2\u5f20\u91cfQ: \u89e3\u7801\u5668\u6bcf\u4e00\u6b65\u8f93\u51fa\u6216\u8005\u662f\u5f53\u524d\u8f93\u5165\u7684x \u952e\u5f20\u91cfK: \u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \u503c\u5f20\u91cfV:\u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \u7b2c\u4e8c\u79cdPytorch\u7248\u672c(\u6539\u8fdb\u7248)\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u4e0a\u56fe\u7ffb\u8bd1\u5e94\u7528\u4e2d\u7684Q\u3001K\u3001V\u89e3\u91ca \u67e5\u8be2\u5f20\u91cfQ: \u89e3\u7801\u5668\u6bcf\u4e00\u6b65\u7684\u8f93\u51fa\u6216\u8005\u662f\u5f53\u524d\u8f93\u5165\u7684x \u952e\u5f20\u91cfK: \u89e3\u7801\u5668\u4e0a\u4e00\u6b65\u7684\u9690\u85cf\u5c42\u8f93\u51fa \u503c\u5f20\u91cfV:\u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u8f93\u51fa\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \u4e24\u4e2a\u7248\u672c\u5bf9\u6bd4\uff1a pytorch\u7248\u672c\u7684\u662f\u4e58\u578battention\uff0ctensorflow\u7248\u672c\u7684\u662f\u52a0\u578battention\u3002pytorch\u8fd9\u91cc\u76f4\u63a5\u5c06\u4e0e\u4e0a\u4e00\u4e2aunit\u9690\u72b6\u6001prev_hidden\u62fc\u63a5\u8d77\u6765\u2716W\u5f97\u5230score\uff0c\u4e4b\u540e\u5c06score\u8fc7softmax\u5f97\u5230attenion_weights. \u89e3\u7801\u8fc7\u7a0b\u5982\u4e0b\uff1a \uff081\uff09\u91c7\u7528\u81ea\u56de\u5f52\u673a\u5236\uff0c\u6bd4\u5982\uff1a\u8f93\u5165\u201cgo\u201d\u6765\u9884\u6d4b\u201cwelcome\u201d\uff0c\u8f93\u5165\u201cwelcome\u201d\u6765\u9884\u6d4b\"to\",\u8f93\u5165\u201cto\u201d\u6765\u9884\u6d4b\u201cBeijing\u201d\u3002\u5728\u8f93\u5165\u201cwelcome\u201d\u6765\u9884\u6d4b\"to\"\u89e3\u7801\u4e2d\uff0c\u53ef\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236 \uff082\uff09\u67e5\u8be2\u5f20\u91cfQ\uff1a\u4e00\u822c\u53ef\u4ee5\u662f\u201cwelcome\u201d\u8bcd\u5d4c\u5165\u5c42\u4ee5\u540e\u7684\u7ed3\u679c\uff0c\u67e5\u8be2\u5f20\u91cfQ\u4e3a\u751f\u6210\u8c01\u5c31\u662f\u8c01\u7684\u67e5\u8be2\u5f20\u91cf\uff08\u6bd4\u5982\u8fd9\u91cc\u4e3a\u4e86\u751f\u6210\u201cto\u201d\uff0c\u5219\u67e5\u8be2\u5f20\u91cf\u5c31\u662f\u201cto\u201d\u7684\u67e5\u8be2\u5f20\u91cf\uff0c\u8bf7\u4ed4\u7ec6\u4f53\u4f1a\u8fd9\u4e00\u70b9\uff09 \uff083\uff09 \u952e\u5411\u91cfK\uff1a\u4e00\u822c\u53ef\u4ee5\u662f\u4e0a\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u85cf\u5c42\u8f93\u51fa \uff084\uff09\u503c\u5411\u91cfV\uff1a\u4e00\u822c\u53ef\u4ee5\u662f\u7f16\u7801\u90e8\u5206\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u7ed3\u679c\u7ec4\u5408\u800c\u6210 \uff085\uff09\u67e5\u8be2\u5f20\u91cfQ\u6765\u751f\u6210\u201cto\u201d\uff0c\u53bb\u68c0\u7d22\u201cto\u201d\u5355\u8bcd\u548c\u201c\u6b22\u8fce\u201d\u3001\u201c\u6765\u201d\u3001\u201c\u5317\u4eac\u201d\u4e09\u4e2a\u5355\u8bcd\u7684\u6743\u91cd\u5206\u5e03\uff0c\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\uff08\u7528\u6743\u91cd\u5206\u5e03 \u4e58\u4ee5\u5185\u5bb9V\uff09","title":"1 \u6ce8\u610f\u529b\u673a\u5236\u89c4\u5219"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#13","text":"\u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. $$ Attention(Q,K,V)=Softmax(Linear([Q,K]))\\cdot V $$ \u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316\u540e\u518d\u4f7f\u7528tanh\u51fd\u6570\u6fc0\u6d3b, \u7136\u540e\u518d\u8fdb\u884c\u5185\u90e8\u6c42\u548c, \u6700\u540e\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u518d\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. $$ Attention(Q,K,V)=Softmax(sum(tanh(Linear([Q,K]))))\\cdot V $$ \u5c06Q\u4e0eK\u7684\u8f6c\u7f6e\u505a\u70b9\u79ef\u8fd0\u7b97, \u7136\u540e\u9664\u4ee5\u4e00\u4e2a\u7f29\u653e\u7cfb\u6570, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. $$ Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V $$ \u8bf4\u660e\uff1a\u5f53\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635\u548cV\u90fd\u662f\u4e09\u7ef4\u5f20\u91cf\u4e14\u7b2c\u4e00\u7ef4\u4ee3\u8868\u4e3abatch\u6761\u6570\u65f6, \u5219\u505abmm\u8fd0\u7b97.bmm\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u5f20\u91cf\u4e58\u6cd5\u8fd0\u7b97. bmm\u8fd0\u7b97\u6f14\u793a: # \u5982\u679c\u53c2\u65701\u5f62\u72b6\u662f(b \u00d7 n \u00d7 m), \u53c2\u65702\u5f62\u72b6\u662f(b \u00d7 m \u00d7 p), \u5219\u8f93\u51fa\u4e3a(b \u00d7 n \u00d7 p) >>> input = torch . randn ( 10 , 3 , 4 ) >>> mat2 = torch . randn ( 10 , 4 , 5 ) >>> res = torch . bmm ( input , mat2 ) >>> res . size () torch . Size ([ 10 , 3 , 5 ])","title":"1.3 \u5e38\u89c1\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#2","text":"\u6ce8\u610f\u529b\u673a\u5236\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u80fd\u591f\u5e94\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u7684\u8f7d\u4f53, \u540c\u65f6\u5305\u62ec\u4e00\u4e9b\u5fc5\u8981\u7684\u5168\u8fde\u63a5\u5c42\u4ee5\u53ca\u76f8\u5173\u5f20\u91cf\u5904\u7406, \u4f7f\u5176\u4e0e\u5e94\u7528\u7f51\u7edc\u878d\u4e3a\u4e00\u4f53. \u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u6ce8\u610f\u529b\u673a\u5236\u79f0\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236. \u8bf4\u660e: NLP\u9886\u57df\u4e2d, \u5f53\u524d\u7684\u6ce8\u610f\u529b\u673a\u5236\u5927\u591a\u6570\u5e94\u7528\u4e8eseq2seq\u67b6\u6784, \u5373\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b. \u8bf7\u601d\u8003\uff1a\u4e3a\u4ec0\u4e48\u8981\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff1f 1\u3001rnn\u7b49\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u968f\u7740\u65f6\u95f4\u6b65\u7684\u589e\u957f\uff0c\u524d\u9762\u5355\u8bcd\u7684\u7279\u5f81\u4f1a\u9057\u5fd8\uff0c\u9020\u6210\u5bf9\u53e5\u5b50\u7279\u5f81\u63d0\u53d6\u4e0d\u5145\u5206 2\u3001rnn\u7b49\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u662f\u4e00\u4e2a\u65f6\u95f4\u6b65\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u63d0\u53d6\u5e8f\u5217\u7279\u5f81\uff0c\u6548\u7387\u4f4e\u4e0b 3\u3001\u7814\u7a76\u8005\u5f00\u59cb\u601d\u8003\uff0c\u80fd\u4e0d\u80fd\u5bf932\u4e2a\u5355\u8bcd\uff08\u5e8f\u5217\uff09\u540c\u65f6\u63d0\u53d6\u4e8b\u7269\u7279\u5f81\uff0c\u800c\u4e14\u8fd8\u662f\u5e76\u884c\u7684\uff0c\u6240\u4ee5\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff01","title":"2 \u4ec0\u4e48\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6ce8\u610f\u529b\u673a\u5236"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#3","text":"\u5728\u89e3\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u80fd\u591f\u6839\u636e\u6a21\u578b\u76ee\u6807\u6709\u6548\u7684\u805a\u7126\u7f16\u7801\u5668\u7684\u8f93\u51fa\u7ed3\u679c, \u5f53\u5176\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u65f6\u63d0\u5347\u6548\u679c. \u6539\u5584\u4ee5\u5f80\u7f16\u7801\u5668\u8f93\u51fa\u662f\u5355\u4e00\u5b9a\u957f\u5f20\u91cf, \u65e0\u6cd5\u5b58\u50a8\u8fc7\u591a\u4fe1\u606f\u7684\u60c5\u51b5. \u5728\u7f16\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u4e3b\u8981\u89e3\u51b3\u8868\u5f81\u95ee\u9898, \u76f8\u5f53\u4e8e\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5f97\u5230\u8f93\u5165\u7684\u6ce8\u610f\u529b\u8868\u793a. \u4e00\u822c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b(self-attention). \u6ce8\u610f\u529b\u673a\u5236\u5728\u7f51\u7edc\u4e2d\u5b9e\u73b0\u7684\u56fe\u5f62\u8868\u793a:","title":"3 \u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#4","text":"","title":"4 \u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6b65\u9aa4"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#41","text":"\u7b2c\u4e00\u6b65: \u6839\u636e\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219, \u5bf9Q\uff0cK\uff0cV\u8fdb\u884c\u76f8\u5e94\u7684\u8ba1\u7b97. \u7b2c\u4e8c\u6b65: \u6839\u636e\u7b2c\u4e00\u6b65\u91c7\u7528\u7684\u8ba1\u7b97\u65b9\u6cd5, \u5982\u679c\u662f\u62fc\u63a5\u65b9\u6cd5\uff0c\u5219\u9700\u8981\u5c06Q\u4e0e\u7b2c\u4e8c\u6b65\u7684\u8ba1\u7b97\u7ed3\u679c\u518d\u8fdb\u884c\u62fc\u63a5, \u5982\u679c\u662f\u8f6c\u7f6e\u70b9\u79ef, \u4e00\u822c\u662f\u81ea\u6ce8\u610f\u529b, Q\u4e0eV\u76f8\u540c, \u5219\u4e0d\u9700\u8981\u8fdb\u884c\u4e0eQ\u7684\u62fc\u63a5. \u7b2c\u4e09\u6b65: \u6700\u540e\u4e3a\u4e86\u4f7f\u6574\u4e2aattention\u673a\u5236\u6309\u7167\u6307\u5b9a\u5c3a\u5bf8\u8f93\u51fa, \u4f7f\u7528\u7ebf\u6027\u5c42\u4f5c\u7528\u5728\u7b2c\u4e8c\u6b65\u7684\u7ed3\u679c\u4e0a\u505a\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362, \u5f97\u5230\u6700\u7ec8\u5bf9Q\u7684\u6ce8\u610f\u529b\u8868\u793a.","title":"4.1 \u6b65\u9aa4"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#42","text":"\u5e38\u89c1\u6ce8\u610f\u529b\u673a\u5236\u7684\u4ee3\u7801\u5206\u6790: # \u4efb\u52a1\u63cf\u8ff0\uff1a # \u6709QKV\uff1av\u662f\u5185\u5bb9\u6bd4\u598232\u4e2a\u5355\u8bcd\uff0c\u6bcf\u4e2a\u5355\u8bcd64\u4e2a\u7279\u5f81\uff0ck\u662f32\u4e2a\u5355\u8bcd\u7684\u7d22\u5f15\uff0cq\u662f\u67e5\u8be2\u5f20\u91cf # \u6211\u4eec\u7684\u4efb\u52a1\uff1a\u8f93\u5165\u67e5\u8be2\u5f20\u91cfq\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6765\u8ba1\u7b97\u5982\u4e0b\u4fe1\u606f\uff1a # 1\u3001\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03\uff1a\u67e5\u8be2\u5f20\u91cfq\u548c\u5176\u4ed632\u4e2a\u5355\u8bcd\u76f8\u5173\u6027\uff08\u76f8\u8bc6\u5ea6\uff09 # 2\u3001\u67e5\u8be2\u5f20\u91cfq\u7684\u7ed3\u679c\u8868\u793a\uff1a\u6709\u4e00\u4e2a\u666e\u901a\u7684q\u5347\u7ea7\u6210\u4e00\u4e2a\u66f4\u5f3a\u5927q\uff1b\u7528q\u548cv\u505abmm\u8fd0\u7b97 # 3 \u6ce8\u610f\uff1a\u67e5\u8be2\u5f20\u91cfq\u67e5\u8be2\u7684\u76ee\u6807\u662f\u8c01\uff0c\u5c31\u662f\u8c01\u7684\u67e5\u8be2\u5f20\u91cf\u3002 # eg\uff1a\u6bd4\u5982\u67e5\u8be2\u5f20\u91cfq\u662f\u6765\u67e5\u8be2\u5355\u8bcd\"\u6211\"\uff0c\u5219q\u5c31\u662f\u6211\u7684\u67e5\u8be2\u5f20\u91cf import torch import torch.nn as nn import torch.nn.functional as F # MyAtt\u7c7b\u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, query_size, key_size, value_size1, value_size2, output_size) # \u51c6\u59072\u4e2a\u7ebf\u6027\u5c42 \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03self.attn \u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8fdb\u884c\u8f93\u51fa\u5c42 self.attn_combine # 2 forward(self, Q, K, V): # \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,32] # \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,64] # q \u4e0e attn_applied \u878d\u5408\uff0c\u518d\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,32] # \u8fd4\u56de\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput:[1,1,32], \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights:[1,32] class MyAtt ( nn . Module ): # 32 32 32 64 32 def __init__ ( self , query_size , key_size , value_size1 , value_size2 , output_size ): super ( MyAtt , self ) . __init__ () self . query_size = query_size self . key_size = key_size self . value_size1 = value_size1 self . value_size2 = value_size2 self . output_size = output_size # \u7ebf\u6027\u5c421 \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 self . attn = nn . Linear ( self . query_size + self . key_size , self . value_size1 ) # \u7ebf\u6027\u5c422 \u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa\u5c42 self.attn_combine self . attn_combine = nn . Linear ( self . query_size + self . value_size2 , output_size ) def forward ( self , Q , K , V ): # 1 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,32] # [1,1,32],[1,1,32]--> [1,32],[1,32]->[1,64] # [1,64] --> [1,32] # tmp1 = torch.cat( (Q[0], K[0]), dim=1) # tmp2 = self.attn(tmp1) # tmp3 = F.softmax(tmp2, dim=1) attn_weights = F . softmax ( self . attn ( torch . cat ( ( Q [ 0 ], K [ 0 ]), dim =- 1 )), dim =- 1 ) # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,64] # [1,1,32] * [1,32,64] ---> [1,1,64] attn_applied = torch . bmm ( attn_weights . unsqueeze ( 0 ), V ) # 3 q \u4e0e attn_applied \u878d\u5408\uff0c\u518d\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,64] # 3-1 q\u4e0e\u7ed3\u679c\u8868\u793a\u62fc\u63a5 [1,32],[1,64] ---> [1,96] output = torch . cat (( Q [ 0 ], attn_applied [ 0 ]), dim =- 1 ) # 3-2 shape [1,96] ---> [1,32] output = self . attn_combine ( output ) . unsqueeze ( 0 ) # 4 \u8fd4\u56de\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput:[1,1,32], \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights:[1,32] return output , attn_weights \u8c03\u7528: if __name__ == '__main__' : query_size = 32 key_size = 32 value_size1 = 32 # 32\u4e2a\u5355\u8bcd value_size2 = 64 # 64\u4e2a\u7279\u5f81 output_size = 32 Q = torch . randn ( 1 , 1 , 32 ) K = torch . randn ( 1 , 1 , 32 ) V = torch . randn ( 1 , 32 , 64 ) # V = torch.randn(1, value_size1, value_size2) # 1 \u5b9e\u4f8b\u5316\u6ce8\u610f\u529b\u7c7b \u5bf9\u8c61 myattobj = MyAtt ( query_size , key_size , value_size1 , value_size2 , output_size ) # 2 \u628aQKV\u6570\u636e\u6254\u7ed9\u6ce8\u610f\u673a\u5236\uff0c\u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u3001\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 output , attn_weights = myattobj ( Q , K , V ) print ( '\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput--->' , output . shape , output ) print ( '\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights--->' , attn_weights . shape , attn_weights ) \u8f93\u51fa\u6548\u679c: \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793aoutput ---> torch . Size ([ 1 , 1 , 32 ]) tensor ([[[ 0.3135 , - 0.0539 , 0.0597 , - 0.0046 , - 0.3389 , - 0.1238 , 1.0385 , 0.8896 , - 0.0268 , - 0.0705 , - 0.8409 , 0.6547 , 0.5909 , - 0.6048 , 0.6303 , - 0.2233 , 0.7678 , - 0.3140 , 0.3635 , - 0.3234 , - 0.1053 , 0.5845 , 0.1163 , - 0.2203 , - 0.0812 , - 0.0868 , 0.0218 , - 0.0597 , 0.6923 , - 0.1848 , - 0.8266 , - 0.0614 ]]], grad_fn =< UnsqueezeBackward0 > ) \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03attn_weights ---> torch . Size ([ 1 , 32 ]) tensor ([[ 0.0843 , 0.0174 , 0.0138 , 0.0431 , 0.0110 , 0.0308 , 0.0608 , 0.0216 , 0.0101 , 0.0406 , 0.0462 , 0.0111 , 0.0349 , 0.0065 , 0.0383 , 0.0526 , 0.0151 , 0.0193 , 0.0294 , 0.0632 , 0.0322 , 0.0072 , 0.0294 , 0.0388 , 0.0135 , 0.0443 , 0.0594 , 0.0332 , 0.0117 , 0.0168 , 0.0293 , 0.0344 ]], grad_fn =< SoftmaxBackward0 > ) \u66f4\u591a\u6709\u5173\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e94\u7528\u6211\u4eec\u5c06\u5728\u6848\u4f8b\u4e2d\u8fdb\u884c\u8be6\u5c3d\u7684\u7406\u89e3\u5206\u6790.","title":"4.2 \u4ee3\u7801\u5b9e\u73b0"},{"location":"03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html#5","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219: \u5b83\u9700\u8981\u4e09\u4e2a\u6307\u5b9a\u7684\u8f93\u5165Q(query), K(key), V(value), \u7136\u540e\u901a\u8fc7\u8ba1\u7b97\u516c\u5f0f\u5f97\u5230\u6ce8\u610f\u529b\u7684\u7ed3\u679c, \u8fd9\u4e2a\u7ed3\u679c\u4ee3\u8868query\u5728key\u548cvalue\u4f5c\u7528\u4e0b\u7684\u6ce8\u610f\u529b\u8868\u793a. \u5f53\u8f93\u5165\u7684Q=K=V\u65f6, \u79f0\u4f5c\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219. \u5e38\u89c1\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219: \u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. \u5c06Q\uff0cK\u8fdb\u884c\u7eb5\u8f74\u62fc\u63a5, \u505a\u4e00\u6b21\u7ebf\u6027\u53d8\u5316\u540e\u518d\u4f7f\u7528tanh\u51fd\u6570\u6fc0\u6d3b, \u7136\u540e\u518d\u8fdb\u884c\u5185\u90e8\u6c42\u548c, \u6700\u540e\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u518d\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. \u5c06Q\u4e0eK\u7684\u8f6c\u7f6e\u505a\u70b9\u79ef\u8fd0\u7b97, \u7136\u540e\u9664\u4ee5\u4e00\u4e2a\u7f29\u653e\u7cfb\u6570, \u518d\u4f7f\u7528softmax\u5904\u7406\u83b7\u5f97\u7ed3\u679c\u6700\u540e\u4e0eV\u505a\u5f20\u91cf\u4e58\u6cd5. \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6df1\u5ea6\u5b66\u4e60\u6ce8\u610f\u529b\u673a\u5236: \u6ce8\u610f\u529b\u673a\u5236\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u80fd\u591f\u5e94\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u7684\u8f7d\u4f53, \u540c\u65f6\u5305\u62ec\u4e00\u4e9b\u5fc5\u8981\u7684\u5168\u8fde\u63a5\u5c42\u4ee5\u53ca\u76f8\u5173\u5f20\u91cf\u5904\u7406, \u4f7f\u5176\u4e0e\u5e94\u7528\u7f51\u7edc\u878d\u4e3a\u4e00\u4f53. \u4f7f\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u6ce8\u610f\u529b\u673a\u5236\u79f0\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236. \u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528: \u5728\u89e3\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u80fd\u591f\u6839\u636e\u6a21\u578b\u76ee\u6807\u6709\u6548\u7684\u805a\u7126\u7f16\u7801\u5668\u7684\u8f93\u51fa\u7ed3\u679c, \u5f53\u5176\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u65f6\u63d0\u5347\u6548\u679c. \u6539\u5584\u4ee5\u5f80\u7f16\u7801\u5668\u8f93\u51fa\u662f\u5355\u4e00\u5b9a\u957f\u5f20\u91cf, \u65e0\u6cd5\u5b58\u50a8\u8fc7\u591a\u4fe1\u606f\u7684\u60c5\u51b5. \u5728\u7f16\u7801\u5668\u7aef\u7684\u6ce8\u610f\u529b\u673a\u5236: \u4e3b\u8981\u89e3\u51b3\u8868\u5f81\u95ee\u9898, \u76f8\u5f53\u4e8e\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5f97\u5230\u8f93\u5165\u7684\u6ce8\u610f\u529b\u8868\u793a. \u4e00\u822c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b(self-attention). \u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u6839\u636e\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219, \u5bf9Q\uff0cK\uff0cV\u8fdb\u884c\u76f8\u5e94\u7684\u8ba1\u7b97. \u7b2c\u4e8c\u6b65: \u6839\u636e\u7b2c\u4e00\u6b65\u91c7\u7528\u7684\u8ba1\u7b97\u65b9\u6cd5, \u5982\u679c\u662f\u62fc\u63a5\u65b9\u6cd5\uff0c\u5219\u9700\u8981\u5c06Q\u4e0e\u7b2c\u4e8c\u6b65\u7684\u8ba1\u7b97\u7ed3\u679c\u518d\u8fdb\u884c\u62fc\u63a5, \u5982\u679c\u662f\u8f6c\u7f6e\u70b9\u79ef, \u4e00\u822c\u662f\u81ea\u6ce8\u610f\u529b, Q\u4e0eV\u76f8\u540c, \u5219\u4e0d\u9700\u8981\u8fdb\u884c\u4e0eQ\u7684\u62fc\u63a5. \u7b2c\u4e09\u6b65: \u6700\u540e\u4e3a\u4e86\u4f7f\u6574\u4e2aattention\u673a\u5236\u6309\u7167\u6307\u5b9a\u5c3a\u5bf8\u8f93\u51fa, \u4f7f\u7528\u7ebf\u6027\u5c42\u4f5c\u7528\u5728\u7b2c\u4e8c\u6b65\u7684\u7ed3\u679c\u4e0a\u505a\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362, \u5f97\u5230\u6700\u7ec8\u5bf9Q\u7684\u6ce8\u610f\u529b\u8868\u793a. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u5e38\u89c1\u7684\u6ce8\u610f\u529b\u673a\u5236\u7684\u7c7bAttn.","title":"5 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u66f4\u6df1\u4e00\u6b65\u4e86\u89e3seq2seq\u6a21\u578b\u67b6\u6784\u548c\u7ffb\u8bd1\u6570\u636e\u96c6 \u638c\u63e1\u4f7f\u7528\u57fa\u4e8eGRU\u7684seq2seq\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u7ffb\u8bd1\u7684\u8fc7\u7a0b \u638c\u63e1Attention\u673a\u5236\u5728\u89e3\u7801\u5668\u7aef\u7684\u5b9e\u73b0\u8fc7\u7a0b 1 seq2seq\u4ecb\u7ecd \u00b6 1.1 seq2seq\u6a21\u578b\u67b6\u6784 \u00b6 seq2seq\u6a21\u578b\u67b6\u6784\u5206\u6790: seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002\u5176\u4e2d\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5185\u90e8\u5b9e\u73b0\u90fd\u4f7f\u7528\u4e86GRU\u6a21\u578b \u56fe\u4e2d\u8868\u793a\u7684\u662f\u4e00\u4e2a\u4e2d\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1\uff1a\u6b22\u8fce \u6765 \u5317\u4eac \u2192 welcome to BeiJing\u3002\u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"\u6b22\u8fce \u6765 \u5317\u4eac\"\uff0c\u901a\u8fc7GRU\u6a21\u578b\u83b7\u5f97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6700\u540e\u5c06\u5b83\u4eec\u62fc\u63a5\u6210\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\uff1b\u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00 \u6211\u4eec\u7684\u6848\u4f8b\u901a\u8fc7\u82f1\u8bd1\u6cd5\u6765\u8bb2\u89e3seq2seq\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u3002 2 \u6570\u636e\u96c6\u4ecb\u7ecd \u00b6 # \u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/\u4e0b - data/ - eng-fra-v2.txt i am from brazil . je viens du bresil . i am from france . je viens de france . i am from russia . je viens de russie . i am frying fish . je fais frire du poisson . i am not kidding . je ne blague pas . i am on duty now . maintenant je suis en service . i am on duty now . je suis actuellement en service . i am only joking . je ne fais que blaguer . i am out of time . je suis a court de temps . i am out of work . je suis au chomage . i am out of work . je suis sans travail . i am paid weekly . je suis payee a la semaine . i am pretty sure . je suis relativement sur . i am truly sorry . je suis vraiment desole . i am truly sorry . je suis vraiment desolee . 3 \u6848\u4f8b\u6b65\u9aa4 \u00b6 \u57fa\u4e8eGRU\u7684seq2seq\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u7ffb\u8bd1\u7684\u8fc7\u7a0b: \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5de5\u5177\u5305\u548c\u5de5\u5177\u51fd\u6570 \u7b2c\u4e8c\u6b65: \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u7b2c\u56db\u6b65: \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65: \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570, \u5e76\u8fdb\u884c\u6d4b\u8bd5\u4ee5\u53caAttention\u6548\u679c\u5206\u6790 1 \u5bfc\u5165\u5de5\u5177\u5305\u548c\u5de5\u5177\u51fd\u6570 \u00b6 # \u7528\u4e8e\u6b63\u5219\u8868\u8fbe\u5f0f import re # \u7528\u4e8e\u6784\u5efa\u7f51\u7edc\u7ed3\u6784\u548c\u51fd\u6570\u7684torch\u5de5\u5177\u5305 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader # torch\u4e2d\u9884\u5b9a\u4e49\u7684\u4f18\u5316\u65b9\u6cd5\u5de5\u5177\u5305 import torch.optim as optim import time # \u7528\u4e8e\u968f\u673a\u751f\u6210\u6570\u636e import random import matplotlib.pyplot as plt # \u8bbe\u5907\u9009\u62e9, \u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5728cuda\u6216\u8005cpu\u4e0a\u8fd0\u884c\u4f60\u7684\u4ee3\u7801 device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) # \u8d77\u59cb\u6807\u5fd7 SOS_token = 0 # \u7ed3\u675f\u6807\u5fd7 EOS_token = 1 # \u6700\u5927\u53e5\u5b50\u957f\u5ea6\u4e0d\u80fd\u8d85\u8fc710\u4e2a (\u5305\u542b\u6807\u70b9) MAX_LENGTH = 10 # \u6570\u636e\u6587\u4ef6\u8def\u5f84 data_path = './data/eng-fra-v2.txt' # \u6587\u672c\u6e05\u6d17\u5de5\u5177\u51fd\u6570 def normalizeString ( s ): \"\"\"\u5b57\u7b26\u4e32\u89c4\u8303\u5316\u51fd\u6570, \u53c2\u6570s\u4ee3\u8868\u4f20\u5165\u7684\u5b57\u7b26\u4e32\"\"\" s = s . lower () . strip () # \u5728.!?\u524d\u52a0\u4e00\u4e2a\u7a7a\u683c \u8fd9\u91cc\u7684\\1\u8868\u793a\u7b2c\u4e00\u4e2a\u5206\u7ec4 \u6b63\u5219\u4e2d\u7684\\num s = re . sub ( r \"([.!?])\" , r \" \\1\" , s ) # s = re.sub(r\"([.!?])\", r\" \", s) # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u5c06\u5b57\u7b26\u4e32\u4e2d \u4e0d\u662f \u5927\u5c0f\u5199\u5b57\u6bcd\u548c\u6b63\u5e38\u6807\u70b9\u7684\u90fd\u66ff\u6362\u6210\u7a7a\u683c s = re . sub ( r \"[^a-zA-Z.!?]+\" , r \" \" , s ) return s 2 \u6570\u636e\u9884\u5904\u7406 \u00b6 \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42 1 \u6e05\u6d17\u6587\u672c\u548c\u6784\u5efa\u6587\u672c\u5b57\u5178 \u00b6 \u6e05\u6d17\u6587\u672c\u548c\u6784\u5efa\u6587\u672c\u5b57\u5178\u601d\u8def\u5206\u6790 # my_getdata() \u6e05\u6d17\u6587\u672c\u6784\u5efa\u5b57\u5178\u601d\u8def\u5206\u6790 # 1 \u6309\u884c\u8bfb\u6587\u4ef6 open().read().strip().split(\\n) my_lines # 2 \u6309\u884c\u6e05\u6d17\u6587\u672c \u6784\u5efa\u8bed\u8a00\u5bf9 my_pairs[] tmppair[] # 2-1\u683c\u5f0f [['\u82f1\u6587', '\u6cd5\u6587'], ['\u82f1\u6587', '\u6cd5\u6587'], ['\u82f1\u6587', '\u6cd5\u6587'], ['\u82f1\u6587', '\u6cd5\u6587']....] # 2-2\u8c03\u7528\u6e05\u6d17\u6587\u672c\u5de5\u5177\u51fd\u6570normalizeString(s) # 3 \u904d\u5386\u8bed\u8a00\u5bf9 \u6784\u5efa\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 my_pairs->pair->pair[0].split(' ') pair[1].split(' ')->word # 3-1 english_word2index english_word_n french_word2index french_word_n # \u5176\u4e2d english_word2index = {0: \"SOS\", 1: \"EOS\"} english_word_n=2 # 3-2 english_index2word french_index2word # 4 \u8fd4\u56de\u6570\u636e\u76847\u4e2a\u7ed3\u679c # english_word2index, english_index2word, english_word_n, # french_word2index, french_index2word, french_word_n, my_pairs \u4ee3\u7801\u5b9e\u73b0 def my_getdata(): # 1 \u6309\u884c\u8bfb\u6587\u4ef6 open().read().strip().split(\\n) my_lines = open(data_path, encoding='utf-8').read().strip().split('\\n') print('my_lines--->', len(my_lines)) # 2 \u6309\u884c\u6e05\u6d17\u6587\u672c \u6784\u5efa\u8bed\u8a00\u5bf9 my_pairs # \u683c\u5f0f [['\u82f1\u6587\u53e5\u5b50', '\u6cd5\u6587\u53e5\u5b50'], ['\u82f1\u6587\u53e5\u5b50', '\u6cd5\u6587\u53e5\u5b50'], ['\u82f1\u6587\u53e5\u5b50', '\u6cd5\u6587\u53e5\u5b50'], ... ] # tmp_pair, my_pairs = [], [] # for l in my_lines: # for s in l.split('\\t'): # tmp_pair.append(normalizeString(s)) # my_pairs.append(tmp_pair) # tmp_pair = [] my_pairs = [[normalizeString(s) for s in l.split('\\t')] for l in my_lines] print('len(pairs)--->', len(my_pairs)) # \u6253\u5370\u524d4\u6761\u6570\u636e print(my_pairs[:4]) # \u6253\u5370\u7b2c8000\u6761\u7684\u82f1\u6587 \u6cd5\u6587\u6570\u636e print('my_pairs[8000][0]--->', my_pairs[8000][0]) print('my_pairs[8000][1]--->', my_pairs[8000][1]) # 3 \u904d\u5386\u8bed\u8a00\u5bf9 \u6784\u5efa\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 # 3-1 english_word2index english_word_n french_word2index french_word_n english_word2index = {\"SOS\": 0, \"EOS\": 1} english_word_n = 2 french_word2index = {\"SOS\": 0, \"EOS\": 1} french_word_n = 2 # \u904d\u5386\u8bed\u8a00\u5bf9 \u83b7\u53d6\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 for pair in my_pairs: for word in pair[0].split(' '): if word not in english_word2index: english_word2index[word] = english_word_n english_word_n += 1 for word in pair[1].split(' '): if word not in french_word2index: french_word2index[word] = french_word_n french_word_n += 1 # 3-2 english_index2word french_index2word english_index2word = {v:k for k, v in english_word2index.items()} french_index2word = {v:k for k, v in french_word2index.items()} print('len(english_word2index)-->', len(english_word2index)) print('len(french_word2index)-->', len(french_word2index)) print('english_word_n--->', english_word_n, 'french_word_n-->', french_word_n) return english_word2index, english_index2word, english_word_n, french_word2index, french_index2word, french_word_n, my_pairs \u8c03\u7528 # \u5168\u5c40\u51fd\u6570 \u83b7\u53d6\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 \u8bed\u8a00\u5bf9\u5217\u8868my_pairs english_word2index , english_index2word , english_word_n , \\ french_word2index , french_index2word , french_word_n , \\ my_pairs = my_getdata () \u8f93\u51fa\u6548\u679c: my_lines ---> 10599 len ( pairs ) ---> 10599 [[ 'i m .' , 'j ai ans .' ], [ 'i m ok .' , 'je vais bien .' ], [ 'i m ok .' , 'ca va .' ], [ 'i m fat .' , 'je suis gras .' ]] my_pairs [ 8000 ][ 0 ] ---> they re in the science lab . my_pairs [ 8000 ][ 1 ] ---> elles sont dans le laboratoire de sciences . len ( english_word2index ) --> 2803 len ( french_word2index ) --> 4345 english_word_n ---> 2803 french_word_n --> 4345 x . shape torch . Size ([ 1 , 9 ]) tensor ([[ 75 , 40 , 102 , 103 , 677 , 42 , 21 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 119 , 25 , 164 , 165 , 3222 , 5 , 1 ]]) x . shape torch . Size ([ 1 , 5 ]) tensor ([[ 14 , 15 , 44 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 5 ]) tensor ([[ 24 , 25 , 62 , 5 , 1 ]]) x . shape torch . Size ([ 1 , 8 ]) tensor ([[ 2 , 3 , 147 , 61 , 532 , 1143 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 6 , 297 , 7 , 246 , 102 , 5 , 1 ]]) 2 \u6784\u5efa\u6570\u636e\u6e90\u5bf9\u8c61 \u00b6 # \u539f\u59cb\u6570\u636e -> \u6570\u636e\u6e90MyPairsDataset --> \u6570\u636e\u8fed\u4ee3\u5668DataLoader # \u6784\u9020\u6570\u636e\u6e90 MyPairsDataset\uff0c\u628a\u8bed\u6599xy \u6587\u672c\u6570\u503c\u5316 \u518d\u8f6c\u6210tensor_x tensor_y # 1 __init__(self, my_pairs)\u51fd\u6570 \u8bbe\u7f6eself.my_pairs \u6761\u76ee\u6570self.sample_len # 2 __len__(self)\u51fd\u6570 \u83b7\u53d6\u6837\u672c\u6761\u6570 # 3 __getitem__(self, index)\u51fd\u6570 \u83b7\u53d6\u7b2c\u51e0\u6761\u6837\u672c\u6570\u636e # \u6309\u7d22\u5f15 \u83b7\u53d6\u6570\u636e\u6837\u672c x y # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 word2id x.append(EOS_token) # \u6837\u672cy \u6587\u672c\u6570\u503c\u5316 word2id y.append(EOS_token) # \u8fd4\u56detensor_x, tensor_y class MyPairsDataset ( Dataset ): def __init__ ( self , my_pairs ): # \u6837\u672cx self . my_pairs = my_pairs # \u6837\u672c\u6761\u76ee\u6570 self . sample_len = len ( my_pairs ) # \u83b7\u53d6\u6837\u672c\u6761\u6570 def __len__ ( self ): return self . sample_len # \u83b7\u53d6\u7b2c\u51e0\u6761 \u6837\u672c\u6570\u636e def __getitem__ ( self , index ): # \u5bf9index\u5f02\u5e38\u503c\u8fdb\u884c\u4fee\u6b63 [0, self.sample_len-1] index = min ( max ( index , 0 ), self . sample_len - 1 ) # \u6309\u7d22\u5f15\u83b7\u53d6 \u6570\u636e\u6837\u672c x y x = self . my_pairs [ index ][ 0 ] y = self . my_pairs [ index ][ 1 ] # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 x = [ english_word2index [ word ] for word in x . split ( ' ' )] x . append ( EOS_token ) tensor_x = torch . tensor ( x , dtype = torch . long , device = device ) # \u6837\u672cy \u6587\u672c\u6570\u503c\u5316 y = [ french_word2index [ word ] for word in y . split ( ' ' )] y . append ( EOS_token ) tensor_y = torch . tensor ( y , dtype = torch . long , device = device ) # \u6ce8\u610f tensor_x tensor_y\u90fd\u662f\u4e00\u7ef4\u6570\u7ec4\uff0c\u901a\u8fc7DataLoader\u62ff\u51fa\u6570\u636e\u662f\u4e8c\u7ef4\u6570\u636e # print('tensor_y.shape===>', tensor_y.shape, tensor_y) # \u8fd4\u56de\u7ed3\u679c return tensor_x , tensor_y 3 \u6784\u5efa\u6570\u636e\u8fed\u4ee3\u5668 \u00b6 def dm_test_MyPairsDataset (): # 1 \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # 2 \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) if i == 1 : break \u8f93\u51fa\u6548\u679c: x . shape torch . Size ([ 1 , 8 ]) tensor ([[ 2 , 16 , 33 , 518 , 589 , 1460 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 8 ]) tensor ([[ 6 , 11 , 52 , 101 , 1358 , 964 , 5 , 1 ]]) x . shape torch . Size ([ 1 , 6 ]) tensor ([[ 129 , 78 , 677 , 429 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 118 , 214 , 1073 , 194 , 778 , 5 , 1 ]]) 3 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u00b6 1 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668 \u00b6 \u7f16\u7801\u5668\u7ed3\u6784\u56fe: \u5b9e\u73b0\u601d\u8def\u5206\u6790 # EncoderRNN\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u5b9a\u4e492\u4e2a\u5c42 self.embedding self.gru (batch_first=True) # def __init__(self, input_size, hidden_size): # 2803 256 # 2 forward(input, hidden)\u51fd\u6570\uff0c\u8fd4\u56deoutput, hidden # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,6] --> [1,6,256] # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u5f62\u72b6\u53d8\u5316 gru([1,6,256],[1,1,256]) --> [1,6,256] [1,1,256] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6 torch.zeros(1, 1, self.hidden_size, device=device) \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668 class EncoderRNN ( nn . Module ): def __init__ ( self , input_size , hidden_size ): # input_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u5355\u8bcd\u6570 eg\uff1a2803 # hidden_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u6570 eg 256 super ( EncoderRNN , self ) . __init__ () self . input_size = input_size self . hidden_size = hidden_size # \u5b9e\u4f8b\u5316nn.Embedding\u5c42 self . embedding = nn . Embedding ( input_size , hidden_size ) # \u5b9e\u4f8b\u5316nn.GRU\u5c42 \u6ce8\u610f\u53c2\u6570batch_first=True self . gru = nn . GRU ( hidden_size , hidden_size , batch_first = True ) def forward ( self , input , hidden ): # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,6] --> [1,6,256] output = self . embedding ( input ) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u6570\u636e\u5f62\u72b6 gru([1,6,256],[1,1,256]) --> [1,6,256] [1,1,256] output , hidden = self . gru ( output , hidden ) return output , hidden def inithidden ( self ): # \u5c06\u9690\u5c42\u5f20\u91cf\u521d\u59cb\u5316\u6210\u4e3a1x1xself.hidden_size\u5927\u5c0f\u7684\u5f20\u91cf return torch . zeros ( 1 , 1 , self . hidden_size , device = device ) \u8c03\u7528 def dm_test_EncoderRNN (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # my_encoderrnn = EncoderRNN ( input_size , hidden_size ) print ( 'my_encoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_encoderrnn ) # \u7ed9encode\u6a21\u578b\u5582\u6570\u636e for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) # \u4e00\u6b21\u6027\u7684\u9001\u6570\u636e hidden = my_encoderrnn . inithidden () encode_output_c , hidden = my_encoderrnn ( x , hidden ) print ( 'encode_output_c.shape--->' , encode_output_c . shape , encode_output_c ) # \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7ed9\u4e3a\u6a21\u578b\u5582\u6570\u636e hidden = my_encoderrnn . inithidden () for i in range ( x . shape [ 1 ]): tmp = x [ 0 ][ i ] . view ( 1 , - 1 ) output , hidden = my_encoderrnn ( tmp , hidden ) print ( '\u89c2\u5bdf\uff1a\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65output\u8f93\u51fa\u662f\u5426\u76f8\u7b49' ) # hidden_size = 8 \u6548\u679c\u6bd4\u8f83\u597d print ( 'encode_output_c[0][-1]===>' , encode_output_c [ 0 ][ - 1 ]) print ( 'output===>' , output ) break \u8f93\u51fa\u6548\u679c: # \u672c\u8f93\u51fa\u6548\u679c\u4e3ahidden_size = 8 x . shape torch . Size ([ 1 , 6 ]) tensor ([[ 129 , 124 , 270 , 558 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 118 , 214 , 101 , 1253 , 1028 , 5 , 1 ]]) encode_output_c . shape ---> torch . Size ([ 1 , 6 , 8 ]) tensor ([[[ - 0.0984 , 0.4267 , - 0.2120 , 0.0923 , 0.1525 , - 0.0378 , 0.2493 , - 0.2665 ], [ - 0.1388 , 0.5363 , - 0.4522 , - 0.2819 , - 0.2070 , 0.0795 , 0.6262 , - 0.2359 ], [ - 0.4593 , 0.2499 , 0.1159 , 0.3519 , - 0.0852 , - 0.3621 , 0.1980 , - 0.1853 ], [ - 0.4407 , 0.1974 , 0.6873 , - 0.0483 , - 0.2730 , - 0.2190 , 0.0587 , 0.2320 ], [ - 0.6544 , 0.1990 , 0.7534 , - 0.2347 , - 0.0686 , - 0.5532 , 0.0624 , 0.4083 ], [ - 0.2941 , - 0.0427 , 0.1017 , - 0.1057 , 0.1983 , - 0.1066 , 0.0881 , - 0.3936 ]]], grad_fn =< TransposeBackward1 > ) \u89c2\u5bdf \uff1a \u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65output\u8f93\u51fa\u662f\u5426\u76f8\u7b49 encode_output_c [ 0 ][ - 1 ] ===> tensor ([ - 0.2941 , - 0.0427 , 0.1017 , - 0.1057 , 0.1983 , - 0.1066 , 0.0881 , - 0.3936 ], grad_fn =< SelectBackward0 > ) output ===> tensor ([[[ - 0.2941 , - 0.0427 , 0.1017 , - 0.1057 , 0.1983 , - 0.1066 , 0.0881 , - 0.3936 ]]], grad_fn =< TransposeBackward1 > ) 2 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u89e3\u7801\u5668 \u00b6 \u89e3\u7801\u5668\u7ed3\u6784\u56fe: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u89e3\u7801\u5668\u5b9e\u73b0\u601d\u8def\u5206\u6790 # DecoderRNN \u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # \u89e3\u7801\u5668\u7684\u4f5c\u7528\uff1a\u63d0\u53d6\u4e8b\u7269\u7279\u5f81 \u8fdb\u884c\u5206\u7c7b\uff08\u6240\u4ee5\u6bd4 \u7f16\u7801\u5668 \u591a\u4e86 \u7ebf\u6027\u5c42 \u548c softmax\u5c42\uff09 # 1 init\u51fd\u6570 \u5b9a\u4e49\u56db\u4e2a\u5c42 self.embedding self.gru self.out self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, output_size, hidden_size): # 4345 256 # 2 forward(input, hidden)\u51fd\u6570\uff0c\u8fd4\u56deoutput, hidden # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] # \u6570\u636e\u7ecf\u8fc7relu()\u5c42 output = F.relu(output) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u5f62\u72b6\u53d8\u5316 gru([1,1,256],[1,1,256]) --> [1,1,256] [1,1,256] # \u6570\u636e\u7ed3\u679cout\u5c42 \u5f62\u72b6\u53d8\u5316 [1,1,256]->[1,256]-->[1,4345] # \u8fd4\u56de \u89e3\u7801\u5668\u5206\u7c7boutput[1,4345]\uff0c\u6700\u540e\u9690\u5c42\u5f20\u91cfhidden[1,1,256] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6 torch.zeros(1, 1, self.hidden_size, device=device) \u7f16\u7801\u5b9e\u73b0 class DecoderRNN ( nn . Module ): def __init__ ( self , output_size , hidden_size ): # output_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u5355\u8bcd\u6570 eg\uff1a4345 # hidden_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u6570 eg 256 super ( DecoderRNN , self ) . __init__ () self . output_size = output_size self . hidden_size = hidden_size # \u5b9e\u4f8b\u5316\u8bcd\u5d4c\u5165\u5c42 self . embedding = nn . Embedding ( output_size , hidden_size ) # \u5b9e\u4f8b\u5316gru\u5c42\uff0c\u8f93\u5165\u5c3a\u5bf8256 \u8f93\u51fa\u5c3a\u5bf8256 # \u56e0\u89e3\u7801\u5668\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u89e3\u7801 batch_first=True \u610f\u4e49\u4e0d\u5927 self . gru = nn . GRU ( hidden_size , hidden_size , batch_first = True ) # \u5b9e\u4f8b\u5316\u7ebf\u6027\u8f93\u51fa\u5c42out \u8f93\u5165\u5c3a\u5bf8256 \u8f93\u51fa\u5c3a\u5bf84345 self . out = nn . Linear ( hidden_size , output_size ) # \u5b9e\u4f8b\u5316softomax\u5c42 \u6570\u503c\u5f52\u4e00\u5316 \u4ee5\u4fbf\u5206\u7c7b self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden ): # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 # \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] or [1,6]--->[1,6,256] output = self . embedding ( input ) # \u6570\u636e\u7ed3\u679crelu\u5c42\u4f7fEmbedding\u77e9\u9635\u66f4\u7a00\u758f\uff0c\u4ee5\u9632\u6b62\u8fc7\u62df\u5408 output = F . relu ( output ) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 # \u6570\u636e\u5f62\u72b6 gru([1,1,256],[1,1,256]) --> [1,1,256] [1,1,256] output , hidden = self . gru ( output , hidden ) # \u6570\u636e\u7ecf\u8fc7softmax\u5c42 \u5f52\u4e00\u5316 # \u6570\u636e\u5f62\u72b6\u53d8\u5316 [1,1,256]->[1,256] ---> [1,4345] output = self . softmax ( self . out ( output [ 0 ])) return output , hidden def inithidden ( self ): # \u5c06\u9690\u5c42\u5f20\u91cf\u521d\u59cb\u5316\u6210\u4e3a1x1xself.hidden_size\u5927\u5c0f\u7684\u5f20\u91cf return torch . zeros ( 1 , 1 , self . hidden_size , device = device ) \u8c03\u7528 def dm03_test_DecoderRNN (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_encoderrnn = EncoderRNN ( input_size , hidden_size ) print ( 'my_encoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_encoderrnn ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = french_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_decoderrnn = DecoderRNN ( input_size , hidden_size ) print ( 'my_decoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_decoderrnn ) # \u7ed9\u6a21\u578b\u5582\u6570\u636e \u5b8c\u6574\u6f14\u793a\u7f16\u7801 \u89e3\u7801\u6d41\u7a0b for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) # 1 \u7f16\u7801\uff1a\u4e00\u6b21\u6027\u7684\u9001\u6570\u636e hidden = my_encoderrnn . inithidden () encode_output_c , hidden = my_encoderrnn ( x , hidden ) print ( 'encode_output_c.shape--->' , encode_output_c . shape , encode_output_c ) print ( '\u89c2\u5bdf\uff1a\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65output\u8f93\u51fa' ) # hidden_size = 8 \u6548\u679c\u6bd4\u8f83\u597d print ( 'encode_output_c[0][-1]===>' , encode_output_c [ 0 ][ - 1 ]) # 2 \u89e3\u7801: \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u89e3\u7801 # \u6700\u540e1\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa \u4f5c\u4e3a \u89e3\u7801\u5668\u7684\u7b2c1\u4e2a\u65f6\u95f4\u6b65\u9690\u85cf\u5c42\u8f93\u5165 for i in range ( y . shape [ 1 ]): tmp = y [ 0 ][ i ] . view ( 1 , - 1 ) output , hidden = my_decoderrnn ( tmp , hidden ) print ( '\u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output===>' , output . shape ) break \u8f93\u51fa\u6548\u679c my_encoderrnn\u6a21\u578b\u7ed3\u6784 ---> EncoderRNN ( ( embedding ): Embedding ( 2803 , 256 ) ( gru ): GRU ( 256 , 256 , batch_first = True ) ) my_decoderrnn\u6a21\u578b\u7ed3\u6784 ---> DecoderRNN ( ( embedding ): Embedding ( 4345 , 256 ) ( gru ): GRU ( 256 , 256 , batch_first = True ) ( out ): Linear ( in_features = 256 , out_features = 4345 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) x . shape torch . Size ([ 1 , 8 ]) tensor ([[ 14 , 40 , 883 , 677 , 589 , 609 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 6 ]) tensor ([[ 1358 , 1125 , 247 , 2863 , 5 , 1 ]]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) 3 \u6784\u5efa\u57fa\u4e8eGRU\u548cAttention\u7684\u89e3\u7801\u5668 \u00b6 \u89e3\u7801\u5668\u7ed3\u6784\u56fe: \u5b9e\u73b0\u601d\u8def\u5206\u6790 # \u6784\u5efa\u57fa\u4e8eGRU\u548cAttention\u7684\u89e3\u7801\u5668 # AttnDecoderRNN \u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u5b9a\u4e49\u516d\u4e2a\u5c42 # self.embedding self.attn self.attn_combine # self.gru self.out self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, output_size, hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):: # 4345 256 # 2 forward(input, hidden, encoder_outputs)\u51fd\u6570\uff0c\u8fd4\u56deoutput, hidden # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] # 1 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,10] # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,256] # 3 q \u4e0e attn_applied \u878d\u5408\uff0c\u7ecf\u8fc7\u5c42attn_combine \u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,256] # \u6570\u636e\u7ecf\u8fc7relu()\u5c42 output = F.relu(output) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u5f62\u72b6\u53d8\u5316 gru([1,1,256],[1,1,256]) --> [1,1,256] [1,1,256] # \u8fd4\u56de # \u8fd4\u56de\u89e3\u7801\u5668\u5206\u7c7boutput[1,4345]\uff0c\u6700\u540e\u9690\u5c42\u5f20\u91cfhidden[1,1,256] \u6ce8\u610f\u529b\u6743\u91cd\u5f20\u91cfattn_weights[1,10] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6 torch.zeros(1, 1, self.hidden_size, device=device) # \u76f8\u5bf9\u4f20\u7edfRNN\u89e3\u7801 AttnDecoderRNN\u7c7b\u591a\u4e86\u6ce8\u610f\u529b\u673a\u5236,\u9700\u8981\u6784\u5efaQKV # 1 \u5728init\u51fd\u6570\u4e2d (self, output_size, hidden_size, dropout_p=0.1, max_length=MAX_LENGTH) # \u589e\u52a0\u5c42 self.attn self.attn_combine self.dropout # 2 \u589e\u52a0\u51fd\u6570 attentionQKV(self, Q, K, V) # 3 \u51fd\u6570forward(self, input, hidden, encoder_outputs) # encoder_outputs \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51c6\u5907qkv \u8c03\u7528attentionQKV # \u51fd\u6570\u8fd4\u56de\u503c output, hidden, attn_weights # 4 \u8c03\u7528\u9700\u8981\u51c6\u5907\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfC encode_output_c \u7f16\u7801\u5b9e\u73b0 class AttnDecoderRNN ( nn . Module ): def __init__ ( self , output_size , hidden_size , dropout_p = 0.1 , max_length = MAX_LENGTH ): # output_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u5355\u8bcd\u6570 eg\uff1a4345 # hidden_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u6570 eg 256 # dropout_p \u7f6e\u96f6\u6bd4\u7387\uff0c\u9ed8\u8ba40.1, # max_length \u6700\u5927\u957f\u5ea610 super ( AttnDecoderRNN , self ) . __init__ () self . output_size = output_size self . hidden_size = hidden_size self . dropout_p = dropout_p self . max_length = max_length # \u5b9a\u4e49nn.Embedding\u5c42 nn.Embedding(4345,256) self . embedding = nn . Embedding ( self . output_size , self . hidden_size ) # \u5b9a\u4e49\u7ebf\u6027\u5c421\uff1a\u6c42q\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 self . attn = nn . Linear ( self . hidden_size * 2 , self . max_length ) # \u5b9a\u4e49\u7ebf\u6027\u5c422\uff1aq+\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u878d\u5408\u540e\uff0c\u5728\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa self . attn_combine = nn . Linear ( self . hidden_size * 2 , self . hidden_size ) # \u5b9a\u4e49dropout\u5c42 self . dropout = nn . Dropout ( self . dropout_p ) # \u5b9a\u4e49gru\u5c42 self . gru = nn . GRU ( self . hidden_size , self . hidden_size , batch_first = True ) # \u5b9a\u4e49out\u5c42 \u89e3\u7801\u5668\u6309\u7167\u7c7b\u522b\u8fdb\u884c\u8f93\u51fa(256,4345) self . out = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9e\u4f8b\u5316softomax\u5c42 \u6570\u503c\u5f52\u4e00\u5316 \u4ee5\u4fbf\u5206\u7c7b self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden , encoder_outputs ): # input\u4ee3\u8868q [1,1] \u4e8c\u7ef4\u6570\u636e hidden\u4ee3\u8868k [1,1,256] encoder_outputs\u4ee3\u8868v [10,256] # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 # \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] embedded = self . embedding ( input ) # \u4f7f\u7528dropout\u8fdb\u884c\u968f\u673a\u4e22\u5f03\uff0c\u9632\u6b62\u8fc7\u62df\u5408 embedded = self . dropout ( embedded ) # 1 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,10] attn_weights = F . softmax ( self . attn ( torch . cat (( embedded [ 0 ], hidden [ 0 ]), 1 )), dim = 1 ) # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,256] # [1,1,10],[1,10,256] ---> [1,1,256] attn_applied = torch . bmm ( attn_weights . unsqueeze ( 0 ), encoder_outputs . unsqueeze ( 0 )) # 3 q \u4e0e attn_applied \u878d\u5408\uff0c\u518d\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,256] output = torch . cat (( embedded [ 0 ], attn_applied [ 0 ]), 1 ) output = self . attn_combine ( output ) . unsqueeze ( 0 ) # \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a \u4f7f\u7528relu\u6fc0\u6d3b output = F . relu ( output ) # \u67e5\u8be2\u5f20\u91cf\u7ecf\u8fc7gru\u3001softmax\u8fdb\u884c\u5206\u7c7b\u7ed3\u679c\u8f93\u51fa # \u6570\u636e\u5f62\u72b6[1,1,256],[1,1,256] --> [1,1,256], [1,1,256] output , hidden = self . gru ( output , hidden ) # \u6570\u636e\u5f62\u72b6[1,1,256]->[1,256]->[1,4345] output = self . softmax ( self . out ( output [ 0 ])) # \u8fd4\u56de\u89e3\u7801\u5668\u5206\u7c7boutput[1,4345]\uff0c\u6700\u540e\u9690\u5c42\u5f20\u91cfhidden[1,1,256] \u6ce8\u610f\u529b\u6743\u91cd\u5f20\u91cfattn_weights[1,10] return output , hidden , attn_weights def inithidden ( self ): # \u5c06\u9690\u5c42\u5f20\u91cf\u521d\u59cb\u5316\u6210\u4e3a1x1xself.hidden_size\u5927\u5c0f\u7684\u5f20\u91cf return torch . zeros ( 1 , 1 , self . hidden_size , device = device ) \u8c03\u7528 def dm_test_AttnDecoderRNN (): # 1 \u5b9e\u4f8b\u5316 \u6570\u636e\u96c6\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # 2 \u5b9e\u4f8b\u5316 \u6570\u636e\u52a0\u8f7d\u5668\u5bf9\u8c61 mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316 \u7f16\u7801\u5668my_encoderrnn my_encoderrnn = EncoderRNN ( english_word_n , 256 ) # \u5b9e\u4f8b\u5316 \u89e3\u7801\u5668DecoderRNN my_attndecoderrnn = AttnDecoderRNN ( french_word_n , 256 ) # 3 \u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668 for i , ( x , y ) in enumerate ( mydataloader ): # \u7f16\u7801-\u65b9\u6cd51 \u4e00\u6b21\u6027\u7ed9\u6a21\u578b\u9001\u6570\u636e hidden = my_encoderrnn . inithidden () print ( 'x--->' , x . shape , x ) print ( 'y--->' , y . shape , y ) # [1, 6, 256], [1, 1, 256]) --> [1, 6, 256][1, 1, 256] output , hidden = my_encoderrnn ( x , hidden ) # print('output-->', output.shape, output) # print('\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u53d6\u51faoutput[0,-1]-->', output[0, -1].shape, output[0, -1]) # \u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfC encode_output_c = torch . zeros ( MAX_LENGTH , my_encoderrnn . hidden_size , device = device ) for idx in range ( output . shape [ 1 ]): encode_output_c [ idx ] = output [ 0 , idx ] # # \u7f16\u7801-\u65b9\u6cd52 \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7ed9\u6a21\u578b\u9001\u6570\u636e # hidden = my_encoderrnn.inithidden() # for i in range(x.shape[1]): # tmp = x[0][i].view(1, -1) # # [1, 1, 256], [1, 1, 256]) --> [1, 1, 256][1, 1, 256] # output, hidden = my_encoderrnn(tmp, hidden) # print('\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26output', output.shape, output) # \u89e3\u7801-\u5fc5\u987b\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u89e3\u7801 for i in range ( y . shape [ 1 ]): tmp = y [ 0 ][ i ] . view ( 1 , - 1 ) output , hidden , attn_weights = my_attndecoderrnn ( tmp , hidden , encode_output_c ) print ( '\u89e3\u7801output.shape' , output . shape ) print ( '\u89e3\u7801hidden.shape' , hidden . shape ) print ( '\u89e3\u7801attn_weights.shape' , attn_weights . shape ) break \u8f93\u51fa\u6548\u679c: x ---> torch . Size ([ 1 , 7 ]) tensor ([[ 129 , 78 , 1873 , 294 , 1215 , 4 , 1 ]]) y ---> torch . Size ([ 1 , 6 ]) tensor ([[ 210 , 3097 , 248 , 3095 , 5 , 1 ]]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) 4 \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3 \u00b6 1 teacher_forcing\u4ecb\u7ecd \u00b6 \u5b83\u662f\u4e00\u79cd\u7528\u4e8e\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u7684\u8bad\u7ec3\u6280\u5de7, \u5728seq2seq\u67b6\u6784\u4e2d, \u6839\u636e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\uff0c\u89e3\u7801\u5668\u6bcf\u6b21\u5e94\u8be5\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\u7684\u4e00\u90e8\u5206, \u4f46\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u65e6\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u662f\u9519\u8bef\u7684\uff0c\u5c31\u4f1a\u5bfc\u81f4\u8fd9\u79cd\u9519\u8bef\u88ab\u7d2f\u79ef\uff0c\u65e0\u6cd5\u8fbe\u5230\u8bad\u7ec3\u6548\u679c, \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u673a\u5236\u6539\u53d8\u4e0a\u4e00\u6b65\u51fa\u9519\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u8bad\u7ec3\u65f6\u6211\u4eec\u662f\u5df2\u77e5\u6b63\u786e\u7684\u8f93\u51fa\u5e94\u8be5\u662f\u4ec0\u4e48\uff0c\u56e0\u6b64\u53ef\u4ee5\u5f3a\u5236\u5c06\u4e0a\u4e00\u6b65\u7ed3\u679c\u8bbe\u7f6e\u6210\u6b63\u786e\u7684\u8f93\u51fa, \u8fd9\u79cd\u65b9\u5f0f\u5c31\u53eb\u505ateacher_forcing. 2 teacher_forcing\u7684\u4f5c\u7528 \u00b6 \u80fd\u591f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u77eb\u6b63\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u907f\u514d\u5728\u5e8f\u5217\u751f\u6210\u7684\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u8fdb\u4e00\u6b65\u653e\u5927. teacher_forcing\u80fd\u591f\u6781\u5927\u7684\u52a0\u5feb\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4ee4\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u5feb\u66f4\u5e73\u7a33. 3 \u6784\u5efa\u5185\u90e8\u8fed\u4ee3\u8bad\u7ec3\u51fd\u6570 \u00b6 \u6a21\u578b\u8bad\u7ec3\u53c2\u6570 # \u6a21\u578b\u8bad\u7ec3\u53c2\u6570 mylr = 1e-4 epochs = 2 # \u8bbe\u7f6eteacher_forcing\u6bd4\u7387\u4e3a0.5 teacher_forcing_ratio = 0.5 print_interval_num = 1000 plot_interval_num = 100 \u5b9e\u73b0\u601d\u8def\u5206\u6790 # \u5185\u90e8\u8fed\u4ee3\u8bad\u7ec3\u51fd\u6570Train_Iters # 1 \u7f16\u7801 encode_output, encode_hidden = my_encoderrnn(x, encode_hidden) # \u6570\u636e\u5f62\u72b6 eg [1,6],[1,1,256] --> [1,6,256],[1,1,256] # 2 \u89e3\u7801\u53c2\u6570\u51c6\u5907\u548c\u89e3\u7801 # \u89e3\u7801\u53c2\u65701 \u56fa\u5b9a\u957f\u5ea6C encoder_outputs_c = torch.zeros(MAX_LENGTH, my_encoderrnn.hidden_size, device=device) # \u89e3\u7801\u53c2\u65702 decode_hidden # \u89e3\u7801\u53c2\u65703 input_y = torch.tensor([[SOS_token]], device=device) # \u6570\u636e\u5f62\u72b6\u6570\u636e\u5f62\u72b6 [1,1],[1,1,256],[10,256] ---> [1,4345],[1,1,256],[1,10] # output_y, decode_hidden, attn_weight = my_attndecoderrnn(input_y, decode_hidden, encode_output_c) # \u8ba1\u7b97\u635f\u5931 target_y = y[0][idx].view(1) # \u6bcf\u4e2a\u65f6\u95f4\u6b65\u5904\u7406 for idx in range(y_len): \u5904\u7406\u4e09\u8005\u4e4b\u95f4\u5173\u7cfbinput_y output_y target_y # 3 \u8bad\u7ec3\u7b56\u7565 use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False # teacher_forcing \u628a\u6837\u672c\u771f\u5b9e\u503cy\u4f5c\u4e3a\u4e0b\u4e00\u6b21\u8f93\u5165 input_y = y[0][idx].view(1, -1) # not teacher_forcing \u628a\u9884\u6d4b\u503cy\u4f5c\u4e3a\u4e0b\u4e00\u6b21\u8f93\u5165 # topv,topi = output_y.topk(1) # if topi.squeeze().item() == EOS_token: break input_y = topi.detach() # 4 \u5176\u4ed6 # \u8ba1\u7b97\u635f\u5931 # \u68af\u5ea6\u6e05\u96f6 # \u53cd\u5411\u4f20\u64ad # \u68af\u5ea6\u66f4\u65b0 # \u8fd4\u56de \u635f\u5931\u5217\u8868myloss.item()/y_len \u7f16\u7801\u5b9e\u73b0 def Train_Iters ( x , y , my_encoderrnn , my_attndecoderrnn , myadam_encode , myadam_decode , mycrossentropyloss ): # 1 \u7f16\u7801 encode_output, encode_hidden = my_encoderrnn(x, encode_hidden) encode_hidden = my_encoderrnn . inithidden () encode_output , encode_hidden = my_encoderrnn ( x , encode_hidden ) # \u4e00\u6b21\u6027\u9001\u6570\u636e # [1,6],[1,1,256] --> [1,6,256],[1,1,256] # 2 \u89e3\u7801\u53c2\u6570\u51c6\u5907\u548c\u89e3\u7801 # \u89e3\u7801\u53c2\u65701 encode_output_c [10,256] encode_output_c = torch . zeros ( MAX_LENGTH , my_encoderrnn . hidden_size , device = device ) for idx in range ( x . shape [ 1 ]): encode_output_c [ idx ] = encode_output [ 0 , idx ] # \u89e3\u7801\u53c2\u65702 decode_hidden = encode_hidden # \u89e3\u7801\u53c2\u65703 input_y = torch . tensor ([[ SOS_token ]], device = device ) myloss = 0.0 y_len = y . shape [ 1 ] use_teacher_forcing = True if random . random () < teacher_forcing_ratio else False if use_teacher_forcing : for idx in range ( y_len ): # \u6570\u636e\u5f62\u72b6\u6570\u636e\u5f62\u72b6 [1,1],[1,1,256],[10,256] ---> [1,4345],[1,1,256],[1,10] output_y , decode_hidden , attn_weight = my_attndecoderrnn ( input_y , decode_hidden , encode_output_c ) target_y = y [ 0 ][ idx ] . view ( 1 ) myloss = myloss + mycrossentropyloss ( output_y , target_y ) input_y = y [ 0 ][ idx ] . view ( 1 , - 1 ) else : for idx in range ( y_len ): # \u6570\u636e\u5f62\u72b6\u6570\u636e\u5f62\u72b6 [1,1],[1,1,256],[10,256] ---> [1,4345],[1,1,256],[1,10] output_y , decode_hidden , attn_weight = my_attndecoderrnn ( input_y , decode_hidden , encode_output_c ) target_y = y [ 0 ][ idx ] . view ( 1 ) myloss = myloss + mycrossentropyloss ( output_y , target_y ) topv , topi = output_y . topk ( 1 ) if topi . squeeze () . item () == EOS_token : break input_y = topi . detach () # \u68af\u5ea6\u6e05\u96f6 myadam_encode . zero_grad () myadam_decode . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam_encode . step () myadam_decode . step () # \u8fd4\u56de \u635f\u5931\u5217\u8868myloss.item()/y_len return myloss . item () / y_len 4 \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570 \u00b6 \u5b9e\u73b0\u601d\u8def\u5206\u6790 # Train_seq2seq() \u601d\u8def\u5206\u6790 # \u5b9e\u4f8b\u5316 mypairsdataset\u5bf9\u8c61 \u5b9e\u4f8b\u5316 mydataloader # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668 my_encoderrnn \u5b9e\u4f8b\u5316\u89e3\u7801\u5668 my_attndecoderrnn # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u4f18\u5316\u5668 myadam_encode \u5b9e\u4f8b\u5316\u89e3\u7801\u5668\u4f18\u5316\u5668 myadam_decode # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570 mycrossentropyloss = nn.NLLLoss() # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 # epoches mylr=1e4 teacher_forcing_ratio print_interval_num plot_interval_num (\u5168\u5c40) # plot_loss_list = [] (\u8fd4\u56de) print_loss_total plot_loss_total starttime (\u6bcf\u8f6e\u5185\u90e8) # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range(1, 1+epochs): # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 # for item, (x, y) in enumerate(mydataloader, start=1): # \u8c03\u7528\u5185\u90e8\u8bad\u7ec3\u51fd\u6570 Train_Iters(x, y, my_encoderrnn, my_attndecoderrnn, myadam_encode, myadam_decode, mycrossentropyloss) # \u8ba1\u7b97\u8f85\u52a9\u4fe1\u606f # \u8ba1\u7b97\u6253\u5370\u5c4f\u5e55\u95f4\u9694\u635f\u5931-\u6bcf\u96941000\u6b21 # \u8ba1\u7b97\u753b\u56fe\u95f4\u9694\u635f\u5931-\u6bcf\u9694100\u6b21 # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch.save(my_encoderrnn.state_dict(), PATH1) # \u6240\u6709\u8f6e\u6b21\u8bad\u7ec3\u5b8c\u6bd5 \u753b\u635f\u5931\u56fe plt.figure() .plot(plot_loss_list) .save('x.png') .show() \u7f16\u7801\u5b9e\u73b0 def Train_seq2seq (): # \u5b9e\u4f8b\u5316 mypairsdataset\u5bf9\u8c61 \u5b9e\u4f8b\u5316 mydataloader mypairsdataset = MyPairsDataset ( my_pairs ) mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668 my_encoderrnn \u5b9e\u4f8b\u5316\u89e3\u7801\u5668 my_attndecoderrnn my_encoderrnn = EncoderRNN ( 2803 , 256 ) my_attndecoderrnn = AttnDecoderRNN ( output_size = 4345 , hidden_size = 256 , dropout_p = 0.1 , max_length = 10 ) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u4f18\u5316\u5668 myadam_encode \u5b9e\u4f8b\u5316\u89e3\u7801\u5668\u4f18\u5316\u5668 myadam_decode myadam_encode = optim . Adam ( my_encoderrnn . parameters (), lr = mylr ) myadam_decode = optim . Adam ( my_attndecoderrnn . parameters (), lr = mylr ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570 mycrossentropyloss = nn.NLLLoss() mycrossentropyloss = nn . NLLLoss () # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 plot_loss_list = [] # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range(1, 1+epochs): for epoch_idx in range ( 1 , 1 + epochs ): print_loss_total , plot_loss_total = 0.0 , 0.0 starttime = time . time () # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for item , ( x , y ) in enumerate ( mydataloader , start = 1 ): # \u8c03\u7528\u5185\u90e8\u8bad\u7ec3\u51fd\u6570 myloss = Train_Iters ( x , y , my_encoderrnn , my_attndecoderrnn , myadam_encode , myadam_decode , mycrossentropyloss ) print_loss_total += myloss plot_loss_total += myloss # \u8ba1\u7b97\u6253\u5370\u5c4f\u5e55\u95f4\u9694\u635f\u5931-\u6bcf\u96941000\u6b21 if item % print_interval_num == 0 : print_loss_avg = print_loss_total / print_interval_num # \u5c06\u603b\u635f\u5931\u5f520 print_loss_total = 0 # \u6253\u5370\u65e5\u5fd7\uff0c\u65e5\u5fd7\u5185\u5bb9\u5206\u522b\u662f\uff1a\u8bad\u7ec3\u8017\u65f6\uff0c\u5f53\u524d\u8fed\u4ee3\u6b65\uff0c\u5f53\u524d\u8fdb\u5ea6\u767e\u5206\u6bd4\uff0c\u5f53\u524d\u5e73\u5747\u635f\u5931 print ( '\u8f6e\u6b21 %d \u635f\u5931 %.6f \u65f6\u95f4: %d ' % ( epoch_idx , print_loss_avg , time . time () - starttime )) # \u8ba1\u7b97\u753b\u56fe\u95f4\u9694\u635f\u5931-\u6bcf\u9694100\u6b21 if item % plot_interval_num == 0 : # \u901a\u8fc7\u603b\u635f\u5931\u9664\u4ee5\u95f4\u9694\u5f97\u5230\u5e73\u5747\u635f\u5931 plot_loss_avg = plot_loss_total / plot_interval_num # \u5c06\u5e73\u5747\u635f\u5931\u6dfb\u52a0plot_loss_list\u5217\u8868\u4e2d plot_loss_list . append ( plot_loss_avg ) # \u603b\u635f\u5931\u5f520 plot_loss_total = 0 # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_encoderrnn . state_dict (), './my_encoderrnn_ %d .pth' % epoch_idx ) torch . save ( my_attndecoderrnn . state_dict (), './my_attndecoderrnn_ %d .pth' % epoch_idx ) # \u6240\u6709\u8f6e\u6b21\u8bad\u7ec3\u5b8c\u6bd5 \u753b\u635f\u5931\u56fe plt . figure () plt . plot ( plot_loss_list ) plt . savefig ( './s2sq_loss.png' ) plt . show () return plot_loss_list \u8f93\u51fa\u6548\u679c \u8f6e\u6b211 \u635f\u59318.123402 \u65f6\u95f4:4 \u8f6e\u6b211 \u635f\u59316.658305 \u65f6\u95f4:8 \u8f6e\u6b211 \u635f\u59315.252497 \u65f6\u95f4:12 \u8f6e\u6b211 \u635f\u59314.906939 \u65f6\u95f4:16 \u8f6e\u6b211 \u635f\u59314.813769 \u65f6\u95f4:19 \u8f6e\u6b211 \u635f\u59314.780460 \u65f6\u95f4:23 \u8f6e\u6b211 \u635f\u59314.621599 \u65f6\u95f4:27 \u8f6e\u6b211 \u635f\u59314.487508 \u65f6\u95f4:31 \u8f6e\u6b211 \u635f\u59314.478538 \u65f6\u95f4:35 \u8f6e\u6b211 \u635f\u59314.245148 \u65f6\u95f4:39 \u8f6e\u6b211 \u635f\u59314.602579 \u65f6\u95f4:44 \u8f6e\u6b211 \u635f\u59314.256789 \u65f6\u95f4:48 \u8f6e\u6b211 \u635f\u59314.218111 \u65f6\u95f4:52 \u8f6e\u6b211 \u635f\u59314.393134 \u65f6\u95f4:56 \u8f6e\u6b211 \u635f\u59314.134959 \u65f6\u95f4:60 \u8f6e\u6b211 \u635f\u59314.164878 \u65f6\u95f4:63 5 \u635f\u5931\u66f2\u7ebf\u5206\u6790 \u00b6 \u635f\u5931\u4e0b\u964d\u66f2\u7ebf \u4e00\u76f4\u4e0b\u964d\u7684\u635f\u5931\u66f2\u7ebf, \u8bf4\u660e\u6a21\u578b\u6b63\u5728\u6536\u655b, \u80fd\u591f\u4ece\u6570\u636e\u4e2d\u627e\u5230\u4e00\u4e9b\u89c4\u5f8b\u5e94\u7528\u4e8e\u6570\u636e. 5 \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570\u5e76\u6d4b\u8bd5 \u00b6 1 \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570 \u00b6 # \u6a21\u578b\u8bc4\u4f30\u4ee3\u7801\u4e0e\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7c7b\u4f3c\uff0c\u9700\u8981\u6ce8\u610f\u4f7f\u7528with torch.no_grad() # \u6a21\u578b\u9884\u6d4b\u65f6\uff0c\u7b2c\u4e00\u4e2a\u65f6\u95f4\u6b65\u4f7f\u7528SOS_token\u4f5c\u4e3a\u8f93\u5165 \u540e\u7eed\u65f6\u95f4\u6b65\u91c7\u7528\u9884\u6d4b\u503c\u4f5c\u4e3a\u8f93\u5165\uff0c\u4e5f\u5c31\u662f\u81ea\u56de\u5f52\u673a\u5236 def Seq2Seq_Evaluate ( x , my_encoderrnn , my_attndecoderrnn ): with torch . no_grad (): # 1 \u7f16\u7801\uff1a\u4e00\u6b21\u6027\u7684\u9001\u6570\u636e encode_hidden = my_encoderrnn . inithidden () encode_output , encode_hidden = my_encoderrnn ( x , encode_hidden ) # 2 \u89e3\u7801\u53c2\u6570\u51c6\u5907 # \u89e3\u7801\u53c2\u65701 \u56fa\u5b9a\u957f\u5ea6\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc encoder_outputs_c = torch . zeros ( MAX_LENGTH , my_encoderrnn . hidden_size , device = device ) x_len = x . shape [ 1 ] for idx in range ( x_len ): encoder_outputs_c [ idx ] = encode_output [ 0 , idx ] # \u89e3\u7801\u53c2\u65702 \u6700\u540e1\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa \u4f5c\u4e3a \u89e3\u7801\u5668\u7684\u7b2c1\u4e2a\u65f6\u95f4\u6b65\u9690\u85cf\u5c42\u8f93\u5165 decode_hidden = encode_hidden # \u89e3\u7801\u53c2\u65703 \u89e3\u7801\u5668\u7b2c\u4e00\u4e2a\u65f6\u95f4\u6b65\u8d77\u59cb\u7b26 input_y = torch . tensor ([[ SOS_token ]], device = device ) # 3 \u81ea\u56de\u5f52\u65b9\u5f0f\u89e3\u7801 # \u521d\u59cb\u5316\u9884\u6d4b\u7684\u8bcd\u6c47\u5217\u8868 decoded_words = [] # \u521d\u59cb\u5316attention\u5f20\u91cf decoder_attentions = torch . zeros ( MAX_LENGTH , MAX_LENGTH ) for idx in range ( MAX_LENGTH ): # note:MAX_LENGTH=10 output_y , decode_hidden , attn_weights = my_attndecoderrnn ( input_y , decode_hidden , encoder_outputs_c ) # \u9884\u6d4b\u503c\u4f5c\u4e3a\u4e3a\u4e0b\u4e00\u6b21\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u503c topv , topi = output_y . topk ( 1 ) decoder_attentions [ idx ] = attn_weights # \u5982\u679c\u8f93\u51fa\u503c\u662f\u7ec8\u6b62\u7b26\uff0c\u5219\u5faa\u73af\u505c\u6b62 if topi . squeeze () . item () == EOS_token : decoded_words . append ( '<EOS>' ) break else : decoded_words . append ( french_index2word [ topi . item ()]) # \u5c06\u672c\u6b21\u9884\u6d4b\u7684\u7d22\u5f15\u8d4b\u503c\u7ed9 input_y\uff0c\u8fdb\u884c\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u9884\u6d4b input_y = topi . detach () # \u8fd4\u56de\u7ed3\u679cdecoded_words\uff0c \u6ce8\u610f\u529b\u5f20\u91cf\u6743\u91cd\u5206\u5e03\u8868(\u628a\u6ca1\u6709\u7528\u5230\u7684\u90e8\u5206\u5207\u6389) return decoded_words , decoder_attentions [: idx + 1 ] 2 \u6a21\u578b\u8bc4\u4f30\u51fd\u6570\u8c03\u7528 \u00b6 # \u52a0\u8f7d\u6a21\u578b PATH1 = './gpumodel/my_encoderrnn.pth' PATH2 = './gpumodel/my_attndecoderrnn.pth' def dm_test_Seq2Seq_Evaluate (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_encoderrnn = EncoderRNN ( input_size , hidden_size ) # my_encoderrnn.load_state_dict(torch.load(PATH1)) my_encoderrnn . load_state_dict ( torch . load ( PATH1 , map_location = lambda storage , loc : storage ), False ) print ( 'my_encoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_encoderrnn ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = french_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_attndecoderrnn = AttnDecoderRNN ( input_size , hidden_size ) # my_attndecoderrnn.load_state_dict(torch.load(PATH2)) my_attndecoderrnn . load_state_dict ( torch . load ( PATH2 , map_location = lambda storage , loc : storage ), False ) print ( 'my_decoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_attndecoderrnn ) my_samplepairs = [ [ 'i m impressed with your french .' , 'je suis impressionne par votre francais .' ], [ 'i m more than a friend .' , 'je suis plus qu une amie .' ], [ 'she is beautiful like her mother .' , 'elle est belle comme sa mere .' ] ] print ( 'my_samplepairs--->' , len ( my_samplepairs )) for index , pair in enumerate ( my_samplepairs ): x = pair [ 0 ] y = pair [ 1 ] # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 tmpx = [ english_word2index [ word ] for word in x . split ( ' ' )] tmpx . append ( EOS_token ) tensor_x = torch . tensor ( tmpx , dtype = torch . long , device = device ) . view ( 1 , - 1 ) # \u6a21\u578b\u9884\u6d4b decoded_words , attentions = Seq2Seq_Evaluate ( tensor_x , my_encoderrnn , my_attndecoderrnn ) # print('decoded_words->', decoded_words) output_sentence = ' ' . join ( decoded_words ) print ( ' \\n ' ) print ( '>' , x ) print ( '=' , y ) print ( '<' , output_sentence ) \u8f93\u51fa\u6548\u679c: > i m impressed with your french . = je suis impressionne par votre francais . < je suis impressionnee par votre francais . < EOS > > i m more than a friend . = je suis plus qu une amie . < je suis plus qu une amie . < EOS > > she is beautiful like her mother . = elle est belle comme sa mere . < elle est sa sa mere . < EOS > > you re winning aren t you ? = vous gagnez n est ce pas ? < tu restez n est ce pas ? < EOS > > he is angry with you . = il est en colere apres toi . < il est en colere apres toi . < EOS > > you re very timid . = vous etes tres craintifs . < tu es tres craintive . < EOS > 3 Attention\u5f20\u91cf\u5236\u56fe \u00b6 def dm_test_Attention (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_encoderrnn = EncoderRNN ( input_size , hidden_size ) # my_encoderrnn.load_state_dict(torch.load(PATH1)) my_encoderrnn . load_state_dict ( torch . load ( PATH1 , map_location = lambda storage , loc : storage ), False ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = french_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_attndecoderrnn = AttnDecoderRNN ( input_size , hidden_size ) # my_attndecoderrnn.load_state_dict(torch.load(PATH2)) my_attndecoderrnn . load_state_dict ( torch . load ( PATH2 , map_location = lambda storage , loc : storage ), False ) sentence = \"we re both teachers .\" # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 tmpx = [ english_word2index [ word ] for word in sentence . split ( ' ' )] tmpx . append ( EOS_token ) tensor_x = torch . tensor ( tmpx , dtype = torch . long , device = device ) . view ( 1 , - 1 ) # \u6a21\u578b\u9884\u6d4b decoded_words , attentions = Seq2Seq_Evaluate ( tensor_x , my_encoderrnn , my_attndecoderrnn ) print ( 'decoded_words->' , decoded_words ) # print('\\n') # print('\u82f1\u6587', sentence) # print('\u6cd5\u6587', output_sentence) plt . matshow ( attentions . numpy ()) # \u4ee5\u77e9\u9635\u5217\u8868\u7684\u5f62\u5f0f \u663e\u793a # \u4fdd\u5b58\u56fe\u50cf plt . savefig ( \"./s2s_attn.png\" ) plt . show () print ( 'attentions.numpy()---> \\n ' , attentions . numpy ()) print ( 'attentions.size--->' , attentions . size ()) \u8f93\u51fa\u6548\u679c: decoded_words-> ['nous', 'sommes', 'toutes', 'deux', 'enseignantes', '.', '<EOS>'] Attention\u53ef\u89c6\u5316: Attention\u56fe\u50cf\u7684\u7eb5\u5750\u6807\u4ee3\u8868\u8f93\u5165\u7684\u6e90\u8bed\u8a00\u5404\u4e2a\u8bcd\u6c47\u5bf9\u5e94\u7684\u7d22\u5f15, 0-6\u5206\u522b\u5bf9\u5e94[\"we\", \"re\", \"both\", \"teachers\", \".\", \" \"], \u7eb5\u5750\u6807\u4ee3\u8868\u751f\u6210\u7684\u76ee\u6807\u8bed\u8a00\u5404\u4e2a\u8bcd\u6c47\u5bf9\u5e94\u7684\u7d22\u5f15, 0-7\u4ee3\u8868['nous', 'sommes', 'toutes', 'deux', 'enseignantes', '.', ' '], \u56fe\u4e2d\u6d45\u8272\u5c0f\u65b9\u5757(\u989c\u8272\u8d8a\u6d45\u8bf4\u660e\u5f71\u54cd\u8d8a\u5927)\u4ee3\u8868\u8bcd\u6c47\u4e4b\u95f4\u7684\u5f71\u54cd\u5173\u7cfb, \u6bd4\u5982\u6e90\u8bed\u8a00\u7684\u7b2c1\u4e2a\u8bcd\u6c47\u5bf9\u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684\u7b2c1\u4e2a\u8bcd\u6c47\u5f71\u54cd\u6700\u5927, \u6e90\u8bed\u8a00\u7684\u7b2c4\uff0c5\u4e2a\u8bcd\u5bf9\u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684\u7b2c5\u4e2a\u8bcd\u4f1a\u5f71\u54cd\u6700\u5927, \u901a\u8fc7\u8fd9\u6837\u7684\u53ef\u89c6\u5316\u56fe\u50cf, \u6211\u4eec\u53ef\u4ee5\u77e5\u9053Attention\u7684\u6548\u679c\u597d\u574f, \u4e0e\u6211\u4eec\u4eba\u4e3a\u53bb\u5224\u5b9a\u5230\u5e95\u8fd8\u6709\u591a\u5927\u7684\u5dee\u8ddd. \u8fdb\u800c\u8861\u91cf\u6211\u4eec\u8bad\u7ec3\u6a21\u578b\u7684\u53ef\u7528\u6027. 4 \u5c0f\u7ed3 \u00b6 seq2seq\u6a21\u578b\u67b6\u6784\u5206\u6790 seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002\u5176\u4e2d\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5185\u90e8\u5b9e\u73b0\u90fd\u4f7f\u7528\u4e86GRU\u6a21\u578b \u57fa\u4e8eGRU\u7684seq2seq\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u7ffb\u8bd1\u7684\u8fc7\u7a0b \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305\u548c\u5de5\u5177\u51fd\u6570 \u7b2c\u4e8c\u6b65: \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u7b2c\u56db\u6b65: \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65: \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570, \u5e76\u8fdb\u884c\u6d4b\u8bd5\u4ee5\u53caAttention\u6548\u679c\u5206\u6790 \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 python\u7248\u672c\u4f7f\u75283.6.x, pytorch\u7248\u672c\u4f7f\u75281.3.1 \u7b2c\u4e8c\u6b65: \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42 \u6e05\u6d17\u6587\u672c\u548c\u6784\u5efa\u6587\u672c\u5b57\u5178\u3001\u6784\u5efa\u6570\u636e\u6e90\u3001\u6784\u5efa\u6570\u636e\u8fed\u4ee3\u5668\u3002\u6587\u672c\u5904\u7406\u7684\u672c\u8d28\u5c31\u662f\u6839\u636e\u4efb\u52a1\u6784\u5efa\u6807\u7b7ex\u3001\u6807\u7b7ey \u7b2c\u4e09\u6b65: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u89e3\u7801\u5668 \u6784\u5efa\u57fa\u4e8eGRU\u548cAttention\u7684\u89e3\u7801\u5668 \u7b2c\u56db\u6b65: \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3 \u4ec0\u4e48\u662fteacher_forcing: \u5b83\u662f\u4e00\u79cd\u7528\u4e8e\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u7684\u8bad\u7ec3\u6280\u5de7, \u5728seq2seq\u67b6\u6784\u4e2d, \u6839\u636e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\uff0c\u89e3\u7801\u5668\u6bcf\u6b21\u5e94\u8be5\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\u7684\u4e00\u90e8\u5206, \u4f46\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u65e6\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u662f\u9519\u8bef\u7684\uff0c\u5c31\u4f1a\u5bfc\u81f4\u8fd9\u79cd\u9519\u8bef\u88ab\u7d2f\u79ef\uff0c\u65e0\u6cd5\u8fbe\u5230\u8bad\u7ec3\u6548\u679c, \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u673a\u5236\u6539\u53d8\u4e0a\u4e00\u6b65\u51fa\u9519\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u8bad\u7ec3\u65f6\u6211\u4eec\u662f\u5df2\u77e5\u6b63\u786e\u7684\u8f93\u51fa\u5e94\u8be5\u662f\u4ec0\u4e48\uff0c\u56e0\u6b64\u53ef\u4ee5\u5f3a\u5236\u5c06\u4e0a\u4e00\u6b65\u7ed3\u679c\u8bbe\u7f6e\u6210\u6b63\u786e\u7684\u8f93\u51fa, \u8fd9\u79cd\u65b9\u5f0f\u5c31\u53eb\u505ateacher_forcing teacher_forcing\u7684\u4f5c\u7528: \u80fd\u591f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u77eb\u6b63\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u907f\u514d\u5728\u5e8f\u5217\u751f\u6210\u7684\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u8fdb\u4e00\u6b65\u653e\u5927. \u53e6\u5916, teacher_forcing\u80fd\u591f\u6781\u5927\u7684\u52a0\u5feb\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4ee4\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u5feb\u66f4\u5e73\u7a33 \u6784\u5efa\u8bad\u7ec3\u51fd\u6570train \u8c03\u7528\u8bad\u7ec3\u51fd\u6570\u5e76\u6253\u5370\u65e5\u5fd7\u548c\u5236\u56fe \u635f\u5931\u66f2\u7ebf\u5206\u6790: \u4e00\u76f4\u4e0b\u964d\u7684\u635f\u5931\u66f2\u7ebf, \u8bf4\u660e\u6a21\u578b\u6b63\u5728\u6536\u655b, \u80fd\u591f\u4ece\u6570\u636e\u4e2d\u627e\u5230\u4e00\u4e9b\u89c4\u5f8b\u5e94\u7528\u4e8e\u6570\u636e \u7b2c\u4e94\u6b65: \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570, \u5e76\u8fdb\u884c\u6d4b\u8bd5\u4ee5\u53caAttention\u6548\u679c\u5206\u6790 \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570evaluate \u968f\u673a\u9009\u62e9\u6307\u5b9a\u6570\u91cf\u7684\u6570\u636e\u8fdb\u884c\u8bc4\u4f30 \u8fdb\u884c\u4e86Attention\u53ef\u89c6\u5316\u5206\u6790","title":"8 RNN\u6848\u4f8b seq2seq\u82f1\u8bd1\u6cd5"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#_1","text":"\u66f4\u6df1\u4e00\u6b65\u4e86\u89e3seq2seq\u6a21\u578b\u67b6\u6784\u548c\u7ffb\u8bd1\u6570\u636e\u96c6 \u638c\u63e1\u4f7f\u7528\u57fa\u4e8eGRU\u7684seq2seq\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u7ffb\u8bd1\u7684\u8fc7\u7a0b \u638c\u63e1Attention\u673a\u5236\u5728\u89e3\u7801\u5668\u7aef\u7684\u5b9e\u73b0\u8fc7\u7a0b","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#1-seq2seq","text":"","title":"1 seq2seq\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#11-seq2seq","text":"seq2seq\u6a21\u578b\u67b6\u6784\u5206\u6790: seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002\u5176\u4e2d\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5185\u90e8\u5b9e\u73b0\u90fd\u4f7f\u7528\u4e86GRU\u6a21\u578b \u56fe\u4e2d\u8868\u793a\u7684\u662f\u4e00\u4e2a\u4e2d\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1\uff1a\u6b22\u8fce \u6765 \u5317\u4eac \u2192 welcome to BeiJing\u3002\u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"\u6b22\u8fce \u6765 \u5317\u4eac\"\uff0c\u901a\u8fc7GRU\u6a21\u578b\u83b7\u5f97\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u6700\u540e\u5c06\u5b83\u4eec\u62fc\u63a5\u6210\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\uff1b\u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u9690\u5c42\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00 \u6211\u4eec\u7684\u6848\u4f8b\u901a\u8fc7\u82f1\u8bd1\u6cd5\u6765\u8bb2\u89e3seq2seq\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u3002","title":"1.1 seq2seq\u6a21\u578b\u67b6\u6784"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#2","text":"# \u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/\u4e0b - data/ - eng-fra-v2.txt i am from brazil . je viens du bresil . i am from france . je viens de france . i am from russia . je viens de russie . i am frying fish . je fais frire du poisson . i am not kidding . je ne blague pas . i am on duty now . maintenant je suis en service . i am on duty now . je suis actuellement en service . i am only joking . je ne fais que blaguer . i am out of time . je suis a court de temps . i am out of work . je suis au chomage . i am out of work . je suis sans travail . i am paid weekly . je suis payee a la semaine . i am pretty sure . je suis relativement sur . i am truly sorry . je suis vraiment desole . i am truly sorry . je suis vraiment desolee .","title":"2 \u6570\u636e\u96c6\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#3","text":"\u57fa\u4e8eGRU\u7684seq2seq\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u7ffb\u8bd1\u7684\u8fc7\u7a0b: \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5de5\u5177\u5305\u548c\u5de5\u5177\u51fd\u6570 \u7b2c\u4e8c\u6b65: \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u7b2c\u56db\u6b65: \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65: \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570, \u5e76\u8fdb\u884c\u6d4b\u8bd5\u4ee5\u53caAttention\u6548\u679c\u5206\u6790","title":"3 \u6848\u4f8b\u6b65\u9aa4"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#1","text":"# \u7528\u4e8e\u6b63\u5219\u8868\u8fbe\u5f0f import re # \u7528\u4e8e\u6784\u5efa\u7f51\u7edc\u7ed3\u6784\u548c\u51fd\u6570\u7684torch\u5de5\u5177\u5305 import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader # torch\u4e2d\u9884\u5b9a\u4e49\u7684\u4f18\u5316\u65b9\u6cd5\u5de5\u5177\u5305 import torch.optim as optim import time # \u7528\u4e8e\u968f\u673a\u751f\u6210\u6570\u636e import random import matplotlib.pyplot as plt # \u8bbe\u5907\u9009\u62e9, \u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5728cuda\u6216\u8005cpu\u4e0a\u8fd0\u884c\u4f60\u7684\u4ee3\u7801 device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) # \u8d77\u59cb\u6807\u5fd7 SOS_token = 0 # \u7ed3\u675f\u6807\u5fd7 EOS_token = 1 # \u6700\u5927\u53e5\u5b50\u957f\u5ea6\u4e0d\u80fd\u8d85\u8fc710\u4e2a (\u5305\u542b\u6807\u70b9) MAX_LENGTH = 10 # \u6570\u636e\u6587\u4ef6\u8def\u5f84 data_path = './data/eng-fra-v2.txt' # \u6587\u672c\u6e05\u6d17\u5de5\u5177\u51fd\u6570 def normalizeString ( s ): \"\"\"\u5b57\u7b26\u4e32\u89c4\u8303\u5316\u51fd\u6570, \u53c2\u6570s\u4ee3\u8868\u4f20\u5165\u7684\u5b57\u7b26\u4e32\"\"\" s = s . lower () . strip () # \u5728.!?\u524d\u52a0\u4e00\u4e2a\u7a7a\u683c \u8fd9\u91cc\u7684\\1\u8868\u793a\u7b2c\u4e00\u4e2a\u5206\u7ec4 \u6b63\u5219\u4e2d\u7684\\num s = re . sub ( r \"([.!?])\" , r \" \\1\" , s ) # s = re.sub(r\"([.!?])\", r\" \", s) # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u5c06\u5b57\u7b26\u4e32\u4e2d \u4e0d\u662f \u5927\u5c0f\u5199\u5b57\u6bcd\u548c\u6b63\u5e38\u6807\u70b9\u7684\u90fd\u66ff\u6362\u6210\u7a7a\u683c s = re . sub ( r \"[^a-zA-Z.!?]+\" , r \" \" , s ) return s","title":"1 \u5bfc\u5165\u5de5\u5177\u5305\u548c\u5de5\u5177\u51fd\u6570"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#2_1","text":"\u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42","title":"2 \u6570\u636e\u9884\u5904\u7406"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#1_1","text":"\u6e05\u6d17\u6587\u672c\u548c\u6784\u5efa\u6587\u672c\u5b57\u5178\u601d\u8def\u5206\u6790 # my_getdata() \u6e05\u6d17\u6587\u672c\u6784\u5efa\u5b57\u5178\u601d\u8def\u5206\u6790 # 1 \u6309\u884c\u8bfb\u6587\u4ef6 open().read().strip().split(\\n) my_lines # 2 \u6309\u884c\u6e05\u6d17\u6587\u672c \u6784\u5efa\u8bed\u8a00\u5bf9 my_pairs[] tmppair[] # 2-1\u683c\u5f0f [['\u82f1\u6587', '\u6cd5\u6587'], ['\u82f1\u6587', '\u6cd5\u6587'], ['\u82f1\u6587', '\u6cd5\u6587'], ['\u82f1\u6587', '\u6cd5\u6587']....] # 2-2\u8c03\u7528\u6e05\u6d17\u6587\u672c\u5de5\u5177\u51fd\u6570normalizeString(s) # 3 \u904d\u5386\u8bed\u8a00\u5bf9 \u6784\u5efa\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 my_pairs->pair->pair[0].split(' ') pair[1].split(' ')->word # 3-1 english_word2index english_word_n french_word2index french_word_n # \u5176\u4e2d english_word2index = {0: \"SOS\", 1: \"EOS\"} english_word_n=2 # 3-2 english_index2word french_index2word # 4 \u8fd4\u56de\u6570\u636e\u76847\u4e2a\u7ed3\u679c # english_word2index, english_index2word, english_word_n, # french_word2index, french_index2word, french_word_n, my_pairs \u4ee3\u7801\u5b9e\u73b0 def my_getdata(): # 1 \u6309\u884c\u8bfb\u6587\u4ef6 open().read().strip().split(\\n) my_lines = open(data_path, encoding='utf-8').read().strip().split('\\n') print('my_lines--->', len(my_lines)) # 2 \u6309\u884c\u6e05\u6d17\u6587\u672c \u6784\u5efa\u8bed\u8a00\u5bf9 my_pairs # \u683c\u5f0f [['\u82f1\u6587\u53e5\u5b50', '\u6cd5\u6587\u53e5\u5b50'], ['\u82f1\u6587\u53e5\u5b50', '\u6cd5\u6587\u53e5\u5b50'], ['\u82f1\u6587\u53e5\u5b50', '\u6cd5\u6587\u53e5\u5b50'], ... ] # tmp_pair, my_pairs = [], [] # for l in my_lines: # for s in l.split('\\t'): # tmp_pair.append(normalizeString(s)) # my_pairs.append(tmp_pair) # tmp_pair = [] my_pairs = [[normalizeString(s) for s in l.split('\\t')] for l in my_lines] print('len(pairs)--->', len(my_pairs)) # \u6253\u5370\u524d4\u6761\u6570\u636e print(my_pairs[:4]) # \u6253\u5370\u7b2c8000\u6761\u7684\u82f1\u6587 \u6cd5\u6587\u6570\u636e print('my_pairs[8000][0]--->', my_pairs[8000][0]) print('my_pairs[8000][1]--->', my_pairs[8000][1]) # 3 \u904d\u5386\u8bed\u8a00\u5bf9 \u6784\u5efa\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 # 3-1 english_word2index english_word_n french_word2index french_word_n english_word2index = {\"SOS\": 0, \"EOS\": 1} english_word_n = 2 french_word2index = {\"SOS\": 0, \"EOS\": 1} french_word_n = 2 # \u904d\u5386\u8bed\u8a00\u5bf9 \u83b7\u53d6\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 for pair in my_pairs: for word in pair[0].split(' '): if word not in english_word2index: english_word2index[word] = english_word_n english_word_n += 1 for word in pair[1].split(' '): if word not in french_word2index: french_word2index[word] = french_word_n french_word_n += 1 # 3-2 english_index2word french_index2word english_index2word = {v:k for k, v in english_word2index.items()} french_index2word = {v:k for k, v in french_word2index.items()} print('len(english_word2index)-->', len(english_word2index)) print('len(french_word2index)-->', len(french_word2index)) print('english_word_n--->', english_word_n, 'french_word_n-->', french_word_n) return english_word2index, english_index2word, english_word_n, french_word2index, french_index2word, french_word_n, my_pairs \u8c03\u7528 # \u5168\u5c40\u51fd\u6570 \u83b7\u53d6\u82f1\u8bed\u5355\u8bcd\u5b57\u5178 \u6cd5\u8bed\u5355\u8bcd\u5b57\u5178 \u8bed\u8a00\u5bf9\u5217\u8868my_pairs english_word2index , english_index2word , english_word_n , \\ french_word2index , french_index2word , french_word_n , \\ my_pairs = my_getdata () \u8f93\u51fa\u6548\u679c: my_lines ---> 10599 len ( pairs ) ---> 10599 [[ 'i m .' , 'j ai ans .' ], [ 'i m ok .' , 'je vais bien .' ], [ 'i m ok .' , 'ca va .' ], [ 'i m fat .' , 'je suis gras .' ]] my_pairs [ 8000 ][ 0 ] ---> they re in the science lab . my_pairs [ 8000 ][ 1 ] ---> elles sont dans le laboratoire de sciences . len ( english_word2index ) --> 2803 len ( french_word2index ) --> 4345 english_word_n ---> 2803 french_word_n --> 4345 x . shape torch . Size ([ 1 , 9 ]) tensor ([[ 75 , 40 , 102 , 103 , 677 , 42 , 21 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 119 , 25 , 164 , 165 , 3222 , 5 , 1 ]]) x . shape torch . Size ([ 1 , 5 ]) tensor ([[ 14 , 15 , 44 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 5 ]) tensor ([[ 24 , 25 , 62 , 5 , 1 ]]) x . shape torch . Size ([ 1 , 8 ]) tensor ([[ 2 , 3 , 147 , 61 , 532 , 1143 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 6 , 297 , 7 , 246 , 102 , 5 , 1 ]])","title":"1 \u6e05\u6d17\u6587\u672c\u548c\u6784\u5efa\u6587\u672c\u5b57\u5178"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#2_2","text":"# \u539f\u59cb\u6570\u636e -> \u6570\u636e\u6e90MyPairsDataset --> \u6570\u636e\u8fed\u4ee3\u5668DataLoader # \u6784\u9020\u6570\u636e\u6e90 MyPairsDataset\uff0c\u628a\u8bed\u6599xy \u6587\u672c\u6570\u503c\u5316 \u518d\u8f6c\u6210tensor_x tensor_y # 1 __init__(self, my_pairs)\u51fd\u6570 \u8bbe\u7f6eself.my_pairs \u6761\u76ee\u6570self.sample_len # 2 __len__(self)\u51fd\u6570 \u83b7\u53d6\u6837\u672c\u6761\u6570 # 3 __getitem__(self, index)\u51fd\u6570 \u83b7\u53d6\u7b2c\u51e0\u6761\u6837\u672c\u6570\u636e # \u6309\u7d22\u5f15 \u83b7\u53d6\u6570\u636e\u6837\u672c x y # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 word2id x.append(EOS_token) # \u6837\u672cy \u6587\u672c\u6570\u503c\u5316 word2id y.append(EOS_token) # \u8fd4\u56detensor_x, tensor_y class MyPairsDataset ( Dataset ): def __init__ ( self , my_pairs ): # \u6837\u672cx self . my_pairs = my_pairs # \u6837\u672c\u6761\u76ee\u6570 self . sample_len = len ( my_pairs ) # \u83b7\u53d6\u6837\u672c\u6761\u6570 def __len__ ( self ): return self . sample_len # \u83b7\u53d6\u7b2c\u51e0\u6761 \u6837\u672c\u6570\u636e def __getitem__ ( self , index ): # \u5bf9index\u5f02\u5e38\u503c\u8fdb\u884c\u4fee\u6b63 [0, self.sample_len-1] index = min ( max ( index , 0 ), self . sample_len - 1 ) # \u6309\u7d22\u5f15\u83b7\u53d6 \u6570\u636e\u6837\u672c x y x = self . my_pairs [ index ][ 0 ] y = self . my_pairs [ index ][ 1 ] # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 x = [ english_word2index [ word ] for word in x . split ( ' ' )] x . append ( EOS_token ) tensor_x = torch . tensor ( x , dtype = torch . long , device = device ) # \u6837\u672cy \u6587\u672c\u6570\u503c\u5316 y = [ french_word2index [ word ] for word in y . split ( ' ' )] y . append ( EOS_token ) tensor_y = torch . tensor ( y , dtype = torch . long , device = device ) # \u6ce8\u610f tensor_x tensor_y\u90fd\u662f\u4e00\u7ef4\u6570\u7ec4\uff0c\u901a\u8fc7DataLoader\u62ff\u51fa\u6570\u636e\u662f\u4e8c\u7ef4\u6570\u636e # print('tensor_y.shape===>', tensor_y.shape, tensor_y) # \u8fd4\u56de\u7ed3\u679c return tensor_x , tensor_y","title":"2 \u6784\u5efa\u6570\u636e\u6e90\u5bf9\u8c61"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#3_1","text":"def dm_test_MyPairsDataset (): # 1 \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # 2 \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) if i == 1 : break \u8f93\u51fa\u6548\u679c: x . shape torch . Size ([ 1 , 8 ]) tensor ([[ 2 , 16 , 33 , 518 , 589 , 1460 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 8 ]) tensor ([[ 6 , 11 , 52 , 101 , 1358 , 964 , 5 , 1 ]]) x . shape torch . Size ([ 1 , 6 ]) tensor ([[ 129 , 78 , 677 , 429 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 118 , 214 , 1073 , 194 , 778 , 5 , 1 ]])","title":"3 \u6784\u5efa\u6570\u636e\u8fed\u4ee3\u5668"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#3-gru","text":"","title":"3 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#1-gru","text":"\u7f16\u7801\u5668\u7ed3\u6784\u56fe: \u5b9e\u73b0\u601d\u8def\u5206\u6790 # EncoderRNN\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u5b9a\u4e492\u4e2a\u5c42 self.embedding self.gru (batch_first=True) # def __init__(self, input_size, hidden_size): # 2803 256 # 2 forward(input, hidden)\u51fd\u6570\uff0c\u8fd4\u56deoutput, hidden # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,6] --> [1,6,256] # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u5f62\u72b6\u53d8\u5316 gru([1,6,256],[1,1,256]) --> [1,6,256] [1,1,256] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6 torch.zeros(1, 1, self.hidden_size, device=device) \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668 class EncoderRNN ( nn . Module ): def __init__ ( self , input_size , hidden_size ): # input_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u5355\u8bcd\u6570 eg\uff1a2803 # hidden_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u6570 eg 256 super ( EncoderRNN , self ) . __init__ () self . input_size = input_size self . hidden_size = hidden_size # \u5b9e\u4f8b\u5316nn.Embedding\u5c42 self . embedding = nn . Embedding ( input_size , hidden_size ) # \u5b9e\u4f8b\u5316nn.GRU\u5c42 \u6ce8\u610f\u53c2\u6570batch_first=True self . gru = nn . GRU ( hidden_size , hidden_size , batch_first = True ) def forward ( self , input , hidden ): # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,6] --> [1,6,256] output = self . embedding ( input ) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u6570\u636e\u5f62\u72b6 gru([1,6,256],[1,1,256]) --> [1,6,256] [1,1,256] output , hidden = self . gru ( output , hidden ) return output , hidden def inithidden ( self ): # \u5c06\u9690\u5c42\u5f20\u91cf\u521d\u59cb\u5316\u6210\u4e3a1x1xself.hidden_size\u5927\u5c0f\u7684\u5f20\u91cf return torch . zeros ( 1 , 1 , self . hidden_size , device = device ) \u8c03\u7528 def dm_test_EncoderRNN (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # my_encoderrnn = EncoderRNN ( input_size , hidden_size ) print ( 'my_encoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_encoderrnn ) # \u7ed9encode\u6a21\u578b\u5582\u6570\u636e for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) # \u4e00\u6b21\u6027\u7684\u9001\u6570\u636e hidden = my_encoderrnn . inithidden () encode_output_c , hidden = my_encoderrnn ( x , hidden ) print ( 'encode_output_c.shape--->' , encode_output_c . shape , encode_output_c ) # \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7ed9\u4e3a\u6a21\u578b\u5582\u6570\u636e hidden = my_encoderrnn . inithidden () for i in range ( x . shape [ 1 ]): tmp = x [ 0 ][ i ] . view ( 1 , - 1 ) output , hidden = my_encoderrnn ( tmp , hidden ) print ( '\u89c2\u5bdf\uff1a\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65output\u8f93\u51fa\u662f\u5426\u76f8\u7b49' ) # hidden_size = 8 \u6548\u679c\u6bd4\u8f83\u597d print ( 'encode_output_c[0][-1]===>' , encode_output_c [ 0 ][ - 1 ]) print ( 'output===>' , output ) break \u8f93\u51fa\u6548\u679c: # \u672c\u8f93\u51fa\u6548\u679c\u4e3ahidden_size = 8 x . shape torch . Size ([ 1 , 6 ]) tensor ([[ 129 , 124 , 270 , 558 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 7 ]) tensor ([[ 118 , 214 , 101 , 1253 , 1028 , 5 , 1 ]]) encode_output_c . shape ---> torch . Size ([ 1 , 6 , 8 ]) tensor ([[[ - 0.0984 , 0.4267 , - 0.2120 , 0.0923 , 0.1525 , - 0.0378 , 0.2493 , - 0.2665 ], [ - 0.1388 , 0.5363 , - 0.4522 , - 0.2819 , - 0.2070 , 0.0795 , 0.6262 , - 0.2359 ], [ - 0.4593 , 0.2499 , 0.1159 , 0.3519 , - 0.0852 , - 0.3621 , 0.1980 , - 0.1853 ], [ - 0.4407 , 0.1974 , 0.6873 , - 0.0483 , - 0.2730 , - 0.2190 , 0.0587 , 0.2320 ], [ - 0.6544 , 0.1990 , 0.7534 , - 0.2347 , - 0.0686 , - 0.5532 , 0.0624 , 0.4083 ], [ - 0.2941 , - 0.0427 , 0.1017 , - 0.1057 , 0.1983 , - 0.1066 , 0.0881 , - 0.3936 ]]], grad_fn =< TransposeBackward1 > ) \u89c2\u5bdf \uff1a \u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65output\u8f93\u51fa\u662f\u5426\u76f8\u7b49 encode_output_c [ 0 ][ - 1 ] ===> tensor ([ - 0.2941 , - 0.0427 , 0.1017 , - 0.1057 , 0.1983 , - 0.1066 , 0.0881 , - 0.3936 ], grad_fn =< SelectBackward0 > ) output ===> tensor ([[[ - 0.2941 , - 0.0427 , 0.1017 , - 0.1057 , 0.1983 , - 0.1066 , 0.0881 , - 0.3936 ]]], grad_fn =< TransposeBackward1 > )","title":"1 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#2-gru","text":"\u89e3\u7801\u5668\u7ed3\u6784\u56fe: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u89e3\u7801\u5668\u5b9e\u73b0\u601d\u8def\u5206\u6790 # DecoderRNN \u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # \u89e3\u7801\u5668\u7684\u4f5c\u7528\uff1a\u63d0\u53d6\u4e8b\u7269\u7279\u5f81 \u8fdb\u884c\u5206\u7c7b\uff08\u6240\u4ee5\u6bd4 \u7f16\u7801\u5668 \u591a\u4e86 \u7ebf\u6027\u5c42 \u548c softmax\u5c42\uff09 # 1 init\u51fd\u6570 \u5b9a\u4e49\u56db\u4e2a\u5c42 self.embedding self.gru self.out self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, output_size, hidden_size): # 4345 256 # 2 forward(input, hidden)\u51fd\u6570\uff0c\u8fd4\u56deoutput, hidden # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] # \u6570\u636e\u7ecf\u8fc7relu()\u5c42 output = F.relu(output) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u5f62\u72b6\u53d8\u5316 gru([1,1,256],[1,1,256]) --> [1,1,256] [1,1,256] # \u6570\u636e\u7ed3\u679cout\u5c42 \u5f62\u72b6\u53d8\u5316 [1,1,256]->[1,256]-->[1,4345] # \u8fd4\u56de \u89e3\u7801\u5668\u5206\u7c7boutput[1,4345]\uff0c\u6700\u540e\u9690\u5c42\u5f20\u91cfhidden[1,1,256] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6 torch.zeros(1, 1, self.hidden_size, device=device) \u7f16\u7801\u5b9e\u73b0 class DecoderRNN ( nn . Module ): def __init__ ( self , output_size , hidden_size ): # output_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u5355\u8bcd\u6570 eg\uff1a4345 # hidden_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u6570 eg 256 super ( DecoderRNN , self ) . __init__ () self . output_size = output_size self . hidden_size = hidden_size # \u5b9e\u4f8b\u5316\u8bcd\u5d4c\u5165\u5c42 self . embedding = nn . Embedding ( output_size , hidden_size ) # \u5b9e\u4f8b\u5316gru\u5c42\uff0c\u8f93\u5165\u5c3a\u5bf8256 \u8f93\u51fa\u5c3a\u5bf8256 # \u56e0\u89e3\u7801\u5668\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u89e3\u7801 batch_first=True \u610f\u4e49\u4e0d\u5927 self . gru = nn . GRU ( hidden_size , hidden_size , batch_first = True ) # \u5b9e\u4f8b\u5316\u7ebf\u6027\u8f93\u51fa\u5c42out \u8f93\u5165\u5c3a\u5bf8256 \u8f93\u51fa\u5c3a\u5bf84345 self . out = nn . Linear ( hidden_size , output_size ) # \u5b9e\u4f8b\u5316softomax\u5c42 \u6570\u503c\u5f52\u4e00\u5316 \u4ee5\u4fbf\u5206\u7c7b self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden ): # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 # \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] or [1,6]--->[1,6,256] output = self . embedding ( input ) # \u6570\u636e\u7ed3\u679crelu\u5c42\u4f7fEmbedding\u77e9\u9635\u66f4\u7a00\u758f\uff0c\u4ee5\u9632\u6b62\u8fc7\u62df\u5408 output = F . relu ( output ) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 # \u6570\u636e\u5f62\u72b6 gru([1,1,256],[1,1,256]) --> [1,1,256] [1,1,256] output , hidden = self . gru ( output , hidden ) # \u6570\u636e\u7ecf\u8fc7softmax\u5c42 \u5f52\u4e00\u5316 # \u6570\u636e\u5f62\u72b6\u53d8\u5316 [1,1,256]->[1,256] ---> [1,4345] output = self . softmax ( self . out ( output [ 0 ])) return output , hidden def inithidden ( self ): # \u5c06\u9690\u5c42\u5f20\u91cf\u521d\u59cb\u5316\u6210\u4e3a1x1xself.hidden_size\u5927\u5c0f\u7684\u5f20\u91cf return torch . zeros ( 1 , 1 , self . hidden_size , device = device ) \u8c03\u7528 def dm03_test_DecoderRNN (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_encoderrnn = EncoderRNN ( input_size , hidden_size ) print ( 'my_encoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_encoderrnn ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = french_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_decoderrnn = DecoderRNN ( input_size , hidden_size ) print ( 'my_decoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_decoderrnn ) # \u7ed9\u6a21\u578b\u5582\u6570\u636e \u5b8c\u6574\u6f14\u793a\u7f16\u7801 \u89e3\u7801\u6d41\u7a0b for i , ( x , y ) in enumerate ( mydataloader ): print ( 'x.shape' , x . shape , x ) print ( 'y.shape' , y . shape , y ) # 1 \u7f16\u7801\uff1a\u4e00\u6b21\u6027\u7684\u9001\u6570\u636e hidden = my_encoderrnn . inithidden () encode_output_c , hidden = my_encoderrnn ( x , hidden ) print ( 'encode_output_c.shape--->' , encode_output_c . shape , encode_output_c ) print ( '\u89c2\u5bdf\uff1a\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65output\u8f93\u51fa' ) # hidden_size = 8 \u6548\u679c\u6bd4\u8f83\u597d print ( 'encode_output_c[0][-1]===>' , encode_output_c [ 0 ][ - 1 ]) # 2 \u89e3\u7801: \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u89e3\u7801 # \u6700\u540e1\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa \u4f5c\u4e3a \u89e3\u7801\u5668\u7684\u7b2c1\u4e2a\u65f6\u95f4\u6b65\u9690\u85cf\u5c42\u8f93\u5165 for i in range ( y . shape [ 1 ]): tmp = y [ 0 ][ i ] . view ( 1 , - 1 ) output , hidden = my_decoderrnn ( tmp , hidden ) print ( '\u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output===>' , output . shape ) break \u8f93\u51fa\u6548\u679c my_encoderrnn\u6a21\u578b\u7ed3\u6784 ---> EncoderRNN ( ( embedding ): Embedding ( 2803 , 256 ) ( gru ): GRU ( 256 , 256 , batch_first = True ) ) my_decoderrnn\u6a21\u578b\u7ed3\u6784 ---> DecoderRNN ( ( embedding ): Embedding ( 4345 , 256 ) ( gru ): GRU ( 256 , 256 , batch_first = True ) ( out ): Linear ( in_features = 256 , out_features = 4345 , bias = True ) ( softmax ): LogSoftmax ( dim =- 1 ) ) x . shape torch . Size ([ 1 , 8 ]) tensor ([[ 14 , 40 , 883 , 677 , 589 , 609 , 4 , 1 ]]) y . shape torch . Size ([ 1 , 6 ]) tensor ([[ 1358 , 1125 , 247 , 2863 , 5 , 1 ]]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ]) \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51fa\u67654345\u79cd\u53ef\u80fd output ===> torch . Size ([ 1 , 4345 ])","title":"2 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u89e3\u7801\u5668"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#3-gruattention","text":"\u89e3\u7801\u5668\u7ed3\u6784\u56fe: \u5b9e\u73b0\u601d\u8def\u5206\u6790 # \u6784\u5efa\u57fa\u4e8eGRU\u548cAttention\u7684\u89e3\u7801\u5668 # AttnDecoderRNN \u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790\uff1a # 1 init\u51fd\u6570 \u5b9a\u4e49\u516d\u4e2a\u5c42 # self.embedding self.attn self.attn_combine # self.gru self.out self.softmax=nn.LogSoftmax(dim=-1) # def __init__(self, output_size, hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):: # 4345 256 # 2 forward(input, hidden, encoder_outputs)\u51fd\u6570\uff0c\u8fd4\u56deoutput, hidden # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] # 1 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,10] # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,256] # 3 q \u4e0e attn_applied \u878d\u5408\uff0c\u7ecf\u8fc7\u5c42attn_combine \u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,256] # \u6570\u636e\u7ecf\u8fc7relu()\u5c42 output = F.relu(output) # \u6570\u636e\u7ecf\u8fc7gru\u5c42 \u5f62\u72b6\u53d8\u5316 gru([1,1,256],[1,1,256]) --> [1,1,256] [1,1,256] # \u8fd4\u56de # \u8fd4\u56de\u89e3\u7801\u5668\u5206\u7c7boutput[1,4345]\uff0c\u6700\u540e\u9690\u5c42\u5f20\u91cfhidden[1,1,256] \u6ce8\u610f\u529b\u6743\u91cd\u5f20\u91cfattn_weights[1,10] # 3 \u521d\u59cb\u5316\u9690\u85cf\u5c42\u8f93\u5165\u6570\u636e inithidden() # \u5f62\u72b6 torch.zeros(1, 1, self.hidden_size, device=device) # \u76f8\u5bf9\u4f20\u7edfRNN\u89e3\u7801 AttnDecoderRNN\u7c7b\u591a\u4e86\u6ce8\u610f\u529b\u673a\u5236,\u9700\u8981\u6784\u5efaQKV # 1 \u5728init\u51fd\u6570\u4e2d (self, output_size, hidden_size, dropout_p=0.1, max_length=MAX_LENGTH) # \u589e\u52a0\u5c42 self.attn self.attn_combine self.dropout # 2 \u589e\u52a0\u51fd\u6570 attentionQKV(self, Q, K, V) # 3 \u51fd\u6570forward(self, input, hidden, encoder_outputs) # encoder_outputs \u6bcf\u4e2a\u65f6\u95f4\u6b65\u89e3\u7801\u51c6\u5907qkv \u8c03\u7528attentionQKV # \u51fd\u6570\u8fd4\u56de\u503c output, hidden, attn_weights # 4 \u8c03\u7528\u9700\u8981\u51c6\u5907\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfC encode_output_c \u7f16\u7801\u5b9e\u73b0 class AttnDecoderRNN ( nn . Module ): def __init__ ( self , output_size , hidden_size , dropout_p = 0.1 , max_length = MAX_LENGTH ): # output_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u5355\u8bcd\u6570 eg\uff1a4345 # hidden_size \u7f16\u7801\u5668 \u8bcd\u5d4c\u5165\u5c42\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u6570 eg 256 # dropout_p \u7f6e\u96f6\u6bd4\u7387\uff0c\u9ed8\u8ba40.1, # max_length \u6700\u5927\u957f\u5ea610 super ( AttnDecoderRNN , self ) . __init__ () self . output_size = output_size self . hidden_size = hidden_size self . dropout_p = dropout_p self . max_length = max_length # \u5b9a\u4e49nn.Embedding\u5c42 nn.Embedding(4345,256) self . embedding = nn . Embedding ( self . output_size , self . hidden_size ) # \u5b9a\u4e49\u7ebf\u6027\u5c421\uff1a\u6c42q\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 self . attn = nn . Linear ( self . hidden_size * 2 , self . max_length ) # \u5b9a\u4e49\u7ebf\u6027\u5c422\uff1aq+\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a\u878d\u5408\u540e\uff0c\u5728\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa self . attn_combine = nn . Linear ( self . hidden_size * 2 , self . hidden_size ) # \u5b9a\u4e49dropout\u5c42 self . dropout = nn . Dropout ( self . dropout_p ) # \u5b9a\u4e49gru\u5c42 self . gru = nn . GRU ( self . hidden_size , self . hidden_size , batch_first = True ) # \u5b9a\u4e49out\u5c42 \u89e3\u7801\u5668\u6309\u7167\u7c7b\u522b\u8fdb\u884c\u8f93\u51fa(256,4345) self . out = nn . Linear ( self . hidden_size , self . output_size ) # \u5b9e\u4f8b\u5316softomax\u5c42 \u6570\u503c\u5f52\u4e00\u5316 \u4ee5\u4fbf\u5206\u7c7b self . softmax = nn . LogSoftmax ( dim =- 1 ) def forward ( self , input , hidden , encoder_outputs ): # input\u4ee3\u8868q [1,1] \u4e8c\u7ef4\u6570\u636e hidden\u4ee3\u8868k [1,1,256] encoder_outputs\u4ee3\u8868v [10,256] # \u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 # \u6570\u636e\u5f62\u72b6 [1,1] --> [1,1,256] embedded = self . embedding ( input ) # \u4f7f\u7528dropout\u8fdb\u884c\u968f\u673a\u4e22\u5f03\uff0c\u9632\u6b62\u8fc7\u62df\u5408 embedded = self . dropout ( embedded ) # 1 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03, attn_weights[1,10] attn_weights = F . softmax ( self . attn ( torch . cat (( embedded [ 0 ], hidden [ 0 ]), 1 )), dim = 1 ) # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm\u8fd0\u7b97, attn_applied[1,1,256] # [1,1,10],[1,10,256] ---> [1,1,256] attn_applied = torch . bmm ( attn_weights . unsqueeze ( 0 ), encoder_outputs . unsqueeze ( 0 )) # 3 q \u4e0e attn_applied \u878d\u5408\uff0c\u518d\u6309\u7167\u6307\u5b9a\u7ef4\u5ea6\u8f93\u51fa output[1,1,256] output = torch . cat (( embedded [ 0 ], attn_applied [ 0 ]), 1 ) output = self . attn_combine ( output ) . unsqueeze ( 0 ) # \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a \u4f7f\u7528relu\u6fc0\u6d3b output = F . relu ( output ) # \u67e5\u8be2\u5f20\u91cf\u7ecf\u8fc7gru\u3001softmax\u8fdb\u884c\u5206\u7c7b\u7ed3\u679c\u8f93\u51fa # \u6570\u636e\u5f62\u72b6[1,1,256],[1,1,256] --> [1,1,256], [1,1,256] output , hidden = self . gru ( output , hidden ) # \u6570\u636e\u5f62\u72b6[1,1,256]->[1,256]->[1,4345] output = self . softmax ( self . out ( output [ 0 ])) # \u8fd4\u56de\u89e3\u7801\u5668\u5206\u7c7boutput[1,4345]\uff0c\u6700\u540e\u9690\u5c42\u5f20\u91cfhidden[1,1,256] \u6ce8\u610f\u529b\u6743\u91cd\u5f20\u91cfattn_weights[1,10] return output , hidden , attn_weights def inithidden ( self ): # \u5c06\u9690\u5c42\u5f20\u91cf\u521d\u59cb\u5316\u6210\u4e3a1x1xself.hidden_size\u5927\u5c0f\u7684\u5f20\u91cf return torch . zeros ( 1 , 1 , self . hidden_size , device = device ) \u8c03\u7528 def dm_test_AttnDecoderRNN (): # 1 \u5b9e\u4f8b\u5316 \u6570\u636e\u96c6\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # 2 \u5b9e\u4f8b\u5316 \u6570\u636e\u52a0\u8f7d\u5668\u5bf9\u8c61 mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316 \u7f16\u7801\u5668my_encoderrnn my_encoderrnn = EncoderRNN ( english_word_n , 256 ) # \u5b9e\u4f8b\u5316 \u89e3\u7801\u5668DecoderRNN my_attndecoderrnn = AttnDecoderRNN ( french_word_n , 256 ) # 3 \u904d\u5386\u6570\u636e\u8fed\u4ee3\u5668 for i , ( x , y ) in enumerate ( mydataloader ): # \u7f16\u7801-\u65b9\u6cd51 \u4e00\u6b21\u6027\u7ed9\u6a21\u578b\u9001\u6570\u636e hidden = my_encoderrnn . inithidden () print ( 'x--->' , x . shape , x ) print ( 'y--->' , y . shape , y ) # [1, 6, 256], [1, 1, 256]) --> [1, 6, 256][1, 1, 256] output , hidden = my_encoderrnn ( x , hidden ) # print('output-->', output.shape, output) # print('\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u53d6\u51faoutput[0,-1]-->', output[0, -1].shape, output[0, -1]) # \u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfC encode_output_c = torch . zeros ( MAX_LENGTH , my_encoderrnn . hidden_size , device = device ) for idx in range ( output . shape [ 1 ]): encode_output_c [ idx ] = output [ 0 , idx ] # # \u7f16\u7801-\u65b9\u6cd52 \u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7ed9\u6a21\u578b\u9001\u6570\u636e # hidden = my_encoderrnn.inithidden() # for i in range(x.shape[1]): # tmp = x[0][i].view(1, -1) # # [1, 1, 256], [1, 1, 256]) --> [1, 1, 256][1, 1, 256] # output, hidden = my_encoderrnn(tmp, hidden) # print('\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26output', output.shape, output) # \u89e3\u7801-\u5fc5\u987b\u4e00\u4e2a\u5b57\u7b26\u4e00\u4e2a\u5b57\u7b26\u7684\u89e3\u7801 for i in range ( y . shape [ 1 ]): tmp = y [ 0 ][ i ] . view ( 1 , - 1 ) output , hidden , attn_weights = my_attndecoderrnn ( tmp , hidden , encode_output_c ) print ( '\u89e3\u7801output.shape' , output . shape ) print ( '\u89e3\u7801hidden.shape' , hidden . shape ) print ( '\u89e3\u7801attn_weights.shape' , attn_weights . shape ) break \u8f93\u51fa\u6548\u679c: x ---> torch . Size ([ 1 , 7 ]) tensor ([[ 129 , 78 , 1873 , 294 , 1215 , 4 , 1 ]]) y ---> torch . Size ([ 1 , 6 ]) tensor ([[ 210 , 3097 , 248 , 3095 , 5 , 1 ]]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ]) \u89e3\u7801output . shape torch . Size ([ 1 , 4345 ]) \u89e3\u7801hidden . shape torch . Size ([ 1 , 1 , 256 ]) \u89e3\u7801attn_weights . shape torch . Size ([ 1 , 10 ])","title":"3 \u6784\u5efa\u57fa\u4e8eGRU\u548cAttention\u7684\u89e3\u7801\u5668"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#4","text":"","title":"4 \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#1-teacher_forcing","text":"\u5b83\u662f\u4e00\u79cd\u7528\u4e8e\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u7684\u8bad\u7ec3\u6280\u5de7, \u5728seq2seq\u67b6\u6784\u4e2d, \u6839\u636e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\uff0c\u89e3\u7801\u5668\u6bcf\u6b21\u5e94\u8be5\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\u7684\u4e00\u90e8\u5206, \u4f46\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u65e6\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u662f\u9519\u8bef\u7684\uff0c\u5c31\u4f1a\u5bfc\u81f4\u8fd9\u79cd\u9519\u8bef\u88ab\u7d2f\u79ef\uff0c\u65e0\u6cd5\u8fbe\u5230\u8bad\u7ec3\u6548\u679c, \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u673a\u5236\u6539\u53d8\u4e0a\u4e00\u6b65\u51fa\u9519\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u8bad\u7ec3\u65f6\u6211\u4eec\u662f\u5df2\u77e5\u6b63\u786e\u7684\u8f93\u51fa\u5e94\u8be5\u662f\u4ec0\u4e48\uff0c\u56e0\u6b64\u53ef\u4ee5\u5f3a\u5236\u5c06\u4e0a\u4e00\u6b65\u7ed3\u679c\u8bbe\u7f6e\u6210\u6b63\u786e\u7684\u8f93\u51fa, \u8fd9\u79cd\u65b9\u5f0f\u5c31\u53eb\u505ateacher_forcing.","title":"1 teacher_forcing\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#2-teacher_forcing","text":"\u80fd\u591f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u77eb\u6b63\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u907f\u514d\u5728\u5e8f\u5217\u751f\u6210\u7684\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u8fdb\u4e00\u6b65\u653e\u5927. teacher_forcing\u80fd\u591f\u6781\u5927\u7684\u52a0\u5feb\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4ee4\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u5feb\u66f4\u5e73\u7a33.","title":"2 teacher_forcing\u7684\u4f5c\u7528"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#3_2","text":"\u6a21\u578b\u8bad\u7ec3\u53c2\u6570 # \u6a21\u578b\u8bad\u7ec3\u53c2\u6570 mylr = 1e-4 epochs = 2 # \u8bbe\u7f6eteacher_forcing\u6bd4\u7387\u4e3a0.5 teacher_forcing_ratio = 0.5 print_interval_num = 1000 plot_interval_num = 100 \u5b9e\u73b0\u601d\u8def\u5206\u6790 # \u5185\u90e8\u8fed\u4ee3\u8bad\u7ec3\u51fd\u6570Train_Iters # 1 \u7f16\u7801 encode_output, encode_hidden = my_encoderrnn(x, encode_hidden) # \u6570\u636e\u5f62\u72b6 eg [1,6],[1,1,256] --> [1,6,256],[1,1,256] # 2 \u89e3\u7801\u53c2\u6570\u51c6\u5907\u548c\u89e3\u7801 # \u89e3\u7801\u53c2\u65701 \u56fa\u5b9a\u957f\u5ea6C encoder_outputs_c = torch.zeros(MAX_LENGTH, my_encoderrnn.hidden_size, device=device) # \u89e3\u7801\u53c2\u65702 decode_hidden # \u89e3\u7801\u53c2\u65703 input_y = torch.tensor([[SOS_token]], device=device) # \u6570\u636e\u5f62\u72b6\u6570\u636e\u5f62\u72b6 [1,1],[1,1,256],[10,256] ---> [1,4345],[1,1,256],[1,10] # output_y, decode_hidden, attn_weight = my_attndecoderrnn(input_y, decode_hidden, encode_output_c) # \u8ba1\u7b97\u635f\u5931 target_y = y[0][idx].view(1) # \u6bcf\u4e2a\u65f6\u95f4\u6b65\u5904\u7406 for idx in range(y_len): \u5904\u7406\u4e09\u8005\u4e4b\u95f4\u5173\u7cfbinput_y output_y target_y # 3 \u8bad\u7ec3\u7b56\u7565 use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False # teacher_forcing \u628a\u6837\u672c\u771f\u5b9e\u503cy\u4f5c\u4e3a\u4e0b\u4e00\u6b21\u8f93\u5165 input_y = y[0][idx].view(1, -1) # not teacher_forcing \u628a\u9884\u6d4b\u503cy\u4f5c\u4e3a\u4e0b\u4e00\u6b21\u8f93\u5165 # topv,topi = output_y.topk(1) # if topi.squeeze().item() == EOS_token: break input_y = topi.detach() # 4 \u5176\u4ed6 # \u8ba1\u7b97\u635f\u5931 # \u68af\u5ea6\u6e05\u96f6 # \u53cd\u5411\u4f20\u64ad # \u68af\u5ea6\u66f4\u65b0 # \u8fd4\u56de \u635f\u5931\u5217\u8868myloss.item()/y_len \u7f16\u7801\u5b9e\u73b0 def Train_Iters ( x , y , my_encoderrnn , my_attndecoderrnn , myadam_encode , myadam_decode , mycrossentropyloss ): # 1 \u7f16\u7801 encode_output, encode_hidden = my_encoderrnn(x, encode_hidden) encode_hidden = my_encoderrnn . inithidden () encode_output , encode_hidden = my_encoderrnn ( x , encode_hidden ) # \u4e00\u6b21\u6027\u9001\u6570\u636e # [1,6],[1,1,256] --> [1,6,256],[1,1,256] # 2 \u89e3\u7801\u53c2\u6570\u51c6\u5907\u548c\u89e3\u7801 # \u89e3\u7801\u53c2\u65701 encode_output_c [10,256] encode_output_c = torch . zeros ( MAX_LENGTH , my_encoderrnn . hidden_size , device = device ) for idx in range ( x . shape [ 1 ]): encode_output_c [ idx ] = encode_output [ 0 , idx ] # \u89e3\u7801\u53c2\u65702 decode_hidden = encode_hidden # \u89e3\u7801\u53c2\u65703 input_y = torch . tensor ([[ SOS_token ]], device = device ) myloss = 0.0 y_len = y . shape [ 1 ] use_teacher_forcing = True if random . random () < teacher_forcing_ratio else False if use_teacher_forcing : for idx in range ( y_len ): # \u6570\u636e\u5f62\u72b6\u6570\u636e\u5f62\u72b6 [1,1],[1,1,256],[10,256] ---> [1,4345],[1,1,256],[1,10] output_y , decode_hidden , attn_weight = my_attndecoderrnn ( input_y , decode_hidden , encode_output_c ) target_y = y [ 0 ][ idx ] . view ( 1 ) myloss = myloss + mycrossentropyloss ( output_y , target_y ) input_y = y [ 0 ][ idx ] . view ( 1 , - 1 ) else : for idx in range ( y_len ): # \u6570\u636e\u5f62\u72b6\u6570\u636e\u5f62\u72b6 [1,1],[1,1,256],[10,256] ---> [1,4345],[1,1,256],[1,10] output_y , decode_hidden , attn_weight = my_attndecoderrnn ( input_y , decode_hidden , encode_output_c ) target_y = y [ 0 ][ idx ] . view ( 1 ) myloss = myloss + mycrossentropyloss ( output_y , target_y ) topv , topi = output_y . topk ( 1 ) if topi . squeeze () . item () == EOS_token : break input_y = topi . detach () # \u68af\u5ea6\u6e05\u96f6 myadam_encode . zero_grad () myadam_decode . zero_grad () # \u53cd\u5411\u4f20\u64ad myloss . backward () # \u68af\u5ea6\u66f4\u65b0 myadam_encode . step () myadam_decode . step () # \u8fd4\u56de \u635f\u5931\u5217\u8868myloss.item()/y_len return myloss . item () / y_len","title":"3 \u6784\u5efa\u5185\u90e8\u8fed\u4ee3\u8bad\u7ec3\u51fd\u6570"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#4_1","text":"\u5b9e\u73b0\u601d\u8def\u5206\u6790 # Train_seq2seq() \u601d\u8def\u5206\u6790 # \u5b9e\u4f8b\u5316 mypairsdataset\u5bf9\u8c61 \u5b9e\u4f8b\u5316 mydataloader # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668 my_encoderrnn \u5b9e\u4f8b\u5316\u89e3\u7801\u5668 my_attndecoderrnn # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u4f18\u5316\u5668 myadam_encode \u5b9e\u4f8b\u5316\u89e3\u7801\u5668\u4f18\u5316\u5668 myadam_decode # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570 mycrossentropyloss = nn.NLLLoss() # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 # epoches mylr=1e4 teacher_forcing_ratio print_interval_num plot_interval_num (\u5168\u5c40) # plot_loss_list = [] (\u8fd4\u56de) print_loss_total plot_loss_total starttime (\u6bcf\u8f6e\u5185\u90e8) # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range(1, 1+epochs): # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 # for item, (x, y) in enumerate(mydataloader, start=1): # \u8c03\u7528\u5185\u90e8\u8bad\u7ec3\u51fd\u6570 Train_Iters(x, y, my_encoderrnn, my_attndecoderrnn, myadam_encode, myadam_decode, mycrossentropyloss) # \u8ba1\u7b97\u8f85\u52a9\u4fe1\u606f # \u8ba1\u7b97\u6253\u5370\u5c4f\u5e55\u95f4\u9694\u635f\u5931-\u6bcf\u96941000\u6b21 # \u8ba1\u7b97\u753b\u56fe\u95f4\u9694\u635f\u5931-\u6bcf\u9694100\u6b21 # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch.save(my_encoderrnn.state_dict(), PATH1) # \u6240\u6709\u8f6e\u6b21\u8bad\u7ec3\u5b8c\u6bd5 \u753b\u635f\u5931\u56fe plt.figure() .plot(plot_loss_list) .save('x.png') .show() \u7f16\u7801\u5b9e\u73b0 def Train_seq2seq (): # \u5b9e\u4f8b\u5316 mypairsdataset\u5bf9\u8c61 \u5b9e\u4f8b\u5316 mydataloader mypairsdataset = MyPairsDataset ( my_pairs ) mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668 my_encoderrnn \u5b9e\u4f8b\u5316\u89e3\u7801\u5668 my_attndecoderrnn my_encoderrnn = EncoderRNN ( 2803 , 256 ) my_attndecoderrnn = AttnDecoderRNN ( output_size = 4345 , hidden_size = 256 , dropout_p = 0.1 , max_length = 10 ) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u4f18\u5316\u5668 myadam_encode \u5b9e\u4f8b\u5316\u89e3\u7801\u5668\u4f18\u5316\u5668 myadam_decode myadam_encode = optim . Adam ( my_encoderrnn . parameters (), lr = mylr ) myadam_decode = optim . Adam ( my_attndecoderrnn . parameters (), lr = mylr ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570 mycrossentropyloss = nn.NLLLoss() mycrossentropyloss = nn . NLLLoss () # \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570 plot_loss_list = [] # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for epoch_idx in range(1, 1+epochs): for epoch_idx in range ( 1 , 1 + epochs ): print_loss_total , plot_loss_total = 0.0 , 0.0 starttime = time . time () # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for item , ( x , y ) in enumerate ( mydataloader , start = 1 ): # \u8c03\u7528\u5185\u90e8\u8bad\u7ec3\u51fd\u6570 myloss = Train_Iters ( x , y , my_encoderrnn , my_attndecoderrnn , myadam_encode , myadam_decode , mycrossentropyloss ) print_loss_total += myloss plot_loss_total += myloss # \u8ba1\u7b97\u6253\u5370\u5c4f\u5e55\u95f4\u9694\u635f\u5931-\u6bcf\u96941000\u6b21 if item % print_interval_num == 0 : print_loss_avg = print_loss_total / print_interval_num # \u5c06\u603b\u635f\u5931\u5f520 print_loss_total = 0 # \u6253\u5370\u65e5\u5fd7\uff0c\u65e5\u5fd7\u5185\u5bb9\u5206\u522b\u662f\uff1a\u8bad\u7ec3\u8017\u65f6\uff0c\u5f53\u524d\u8fed\u4ee3\u6b65\uff0c\u5f53\u524d\u8fdb\u5ea6\u767e\u5206\u6bd4\uff0c\u5f53\u524d\u5e73\u5747\u635f\u5931 print ( '\u8f6e\u6b21 %d \u635f\u5931 %.6f \u65f6\u95f4: %d ' % ( epoch_idx , print_loss_avg , time . time () - starttime )) # \u8ba1\u7b97\u753b\u56fe\u95f4\u9694\u635f\u5931-\u6bcf\u9694100\u6b21 if item % plot_interval_num == 0 : # \u901a\u8fc7\u603b\u635f\u5931\u9664\u4ee5\u95f4\u9694\u5f97\u5230\u5e73\u5747\u635f\u5931 plot_loss_avg = plot_loss_total / plot_interval_num # \u5c06\u5e73\u5747\u635f\u5931\u6dfb\u52a0plot_loss_list\u5217\u8868\u4e2d plot_loss_list . append ( plot_loss_avg ) # \u603b\u635f\u5931\u5f520 plot_loss_total = 0 # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_encoderrnn . state_dict (), './my_encoderrnn_ %d .pth' % epoch_idx ) torch . save ( my_attndecoderrnn . state_dict (), './my_attndecoderrnn_ %d .pth' % epoch_idx ) # \u6240\u6709\u8f6e\u6b21\u8bad\u7ec3\u5b8c\u6bd5 \u753b\u635f\u5931\u56fe plt . figure () plt . plot ( plot_loss_list ) plt . savefig ( './s2sq_loss.png' ) plt . show () return plot_loss_list \u8f93\u51fa\u6548\u679c \u8f6e\u6b211 \u635f\u59318.123402 \u65f6\u95f4:4 \u8f6e\u6b211 \u635f\u59316.658305 \u65f6\u95f4:8 \u8f6e\u6b211 \u635f\u59315.252497 \u65f6\u95f4:12 \u8f6e\u6b211 \u635f\u59314.906939 \u65f6\u95f4:16 \u8f6e\u6b211 \u635f\u59314.813769 \u65f6\u95f4:19 \u8f6e\u6b211 \u635f\u59314.780460 \u65f6\u95f4:23 \u8f6e\u6b211 \u635f\u59314.621599 \u65f6\u95f4:27 \u8f6e\u6b211 \u635f\u59314.487508 \u65f6\u95f4:31 \u8f6e\u6b211 \u635f\u59314.478538 \u65f6\u95f4:35 \u8f6e\u6b211 \u635f\u59314.245148 \u65f6\u95f4:39 \u8f6e\u6b211 \u635f\u59314.602579 \u65f6\u95f4:44 \u8f6e\u6b211 \u635f\u59314.256789 \u65f6\u95f4:48 \u8f6e\u6b211 \u635f\u59314.218111 \u65f6\u95f4:52 \u8f6e\u6b211 \u635f\u59314.393134 \u65f6\u95f4:56 \u8f6e\u6b211 \u635f\u59314.134959 \u65f6\u95f4:60 \u8f6e\u6b211 \u635f\u59314.164878 \u65f6\u95f4:63","title":"4 \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#5","text":"\u635f\u5931\u4e0b\u964d\u66f2\u7ebf \u4e00\u76f4\u4e0b\u964d\u7684\u635f\u5931\u66f2\u7ebf, \u8bf4\u660e\u6a21\u578b\u6b63\u5728\u6536\u655b, \u80fd\u591f\u4ece\u6570\u636e\u4e2d\u627e\u5230\u4e00\u4e9b\u89c4\u5f8b\u5e94\u7528\u4e8e\u6570\u636e.","title":"5 \u635f\u5931\u66f2\u7ebf\u5206\u6790"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#5_1","text":"","title":"5 \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570\u5e76\u6d4b\u8bd5"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#1_2","text":"# \u6a21\u578b\u8bc4\u4f30\u4ee3\u7801\u4e0e\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7c7b\u4f3c\uff0c\u9700\u8981\u6ce8\u610f\u4f7f\u7528with torch.no_grad() # \u6a21\u578b\u9884\u6d4b\u65f6\uff0c\u7b2c\u4e00\u4e2a\u65f6\u95f4\u6b65\u4f7f\u7528SOS_token\u4f5c\u4e3a\u8f93\u5165 \u540e\u7eed\u65f6\u95f4\u6b65\u91c7\u7528\u9884\u6d4b\u503c\u4f5c\u4e3a\u8f93\u5165\uff0c\u4e5f\u5c31\u662f\u81ea\u56de\u5f52\u673a\u5236 def Seq2Seq_Evaluate ( x , my_encoderrnn , my_attndecoderrnn ): with torch . no_grad (): # 1 \u7f16\u7801\uff1a\u4e00\u6b21\u6027\u7684\u9001\u6570\u636e encode_hidden = my_encoderrnn . inithidden () encode_output , encode_hidden = my_encoderrnn ( x , encode_hidden ) # 2 \u89e3\u7801\u53c2\u6570\u51c6\u5907 # \u89e3\u7801\u53c2\u65701 \u56fa\u5b9a\u957f\u5ea6\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc encoder_outputs_c = torch . zeros ( MAX_LENGTH , my_encoderrnn . hidden_size , device = device ) x_len = x . shape [ 1 ] for idx in range ( x_len ): encoder_outputs_c [ idx ] = encode_output [ 0 , idx ] # \u89e3\u7801\u53c2\u65702 \u6700\u540e1\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa \u4f5c\u4e3a \u89e3\u7801\u5668\u7684\u7b2c1\u4e2a\u65f6\u95f4\u6b65\u9690\u85cf\u5c42\u8f93\u5165 decode_hidden = encode_hidden # \u89e3\u7801\u53c2\u65703 \u89e3\u7801\u5668\u7b2c\u4e00\u4e2a\u65f6\u95f4\u6b65\u8d77\u59cb\u7b26 input_y = torch . tensor ([[ SOS_token ]], device = device ) # 3 \u81ea\u56de\u5f52\u65b9\u5f0f\u89e3\u7801 # \u521d\u59cb\u5316\u9884\u6d4b\u7684\u8bcd\u6c47\u5217\u8868 decoded_words = [] # \u521d\u59cb\u5316attention\u5f20\u91cf decoder_attentions = torch . zeros ( MAX_LENGTH , MAX_LENGTH ) for idx in range ( MAX_LENGTH ): # note:MAX_LENGTH=10 output_y , decode_hidden , attn_weights = my_attndecoderrnn ( input_y , decode_hidden , encoder_outputs_c ) # \u9884\u6d4b\u503c\u4f5c\u4e3a\u4e3a\u4e0b\u4e00\u6b21\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u503c topv , topi = output_y . topk ( 1 ) decoder_attentions [ idx ] = attn_weights # \u5982\u679c\u8f93\u51fa\u503c\u662f\u7ec8\u6b62\u7b26\uff0c\u5219\u5faa\u73af\u505c\u6b62 if topi . squeeze () . item () == EOS_token : decoded_words . append ( '<EOS>' ) break else : decoded_words . append ( french_index2word [ topi . item ()]) # \u5c06\u672c\u6b21\u9884\u6d4b\u7684\u7d22\u5f15\u8d4b\u503c\u7ed9 input_y\uff0c\u8fdb\u884c\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u9884\u6d4b input_y = topi . detach () # \u8fd4\u56de\u7ed3\u679cdecoded_words\uff0c \u6ce8\u610f\u529b\u5f20\u91cf\u6743\u91cd\u5206\u5e03\u8868(\u628a\u6ca1\u6709\u7528\u5230\u7684\u90e8\u5206\u5207\u6389) return decoded_words , decoder_attentions [: idx + 1 ]","title":"1 \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#2_3","text":"# \u52a0\u8f7d\u6a21\u578b PATH1 = './gpumodel/my_encoderrnn.pth' PATH2 = './gpumodel/my_attndecoderrnn.pth' def dm_test_Seq2Seq_Evaluate (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_encoderrnn = EncoderRNN ( input_size , hidden_size ) # my_encoderrnn.load_state_dict(torch.load(PATH1)) my_encoderrnn . load_state_dict ( torch . load ( PATH1 , map_location = lambda storage , loc : storage ), False ) print ( 'my_encoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_encoderrnn ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = french_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_attndecoderrnn = AttnDecoderRNN ( input_size , hidden_size ) # my_attndecoderrnn.load_state_dict(torch.load(PATH2)) my_attndecoderrnn . load_state_dict ( torch . load ( PATH2 , map_location = lambda storage , loc : storage ), False ) print ( 'my_decoderrnn\u6a21\u578b\u7ed3\u6784--->' , my_attndecoderrnn ) my_samplepairs = [ [ 'i m impressed with your french .' , 'je suis impressionne par votre francais .' ], [ 'i m more than a friend .' , 'je suis plus qu une amie .' ], [ 'she is beautiful like her mother .' , 'elle est belle comme sa mere .' ] ] print ( 'my_samplepairs--->' , len ( my_samplepairs )) for index , pair in enumerate ( my_samplepairs ): x = pair [ 0 ] y = pair [ 1 ] # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 tmpx = [ english_word2index [ word ] for word in x . split ( ' ' )] tmpx . append ( EOS_token ) tensor_x = torch . tensor ( tmpx , dtype = torch . long , device = device ) . view ( 1 , - 1 ) # \u6a21\u578b\u9884\u6d4b decoded_words , attentions = Seq2Seq_Evaluate ( tensor_x , my_encoderrnn , my_attndecoderrnn ) # print('decoded_words->', decoded_words) output_sentence = ' ' . join ( decoded_words ) print ( ' \\n ' ) print ( '>' , x ) print ( '=' , y ) print ( '<' , output_sentence ) \u8f93\u51fa\u6548\u679c: > i m impressed with your french . = je suis impressionne par votre francais . < je suis impressionnee par votre francais . < EOS > > i m more than a friend . = je suis plus qu une amie . < je suis plus qu une amie . < EOS > > she is beautiful like her mother . = elle est belle comme sa mere . < elle est sa sa mere . < EOS > > you re winning aren t you ? = vous gagnez n est ce pas ? < tu restez n est ce pas ? < EOS > > he is angry with you . = il est en colere apres toi . < il est en colere apres toi . < EOS > > you re very timid . = vous etes tres craintifs . < tu es tres craintive . < EOS >","title":"2  \u6a21\u578b\u8bc4\u4f30\u51fd\u6570\u8c03\u7528"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#3-attention","text":"def dm_test_Attention (): # \u5b9e\u4f8b\u5316dataset\u5bf9\u8c61 mypairsdataset = MyPairsDataset ( my_pairs ) # \u5b9e\u4f8b\u5316dataloader mydataloader = DataLoader ( dataset = mypairsdataset , batch_size = 1 , shuffle = True ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = english_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_encoderrnn = EncoderRNN ( input_size , hidden_size ) # my_encoderrnn.load_state_dict(torch.load(PATH1)) my_encoderrnn . load_state_dict ( torch . load ( PATH1 , map_location = lambda storage , loc : storage ), False ) # \u5b9e\u4f8b\u5316\u6a21\u578b input_size = french_word_n hidden_size = 256 # \u89c2\u5bdf\u7ed3\u679c\u6570\u636e \u53ef\u4f7f\u75288 my_attndecoderrnn = AttnDecoderRNN ( input_size , hidden_size ) # my_attndecoderrnn.load_state_dict(torch.load(PATH2)) my_attndecoderrnn . load_state_dict ( torch . load ( PATH2 , map_location = lambda storage , loc : storage ), False ) sentence = \"we re both teachers .\" # \u6837\u672cx \u6587\u672c\u6570\u503c\u5316 tmpx = [ english_word2index [ word ] for word in sentence . split ( ' ' )] tmpx . append ( EOS_token ) tensor_x = torch . tensor ( tmpx , dtype = torch . long , device = device ) . view ( 1 , - 1 ) # \u6a21\u578b\u9884\u6d4b decoded_words , attentions = Seq2Seq_Evaluate ( tensor_x , my_encoderrnn , my_attndecoderrnn ) print ( 'decoded_words->' , decoded_words ) # print('\\n') # print('\u82f1\u6587', sentence) # print('\u6cd5\u6587', output_sentence) plt . matshow ( attentions . numpy ()) # \u4ee5\u77e9\u9635\u5217\u8868\u7684\u5f62\u5f0f \u663e\u793a # \u4fdd\u5b58\u56fe\u50cf plt . savefig ( \"./s2s_attn.png\" ) plt . show () print ( 'attentions.numpy()---> \\n ' , attentions . numpy ()) print ( 'attentions.size--->' , attentions . size ()) \u8f93\u51fa\u6548\u679c: decoded_words-> ['nous', 'sommes', 'toutes', 'deux', 'enseignantes', '.', '<EOS>'] Attention\u53ef\u89c6\u5316: Attention\u56fe\u50cf\u7684\u7eb5\u5750\u6807\u4ee3\u8868\u8f93\u5165\u7684\u6e90\u8bed\u8a00\u5404\u4e2a\u8bcd\u6c47\u5bf9\u5e94\u7684\u7d22\u5f15, 0-6\u5206\u522b\u5bf9\u5e94[\"we\", \"re\", \"both\", \"teachers\", \".\", \" \"], \u7eb5\u5750\u6807\u4ee3\u8868\u751f\u6210\u7684\u76ee\u6807\u8bed\u8a00\u5404\u4e2a\u8bcd\u6c47\u5bf9\u5e94\u7684\u7d22\u5f15, 0-7\u4ee3\u8868['nous', 'sommes', 'toutes', 'deux', 'enseignantes', '.', ' '], \u56fe\u4e2d\u6d45\u8272\u5c0f\u65b9\u5757(\u989c\u8272\u8d8a\u6d45\u8bf4\u660e\u5f71\u54cd\u8d8a\u5927)\u4ee3\u8868\u8bcd\u6c47\u4e4b\u95f4\u7684\u5f71\u54cd\u5173\u7cfb, \u6bd4\u5982\u6e90\u8bed\u8a00\u7684\u7b2c1\u4e2a\u8bcd\u6c47\u5bf9\u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684\u7b2c1\u4e2a\u8bcd\u6c47\u5f71\u54cd\u6700\u5927, \u6e90\u8bed\u8a00\u7684\u7b2c4\uff0c5\u4e2a\u8bcd\u5bf9\u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684\u7b2c5\u4e2a\u8bcd\u4f1a\u5f71\u54cd\u6700\u5927, \u901a\u8fc7\u8fd9\u6837\u7684\u53ef\u89c6\u5316\u56fe\u50cf, \u6211\u4eec\u53ef\u4ee5\u77e5\u9053Attention\u7684\u6548\u679c\u597d\u574f, \u4e0e\u6211\u4eec\u4eba\u4e3a\u53bb\u5224\u5b9a\u5230\u5e95\u8fd8\u6709\u591a\u5927\u7684\u5dee\u8ddd. \u8fdb\u800c\u8861\u91cf\u6211\u4eec\u8bad\u7ec3\u6a21\u578b\u7684\u53ef\u7528\u6027.","title":"3 Attention\u5f20\u91cf\u5236\u56fe"},{"location":"03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html#4_2","text":"seq2seq\u6a21\u578b\u67b6\u6784\u5206\u6790 seq2seq\u6a21\u578b\u67b6\u6784\u5305\u62ec\u4e09\u90e8\u5206\uff0c\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u3001decoder(\u89e3\u7801\u5668)\u3001\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u3002\u5176\u4e2d\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5185\u90e8\u5b9e\u73b0\u90fd\u4f7f\u7528\u4e86GRU\u6a21\u578b \u57fa\u4e8eGRU\u7684seq2seq\u6a21\u578b\u67b6\u6784\u5b9e\u73b0\u7ffb\u8bd1\u7684\u8fc7\u7a0b \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305\u548c\u5de5\u5177\u51fd\u6570 \u7b2c\u4e8c\u6b65: \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42 \u7b2c\u4e09\u6b65: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u7b2c\u56db\u6b65: \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u6b65: \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570, \u5e76\u8fdb\u884c\u6d4b\u8bd5\u4ee5\u53caAttention\u6548\u679c\u5206\u6790 \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 python\u7248\u672c\u4f7f\u75283.6.x, pytorch\u7248\u672c\u4f7f\u75281.3.1 \u7b2c\u4e8c\u6b65: \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406, \u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42 \u6e05\u6d17\u6587\u672c\u548c\u6784\u5efa\u6587\u672c\u5b57\u5178\u3001\u6784\u5efa\u6570\u636e\u6e90\u3001\u6784\u5efa\u6570\u636e\u8fed\u4ee3\u5668\u3002\u6587\u672c\u5904\u7406\u7684\u672c\u8d28\u5c31\u662f\u6839\u636e\u4efb\u52a1\u6784\u5efa\u6807\u7b7ex\u3001\u6807\u7b7ey \u7b2c\u4e09\u6b65: \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668 \u6784\u5efa\u57fa\u4e8eGRU\u7684\u89e3\u7801\u5668 \u6784\u5efa\u57fa\u4e8eGRU\u548cAttention\u7684\u89e3\u7801\u5668 \u7b2c\u56db\u6b65: \u6784\u5efa\u6a21\u578b\u8bad\u7ec3\u51fd\u6570, \u5e76\u8fdb\u884c\u8bad\u7ec3 \u4ec0\u4e48\u662fteacher_forcing: \u5b83\u662f\u4e00\u79cd\u7528\u4e8e\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u7684\u8bad\u7ec3\u6280\u5de7, \u5728seq2seq\u67b6\u6784\u4e2d, \u6839\u636e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\uff0c\u89e3\u7801\u5668\u6bcf\u6b21\u5e94\u8be5\u4f7f\u7528\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u4f5c\u4e3a\u8f93\u5165\u7684\u4e00\u90e8\u5206, \u4f46\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u65e6\u4e0a\u4e00\u6b65\u7684\u7ed3\u679c\u662f\u9519\u8bef\u7684\uff0c\u5c31\u4f1a\u5bfc\u81f4\u8fd9\u79cd\u9519\u8bef\u88ab\u7d2f\u79ef\uff0c\u65e0\u6cd5\u8fbe\u5230\u8bad\u7ec3\u6548\u679c, \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u673a\u5236\u6539\u53d8\u4e0a\u4e00\u6b65\u51fa\u9519\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u8bad\u7ec3\u65f6\u6211\u4eec\u662f\u5df2\u77e5\u6b63\u786e\u7684\u8f93\u51fa\u5e94\u8be5\u662f\u4ec0\u4e48\uff0c\u56e0\u6b64\u53ef\u4ee5\u5f3a\u5236\u5c06\u4e0a\u4e00\u6b65\u7ed3\u679c\u8bbe\u7f6e\u6210\u6b63\u786e\u7684\u8f93\u51fa, \u8fd9\u79cd\u65b9\u5f0f\u5c31\u53eb\u505ateacher_forcing teacher_forcing\u7684\u4f5c\u7528: \u80fd\u591f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u77eb\u6b63\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u907f\u514d\u5728\u5e8f\u5217\u751f\u6210\u7684\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u8fdb\u4e00\u6b65\u653e\u5927. \u53e6\u5916, teacher_forcing\u80fd\u591f\u6781\u5927\u7684\u52a0\u5feb\u6a21\u578b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4ee4\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u5feb\u66f4\u5e73\u7a33 \u6784\u5efa\u8bad\u7ec3\u51fd\u6570train \u8c03\u7528\u8bad\u7ec3\u51fd\u6570\u5e76\u6253\u5370\u65e5\u5fd7\u548c\u5236\u56fe \u635f\u5931\u66f2\u7ebf\u5206\u6790: \u4e00\u76f4\u4e0b\u964d\u7684\u635f\u5931\u66f2\u7ebf, \u8bf4\u660e\u6a21\u578b\u6b63\u5728\u6536\u655b, \u80fd\u591f\u4ece\u6570\u636e\u4e2d\u627e\u5230\u4e00\u4e9b\u89c4\u5f8b\u5e94\u7528\u4e8e\u6570\u636e \u7b2c\u4e94\u6b65: \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570, \u5e76\u8fdb\u884c\u6d4b\u8bd5\u4ee5\u53caAttention\u6548\u679c\u5206\u6790 \u6784\u5efa\u6a21\u578b\u8bc4\u4f30\u51fd\u6570evaluate \u968f\u673a\u9009\u62e9\u6307\u5b9a\u6570\u91cf\u7684\u6570\u636e\u8fdb\u884c\u8bc4\u4f30 \u8fdb\u884c\u4e86Attention\u53ef\u89c6\u5316\u5206\u6790","title":"4 \u5c0f\u7ed3"},{"location":"03_mkdocs_RNN/9%20Self-attention%E6%BC%94%E5%8F%98%E8%BF%87%E7%A8%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u7406\u89e3attention\u548cself-attention\u4e4b\u95f4\u7684\u4e0d\u540c\u70b9 \u7406\u89e3attention\u548cself-attention\u7684\u5e94\u7528\u573a\u666f 1 Self-attention\u4ecb\u7ecd \u00b6 Self-attention\u5c31\u672c\u8d28\u4e0a\u662f\u4e00\u79cd\u7279\u6b8a\u7684attention\u3002\u8fd9\u79cd\u5e94\u7528\u5728transformer\u4e2d\u6700\u91cd\u8981\u7684\u7ed3\u6784\u4e4b\u4e00\u3002\u524d\u9762\u6211\u4eec\u4ecb\u7ecd\u4e86attention\u673a\u5236\uff0c\u5b83\u80fd\u591f\u5e2e\u6211\u4eec\u627e\u5230\u5b50\u5e8f\u5217\u548c\u5168\u5c40\u7684attention\u7684\u5173\u7cfb\uff0c\u4e5f\u5c31\u662f\u627e\u5230\u6743\u91cd\u503c w_i w_i \u3002Self-attention\u5411\u5bf9\u4e8eattention\u7684\u53d8\u5316\uff0c\u5176\u5b9e\u5c31\u662f\u5bfb\u627e\u6743\u91cd\u503c\u7684 w_i w_i \u8fc7\u7a0b\u4e0d\u540c\u3002\u4e0b\u9762\u6211\u4eec\u6765\u770b\u770bself-attention\u7684\u8fd0\u7b97\u8fc7\u7a0b\u3002 \u4e3a\u4e86\u80fd\u591f\u4ea7\u751f\u8f93\u51fa\u7684\u5411\u91cf y_i y_i \uff0cself-attention\u5176\u5b9e\u662f\u5bf9\u6240\u6709\u7684\u8f93\u5165\u505a\u4e86\u4e00\u4e2a\u52a0\u6743\u5e73\u5747\u7684\u64cd\u4f5c\uff0c\u8fd9\u4e2a\u516c\u5f0f\u548c\u4e0a\u9762\u7684attention\u662f\u4e00\u81f4\u7684\u3002 $$ y_i = \\sum w_{ij}x_j $$ j j \u4ee3\u8868\u6574\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5e76\u4e14 j j \u4e2a\u6743\u91cd\u7684\u76f8\u52a0\u4e4b\u548c\u7b49\u4e8e1\u3002\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u8fd9\u91cc\u7684 w_{ij} w_{ij} \u5e76\u4e0d\u662f\u4e00\u4e2a\u9700\u8981\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u5b83\u662f\u6765\u6e90\u4e8e x_i x_i \u548c x_j x_j \u7684\u4e4b\u95f4\u7684\u8ba1\u7b97\u7684\u7ed3\u679c\uff08\u8fd9\u91cc w_{ij} w_{ij} \u7684\u8ba1\u7b97\u53d1\u751f\u4e86\u53d8\u5316)\u3002\u5b83\u4eec\u4e4b\u95f4\u6700\u7b80\u5355\u7684\u4e00\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5c31\u662f\u4f7f\u7528\u70b9\u79ef\u7684\u65b9\u5f0f\u3002 w_{ij}^\\prime = x_{i}^Tx_j w_{ij}^\\prime = x_{i}^Tx_j x_i x_i \u548c x_j x_j \u662f\u4e00\u5bf9\u8f93\u5165\u548c\u8f93\u51fa\u3002\u5bf9\u4e8e\u4e0b\u4e00\u4e2a\u8f93\u51fa\u7684\u5411\u91cf y_{i+1} y_{i+1} \uff0c\u6211\u4eec\u6709\u4e00\u4e2a\u5168\u65b0\u7684\u8f93\u5165\u5e8f\u5217\u548c\u4e00\u4e2a\u4e0d\u540c\u7684\u6743\u91cd\u503c\u3002 \u8fd9\u4e2a\u70b9\u79ef\u7684\u8f93\u51fa\u7684\u53d6\u503c\u8303\u56f4\u5728\u8d1f\u65e0\u7a77\u548c\u6b63\u65e0\u7a77\u4e4b\u95f4\uff0c\u6240\u4ee5\u6211\u4eec\u8981\u4f7f\u7528\u4e00\u4e2a softmax softmax \u628a\u5b83\u6620\u5c04\u5230 \u4e4b\u95f4\uff0c\u5e76\u4e14\u8981\u786e\u4fdd\u5b83\u4eec\u5bf9\u4e8e\u6574\u4e2a\u5e8f\u5217\u800c\u8a00\u7684\u548c\u4e3a1\u3002 $$ w_{ij} = \\frac{exp\\;w_{ij}^{\\prime}}{\\sum_j exp\\;w_{ij}^{\\prime}} $$ \u4ee5\u4e0a\u8fd9\u4e9b\u5c31\u662fself-attention\u6700\u57fa\u672c\u7684\u64cd\u4f5c. 2 Self-attention\u548cAttention\u4f7f\u7528\u65b9\u6cd5 \u00b6 \u6839\u636e\u4ed6\u4eec\u4e4b\u95f4\u7684\u91cd\u8981\u533a\u522b, \u53ef\u4ee5\u533a\u5206\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u4f7f\u7528\u65b9\u6cd5: \u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u901a\u5e38\u6765\u8bf4\u4f60\u4f1a\u6709\u8f93\u5165\u5c42\uff08input\uff09\uff0c\u5e94\u7528\u6fc0\u6d3b\u51fd\u6570\u540e\u7684\u8f93\u51fa\u5c42\uff08output\uff09\uff0c\u5728RNN\u5f53\u4e2d\u4f60\u4f1a\u6709\u72b6\u6001\uff08state\uff09\u3002\u5982\u679cattention (AT) \u88ab\u5e94\u7528\u5728\u67d0\u4e00\u5c42\u7684\u8bdd\uff0c\u5b83\u66f4\u591a\u7684\u662f\u88ab\u5e94\u7528\u5728\u8f93\u51fa\u6216\u8005\u662f\u72b6\u6001\u5c42\u4e0a\uff0c\u800c\u5f53\u6211\u4eec\u4f7f\u7528self-attention\uff08SA\uff09\uff0c\u8fd9\u79cd\u6ce8\u610f\u529b\u7684\u673a\u5236\u66f4\u591a\u7684\u5b9e\u5728\u5173\u6ce8input\u4e0a\u3002 Attention (AT) \u7ecf\u5e38\u88ab\u5e94\u7528\u5728\u4ece\u7f16\u7801\u5668\uff08encoder\uff09\u8f6c\u6362\u5230\u89e3\u7801\u5668\uff08decoder\uff09\u3002\u6bd4\u5982\u8bf4\uff0c\u89e3\u7801\u5668\u7684\u795e\u7ecf\u5143\u4f1a\u63a5\u53d7\u4e00\u4e9bAT\u4ece\u7f16\u7801\u5c42\u751f\u6210\u7684\u8f93\u5165\u4fe1\u606f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cAT\u8fde\u63a5\u7684\u662f**\u4e24\u4e2a\u4e0d\u540c\u7684\u7ec4\u4ef6**\uff08component\uff09\uff0c\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002\u4f46\u662f\u5982\u679c\u6211\u4eec\u7528**SA**\uff0c\u5b83\u5c31\u4e0d\u662f\u5173\u6ce8\u7684\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5b83\u53ea\u662f\u5728\u5173\u6ce8\u4f60\u5e94\u7528\u7684**\u90a3\u4e00\u4e2a\u7ec4\u4ef6**\u3002\u90a3\u8fd9\u91cc\u4ed6\u5c31\u4e0d\u4f1a\u53bb\u5173\u6ce8\u89e3\u7801\u5668\u4e86\uff0c\u5c31\u6bd4\u5982\u8bf4\u5728Bert\u4e2d\uff0c\u4f7f\u7528\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u5c31\u6ca1\u6709\u89e3\u7801\u5668\u3002 SA\u53ef\u4ee5\u5728\u4e00\u4e2a\u6a21\u578b\u5f53\u4e2d\u88ab\u591a\u6b21\u7684\u3001\u72ec\u7acb\u7684\u4f7f\u7528\uff08\u6bd4\u5982\u8bf4\u5728Transformer\u4e2d\uff0c\u4f7f\u7528\u4e8618\u6b21\uff1b\u5728Bert\u5f53\u4e2d\u4f7f\u752812\u6b21\uff09\u3002\u4f46\u662f\uff0cAT\u5728\u4e00\u4e2a\u6a21\u578b\u5f53\u4e2d\u7ecf\u5e38\u53ea\u662f\u88ab\u4f7f\u7528\u4e00\u6b21\uff0c\u5e76\u4e14\u8d77\u5230\u8fde\u63a5\u4e24\u4e2a\u7ec4\u4ef6\u7684\u4f5c\u7528\u3002 SA\u6bd4\u8f83\u64c5\u957f\u5728\u4e00\u4e2a\u5e8f\u5217\u5f53\u4e2d\uff0c\u5bfb\u627e\u4e0d\u540c\u90e8\u5206\u4e4b\u95f4\u7684\u5173\u7cfb \u3002\u6bd4\u5982\u8bf4\uff0c\u5728\u8bcd\u6cd5\u5206\u6790\u7684\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u5e2e\u52a9\u53bb\u7406\u89e3\u4e0d\u540c\u8bcd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 AT\u5374\u66f4\u64c5\u957f\u5bfb\u627e\u4e24\u4e2a\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb \uff0c\u6bd4\u5982\u8bf4\u5728\u7ffb\u8bd1\u4efb\u52a1\u5f53\u4e2d\uff0c\u539f\u59cb\u7684\u6587\u672c\u548c\u7ffb\u8bd1\u540e\u7684\u6587\u672c\u3002\u8fd9\u91cc\u4e5f\u8981\u6ce8\u610f\uff0c\u5728\u7ffb\u8bd1\u4efb\u52a1\u91cd\uff0cSA\u4e5f\u5f88\u64c5\u957f\uff0c\u6bd4\u5982\u8bf4Transformer\u3002 AT\u53ef\u4ee5\u8fde\u63a5\u4e24\u79cd\u4e0d\u540c\u7684\u6a21\u6001\uff0c\u6bd4\u5982\u8bf4\u56fe\u7247\u548c\u6587\u5b57\u3002SA\u66f4\u591a\u7684\u662f\u88ab\u5e94\u7528\u5728\u540c\u4e00\u79cd\u6a21\u6001\u4e0a\uff0c\u4f46\u662f\u5982\u679c\u4e00\u5b9a\u8981\u4f7f\u7528SA\u6765\u505a\u7684\u8bdd\uff0c\u4e5f\u53ef\u4ee5\u5c06\u4e0d\u540c\u7684\u6a21\u6001\u7ec4\u5408\u6210\u4e00\u4e2a\u5e8f\u5217\uff0c\u518d\u4f7f\u7528SA\u3002 \u5176\u5b9e\u6709\u65f6\u5019\u5927\u90e8\u5206\u60c5\u51b5\uff0cSA\u8fd9\u79cd\u7ed3\u6784\u66f4\u52a0\u7684general\uff0c\u5728\u5f88\u591a\u4efb\u52a1\u4f5c\u4e3a\u964d\u7ef4\u3001\u7279\u5f81\u8868\u793a\u3001\u7279\u5f81\u4ea4\u53c9\u7b49\u529f\u80fd\u5c1d\u8bd5\u7740\u5e94\u7528\uff0c\u5f88\u591a\u65f6\u5019\u6548\u679c\u90fd\u4e0d\u9519\u3002","title":"9 Self attention\u6f14\u53d8\u8fc7\u7a0b"},{"location":"03_mkdocs_RNN/9%20Self-attention%E6%BC%94%E5%8F%98%E8%BF%87%E7%A8%8B.html#_1","text":"\u7406\u89e3attention\u548cself-attention\u4e4b\u95f4\u7684\u4e0d\u540c\u70b9 \u7406\u89e3attention\u548cself-attention\u7684\u5e94\u7528\u573a\u666f","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/9%20Self-attention%E6%BC%94%E5%8F%98%E8%BF%87%E7%A8%8B.html#1-self-attention","text":"Self-attention\u5c31\u672c\u8d28\u4e0a\u662f\u4e00\u79cd\u7279\u6b8a\u7684attention\u3002\u8fd9\u79cd\u5e94\u7528\u5728transformer\u4e2d\u6700\u91cd\u8981\u7684\u7ed3\u6784\u4e4b\u4e00\u3002\u524d\u9762\u6211\u4eec\u4ecb\u7ecd\u4e86attention\u673a\u5236\uff0c\u5b83\u80fd\u591f\u5e2e\u6211\u4eec\u627e\u5230\u5b50\u5e8f\u5217\u548c\u5168\u5c40\u7684attention\u7684\u5173\u7cfb\uff0c\u4e5f\u5c31\u662f\u627e\u5230\u6743\u91cd\u503c w_i w_i \u3002Self-attention\u5411\u5bf9\u4e8eattention\u7684\u53d8\u5316\uff0c\u5176\u5b9e\u5c31\u662f\u5bfb\u627e\u6743\u91cd\u503c\u7684 w_i w_i \u8fc7\u7a0b\u4e0d\u540c\u3002\u4e0b\u9762\u6211\u4eec\u6765\u770b\u770bself-attention\u7684\u8fd0\u7b97\u8fc7\u7a0b\u3002 \u4e3a\u4e86\u80fd\u591f\u4ea7\u751f\u8f93\u51fa\u7684\u5411\u91cf y_i y_i \uff0cself-attention\u5176\u5b9e\u662f\u5bf9\u6240\u6709\u7684\u8f93\u5165\u505a\u4e86\u4e00\u4e2a\u52a0\u6743\u5e73\u5747\u7684\u64cd\u4f5c\uff0c\u8fd9\u4e2a\u516c\u5f0f\u548c\u4e0a\u9762\u7684attention\u662f\u4e00\u81f4\u7684\u3002 $$ y_i = \\sum w_{ij}x_j $$ j j \u4ee3\u8868\u6574\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5e76\u4e14 j j \u4e2a\u6743\u91cd\u7684\u76f8\u52a0\u4e4b\u548c\u7b49\u4e8e1\u3002\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u8fd9\u91cc\u7684 w_{ij} w_{ij} \u5e76\u4e0d\u662f\u4e00\u4e2a\u9700\u8981\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u5b83\u662f\u6765\u6e90\u4e8e x_i x_i \u548c x_j x_j \u7684\u4e4b\u95f4\u7684\u8ba1\u7b97\u7684\u7ed3\u679c\uff08\u8fd9\u91cc w_{ij} w_{ij} \u7684\u8ba1\u7b97\u53d1\u751f\u4e86\u53d8\u5316)\u3002\u5b83\u4eec\u4e4b\u95f4\u6700\u7b80\u5355\u7684\u4e00\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff0c\u5c31\u662f\u4f7f\u7528\u70b9\u79ef\u7684\u65b9\u5f0f\u3002 w_{ij}^\\prime = x_{i}^Tx_j w_{ij}^\\prime = x_{i}^Tx_j x_i x_i \u548c x_j x_j \u662f\u4e00\u5bf9\u8f93\u5165\u548c\u8f93\u51fa\u3002\u5bf9\u4e8e\u4e0b\u4e00\u4e2a\u8f93\u51fa\u7684\u5411\u91cf y_{i+1} y_{i+1} \uff0c\u6211\u4eec\u6709\u4e00\u4e2a\u5168\u65b0\u7684\u8f93\u5165\u5e8f\u5217\u548c\u4e00\u4e2a\u4e0d\u540c\u7684\u6743\u91cd\u503c\u3002 \u8fd9\u4e2a\u70b9\u79ef\u7684\u8f93\u51fa\u7684\u53d6\u503c\u8303\u56f4\u5728\u8d1f\u65e0\u7a77\u548c\u6b63\u65e0\u7a77\u4e4b\u95f4\uff0c\u6240\u4ee5\u6211\u4eec\u8981\u4f7f\u7528\u4e00\u4e2a softmax softmax \u628a\u5b83\u6620\u5c04\u5230 \u4e4b\u95f4\uff0c\u5e76\u4e14\u8981\u786e\u4fdd\u5b83\u4eec\u5bf9\u4e8e\u6574\u4e2a\u5e8f\u5217\u800c\u8a00\u7684\u548c\u4e3a1\u3002 $$ w_{ij} = \\frac{exp\\;w_{ij}^{\\prime}}{\\sum_j exp\\;w_{ij}^{\\prime}} $$ \u4ee5\u4e0a\u8fd9\u4e9b\u5c31\u662fself-attention\u6700\u57fa\u672c\u7684\u64cd\u4f5c.","title":"1 Self-attention\u4ecb\u7ecd"},{"location":"03_mkdocs_RNN/9%20Self-attention%E6%BC%94%E5%8F%98%E8%BF%87%E7%A8%8B.html#2-self-attentionattention","text":"\u6839\u636e\u4ed6\u4eec\u4e4b\u95f4\u7684\u91cd\u8981\u533a\u522b, \u53ef\u4ee5\u533a\u5206\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u4f7f\u7528\u65b9\u6cd5: \u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u901a\u5e38\u6765\u8bf4\u4f60\u4f1a\u6709\u8f93\u5165\u5c42\uff08input\uff09\uff0c\u5e94\u7528\u6fc0\u6d3b\u51fd\u6570\u540e\u7684\u8f93\u51fa\u5c42\uff08output\uff09\uff0c\u5728RNN\u5f53\u4e2d\u4f60\u4f1a\u6709\u72b6\u6001\uff08state\uff09\u3002\u5982\u679cattention (AT) \u88ab\u5e94\u7528\u5728\u67d0\u4e00\u5c42\u7684\u8bdd\uff0c\u5b83\u66f4\u591a\u7684\u662f\u88ab\u5e94\u7528\u5728\u8f93\u51fa\u6216\u8005\u662f\u72b6\u6001\u5c42\u4e0a\uff0c\u800c\u5f53\u6211\u4eec\u4f7f\u7528self-attention\uff08SA\uff09\uff0c\u8fd9\u79cd\u6ce8\u610f\u529b\u7684\u673a\u5236\u66f4\u591a\u7684\u5b9e\u5728\u5173\u6ce8input\u4e0a\u3002 Attention (AT) \u7ecf\u5e38\u88ab\u5e94\u7528\u5728\u4ece\u7f16\u7801\u5668\uff08encoder\uff09\u8f6c\u6362\u5230\u89e3\u7801\u5668\uff08decoder\uff09\u3002\u6bd4\u5982\u8bf4\uff0c\u89e3\u7801\u5668\u7684\u795e\u7ecf\u5143\u4f1a\u63a5\u53d7\u4e00\u4e9bAT\u4ece\u7f16\u7801\u5c42\u751f\u6210\u7684\u8f93\u5165\u4fe1\u606f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cAT\u8fde\u63a5\u7684\u662f**\u4e24\u4e2a\u4e0d\u540c\u7684\u7ec4\u4ef6**\uff08component\uff09\uff0c\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002\u4f46\u662f\u5982\u679c\u6211\u4eec\u7528**SA**\uff0c\u5b83\u5c31\u4e0d\u662f\u5173\u6ce8\u7684\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5b83\u53ea\u662f\u5728\u5173\u6ce8\u4f60\u5e94\u7528\u7684**\u90a3\u4e00\u4e2a\u7ec4\u4ef6**\u3002\u90a3\u8fd9\u91cc\u4ed6\u5c31\u4e0d\u4f1a\u53bb\u5173\u6ce8\u89e3\u7801\u5668\u4e86\uff0c\u5c31\u6bd4\u5982\u8bf4\u5728Bert\u4e2d\uff0c\u4f7f\u7528\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u5c31\u6ca1\u6709\u89e3\u7801\u5668\u3002 SA\u53ef\u4ee5\u5728\u4e00\u4e2a\u6a21\u578b\u5f53\u4e2d\u88ab\u591a\u6b21\u7684\u3001\u72ec\u7acb\u7684\u4f7f\u7528\uff08\u6bd4\u5982\u8bf4\u5728Transformer\u4e2d\uff0c\u4f7f\u7528\u4e8618\u6b21\uff1b\u5728Bert\u5f53\u4e2d\u4f7f\u752812\u6b21\uff09\u3002\u4f46\u662f\uff0cAT\u5728\u4e00\u4e2a\u6a21\u578b\u5f53\u4e2d\u7ecf\u5e38\u53ea\u662f\u88ab\u4f7f\u7528\u4e00\u6b21\uff0c\u5e76\u4e14\u8d77\u5230\u8fde\u63a5\u4e24\u4e2a\u7ec4\u4ef6\u7684\u4f5c\u7528\u3002 SA\u6bd4\u8f83\u64c5\u957f\u5728\u4e00\u4e2a\u5e8f\u5217\u5f53\u4e2d\uff0c\u5bfb\u627e\u4e0d\u540c\u90e8\u5206\u4e4b\u95f4\u7684\u5173\u7cfb \u3002\u6bd4\u5982\u8bf4\uff0c\u5728\u8bcd\u6cd5\u5206\u6790\u7684\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u5e2e\u52a9\u53bb\u7406\u89e3\u4e0d\u540c\u8bcd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 AT\u5374\u66f4\u64c5\u957f\u5bfb\u627e\u4e24\u4e2a\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb \uff0c\u6bd4\u5982\u8bf4\u5728\u7ffb\u8bd1\u4efb\u52a1\u5f53\u4e2d\uff0c\u539f\u59cb\u7684\u6587\u672c\u548c\u7ffb\u8bd1\u540e\u7684\u6587\u672c\u3002\u8fd9\u91cc\u4e5f\u8981\u6ce8\u610f\uff0c\u5728\u7ffb\u8bd1\u4efb\u52a1\u91cd\uff0cSA\u4e5f\u5f88\u64c5\u957f\uff0c\u6bd4\u5982\u8bf4Transformer\u3002 AT\u53ef\u4ee5\u8fde\u63a5\u4e24\u79cd\u4e0d\u540c\u7684\u6a21\u6001\uff0c\u6bd4\u5982\u8bf4\u56fe\u7247\u548c\u6587\u5b57\u3002SA\u66f4\u591a\u7684\u662f\u88ab\u5e94\u7528\u5728\u540c\u4e00\u79cd\u6a21\u6001\u4e0a\uff0c\u4f46\u662f\u5982\u679c\u4e00\u5b9a\u8981\u4f7f\u7528SA\u6765\u505a\u7684\u8bdd\uff0c\u4e5f\u53ef\u4ee5\u5c06\u4e0d\u540c\u7684\u6a21\u6001\u7ec4\u5408\u6210\u4e00\u4e2a\u5e8f\u5217\uff0c\u518d\u4f7f\u7528SA\u3002 \u5176\u5b9e\u6709\u65f6\u5019\u5927\u90e8\u5206\u60c5\u51b5\uff0cSA\u8fd9\u79cd\u7ed3\u6784\u66f4\u52a0\u7684general\uff0c\u5728\u5f88\u591a\u4efb\u52a1\u4f5c\u4e3a\u964d\u7ef4\u3001\u7279\u5f81\u8868\u793a\u3001\u7279\u5f81\u4ea4\u53c9\u7b49\u529f\u80fd\u5c1d\u8bd5\u7740\u5e94\u7528\uff0c\u5f88\u591a\u65f6\u5019\u6548\u679c\u90fd\u4e0d\u9519\u3002","title":"2 Self-attention\u548cAttention\u4f7f\u7528\u65b9\u6cd5"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u77e5\u9053\u6ce8\u610f\u529b\u673a\u5236\u7684\u5de5\u4f5c\u6d41\u7a0b \u7406\u89e3\u4e09\u4e2a\u9636\u6bb5\u8fc7\u7a0b 1 \u6ce8\u610f\u529b\u673a\u5236\u539f\u7406 \u00b6 1.1 \u6ce8\u610f\u529b\u673a\u5236\u793a\u610f\u56fe \u00b6 Attention\u673a\u5236\u7684\u5de5\u4f5c\u539f\u7406\u5e76\u4e0d\u590d\u6742\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u4e0b\u9762\u8fd9\u5f20\u56fe\u505a\u4e00\u4e2a\u603b\u7ed3 1.2 Attention\u8ba1\u7b97\u8fc7\u7a0b \u00b6 \u9636\u6bb5\u4e00: query \u548c key \u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2aquery \u548c key \u76f8\u5173\u6027\u7684\u5206\u503c \u9636\u6bb5\u4e8c: \u5c06\u8fd9\u4e2a\u5206\u503c\u8fdb\u884c\u5f52\u4e00\u5316(softmax)\uff0c\u5f97\u5230\u4e00\u4e2a\u6ce8\u610f\u529b\u7684\u5206\u5e03 \u9636\u6bb5\u4e09: \u4f7f\u7528\u6ce8\u610f\u529b\u5206\u5e03\u548c value \u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2a\u878d\u5408\u6ce8\u610f\u529b\u7684\u66f4\u597d\u7684 value \u503c \u4e3a\u4e86\u66f4\u597d\u7684\u8bf4\u660e\u4e0a\u9762\u7684\u60c5\u51b5, \u6211\u4eec\u901a\u8fc7\u6ce8\u610f\u529b\u6765\u505a\u4e00\u4e2a\u673a\u5668\u7ffb\u8bd1(NMT) \u7684\u4efb\u52a1\uff0c\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u6211\u4eec\u4f1a\u4f7f\u7528 seq2seq \u7684\u67b6\u6784\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4ece\u8bcd\u5178\u91cc\u751f\u6210\u4e00\u4e2a\u7ffb\u8bd1\u7684\u7ed3\u679c\u3002\u5c31\u50cf\u4e0b\u9762\u8fd9\u5f20\u56fe\u4e00\u6837. \u5728\u6ca1\u6709\u6ce8\u610f\u529b\u4e4b\u524d\uff0c\u6211\u4eec\u6bcf\u6b21\u90fd\u662f\u6839\u636e Encoder \u90e8\u5206\u7684\u8f93\u51fa\u7ed3\u679c\u6765\u8fdb\u884c\u751f\u6210\uff0c\u63d0\u51fa\u6ce8\u610f\u529b\u540e\uff0c\u5c31\u662f\u60f3\u5728\u751f\u6210\u7ffb\u8bd1\u7ed3\u679c\u65f6\u5e76\u4e0d\u662f\u770b Encoder \u4e2d\u6240\u6709\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u800c\u662f\u5148\u6765\u770b\u770b\u60f3\u751f\u6210\u7684\u8fd9\u90e8\u5206\u548c\u54ea\u4e9b\u5355\u8bcd\u53ef\u80fd\u5173\u7cfb\u4f1a\u6bd4\u8f83\u5927\uff0c\u5173\u7cfb\u5927\u7684\u6211\u591a\u501f\u9274\u4e9b\uff1b\u5173\u7cfb\u5c0f\u7684\uff0c\u5c11\u501f\u9274\u4e9b\u3002\u5c31\u662f\u8fd9\u6837\u4e00\u4e2a\u60f3\u6cd5\uff0c\u6211\u4eec\u770b\u770b\u8be5\u5982\u4f55\u64cd\u4f5c\u3002 \u8fd9\u91cc\u4e3a\u4e86\u751f\u6210\u5355\u8bcd\uff0c\u6211\u4eec\u628a Decoder \u90e8\u5206\u8f93\u5165\u540e\u5f97\u5230\u7684\u5411\u91cf\u4f5c\u4e3a query\uff1b\u628a Encoder \u90e8\u5206\u6bcf\u4e2a\u5355\u8bcd\u7684\u5411\u91cf\u4f5c\u4e3a key\u3002\u9996\u5148\u6211\u4eec\u5148\u628a query \u548c \u6bcf\u4e00\u4e2a\u5355\u8bcd\u8fdb\u884c\u70b9\u4e58 score=query\\cdot key score=query\\cdot key \uff0c\u5f97\u5230\u76f8\u5173\u6027\u7684\u5206\u503c\uff1b \u6709\u4e86\u8fd9\u4e9b\u5206\u503c\u540e\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e9b\u5206\u503c\u505a\u4e00\u4e2a softmax softmax \uff0c\u5f97\u5230\u4e00\u4e2a\u6ce8\u610f\u529b\u7684\u5206\u5e03 \u6709\u4e86\u8fd9\u4e2a\u6ce8\u610f\u529b\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u7528\u5b83\u548c Encoder \u7684\u8f93\u51fa\u503c (value) \u8fdb\u884c\u76f8\u4e58\uff0c\u5f97\u5230\u4e00\u4e2a\u52a0\u6743\u6c42\u548c\u540e\u7684\u503c\uff0c\u8fd9\u4e2a\u503c\u5c31\u5305\u542b\u6ce8\u610f\u529b\u7684\u8868\u793a\uff0c\u6211\u4eec\u7528\u5b83\u6765\u9884\u6d4b\u8981\u751f\u6210\u7684\u8bcd\u3002 \u8fd9\u4e2a\u8fc7\u7a0b\u6211\u4eec\u53ef\u4ee5\u770b\u770b\u4e00\u4e2a\u52a8\u56fe\u7684\u4e8b\u4f8b\u7406\u89e3\u4e00\u4e0b: 1.3 Attention\u8ba1\u7b97\u903b\u8f91 \u00b6 \u5f53\u7136\uff0cAttention \u5e76\u4e0d\u662f\u53ea\u6709\u8fd9\u4e00\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff0c\u540e\u6765\u8fd8\u6709\u5f88\u591a\u4eba\u627e\u5230\u4e86\u5404\u79cd\u5404\u6837\u7684\u8ba1\u7b97\u6ce8\u610f\u529b\u7684\u65b9\u6cd5, \u6bd4\u5982\u6211\u4eec\u4e0a\u9762\u4ecb\u7ecd\u7684\u4e09\u79cd\u8ba1\u7b97\u89c4\u5219, \u4f46\u662f\u4ece\u672c\u8d28\u4e0a\uff0c\u5b83\u4eec\u90fd\u9075\u5faa\u7740\u8fd9\u4e2a\u4e09\u6b65\u8d70\u7684\u903b\u8f91: query \u548c key \u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2aquery \u548c key \u76f8\u5173\u6027\u7684\u5206\u503c \u5c06\u8fd9\u4e2a\u5206\u503c\u8fdb\u884c\u5f52\u4e00\u5316(softmax)\uff0c\u5f97\u5230\u4e00\u4e2a\u6ce8\u610f\u529b\u7684\u5206\u5e03 \u4f7f\u7528\u6ce8\u610f\u529b\u5206\u5e03\u548c value \u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2a\u878d\u5408\u6ce8\u610f\u529b\u7684\u66f4\u597d\u7684 value \u503c 1.4 \u6709\u65e0attention\u6a21\u578b\u5bf9\u6bd4 \u00b6 1 \u65e0attention\u673a\u5236\u7684\u6a21\u578b \u00b6 \u6587\u672c\u5904\u7406\u9886\u57df\u7684Encoder-Decoder\u6846\u67b6\u53ef\u4ee5\u8fd9\u4e48\u76f4\u89c2\u5730\u53bb\u7406\u89e3\uff1a\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u9002\u5408\u5904\u7406\u7531\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u751f\u6210\u53e6\u5916\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u7684\u901a\u7528\u5904\u7406\u6a21\u578b\u3002\u5bf9\u4e8e\u53e5\u5b50\u5bf9 \uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u7ed9\u5b9a\u8f93\u5165\u53e5\u5b50Source\uff0c\u671f\u5f85\u901a\u8fc7Encoder-Decoder\u6846\u67b6\u6765\u751f\u6210\u76ee\u6807\u53e5\u5b50Target\u3002Source\u548cTarget\u53ef\u4ee5\u662f\u540c\u4e00\u79cd\u8bed\u8a00\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e24\u79cd\u4e0d\u540c\u7684\u8bed\u8a00\u3002\u800cSource\u548cTarget\u5206\u522b\u7531\u5404\u81ea\u7684\u5355\u8bcd\u5e8f\u5217\u6784\u6210\uff1a $$ Source = \\langle X_1,X_2 \\cdots X_m \\rangle \\\\ Target = \\langle y_1,y_2 \\cdots y_n \\rangle $$ encoder\u987e\u540d\u601d\u4e49\u5c31\u662f\u5bf9\u8f93\u5165\u53e5\u5b50Source\u8fdb\u884c\u7f16\u7801\uff0c\u5c06\u8f93\u5165\u53e5\u5b50\u901a\u8fc7\u975e\u7ebf\u6027\u53d8\u6362\u8f6c\u5316\u4e3a\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\uff1a $$ C = F(X_1,X_2 \\cdots X_m) $$ \u5bf9\u4e8e\u89e3\u7801\u5668Decoder\u6765\u8bf4\uff0c\u5176\u4efb\u52a1\u662f\u6839\u636e\u53e5\u5b50Source\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u548c\u4e4b\u524d\u5df2\u7ecf\u751f\u6210\u7684\u5386\u53f2\u4fe1\u606f,y_1, y_2\u2026y_i-1\u6765\u751f\u6210i\u65f6\u523b\u8981\u751f\u6210\u7684\u5355\u8bcdy_i $$ y_i = G(C,y_1,y_2 \\cdots y_{i-1}) $$ \u4e0a\u8ff0\u56fe\u4e2d\u5c55\u793a\u7684Encoder-Decoder\u6846\u67b6\u662f\u6ca1\u6709\u4f53\u73b0\u51fa\u201c\u6ce8\u610f\u529b\u6a21\u578b\u201d\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u662f\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u7684\u5206\u5fc3\u6a21\u578b\u3002\u4e3a\u4ec0\u4e48\u8bf4\u5b83\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u5462\uff1f\u8bf7\u89c2\u5bdf\u4e0b\u76ee\u6807\u53e5\u5b50Target\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u751f\u6210\u8fc7\u7a0b\u5982\u4e0b\uff1a $$ y_1 = f(C) \\\\ y_2 = f(C, y_1) \\\\ y_3 = f(C, y_1, y_2) $$ \u5176\u4e2df\u662fDecoder\u7684\u975e\u7ebf\u6027\u53d8\u6362\u51fd\u6570\u3002\u4ece\u8fd9\u91cc\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u751f\u6210\u76ee\u6807\u53e5\u5b50\u7684\u5355\u8bcd\u65f6\uff0c\u4e0d\u8bba\u751f\u6210\u54ea\u4e2a\u5355\u8bcd\uff0c\u5b83\u4eec\u4f7f\u7528\u7684\u8f93\u5165\u53e5\u5b50Source\u7684\u8bed\u4e49\u7f16\u7801C\u90fd\u662f\u4e00\u6837\u7684\uff0c\u6ca1\u6709\u4efb\u4f55\u533a\u522b\u3002 \u6bcf\u4e2ayi\u90fd\u4f9d\u6b21\u8fd9\u4e48\u4ea7\u751f\uff0c\u90a3\u4e48\u770b\u8d77\u6765\u5c31\u662f\u6574\u4e2a\u7cfb\u7edf\u6839\u636e\u8f93\u5165\u53e5\u5b50Source\u751f\u6210\u4e86\u76ee\u6807\u53e5\u5b50Target\u3002\u5982\u679cSource\u662f\u4e2d\u6587\u53e5\u5b50\uff0cTarget\u662f\u82f1\u6587\u53e5\u5b50\uff0c\u90a3\u4e48\u8fd9\u5c31\u662f\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u95ee\u9898\u7684Encoder-Decoder\u6846\u67b6\uff1b\u5982\u679cSource\u662f\u4e00\u7bc7\u6587\u7ae0\uff0cTarget\u662f\u6982\u62ec\u6027\u7684\u51e0\u53e5\u63cf\u8ff0\u8bed\u53e5\uff0c\u90a3\u4e48\u8fd9\u662f\u6587\u672c\u6458\u8981\u7684Encoder-Decoder\u6846\u67b6\uff1b\u5982\u679cSource\u662f\u4e00\u53e5\u95ee\u53e5\uff0cTarget\u662f\u4e00\u53e5\u56de\u7b54\uff0c\u90a3\u4e48\u8fd9\u662f\u95ee\u7b54\u7cfb\u7edf\u6216\u8005\u5bf9\u8bdd\u673a\u5668\u4eba\u7684Encoder-Decoder\u6846\u67b6\u3002\u7531\u6b64\u53ef\u89c1\uff0c\u5728\u6587\u672c\u5904\u7406\u9886\u57df\uff0cEncoder-Decoder\u7684\u5e94\u7528\u9886\u57df\u76f8\u5f53\u5e7f\u6cdb\u3002 \u95ee\u9898\u70b9\u662f : \u8bed\u4e49\u7f16\u7801C\u662f\u7531\u53e5\u5b50Source\u7684\u6bcf\u4e2a\u5355\u8bcd\u7ecf\u8fc7Encoder \u7f16\u7801\u4ea7\u751f\u7684\uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u8bba\u662f\u751f\u6210\u54ea\u4e2a\u5355\u8bcd\uff0c\u8fd8\u662f\uff0c\u5176\u5b9e\u53e5\u5b50Source\u4e2d\u4efb\u610f\u5355\u8bcd\u5bf9\u751f\u6210\u67d0\u4e2a\u76ee\u6807\u5355\u8bcdyi\u6765\u8bf4\u5f71\u54cd\u529b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u662f\u4e3a\u4f55\u8bf4\u8fd9\u4e2a\u6a21\u578b\u6ca1\u6709\u4f53\u73b0\u51fa\u6ce8\u610f\u529b\u7684\u7f18\u7531\u3002\u8fd9\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u770b\u5230\u773c\u524d\u7684\u753b\u9762\uff0c\u4f46\u662f\u773c\u4e2d\u5374\u6ca1\u6709\u6ce8\u610f\u7126\u70b9\u4e00\u6837. 2 \u6709attention\u673a\u5236\u7684\u6a21\u578b \u00b6 \u5982\u679c\u62ff\u673a\u5668\u7ffb\u8bd1\u6765\u89e3\u91ca\u8fd9\u4e2a\u5206\u5fc3\u6a21\u578b\u7684Encoder-Decoder\u6846\u67b6\u66f4\u597d\u7406\u89e3\uff0c\u6bd4\u5982\u8f93\u5165\u7684\u662f\u82f1\u6587\u53e5\u5b50\uff1aTom chase Jerry\uff0cEncoder-Decoder\u6846\u67b6\u9010\u6b65\u751f\u6210\u4e2d\u6587\u5355\u8bcd\uff1a\u201c\u6c64\u59c6\u201d\uff0c\u201c\u8ffd\u9010\u201d\uff0c\u201c\u6770\u745e\u201d\u3002\u5728\u7ffb\u8bd1\u201c\u6770\u745e\u201d\u8fd9\u4e2a\u4e2d\u6587\u5355\u8bcd\u7684\u65f6\u5019\uff0c\u5206\u5fc3\u6a21\u578b\u91cc\u9762\u7684\u6bcf\u4e2a\u82f1\u6587\u5355\u8bcd\u5bf9\u4e8e\u7ffb\u8bd1\u76ee\u6807\u5355\u8bcd\u201c\u6770\u745e\u201d\u8d21\u732e\u662f\u76f8\u540c\u7684\uff0c\u5f88\u660e\u663e\u8fd9\u91cc\u4e0d\u592a\u5408\u7406\uff0c\u663e\u7136\u201cJerry\u201d\u5bf9\u4e8e\u7ffb\u8bd1\u6210\u201c\u6770\u745e\u201d\u66f4\u91cd\u8981\uff0c\u4f46\u662f\u5206\u5fc3\u6a21\u578b\u662f\u65e0\u6cd5\u4f53\u73b0\u8fd9\u4e00\u70b9\u7684\uff0c\u8fd9\u5c31\u662f\u4e3a\u4f55\u8bf4\u5b83\u6ca1\u6709\u5f15\u5165\u6ce8\u610f\u529b\u7684\u539f\u56e0\u3002 \u6ca1\u6709\u5f15\u5165\u6ce8\u610f\u529b\u7684\u6a21\u578b\u5728\u8f93\u5165\u53e5\u5b50\u6bd4\u8f83\u77ed\u7684\u65f6\u5019\u95ee\u9898\u4e0d\u5927\uff0c\u4f46\u662f\u5982\u679c\u8f93\u5165\u53e5\u5b50\u6bd4\u8f83\u957f\uff0c\u6b64\u65f6\u6240\u6709\u8bed\u4e49\u5b8c\u5168\u901a\u8fc7\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5411\u91cf\u6765\u8868\u793a\uff0c\u5355\u8bcd\u81ea\u8eab\u7684\u4fe1\u606f\u5df2\u7ecf\u6d88\u5931\uff0c\u53ef\u60f3\u800c\u77e5\u4f1a\u4e22\u5931\u5f88\u591a\u7ec6\u8282\u4fe1\u606f\uff0c\u8fd9\u4e5f\u662f\u4e3a\u4f55\u8981\u5f15\u5165\u6ce8\u610f\u529b\u6a21\u578b\u7684\u91cd\u8981\u539f\u56e0\u3002 \u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u5982\u679c\u5f15\u5165Attention\u6a21\u578b\u7684\u8bdd\uff0c\u5e94\u8be5\u5728\u7ffb\u8bd1\u201c\u6770\u745e\u201d\u7684\u65f6\u5019\uff0c\u4f53\u73b0\u51fa\u82f1\u6587\u5355\u8bcd\u5bf9\u4e8e\u7ffb\u8bd1\u5f53\u524d\u4e2d\u6587\u5355\u8bcd\u4e0d\u540c\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u6bd4\u5982\u7ed9\u51fa\u7c7b\u4f3c\u4e0b\u9762\u4e00\u4e2a\u6982\u7387\u5206\u5e03\u503c\uff1a\uff08Tom,0.3\uff09(Chase,0.2) (Jerry,0.5).\u6bcf\u4e2a\u82f1\u6587\u5355\u8bcd\u7684\u6982\u7387\u4ee3\u8868\u4e86\u7ffb\u8bd1\u5f53\u524d\u5355\u8bcd\u201c\u6770\u745e\u201d\u65f6\uff0c\u6ce8\u610f\u529b\u5206\u914d\u6a21\u578b\u5206\u914d\u7ed9\u4e0d\u540c\u82f1\u6587\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5927\u5c0f\u3002\u8fd9\u5bf9\u4e8e\u6b63\u786e\u7ffb\u8bd1\u76ee\u6807\u8bed\u5355\u8bcd\u80af\u5b9a\u662f\u6709\u5e2e\u52a9\u7684\uff0c\u56e0\u4e3a\u5f15\u5165\u4e86\u65b0\u7684\u4fe1\u606f\u3002 \u540c\u7406\uff0c\u76ee\u6807\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u90fd\u5e94\u8be5\u5b66\u4f1a\u5176\u5bf9\u5e94\u7684\u6e90\u8bed\u53e5\u5b50\u4e2d\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u4fe1\u606f\u3002\u8fd9\u610f\u5473\u7740\u5728\u751f\u6210\u6bcf\u4e2a\u5355\u8bcd\u7684\u65f6\u5019\uff0c\u539f\u5148\u90fd\u662f\u76f8\u540c\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u4f1a\u88ab\u66ff\u6362\u6210\u6839\u636e\u5f53\u524d\u751f\u6210\u5355\u8bcd\u800c\u4e0d\u65ad\u53d8\u5316\u7684\u3002\u7406\u89e3Attention\u6a21\u578b\u7684\u5173\u952e\u5c31\u662f\u8fd9\u91cc\uff0c\u5373\u7531\u56fa\u5b9a\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u6362\u6210\u4e86\u6839\u636e\u5f53\u524d\u8f93\u51fa\u5355\u8bcd\u6765\u8c03\u6574\u6210\u52a0\u5165\u6ce8\u610f\u529b\u6a21\u578b\u7684\u53d8\u5316\u7684\u3002\u589e\u52a0\u4e86\u6ce8\u610f\u529b\u6a21\u578b\u7684Encoder-Decoder\u6846\u67b6\u7406\u89e3\u8d77\u6765\u5982\u4e0b\u56fe\u6240\u793a: \u5373\u751f\u6210\u76ee\u6807\u53e5\u5b50\u5355\u8bcd\u7684\u8fc7\u7a0b\u6210\u4e86\u4e0b\u9762\u7684\u5f62\u5f0f\uff1a $$ y_1 = f1(C_1) \\\\ y_2 = f1(C_2, y_1) \\\\ y_3 = f1(C_3, y_1, y_2) $$ \u800c\u6bcf\u4e2aCi\u53ef\u80fd\u5bf9\u5e94\u7740\u4e0d\u540c\u7684\u6e90\u8bed\u53e5\u5b50\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\uff0c\u6bd4\u5982\u5bf9\u4e8e\u4e0a\u9762\u7684\u82f1\u6c49\u7ffb\u8bd1\u6765\u8bf4\uff0c\u5176\u5bf9\u5e94\u7684\u4fe1\u606f\u53ef\u80fd\u5982\u4e0b: $$ C_{Tom}=g(0.6*f2(Tom), 0.2*f2(Chase), 0.2*f2(Jerry)) \\\\ C_{Chase}=g(0.2*f2(Tom), 0.7*f2(Chase), 0.1*f2(Jerry)) \\\\ C_{Jerry}=g(0.3*f2(Tom), 0.2*f2(Chase), 0.5*f2(Jerry)) $$ f2\u51fd\u6570\u4ee3\u8868Encoder\u5bf9\u8f93\u5165\u82f1\u6587\u5355\u8bcd\u7684\u67d0\u79cd\u53d8\u6362\u51fd\u6570\uff0c\u6bd4\u5982\u5982\u679cEncoder\u662f\u7528\u7684RNN\u6a21\u578b\u7684\u8bdd\uff0c\u8fd9\u4e2af2\u51fd\u6570\u7684\u7ed3\u679c\u5f80\u5f80\u662f\u67d0\u4e2a\u65f6\u523b\u8f93\u5165\u540e\u9690\u5c42\u8282\u70b9\u7684\u72b6\u6001\u503c\uff1bg\u4ee3\u8868Encoder\u6839\u636e\u5355\u8bcd\u7684\u4e2d\u95f4\u8868\u793a\u5408\u6210\u6574\u4e2a\u53e5\u5b50\u4e2d\u95f4\u8bed\u4e49\u8868\u793a\u7684\u53d8\u6362\u51fd\u6570\uff0c\u4e00\u822c\u7684\u505a\u6cd5\u4e2d\uff0cg\u51fd\u6570\u5c31\u662f\u5bf9\u6784\u6210\u5143\u7d20\u52a0\u6743\u6c42\u548c\uff0c\u5373\u4e0b\u5217\u516c\u5f0f $$ C_i = \\sum_{j=1}^{L_x}a_{ij}h_j $$ Lx\u4ee3\u8868\u8f93\u5165\u53e5\u5b50source\u7684\u957f\u5ea6, a_ij\u4ee3\u8868\u5728Target\u8f93\u51fa\u7b2ci\u4e2a\u5355\u8bcd\u65f6source\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u7b2cj\u4e2a\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u7cfb\u6570, \u800chj\u5219\u662fsource\u8f93\u5165\u53e5\u5b50\u4e2d\u7b2cj\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5047\u8bbeCi\u4e0b\u6807i\u5c31\u662f\u4e0a\u9762\u4f8b\u5b50\u6240\u8bf4\u7684'\u6c64\u59c6', \u90a3\u4e48Lx\u5c31\u662f3, h1=f('Tom'), h2=f('Chase'),h3=f('jerry')\u5206\u522b\u8f93\u5165\u53e5\u5b50\u6bcf\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u6a21\u578b\u6743\u503c\u5219\u5206\u522b\u662f0.6, 0.2, 0.2, \u6240\u4ee5g\u51fd\u6570\u672c\u8d28\u4e0a\u5c31\u662f\u52a0\u6743\u6c42\u548c\u51fd\u6570, \u5982\u679c\u5f62\u8c61\u8868\u793a\u7684\u8bdd, \u7ffb\u8bd1\u4e2d\u6587\u5355\u8bcd'\u6c64\u59c6'\u7684\u65f6\u5019, \u6570\u5b66\u516c\u5f0f\u5bf9\u5e94\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aCi\u7684\u5f62\u6210\u8fc7\u7a0b\u7c7b\u4f3c\u4e0b\u56fe:","title":"9 \u6ce8\u610f\u529b\u673a\u5236\u62d3\u5c55\u9605\u8bfb"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#_1","text":"\u77e5\u9053\u6ce8\u610f\u529b\u673a\u5236\u7684\u5de5\u4f5c\u6d41\u7a0b \u7406\u89e3\u4e09\u4e2a\u9636\u6bb5\u8fc7\u7a0b","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#1","text":"","title":"1 \u6ce8\u610f\u529b\u673a\u5236\u539f\u7406"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#11","text":"Attention\u673a\u5236\u7684\u5de5\u4f5c\u539f\u7406\u5e76\u4e0d\u590d\u6742\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u4e0b\u9762\u8fd9\u5f20\u56fe\u505a\u4e00\u4e2a\u603b\u7ed3","title":"1.1 \u6ce8\u610f\u529b\u673a\u5236\u793a\u610f\u56fe"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#12-attention","text":"\u9636\u6bb5\u4e00: query \u548c key \u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2aquery \u548c key \u76f8\u5173\u6027\u7684\u5206\u503c \u9636\u6bb5\u4e8c: \u5c06\u8fd9\u4e2a\u5206\u503c\u8fdb\u884c\u5f52\u4e00\u5316(softmax)\uff0c\u5f97\u5230\u4e00\u4e2a\u6ce8\u610f\u529b\u7684\u5206\u5e03 \u9636\u6bb5\u4e09: \u4f7f\u7528\u6ce8\u610f\u529b\u5206\u5e03\u548c value \u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2a\u878d\u5408\u6ce8\u610f\u529b\u7684\u66f4\u597d\u7684 value \u503c \u4e3a\u4e86\u66f4\u597d\u7684\u8bf4\u660e\u4e0a\u9762\u7684\u60c5\u51b5, \u6211\u4eec\u901a\u8fc7\u6ce8\u610f\u529b\u6765\u505a\u4e00\u4e2a\u673a\u5668\u7ffb\u8bd1(NMT) \u7684\u4efb\u52a1\uff0c\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u6211\u4eec\u4f1a\u4f7f\u7528 seq2seq \u7684\u67b6\u6784\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4ece\u8bcd\u5178\u91cc\u751f\u6210\u4e00\u4e2a\u7ffb\u8bd1\u7684\u7ed3\u679c\u3002\u5c31\u50cf\u4e0b\u9762\u8fd9\u5f20\u56fe\u4e00\u6837. \u5728\u6ca1\u6709\u6ce8\u610f\u529b\u4e4b\u524d\uff0c\u6211\u4eec\u6bcf\u6b21\u90fd\u662f\u6839\u636e Encoder \u90e8\u5206\u7684\u8f93\u51fa\u7ed3\u679c\u6765\u8fdb\u884c\u751f\u6210\uff0c\u63d0\u51fa\u6ce8\u610f\u529b\u540e\uff0c\u5c31\u662f\u60f3\u5728\u751f\u6210\u7ffb\u8bd1\u7ed3\u679c\u65f6\u5e76\u4e0d\u662f\u770b Encoder \u4e2d\u6240\u6709\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u800c\u662f\u5148\u6765\u770b\u770b\u60f3\u751f\u6210\u7684\u8fd9\u90e8\u5206\u548c\u54ea\u4e9b\u5355\u8bcd\u53ef\u80fd\u5173\u7cfb\u4f1a\u6bd4\u8f83\u5927\uff0c\u5173\u7cfb\u5927\u7684\u6211\u591a\u501f\u9274\u4e9b\uff1b\u5173\u7cfb\u5c0f\u7684\uff0c\u5c11\u501f\u9274\u4e9b\u3002\u5c31\u662f\u8fd9\u6837\u4e00\u4e2a\u60f3\u6cd5\uff0c\u6211\u4eec\u770b\u770b\u8be5\u5982\u4f55\u64cd\u4f5c\u3002 \u8fd9\u91cc\u4e3a\u4e86\u751f\u6210\u5355\u8bcd\uff0c\u6211\u4eec\u628a Decoder \u90e8\u5206\u8f93\u5165\u540e\u5f97\u5230\u7684\u5411\u91cf\u4f5c\u4e3a query\uff1b\u628a Encoder \u90e8\u5206\u6bcf\u4e2a\u5355\u8bcd\u7684\u5411\u91cf\u4f5c\u4e3a key\u3002\u9996\u5148\u6211\u4eec\u5148\u628a query \u548c \u6bcf\u4e00\u4e2a\u5355\u8bcd\u8fdb\u884c\u70b9\u4e58 score=query\\cdot key score=query\\cdot key \uff0c\u5f97\u5230\u76f8\u5173\u6027\u7684\u5206\u503c\uff1b \u6709\u4e86\u8fd9\u4e9b\u5206\u503c\u540e\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e9b\u5206\u503c\u505a\u4e00\u4e2a softmax softmax \uff0c\u5f97\u5230\u4e00\u4e2a\u6ce8\u610f\u529b\u7684\u5206\u5e03 \u6709\u4e86\u8fd9\u4e2a\u6ce8\u610f\u529b\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u7528\u5b83\u548c Encoder \u7684\u8f93\u51fa\u503c (value) \u8fdb\u884c\u76f8\u4e58\uff0c\u5f97\u5230\u4e00\u4e2a\u52a0\u6743\u6c42\u548c\u540e\u7684\u503c\uff0c\u8fd9\u4e2a\u503c\u5c31\u5305\u542b\u6ce8\u610f\u529b\u7684\u8868\u793a\uff0c\u6211\u4eec\u7528\u5b83\u6765\u9884\u6d4b\u8981\u751f\u6210\u7684\u8bcd\u3002 \u8fd9\u4e2a\u8fc7\u7a0b\u6211\u4eec\u53ef\u4ee5\u770b\u770b\u4e00\u4e2a\u52a8\u56fe\u7684\u4e8b\u4f8b\u7406\u89e3\u4e00\u4e0b:","title":"1.2 Attention\u8ba1\u7b97\u8fc7\u7a0b"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#13-attention","text":"\u5f53\u7136\uff0cAttention \u5e76\u4e0d\u662f\u53ea\u6709\u8fd9\u4e00\u79cd\u8ba1\u7b97\u65b9\u5f0f\uff0c\u540e\u6765\u8fd8\u6709\u5f88\u591a\u4eba\u627e\u5230\u4e86\u5404\u79cd\u5404\u6837\u7684\u8ba1\u7b97\u6ce8\u610f\u529b\u7684\u65b9\u6cd5, \u6bd4\u5982\u6211\u4eec\u4e0a\u9762\u4ecb\u7ecd\u7684\u4e09\u79cd\u8ba1\u7b97\u89c4\u5219, \u4f46\u662f\u4ece\u672c\u8d28\u4e0a\uff0c\u5b83\u4eec\u90fd\u9075\u5faa\u7740\u8fd9\u4e2a\u4e09\u6b65\u8d70\u7684\u903b\u8f91: query \u548c key \u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2aquery \u548c key \u76f8\u5173\u6027\u7684\u5206\u503c \u5c06\u8fd9\u4e2a\u5206\u503c\u8fdb\u884c\u5f52\u4e00\u5316(softmax)\uff0c\u5f97\u5230\u4e00\u4e2a\u6ce8\u610f\u529b\u7684\u5206\u5e03 \u4f7f\u7528\u6ce8\u610f\u529b\u5206\u5e03\u548c value \u8fdb\u884c\u8ba1\u7b97\uff0c\u5f97\u5230\u4e00\u4e2a\u878d\u5408\u6ce8\u610f\u529b\u7684\u66f4\u597d\u7684 value \u503c","title":"1.3 Attention\u8ba1\u7b97\u903b\u8f91"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#14-attention","text":"","title":"1.4 \u6709\u65e0attention\u6a21\u578b\u5bf9\u6bd4"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#1-attention","text":"\u6587\u672c\u5904\u7406\u9886\u57df\u7684Encoder-Decoder\u6846\u67b6\u53ef\u4ee5\u8fd9\u4e48\u76f4\u89c2\u5730\u53bb\u7406\u89e3\uff1a\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u9002\u5408\u5904\u7406\u7531\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u751f\u6210\u53e6\u5916\u4e00\u4e2a\u53e5\u5b50\uff08\u6216\u7bc7\u7ae0\uff09\u7684\u901a\u7528\u5904\u7406\u6a21\u578b\u3002\u5bf9\u4e8e\u53e5\u5b50\u5bf9 \uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u7ed9\u5b9a\u8f93\u5165\u53e5\u5b50Source\uff0c\u671f\u5f85\u901a\u8fc7Encoder-Decoder\u6846\u67b6\u6765\u751f\u6210\u76ee\u6807\u53e5\u5b50Target\u3002Source\u548cTarget\u53ef\u4ee5\u662f\u540c\u4e00\u79cd\u8bed\u8a00\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e24\u79cd\u4e0d\u540c\u7684\u8bed\u8a00\u3002\u800cSource\u548cTarget\u5206\u522b\u7531\u5404\u81ea\u7684\u5355\u8bcd\u5e8f\u5217\u6784\u6210\uff1a $$ Source = \\langle X_1,X_2 \\cdots X_m \\rangle \\\\ Target = \\langle y_1,y_2 \\cdots y_n \\rangle $$ encoder\u987e\u540d\u601d\u4e49\u5c31\u662f\u5bf9\u8f93\u5165\u53e5\u5b50Source\u8fdb\u884c\u7f16\u7801\uff0c\u5c06\u8f93\u5165\u53e5\u5b50\u901a\u8fc7\u975e\u7ebf\u6027\u53d8\u6362\u8f6c\u5316\u4e3a\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\uff1a $$ C = F(X_1,X_2 \\cdots X_m) $$ \u5bf9\u4e8e\u89e3\u7801\u5668Decoder\u6765\u8bf4\uff0c\u5176\u4efb\u52a1\u662f\u6839\u636e\u53e5\u5b50Source\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u548c\u4e4b\u524d\u5df2\u7ecf\u751f\u6210\u7684\u5386\u53f2\u4fe1\u606f,y_1, y_2\u2026y_i-1\u6765\u751f\u6210i\u65f6\u523b\u8981\u751f\u6210\u7684\u5355\u8bcdy_i $$ y_i = G(C,y_1,y_2 \\cdots y_{i-1}) $$ \u4e0a\u8ff0\u56fe\u4e2d\u5c55\u793a\u7684Encoder-Decoder\u6846\u67b6\u662f\u6ca1\u6709\u4f53\u73b0\u51fa\u201c\u6ce8\u610f\u529b\u6a21\u578b\u201d\u7684\uff0c\u6240\u4ee5\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u662f\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u7684\u5206\u5fc3\u6a21\u578b\u3002\u4e3a\u4ec0\u4e48\u8bf4\u5b83\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u5462\uff1f\u8bf7\u89c2\u5bdf\u4e0b\u76ee\u6807\u53e5\u5b50Target\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u751f\u6210\u8fc7\u7a0b\u5982\u4e0b\uff1a $$ y_1 = f(C) \\\\ y_2 = f(C, y_1) \\\\ y_3 = f(C, y_1, y_2) $$ \u5176\u4e2df\u662fDecoder\u7684\u975e\u7ebf\u6027\u53d8\u6362\u51fd\u6570\u3002\u4ece\u8fd9\u91cc\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u751f\u6210\u76ee\u6807\u53e5\u5b50\u7684\u5355\u8bcd\u65f6\uff0c\u4e0d\u8bba\u751f\u6210\u54ea\u4e2a\u5355\u8bcd\uff0c\u5b83\u4eec\u4f7f\u7528\u7684\u8f93\u5165\u53e5\u5b50Source\u7684\u8bed\u4e49\u7f16\u7801C\u90fd\u662f\u4e00\u6837\u7684\uff0c\u6ca1\u6709\u4efb\u4f55\u533a\u522b\u3002 \u6bcf\u4e2ayi\u90fd\u4f9d\u6b21\u8fd9\u4e48\u4ea7\u751f\uff0c\u90a3\u4e48\u770b\u8d77\u6765\u5c31\u662f\u6574\u4e2a\u7cfb\u7edf\u6839\u636e\u8f93\u5165\u53e5\u5b50Source\u751f\u6210\u4e86\u76ee\u6807\u53e5\u5b50Target\u3002\u5982\u679cSource\u662f\u4e2d\u6587\u53e5\u5b50\uff0cTarget\u662f\u82f1\u6587\u53e5\u5b50\uff0c\u90a3\u4e48\u8fd9\u5c31\u662f\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u95ee\u9898\u7684Encoder-Decoder\u6846\u67b6\uff1b\u5982\u679cSource\u662f\u4e00\u7bc7\u6587\u7ae0\uff0cTarget\u662f\u6982\u62ec\u6027\u7684\u51e0\u53e5\u63cf\u8ff0\u8bed\u53e5\uff0c\u90a3\u4e48\u8fd9\u662f\u6587\u672c\u6458\u8981\u7684Encoder-Decoder\u6846\u67b6\uff1b\u5982\u679cSource\u662f\u4e00\u53e5\u95ee\u53e5\uff0cTarget\u662f\u4e00\u53e5\u56de\u7b54\uff0c\u90a3\u4e48\u8fd9\u662f\u95ee\u7b54\u7cfb\u7edf\u6216\u8005\u5bf9\u8bdd\u673a\u5668\u4eba\u7684Encoder-Decoder\u6846\u67b6\u3002\u7531\u6b64\u53ef\u89c1\uff0c\u5728\u6587\u672c\u5904\u7406\u9886\u57df\uff0cEncoder-Decoder\u7684\u5e94\u7528\u9886\u57df\u76f8\u5f53\u5e7f\u6cdb\u3002 \u95ee\u9898\u70b9\u662f : \u8bed\u4e49\u7f16\u7801C\u662f\u7531\u53e5\u5b50Source\u7684\u6bcf\u4e2a\u5355\u8bcd\u7ecf\u8fc7Encoder \u7f16\u7801\u4ea7\u751f\u7684\uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u8bba\u662f\u751f\u6210\u54ea\u4e2a\u5355\u8bcd\uff0c\u8fd8\u662f\uff0c\u5176\u5b9e\u53e5\u5b50Source\u4e2d\u4efb\u610f\u5355\u8bcd\u5bf9\u751f\u6210\u67d0\u4e2a\u76ee\u6807\u5355\u8bcdyi\u6765\u8bf4\u5f71\u54cd\u529b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u662f\u4e3a\u4f55\u8bf4\u8fd9\u4e2a\u6a21\u578b\u6ca1\u6709\u4f53\u73b0\u51fa\u6ce8\u610f\u529b\u7684\u7f18\u7531\u3002\u8fd9\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u770b\u5230\u773c\u524d\u7684\u753b\u9762\uff0c\u4f46\u662f\u773c\u4e2d\u5374\u6ca1\u6709\u6ce8\u610f\u7126\u70b9\u4e00\u6837.","title":"1 \u65e0attention\u673a\u5236\u7684\u6a21\u578b"},{"location":"03_mkdocs_RNN/9%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB.html#2-attention","text":"\u5982\u679c\u62ff\u673a\u5668\u7ffb\u8bd1\u6765\u89e3\u91ca\u8fd9\u4e2a\u5206\u5fc3\u6a21\u578b\u7684Encoder-Decoder\u6846\u67b6\u66f4\u597d\u7406\u89e3\uff0c\u6bd4\u5982\u8f93\u5165\u7684\u662f\u82f1\u6587\u53e5\u5b50\uff1aTom chase Jerry\uff0cEncoder-Decoder\u6846\u67b6\u9010\u6b65\u751f\u6210\u4e2d\u6587\u5355\u8bcd\uff1a\u201c\u6c64\u59c6\u201d\uff0c\u201c\u8ffd\u9010\u201d\uff0c\u201c\u6770\u745e\u201d\u3002\u5728\u7ffb\u8bd1\u201c\u6770\u745e\u201d\u8fd9\u4e2a\u4e2d\u6587\u5355\u8bcd\u7684\u65f6\u5019\uff0c\u5206\u5fc3\u6a21\u578b\u91cc\u9762\u7684\u6bcf\u4e2a\u82f1\u6587\u5355\u8bcd\u5bf9\u4e8e\u7ffb\u8bd1\u76ee\u6807\u5355\u8bcd\u201c\u6770\u745e\u201d\u8d21\u732e\u662f\u76f8\u540c\u7684\uff0c\u5f88\u660e\u663e\u8fd9\u91cc\u4e0d\u592a\u5408\u7406\uff0c\u663e\u7136\u201cJerry\u201d\u5bf9\u4e8e\u7ffb\u8bd1\u6210\u201c\u6770\u745e\u201d\u66f4\u91cd\u8981\uff0c\u4f46\u662f\u5206\u5fc3\u6a21\u578b\u662f\u65e0\u6cd5\u4f53\u73b0\u8fd9\u4e00\u70b9\u7684\uff0c\u8fd9\u5c31\u662f\u4e3a\u4f55\u8bf4\u5b83\u6ca1\u6709\u5f15\u5165\u6ce8\u610f\u529b\u7684\u539f\u56e0\u3002 \u6ca1\u6709\u5f15\u5165\u6ce8\u610f\u529b\u7684\u6a21\u578b\u5728\u8f93\u5165\u53e5\u5b50\u6bd4\u8f83\u77ed\u7684\u65f6\u5019\u95ee\u9898\u4e0d\u5927\uff0c\u4f46\u662f\u5982\u679c\u8f93\u5165\u53e5\u5b50\u6bd4\u8f83\u957f\uff0c\u6b64\u65f6\u6240\u6709\u8bed\u4e49\u5b8c\u5168\u901a\u8fc7\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5411\u91cf\u6765\u8868\u793a\uff0c\u5355\u8bcd\u81ea\u8eab\u7684\u4fe1\u606f\u5df2\u7ecf\u6d88\u5931\uff0c\u53ef\u60f3\u800c\u77e5\u4f1a\u4e22\u5931\u5f88\u591a\u7ec6\u8282\u4fe1\u606f\uff0c\u8fd9\u4e5f\u662f\u4e3a\u4f55\u8981\u5f15\u5165\u6ce8\u610f\u529b\u6a21\u578b\u7684\u91cd\u8981\u539f\u56e0\u3002 \u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u5982\u679c\u5f15\u5165Attention\u6a21\u578b\u7684\u8bdd\uff0c\u5e94\u8be5\u5728\u7ffb\u8bd1\u201c\u6770\u745e\u201d\u7684\u65f6\u5019\uff0c\u4f53\u73b0\u51fa\u82f1\u6587\u5355\u8bcd\u5bf9\u4e8e\u7ffb\u8bd1\u5f53\u524d\u4e2d\u6587\u5355\u8bcd\u4e0d\u540c\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u6bd4\u5982\u7ed9\u51fa\u7c7b\u4f3c\u4e0b\u9762\u4e00\u4e2a\u6982\u7387\u5206\u5e03\u503c\uff1a\uff08Tom,0.3\uff09(Chase,0.2) (Jerry,0.5).\u6bcf\u4e2a\u82f1\u6587\u5355\u8bcd\u7684\u6982\u7387\u4ee3\u8868\u4e86\u7ffb\u8bd1\u5f53\u524d\u5355\u8bcd\u201c\u6770\u745e\u201d\u65f6\uff0c\u6ce8\u610f\u529b\u5206\u914d\u6a21\u578b\u5206\u914d\u7ed9\u4e0d\u540c\u82f1\u6587\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5927\u5c0f\u3002\u8fd9\u5bf9\u4e8e\u6b63\u786e\u7ffb\u8bd1\u76ee\u6807\u8bed\u5355\u8bcd\u80af\u5b9a\u662f\u6709\u5e2e\u52a9\u7684\uff0c\u56e0\u4e3a\u5f15\u5165\u4e86\u65b0\u7684\u4fe1\u606f\u3002 \u540c\u7406\uff0c\u76ee\u6807\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u90fd\u5e94\u8be5\u5b66\u4f1a\u5176\u5bf9\u5e94\u7684\u6e90\u8bed\u53e5\u5b50\u4e2d\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u4fe1\u606f\u3002\u8fd9\u610f\u5473\u7740\u5728\u751f\u6210\u6bcf\u4e2a\u5355\u8bcd\u7684\u65f6\u5019\uff0c\u539f\u5148\u90fd\u662f\u76f8\u540c\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u4f1a\u88ab\u66ff\u6362\u6210\u6839\u636e\u5f53\u524d\u751f\u6210\u5355\u8bcd\u800c\u4e0d\u65ad\u53d8\u5316\u7684\u3002\u7406\u89e3Attention\u6a21\u578b\u7684\u5173\u952e\u5c31\u662f\u8fd9\u91cc\uff0c\u5373\u7531\u56fa\u5b9a\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aC\u6362\u6210\u4e86\u6839\u636e\u5f53\u524d\u8f93\u51fa\u5355\u8bcd\u6765\u8c03\u6574\u6210\u52a0\u5165\u6ce8\u610f\u529b\u6a21\u578b\u7684\u53d8\u5316\u7684\u3002\u589e\u52a0\u4e86\u6ce8\u610f\u529b\u6a21\u578b\u7684Encoder-Decoder\u6846\u67b6\u7406\u89e3\u8d77\u6765\u5982\u4e0b\u56fe\u6240\u793a: \u5373\u751f\u6210\u76ee\u6807\u53e5\u5b50\u5355\u8bcd\u7684\u8fc7\u7a0b\u6210\u4e86\u4e0b\u9762\u7684\u5f62\u5f0f\uff1a $$ y_1 = f1(C_1) \\\\ y_2 = f1(C_2, y_1) \\\\ y_3 = f1(C_3, y_1, y_2) $$ \u800c\u6bcf\u4e2aCi\u53ef\u80fd\u5bf9\u5e94\u7740\u4e0d\u540c\u7684\u6e90\u8bed\u53e5\u5b50\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u6982\u7387\u5206\u5e03\uff0c\u6bd4\u5982\u5bf9\u4e8e\u4e0a\u9762\u7684\u82f1\u6c49\u7ffb\u8bd1\u6765\u8bf4\uff0c\u5176\u5bf9\u5e94\u7684\u4fe1\u606f\u53ef\u80fd\u5982\u4e0b: $$ C_{Tom}=g(0.6*f2(Tom), 0.2*f2(Chase), 0.2*f2(Jerry)) \\\\ C_{Chase}=g(0.2*f2(Tom), 0.7*f2(Chase), 0.1*f2(Jerry)) \\\\ C_{Jerry}=g(0.3*f2(Tom), 0.2*f2(Chase), 0.5*f2(Jerry)) $$ f2\u51fd\u6570\u4ee3\u8868Encoder\u5bf9\u8f93\u5165\u82f1\u6587\u5355\u8bcd\u7684\u67d0\u79cd\u53d8\u6362\u51fd\u6570\uff0c\u6bd4\u5982\u5982\u679cEncoder\u662f\u7528\u7684RNN\u6a21\u578b\u7684\u8bdd\uff0c\u8fd9\u4e2af2\u51fd\u6570\u7684\u7ed3\u679c\u5f80\u5f80\u662f\u67d0\u4e2a\u65f6\u523b\u8f93\u5165\u540e\u9690\u5c42\u8282\u70b9\u7684\u72b6\u6001\u503c\uff1bg\u4ee3\u8868Encoder\u6839\u636e\u5355\u8bcd\u7684\u4e2d\u95f4\u8868\u793a\u5408\u6210\u6574\u4e2a\u53e5\u5b50\u4e2d\u95f4\u8bed\u4e49\u8868\u793a\u7684\u53d8\u6362\u51fd\u6570\uff0c\u4e00\u822c\u7684\u505a\u6cd5\u4e2d\uff0cg\u51fd\u6570\u5c31\u662f\u5bf9\u6784\u6210\u5143\u7d20\u52a0\u6743\u6c42\u548c\uff0c\u5373\u4e0b\u5217\u516c\u5f0f $$ C_i = \\sum_{j=1}^{L_x}a_{ij}h_j $$ Lx\u4ee3\u8868\u8f93\u5165\u53e5\u5b50source\u7684\u957f\u5ea6, a_ij\u4ee3\u8868\u5728Target\u8f93\u51fa\u7b2ci\u4e2a\u5355\u8bcd\u65f6source\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u7b2cj\u4e2a\u5355\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u914d\u7cfb\u6570, \u800chj\u5219\u662fsource\u8f93\u5165\u53e5\u5b50\u4e2d\u7b2cj\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5047\u8bbeCi\u4e0b\u6807i\u5c31\u662f\u4e0a\u9762\u4f8b\u5b50\u6240\u8bf4\u7684'\u6c64\u59c6', \u90a3\u4e48Lx\u5c31\u662f3, h1=f('Tom'), h2=f('Chase'),h3=f('jerry')\u5206\u522b\u8f93\u5165\u53e5\u5b50\u6bcf\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7f16\u7801, \u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u6a21\u578b\u6743\u503c\u5219\u5206\u522b\u662f0.6, 0.2, 0.2, \u6240\u4ee5g\u51fd\u6570\u672c\u8d28\u4e0a\u5c31\u662f\u52a0\u6743\u6c42\u548c\u51fd\u6570, \u5982\u679c\u5f62\u8c61\u8868\u793a\u7684\u8bdd, \u7ffb\u8bd1\u4e2d\u6587\u5355\u8bcd'\u6c64\u59c6'\u7684\u65f6\u5019, \u6570\u5b66\u516c\u5f0f\u5bf9\u5e94\u7684\u4e2d\u95f4\u8bed\u4e49\u8868\u793aCi\u7684\u5f62\u6210\u8fc7\u7a0b\u7c7b\u4f3c\u4e0b\u56fe:","title":"2 \u6709attention\u673a\u5236\u7684\u6a21\u578b"},{"location":"04_mkdocs_transformer/index.html","text":"\u00b6","title":"Index"},{"location":"04_mkdocs_transformer/index.html#_1","text":"","title":""},{"location":"04_mkdocs_transformer/1%20Transformer%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3Transformer\u80cc\u666f 1 Transformer\u7684\u8bde\u751f \u00b6 2018\u5e7410\u6708\uff0cGoogle\u53d1\u51fa\u4e00\u7bc7\u8bba\u6587\u300aBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u300b, BERT\u6a21\u578b\u6a2a\u7a7a\u51fa\u4e16, \u5e76\u6a2a\u626bNLP\u9886\u57df11\u9879\u4efb\u52a1\u7684\u6700\u4f73\u6210\u7ee9! \u8bba\u6587\u5730\u5740: https://arxiv.org/pdf/1810.04805.pdf \u800c\u5728BERT\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u7684\u7ed3\u6784\u5c31\u662fTransformer, \u4e4b\u540e\u53c8\u76f8\u7ee7\u51fa\u73b0XLNET\uff0croBERT\u7b49\u6a21\u578b\u51fb\u8d25\u4e86BERT\uff0c\u4f46\u662f\u4ed6\u4eec\u7684\u6838\u5fc3\u6ca1\u6709\u53d8\uff0c\u4ecd\u7136\u662f\uff1aTransformer\u3002 2 Transformer\u7684\u4f18\u52bf \u00b6 \u76f8\u6bd4\u4e4b\u524d\u5360\u9886\u5e02\u573a\u7684LSTM\u548cGRU\u6a21\u578b\uff0cTransformer\u6709\u4e24\u4e2a\u663e\u8457\u7684\u4f18\u52bf: 1\u3001Transformer\u80fd\u591f\u5229\u7528\u5206\u5e03\u5f0fGPU\u8fdb\u884c\u5e76\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u7387. 2\u3001\u5728\u5206\u6790\u9884\u6d4b\u66f4\u957f\u7684\u6587\u672c\u65f6, \u6355\u6349\u95f4\u9694\u8f83\u957f\u7684\u8bed\u4e49\u5173\u8054\u6548\u679c\u66f4\u597d. \u4e0b\u9762\u662f\u4e00\u5f20\u5728\u6d4b\u8bc4\u6bd4\u8f83\u56fe: 3 Transformer\u7684\u5e02\u573a \u00b6 \u5728\u8457\u540d\u7684SOTA\u673a\u5668\u7ffb\u8bd1\u699c\u5355\u4e0a, \u51e0\u4e4e\u6240\u6709\u6392\u540d\u9760\u524d\u7684\u6a21\u578b\u90fd\u4f7f\u7528Transformer, \u5176\u57fa\u672c\u4e0a\u53ef\u4ee5\u770b\u4f5c\u662f\u5de5\u4e1a\u754c\u7684\u98ce\u5411\u6807, \u5e02\u573a\u7a7a\u95f4\u81ea\u7136\u4e0d\u5fc5\u591a\u8bf4\uff01","title":"1 Transformer\u80cc\u666f\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/1%20Transformer%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3Transformer\u80cc\u666f","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/1%20Transformer%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#1-transformer","text":"2018\u5e7410\u6708\uff0cGoogle\u53d1\u51fa\u4e00\u7bc7\u8bba\u6587\u300aBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u300b, BERT\u6a21\u578b\u6a2a\u7a7a\u51fa\u4e16, \u5e76\u6a2a\u626bNLP\u9886\u57df11\u9879\u4efb\u52a1\u7684\u6700\u4f73\u6210\u7ee9! \u8bba\u6587\u5730\u5740: https://arxiv.org/pdf/1810.04805.pdf \u800c\u5728BERT\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u7684\u7ed3\u6784\u5c31\u662fTransformer, \u4e4b\u540e\u53c8\u76f8\u7ee7\u51fa\u73b0XLNET\uff0croBERT\u7b49\u6a21\u578b\u51fb\u8d25\u4e86BERT\uff0c\u4f46\u662f\u4ed6\u4eec\u7684\u6838\u5fc3\u6ca1\u6709\u53d8\uff0c\u4ecd\u7136\u662f\uff1aTransformer\u3002","title":"1 Transformer\u7684\u8bde\u751f"},{"location":"04_mkdocs_transformer/1%20Transformer%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#2-transformer","text":"\u76f8\u6bd4\u4e4b\u524d\u5360\u9886\u5e02\u573a\u7684LSTM\u548cGRU\u6a21\u578b\uff0cTransformer\u6709\u4e24\u4e2a\u663e\u8457\u7684\u4f18\u52bf: 1\u3001Transformer\u80fd\u591f\u5229\u7528\u5206\u5e03\u5f0fGPU\u8fdb\u884c\u5e76\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u7387. 2\u3001\u5728\u5206\u6790\u9884\u6d4b\u66f4\u957f\u7684\u6587\u672c\u65f6, \u6355\u6349\u95f4\u9694\u8f83\u957f\u7684\u8bed\u4e49\u5173\u8054\u6548\u679c\u66f4\u597d. \u4e0b\u9762\u662f\u4e00\u5f20\u5728\u6d4b\u8bc4\u6bd4\u8f83\u56fe:","title":"2 Transformer\u7684\u4f18\u52bf"},{"location":"04_mkdocs_transformer/1%20Transformer%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html#3-transformer","text":"\u5728\u8457\u540d\u7684SOTA\u673a\u5668\u7ffb\u8bd1\u699c\u5355\u4e0a, \u51e0\u4e4e\u6240\u6709\u6392\u540d\u9760\u524d\u7684\u6a21\u578b\u90fd\u4f7f\u7528Transformer, \u5176\u57fa\u672c\u4e0a\u53ef\u4ee5\u770b\u4f5c\u662f\u5de5\u4e1a\u754c\u7684\u98ce\u5411\u6807, \u5e02\u573a\u7a7a\u95f4\u81ea\u7136\u4e0d\u5fc5\u591a\u8bf4\uff01","title":"3 Transformer\u7684\u5e02\u573a"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3Transformer\u6a21\u578b\u7684\u4f5c\u7528. \u4e86\u89e3Transformer\u603b\u4f53\u67b6\u6784\u56fe\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u540d\u79f0. 1 Transformer\u6a21\u578b\u7684\u4f5c\u7528 \u00b6 \u57fa\u4e8eseq2seq\u67b6\u6784\u7684transformer\u6a21\u578b\u53ef\u4ee5\u5b8c\u6210NLP\u9886\u57df\u7814\u7a76\u7684\u5178\u578b\u4efb\u52a1, \u5982\u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7b49. \u540c\u65f6\u53c8\u53ef\u4ee5\u6784\u5efa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60. \u5728\u63a5\u4e0b\u6765\u7684\u67b6\u6784\u5206\u6790\u4e2d, \u6211\u4eec\u5c06\u5047\u8bbe\u4f7f\u7528Transformer\u6a21\u578b\u67b6\u6784\u5904\u7406\u4ece\u4e00\u79cd\u8bed\u8a00\u6587\u672c\u5230\u53e6\u4e00\u79cd\u8bed\u8a00\u6587\u672c\u7684\u7ffb\u8bd1\u5de5\u4f5c, \u56e0\u6b64\u5f88\u591a\u547d\u540d\u65b9\u5f0f\u9075\u5faaNLP\u4e2d\u7684\u89c4\u5219. \u6bd4\u5982: Embeddding\u5c42\u5c06\u79f0\u4f5c\u6587\u672c\u5d4c\u5165\u5c42, Embedding\u5c42\u4ea7\u751f\u7684\u5f20\u91cf\u79f0\u4e3a\u8bcd\u5d4c\u5165\u5f20\u91cf, \u5b83\u7684\u6700\u540e\u4e00\u7ef4\u5c06\u79f0\u4f5c\u8bcd\u5411\u91cf\u7b49. 2 Transformer\u603b\u4f53\u67b6\u6784\u56fe \u00b6 2.1 Transformer\u603b\u4f53\u67b6\u6784 \u00b6 \u8f93\u5165\u90e8\u5206 \u8f93\u51fa\u90e8\u5206 \u7f16\u7801\u5668\u90e8\u5206 \u89e3\u7801\u5668\u90e8\u5206 2.2 \u8f93\u5165\u90e8\u5206\u5305\u542b \u00b6 \u6e90\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u76ee\u6807\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 2.3 \u8f93\u51fa\u90e8\u5206\u5305\u542b \u00b6 \u7ebf\u6027\u5c42 softmax\u5c42 2.4 \u7f16\u7801\u5668\u90e8\u5206 \u00b6 \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u7531\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 2.5 \u89e3\u7801\u5668\u90e8\u5206 \u00b6 \u7531N\u4e2a\u89e3\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u7531\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86Transformer\u6a21\u578b\u7684\u4f5c\u7528: \u57fa\u4e8eseq2seq\u67b6\u6784\u7684transformer\u6a21\u578b\u53ef\u4ee5\u5b8c\u6210NLP\u9886\u57df\u7814\u7a76\u7684\u5178\u578b\u4efb\u52a1, \u5982\u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7b49. \u540c\u65f6\u53c8\u53ef\u4ee5\u6784\u5efa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60. Transformer\u603b\u4f53\u67b6\u6784\u53ef\u5206\u4e3a\u56db\u4e2a\u90e8\u5206: \u8f93\u5165\u90e8\u5206 \u8f93\u51fa\u90e8\u5206 \u7f16\u7801\u5668\u90e8\u5206 \u89e3\u7801\u5668\u90e8\u5206 \u8f93\u5165\u90e8\u5206\u5305\u542b: \u6e90\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u76ee\u6807\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u8f93\u51fa\u90e8\u5206\u5305\u542b: \u7ebf\u6027\u5c42 softmax\u5904\u7406\u5668 \u7f16\u7801\u5668\u90e8\u5206: \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u7531\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u89e3\u7801\u5668\u90e8\u5206: \u7531N\u4e2a\u89e3\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u7531\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5","title":"2 \u8ba4\u8bc6Transformer\u67b6\u6784"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#_1","text":"\u4e86\u89e3Transformer\u6a21\u578b\u7684\u4f5c\u7528. \u4e86\u89e3Transformer\u603b\u4f53\u67b6\u6784\u56fe\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u540d\u79f0.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#1-transformer","text":"\u57fa\u4e8eseq2seq\u67b6\u6784\u7684transformer\u6a21\u578b\u53ef\u4ee5\u5b8c\u6210NLP\u9886\u57df\u7814\u7a76\u7684\u5178\u578b\u4efb\u52a1, \u5982\u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7b49. \u540c\u65f6\u53c8\u53ef\u4ee5\u6784\u5efa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60. \u5728\u63a5\u4e0b\u6765\u7684\u67b6\u6784\u5206\u6790\u4e2d, \u6211\u4eec\u5c06\u5047\u8bbe\u4f7f\u7528Transformer\u6a21\u578b\u67b6\u6784\u5904\u7406\u4ece\u4e00\u79cd\u8bed\u8a00\u6587\u672c\u5230\u53e6\u4e00\u79cd\u8bed\u8a00\u6587\u672c\u7684\u7ffb\u8bd1\u5de5\u4f5c, \u56e0\u6b64\u5f88\u591a\u547d\u540d\u65b9\u5f0f\u9075\u5faaNLP\u4e2d\u7684\u89c4\u5219. \u6bd4\u5982: Embeddding\u5c42\u5c06\u79f0\u4f5c\u6587\u672c\u5d4c\u5165\u5c42, Embedding\u5c42\u4ea7\u751f\u7684\u5f20\u91cf\u79f0\u4e3a\u8bcd\u5d4c\u5165\u5f20\u91cf, \u5b83\u7684\u6700\u540e\u4e00\u7ef4\u5c06\u79f0\u4f5c\u8bcd\u5411\u91cf\u7b49.","title":"1 Transformer\u6a21\u578b\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#2-transformer","text":"","title":"2 Transformer\u603b\u4f53\u67b6\u6784\u56fe"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#21-transformer","text":"\u8f93\u5165\u90e8\u5206 \u8f93\u51fa\u90e8\u5206 \u7f16\u7801\u5668\u90e8\u5206 \u89e3\u7801\u5668\u90e8\u5206","title":"2.1 Transformer\u603b\u4f53\u67b6\u6784"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#22","text":"\u6e90\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u76ee\u6807\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668","title":"2.2 \u8f93\u5165\u90e8\u5206\u5305\u542b"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#23","text":"\u7ebf\u6027\u5c42 softmax\u5c42","title":"2.3 \u8f93\u51fa\u90e8\u5206\u5305\u542b"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#24","text":"\u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u7531\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5","title":"2.4 \u7f16\u7801\u5668\u90e8\u5206"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#25","text":"\u7531N\u4e2a\u89e3\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u7531\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5","title":"2.5 \u89e3\u7801\u5668\u90e8\u5206"},{"location":"04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html#3","text":"\u5b66\u4e60\u4e86Transformer\u6a21\u578b\u7684\u4f5c\u7528: \u57fa\u4e8eseq2seq\u67b6\u6784\u7684transformer\u6a21\u578b\u53ef\u4ee5\u5b8c\u6210NLP\u9886\u57df\u7814\u7a76\u7684\u5178\u578b\u4efb\u52a1, \u5982\u673a\u5668\u7ffb\u8bd1, \u6587\u672c\u751f\u6210\u7b49. \u540c\u65f6\u53c8\u53ef\u4ee5\u6784\u5efa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60. Transformer\u603b\u4f53\u67b6\u6784\u53ef\u5206\u4e3a\u56db\u4e2a\u90e8\u5206: \u8f93\u5165\u90e8\u5206 \u8f93\u51fa\u90e8\u5206 \u7f16\u7801\u5668\u90e8\u5206 \u89e3\u7801\u5668\u90e8\u5206 \u8f93\u5165\u90e8\u5206\u5305\u542b: \u6e90\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u76ee\u6807\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u8f93\u51fa\u90e8\u5206\u5305\u542b: \u7ebf\u6027\u5c42 softmax\u5904\u7406\u5668 \u7f16\u7801\u5668\u90e8\u5206: \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u7531\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u89e3\u7801\u5668\u90e8\u5206: \u7531N\u4e2a\u89e3\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u7531\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5","title":"3 \u5c0f\u7ed3"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6587\u672c\u5d4c\u5165\u5c42\u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u4f5c\u7528. \u638c\u63e1\u6587\u672c\u5d4c\u5165\u5c42\u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u5b9e\u73b0\u8fc7\u7a0b. 1 \u8f93\u5165\u90e8\u5206\u4ecb\u7ecd \u00b6 \u8f93\u5165\u90e8\u5206\u5305\u542b: \u6e90\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u76ee\u6807\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 2 \u6587\u672c\u5d4c\u5165\u5c42\u7684\u4f5c\u7528 \u00b6 \u65e0\u8bba\u662f\u6e90\u6587\u672c\u5d4c\u5165\u8fd8\u662f\u76ee\u6807\u6587\u672c\u5d4c\u5165\uff0c\u90fd\u662f\u4e3a\u4e86\u5c06\u6587\u672c\u4e2d\u8bcd\u6c47\u7684\u6570\u5b57\u8868\u793a\u8f6c\u53d8\u4e3a\u5411\u91cf\u8868\u793a, \u5e0c\u671b\u5728\u8fd9\u6837\u7684\u9ad8\u7ef4\u7a7a\u95f4\u6355\u6349\u8bcd\u6c47\u95f4\u7684\u5173\u7cfb. \u6587\u672c\u5d4c\u5165\u5c42\u7684\u4ee3\u7801\u5206\u6790: # \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 import torch # \u9884\u5b9a\u4e49\u7684\u7f51\u7edc\u5c42torch.nn, \u5de5\u5177\u5f00\u53d1\u8005\u5df2\u7ecf\u5e2e\u52a9\u6211\u4eec\u5f00\u53d1\u597d\u7684\u4e00\u4e9b\u5e38\u7528\u5c42, # \u6bd4\u5982\uff0c\u5377\u79ef\u5c42, lstm\u5c42, embedding\u5c42\u7b49, \u4e0d\u9700\u8981\u6211\u4eec\u518d\u91cd\u65b0\u9020\u8f6e\u5b50. import torch.nn as nn # \u6570\u5b66\u8ba1\u7b97\u5de5\u5177\u5305 import math # torch\u4e2d\u53d8\u91cf\u5c01\u88c5\u51fd\u6570Variable. from torch.autograd import Variable # Embeddings\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, d_model, vocab) # \u8bbe\u7f6e\u7c7b\u5c5e\u6027 \u5b9a\u4e49\u8bcd\u5d4c\u5165\u5c42 self.lut\u5c42 # 2 forward(x)\u51fd\u6570 # self.lut(x) * math.sqrt(self.d_model) class Embeddings ( nn . Module ): def __init__ ( self , d_model , vocab ): # \u53c2\u6570d_model \u6bcf\u4e2a\u8bcd\u6c47\u7684\u7279\u5f81\u5c3a\u5bf8 \u8bcd\u5d4c\u5165\u7ef4\u5ea6 # \u53c2\u6570vocab \u8bcd\u6c47\u8868\u5927\u5c0f super ( Embeddings , self ) . __init__ () self . d_model = d_model self . vocab = vocab # \u5b9a\u4e49\u8bcd\u5d4c\u5165\u5c42 self . lut = nn . Embedding ( self . vocab , self . d_model ) def forward ( self , x ): # \u5c06x\u4f20\u7ed9self.lut\u5e76\u4e0e\u6839\u53f7\u4e0bself.d_model\u76f8\u4e58\u4f5c\u4e3a\u7ed3\u679c\u8fd4\u56de # x\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u540e \u589e\u5927x\u7684\u503c, \u8bcd\u5d4c\u5165\u540e\u7684embedding_vector+\u4f4d\u7f6e\u7f16\u7801\u4fe1\u606f,\u503c\u91cf\u7eb2\u5dee\u5dee\u4e0d\u591a return self . lut ( x ) * math . sqrt ( self . d_model ) nn.Embedding\u6f14\u793a: >>> embedding = nn . Embedding ( 10 , 3 ) >>> input = torch . LongTensor ([[ 1 , 2 , 4 , 5 ],[ 4 , 3 , 2 , 9 ]]) >>> embedding ( input ) tensor ([[[ - 0.0251 , - 1.6902 , 0.7172 ], [ - 0.6431 , 0.0748 , 0.6969 ], [ 1.4970 , 1.3448 , - 0.9685 ], [ - 0.3677 , - 2.7265 , - 0.1685 ]], [[ 1.4970 , 1.3448 , - 0.9685 ], [ 0.4362 , - 0.4004 , 0.9400 ], [ - 0.6431 , 0.0748 , 0.6969 ], [ 0.9124 , - 2.3616 , 1.1151 ]]]) >>> embedding = nn . Embedding ( 10 , 3 , padding_idx = 0 ) >>> input = torch . LongTensor ([[ 0 , 2 , 0 , 5 ]]) >>> embedding ( input ) tensor ([[[ 0.0000 , 0.0000 , 0.0000 ], [ 0.1535 , - 2.0309 , 0.9315 ], [ 0.0000 , 0.0000 , 0.0000 ], [ - 0.1655 , 0.9897 , 0.0635 ]]]) \u8c03\u7528 def dm_test_Embeddings (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u5b9e\u4f8b\u5316\u8bcd\u5d4c\u5165\u5c42 my_embeddings = Embeddings ( d_model , vocab ) x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ],[ 491 , 998 , 1 , 221 ]])) embed = my_embeddings ( x ) print ( 'embed.shape' , embed . shape , ' \\n embed---> \\n ' , embed ) \u8f93\u51fa\u6548\u679c embed.shape torch.Size([2, 4, 512]) embed---> tensor([[[-19.0429, -44.2167, 2.6662, ..., -21.1199, -36.5275, -15.6872], [-25.4621, 25.6046, -45.5382, ..., 43.7159, 0.9437, -3.1733], [-15.7487, 8.1787, -20.6409, ..., -8.7201, -3.2585, -22.1298], [ 21.5044, 2.0660, -1.4059, ..., -6.3673, 3.4387, -22.4600]], [[ 15.7010, 2.6187, 14.1192, ..., -19.1751, 10.5954, 9.1155], [-21.5745, 9.6403, 17.9778, ..., 2.3668, 30.1526, -30.3724], [-17.6655, 33.6687, 19.3059, ..., -10.6276, -0.8653, 10.0715], [ 12.9400, -23.6355, -2.4750, ..., 19.1028, 6.6492, -45.1315]]], grad_fn=<MulBackward0>) 3 \u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u4f5c\u7528 \u00b6 \u56e0\u4e3a\u5728Transformer\u7684\u7f16\u7801\u5668\u7ed3\u6784\u4e2d, \u5e76\u6ca1\u6709\u9488\u5bf9\u8bcd\u6c47\u4f4d\u7f6e\u4fe1\u606f\u7684\u5904\u7406\uff0c\u56e0\u6b64\u9700\u8981\u5728Embedding\u5c42\u540e\u52a0\u5165\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u5c06\u8bcd\u6c47\u4f4d\u7f6e\u4e0d\u540c\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e0d\u540c\u8bed\u4e49\u7684\u4fe1\u606f\u52a0\u5165\u5230\u8bcd\u5d4c\u5165\u5f20\u91cf\u4e2d, \u4ee5\u5f25\u8865\u4f4d\u7f6e\u4fe1\u606f\u7684\u7f3a\u5931. 3.1 \u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u4f4d\u7f6e\u7f16\u7801\u5668\u7c7bPositionalEncoding \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, d_model, dropout, max_len=5000) # super()\u51fd\u6570 \u5b9a\u4e49\u5c42self.dropout # \u5b9a\u4e49\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635pe \u5b9a\u4e49\u4f4d\u7f6e\u5217-\u77e9\u9635position \u5b9a\u4e49\u53d8\u5316\u77e9\u9635div_term # \u5957\u516c\u5f0fdiv_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model)) # \u4f4d\u7f6e\u5217-\u77e9\u9635 * \u53d8\u5316\u77e9\u9635 \u963f\u8fbe\u7801\u79efmy_matmulres # \u7ed9pe\u77e9\u9635\u5076\u6570\u5217\u5947\u6570\u5217\u8d4b\u503c pe[:, 0::2] pe[:, 1::2] # pe\u77e9\u9635\u6ce8\u518c\u5230\u6a21\u578b\u7f13\u51b2\u533a pe.unsqueeze(0)\u4e09\u7ef4 self.register_buffer('pe', pe) # 2 forward(self, x) \u8fd4\u56deself.dropout(x) # \u7ed9x\u6570\u636e\u6dfb\u52a0\u4f4d\u7f6e\u7279\u5f81\u4fe1\u606f x = x + Variable( self.pe[:,:x.size()[1]], requires_grad=False) class PositionalEncoding ( nn . Module ): def __init__ ( self , d_model , dropout , max_len = 5000 ): # \u53c2\u6570d_model \u8bcd\u5d4c\u5165\u7ef4\u5ea6 eg: 512\u4e2a\u7279\u5f81 # \u53c2\u6570max_len \u5355\u8bcdtoken\u4e2a\u6570 eg: 60\u4e2a\u5355\u8bcd super ( PositionalEncoding , self ) . __init__ () # \u5b9a\u4e49dropout\u5c42 self . dropout = nn . Dropout ( p = dropout ) # \u601d\u8def\uff1a\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635 + \u7279\u5f81\u77e9\u9635 \u76f8\u5f53\u4e8e\u7ed9\u7279\u5f81\u589e\u52a0\u4e86\u4f4d\u7f6e\u4fe1\u606f # \u5b9a\u4e49\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635PE eg pe[60, 512], \u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u548c\u7279\u5f81\u77e9\u9635\u5f62\u72b6\u662f\u4e00\u6837\u7684 pe = torch . zeros ( max_len , d_model ) # \u5b9a\u4e49\u4f4d\u7f6e\u5217-\u77e9\u9635position \u6570\u636e\u5f62\u72b6[max_len,1] eg: [0,1,2,3,4...60]^T position = torch . arange ( 0 , max_len ) . unsqueeze ( 1 ) # print('position--->', position.shape, position) # \u5b9a\u4e49\u53d8\u5316\u77e9\u9635div_term [1,256] # torch.arange(start=1, end=512, 2)\u7ed3\u679c\u5e76\u4e0d\u5305\u542bend\u3002\u5728start\u548cend\u4e4b\u95f4\u505a\u4e00\u4e2a\u7b49\u5dee\u6570\u7ec4 [0, 2, 4, 6 ... 510] div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * - ( math . log ( 10000.0 ) / d_model )) # \u4f4d\u7f6e\u5217-\u77e9\u9635 @ \u53d8\u5316\u77e9\u9635 \u505a\u77e9\u9635\u8fd0\u7b97 [60*1]@ [1*256] ==> 60 *256 # \u77e9\u9635\u76f8\u4e58\u4e5f\u5c31\u662f\u884c\u5217\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u4e58\u518d\u76f8\u52a0\uff0c\u5176\u542b\u4e49\uff0c\u7ed9\u6bcf\u4e00\u4e2a\u5217\u5c5e\u6027\uff08\u5217\u7279\u5f81\uff09\u589e\u52a0\u4f4d\u7f6e\u7f16\u7801\u4fe1\u606f my_matmulres = position * div_term # print('my_matmulres--->', my_matmulres.shape, my_matmulres) # \u7ed9\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u5947\u6570\u5217\uff0c\u8d4b\u503csin\u66f2\u7ebf\u7279\u5f81 pe [:, 0 :: 2 ] = torch . sin ( my_matmulres ) # \u7ed9\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u5076\u6570\u5217\uff0c\u8d4b\u503ccos\u66f2\u7ebf\u7279\u5f81 pe [:, 1 :: 2 ] = torch . cos ( my_matmulres ) # \u5f62\u72b6\u53d8\u5316 [60,512]-->[1,60,512] pe = pe . unsqueeze ( 0 ) # \u628ape\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635 \u6ce8\u518c\u6210\u6a21\u578b\u7684\u6301\u4e45\u7f13\u51b2\u533abuffer; \u6a21\u578b\u4fdd\u5b58\u518d\u52a0\u8f7d\u65f6\uff0c\u53ef\u4ee5\u6839\u6a21\u578b\u53c2\u6570\u4e00\u6837\uff0c\u4e00\u540c\u88ab\u52a0\u8f7d # \u4ec0\u4e48\u662fbuffer: \u5bf9\u6a21\u578b\u6548\u679c\u6709\u5e2e\u52a9\u7684\uff0c\u4f46\u662f\u5374\u4e0d\u662f\u6a21\u578b\u7ed3\u6784\u4e2d\u8d85\u53c2\u6570\u6216\u8005\u53c2\u6570\uff0c\u4e0d\u53c2\u4e0e\u6a21\u578b\u8bad\u7ec3 self . register_buffer ( 'pe' , pe ) def forward ( self , x ): # \u6ce8\u610f\uff1a\u8f93\u5165\u7684x\u5f62\u72b62*4*512 pe\u662f1*60*512 \u5f62\u72b6 \u5982\u4f55\u8fdb\u884c\u76f8\u52a0 # \u53ea\u9700\u6309\u7167x\u7684\u5355\u8bcd\u4e2a\u6570 \u7ed9\u7279\u5f81\u589e\u52a0\u4f4d\u7f6e\u4fe1\u606f x = x + Variable ( self . pe [:,: x . size ()[ 1 ]], requires_grad = False ) return self . dropout ( x ) nn.Dropout\u6f14\u793a >>> m = nn . Dropout ( p = 0.2 ) >>> input = torch . randn ( 4 , 5 ) >>> output = m ( input ) >>> output Variable containing : 0.0000 - 0.5856 - 1.4094 0.0000 - 1.0290 2.0591 - 1.3400 - 1.7247 - 0.9885 0.1286 0.5099 1.3715 0.0000 2.2079 - 0.5497 - 0.0000 - 0.7839 - 1.2434 - 0.1222 1.2815 [ torch . FloatTensor of size 4 x5 ] torch.unsqueeze\u6f14\u793a >>> x = torch . tensor ([ 1 , 2 , 3 , 4 ]) >>> torch . unsqueeze ( x , 0 ) tensor ([[ 1 , 2 , 3 , 4 ]]) >>> torch . unsqueeze ( x , 1 ) tensor ([[ 1 ], [ 2 ], [ 3 ], [ 4 ]]) \u8c03\u7528 def dm_test_PositionalEncoding (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # 1 \u5b9e\u4f8b\u5316\u8bcd\u5d4c\u5165\u5c42 my_embeddings = Embeddings ( d_model , vocab ) # 2 \u8ba9\u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 [2,4] --->[2,4,512] x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) embed = my_embeddings ( x ) # print('embed--->', embed.shape) # 3 \u521b\u5efape\u4f4d\u7f6e\u77e9\u9635 \u751f\u6210\u4f4d\u7f6e\u7279\u5f81\u6570\u636e[1,60,512] my_pe = PositionalEncoding ( d_model = d_model , dropout = 0.1 , max_len = 60 ) # 4 \u7ed9\u8bcd\u5d4c\u5165\u6570\u636eembed \u6dfb\u52a0\u4f4d\u7f6e\u7279\u5f81 [2,4,512] ---> [2,4,512] pe_result = my_pe ( embed ) print ( 'pe_result.shape--->' , pe_result . shape ) print ( 'pe_result--->' , pe_result ) \u8f93\u51fa\u6548\u679c pe_result . shape ---> torch . Size ([ 2 , 4 , 512 ]) pe_result ---> tensor ([[[ - 6.3490 , - 19.3785 , - 2.8700 , ... , - 23.4560 , - 31.7405 , 9.0657 ], [ - 27.7453 , 19.5398 , 62.4924 , ... , - 7.7443 , 12.3955 , - 29.1615 ], [ 80.8307 , 4.9565 , - 0.7523 , ... , 8.2715 , 26.7639 , - 6.9124 ], [ 13.3252 , - 21.8653 , 0.0000 , ... , - 8.4563 , 17.7678 , 9.6917 ]], [[ 5.2631 , 22.0867 , 15.3600 , ... , 80.5963 , 2.4491 , - 36.0901 ], [ - 19.0809 , 67.3568 , 10.3016 , ... , - 5.6103 , - 14.2998 , - 51.2010 ], [ - 31.1153 , 44.8199 , - 6.9740 , ... , 39.6247 , 33.6903 , 18.5471 ], [ 13.7074 , 26.4221 , - 27.3353 , ... , 24.1987 , 29.1897 , - 20.5858 ]]], grad_fn =< MulBackward0 > ) 3.2 \u7ed8\u5236\u8bcd\u6c47\u5411\u91cf\u4e2d\u7279\u5f81\u7684\u5206\u5e03\u66f2\u7ebf \u00b6 import matplotlib.pyplot as plt import numpy as np # \u7ed8\u5236PE\u4f4d\u7f6e\u7279\u5f81sin-cos\u66f2\u7ebf def dm_draw_PE_feature (): # 1 \u521b\u5efape\u4f4d\u7f6e\u77e9\u9635[1,5000,20]\uff0c\u6bcf\u4e00\u5217\u6570\u503c\u4fe1\u606f\uff1a\u5947\u6570\u5217sin\u66f2\u7ebf \u5076\u6570\u5217cos\u66f2\u7ebf my_pe = PositionalEncoding ( d_model = 20 , dropout = 0 ) print ( 'my_positionalencoding.shape--->' , my_pe . pe . shape ) # 2 \u521b\u5efa\u6570\u636ex[1,100,20], \u7ed9\u6570\u636ex\u6dfb\u52a0\u4f4d\u7f6e\u7279\u5f81 [1,100,20] ---> [1,100,20] y = my_pe ( Variable ( torch . zeros ( 1 , 100 , 20 ))) print ( 'y--->' , y . shape ) # 3 \u753b\u56fe \u7ed8\u5236pe\u4f4d\u7f6e\u77e9\u9635\u7684\u7b2c4-7\u5217\u7279\u5f81\u66f2\u7ebf plt . figure ( figsize = ( 20 , 20 )) # \u7b2c0\u4e2a\u53e5\u5b50\u7684\uff0c\u6240\u6709\u5355\u8bcd\u7684\uff0c\u7ed8\u52364\u52308\u7ef4\u5ea6\u7684\u7279\u5f81 \u770b\u770bsin-cos\u66f2\u7ebf\u53d8\u5316 plt . plot ( np . arange ( 100 ), y [ 0 , :, 4 : 8 ] . numpy ()) plt . legend ([ \"dim %d \" % p for p in [ 4 , 5 , 6 , 7 ]]) plt . show () # print('\u76f4\u63a5\u67e5\u770bpe\u6570\u636e\u5f62\u72b6--->', my_pe.pe.shape) # [1,5000,20] # \u76f4\u63a5\u7ed8\u5236pe\u6570\u636e\u4e5f\u662fok # plt.figure(figsize=(20, 20)) # # \u7b2c0\u4e2a\u53e5\u5b50\u7684\uff0c\u6240\u6709\u5355\u8bcd\u7684\uff0c\u7ed8\u52364\u52308\u7ef4\u5ea6\u7684\u7279\u5f81 \u770b\u770bsin-cos\u66f2\u7ebf\u53d8\u5316 # plt.plot(np.arange(100), my_pe.pe[0,0:100, 4:8]) # plt.legend([\"dim %d\" %p for p in [4,5,6,7]]) # plt.show() \u8f93\u51fa\u6548\u679c: \u6548\u679c\u5206\u6790 \u6bcf\u6761\u989c\u8272\u7684\u66f2\u7ebf\u4ee3\u8868\u67d0\u4e00\u4e2a\u8bcd\u6c47\u4e2d\u7684\u7279\u5f81\u5728\u4e0d\u540c\u4f4d\u7f6e\u7684\u542b\u4e49 \u4fdd\u8bc1\u540c\u4e00\u8bcd\u6c47\u968f\u7740\u6240\u5728\u4f4d\u7f6e\u4e0d\u540c\u5b83\u5bf9\u5e94\u4f4d\u7f6e\u5d4c\u5165\u5411\u91cf\u4f1a\u53d1\u751f\u53d8\u5316 \u6b63\u5f26\u6ce2\u548c\u4f59\u5f26\u6ce2\u7684\u503c\u57df\u8303\u56f4\u90fd\u662f1\u5230-1\u8fd9\u53c8\u5f88\u597d\u7684\u63a7\u5236\u4e86\u5d4c\u5165\u6570\u503c\u7684\u5927\u5c0f, \u6709\u52a9\u4e8e\u68af\u5ea6\u7684\u5feb\u901f\u8ba1\u7b97 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u6587\u672c\u5d4c\u5165\u5c42\u7684\u4f5c\u7528: \u65e0\u8bba\u662f\u6e90\u6587\u672c\u5d4c\u5165\u8fd8\u662f\u76ee\u6807\u6587\u672c\u5d4c\u5165\uff0c\u90fd\u662f\u4e3a\u4e86\u5c06\u6587\u672c\u4e2d\u8bcd\u6c47\u7684\u6570\u5b57\u8868\u793a\u8f6c\u53d8\u4e3a\u5411\u91cf\u8868\u793a, \u5e0c\u671b\u5728\u8fd9\u6837\u7684\u9ad8\u7ef4\u7a7a\u95f4\u6355\u6349\u8bcd\u6c47\u95f4\u7684\u5173\u7cfb. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6587\u672c\u5d4c\u5165\u5c42\u7684\u7c7b: Embeddings \u521d\u59cb\u5316\u51fd\u6570\u4ee5d_model, \u8bcd\u5d4c\u5165\u7ef4\u5ea6, \u548cvocab, \u8bcd\u6c47\u603b\u6570\u4e3a\u53c2\u6570, \u5185\u90e8\u4e3b\u8981\u4f7f\u7528\u4e86nn\u4e2d\u7684\u9884\u5b9a\u5c42Embedding\u8fdb\u884c\u8bcd\u5d4c\u5165. \u5728forward\u51fd\u6570\u4e2d, \u5c06\u8f93\u5165x\u4f20\u5165\u5230Embedding\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e2d, \u7136\u540e\u4e58\u4ee5\u4e00\u4e2a\u6839\u53f7\u4e0bd_model\u8fdb\u884c\u7f29\u653e, \u63a7\u5236\u6570\u503c\u5927\u5c0f. \u5b83\u7684\u8f93\u51fa\u662f\u6587\u672c\u5d4c\u5165\u540e\u7684\u7ed3\u679c. \u5b66\u4e60\u4e86\u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u4f5c\u7528: \u56e0\u4e3a\u5728Transformer\u7684\u7f16\u7801\u5668\u7ed3\u6784\u4e2d, \u5e76\u6ca1\u6709\u9488\u5bf9\u8bcd\u6c47\u4f4d\u7f6e\u4fe1\u606f\u7684\u5904\u7406\uff0c\u56e0\u6b64\u9700\u8981\u5728Embedding\u5c42\u540e\u52a0\u5165\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u5c06\u8bcd\u6c47\u4f4d\u7f6e\u4e0d\u540c\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e0d\u540c\u8bed\u4e49\u7684\u4fe1\u606f\u52a0\u5165\u5230\u8bcd\u5d4c\u5165\u5f20\u91cf\u4e2d, \u4ee5\u5f25\u8865\u4f4d\u7f6e\u4fe1\u606f\u7684\u7f3a\u5931. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u7c7b: PositionalEncoding \u521d\u59cb\u5316\u51fd\u6570\u4ee5d_model, dropout, max_len\u4e3a\u53c2\u6570, \u5206\u522b\u4ee3\u8868d_model: \u8bcd\u5d4c\u5165\u7ef4\u5ea6, dropout: \u7f6e0\u6bd4\u7387, max_len: \u6bcf\u4e2a\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6. forward\u51fd\u6570\u4e2d\u7684\u8f93\u5165\u53c2\u6570\u4e3ax, \u662fEmbedding\u5c42\u7684\u8f93\u51fa. \u6700\u7ec8\u8f93\u51fa\u4e00\u4e2a\u52a0\u5165\u4e86\u4f4d\u7f6e\u7f16\u7801\u4fe1\u606f\u7684\u8bcd\u5d4c\u5165\u5f20\u91cf. \u5b9e\u73b0\u4e86\u7ed8\u5236\u8bcd\u6c47\u5411\u91cf\u4e2d\u7279\u5f81\u7684\u5206\u5e03\u66f2\u7ebf: \u4fdd\u8bc1\u540c\u4e00\u8bcd\u6c47\u968f\u7740\u6240\u5728\u4f4d\u7f6e\u4e0d\u540c\u5b83\u5bf9\u5e94\u4f4d\u7f6e\u5d4c\u5165\u5411\u91cf\u4f1a\u53d1\u751f\u53d8\u5316. \u6b63\u5f26\u6ce2\u548c\u4f59\u5f26\u6ce2\u7684\u503c\u57df\u8303\u56f4\u90fd\u662f1\u5230-1, \u8fd9\u53c8\u5f88\u597d\u7684\u63a7\u5236\u4e86\u5d4c\u5165\u6570\u503c\u7684\u5927\u5c0f, \u6709\u52a9\u4e8e\u68af\u5ea6\u7684\u5feb\u901f\u8ba1\u7b97.","title":"3 \u8f93\u5165\u90e8\u5206\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#_1","text":"\u4e86\u89e3\u6587\u672c\u5d4c\u5165\u5c42\u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u4f5c\u7528. \u638c\u63e1\u6587\u672c\u5d4c\u5165\u5c42\u548c\u4f4d\u7f6e\u7f16\u7801\u7684\u5b9e\u73b0\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#1","text":"\u8f93\u5165\u90e8\u5206\u5305\u542b: \u6e90\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668 \u76ee\u6807\u6587\u672c\u5d4c\u5165\u5c42\u53ca\u5176\u4f4d\u7f6e\u7f16\u7801\u5668","title":"1 \u8f93\u5165\u90e8\u5206\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#2","text":"\u65e0\u8bba\u662f\u6e90\u6587\u672c\u5d4c\u5165\u8fd8\u662f\u76ee\u6807\u6587\u672c\u5d4c\u5165\uff0c\u90fd\u662f\u4e3a\u4e86\u5c06\u6587\u672c\u4e2d\u8bcd\u6c47\u7684\u6570\u5b57\u8868\u793a\u8f6c\u53d8\u4e3a\u5411\u91cf\u8868\u793a, \u5e0c\u671b\u5728\u8fd9\u6837\u7684\u9ad8\u7ef4\u7a7a\u95f4\u6355\u6349\u8bcd\u6c47\u95f4\u7684\u5173\u7cfb. \u6587\u672c\u5d4c\u5165\u5c42\u7684\u4ee3\u7801\u5206\u6790: # \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 import torch # \u9884\u5b9a\u4e49\u7684\u7f51\u7edc\u5c42torch.nn, \u5de5\u5177\u5f00\u53d1\u8005\u5df2\u7ecf\u5e2e\u52a9\u6211\u4eec\u5f00\u53d1\u597d\u7684\u4e00\u4e9b\u5e38\u7528\u5c42, # \u6bd4\u5982\uff0c\u5377\u79ef\u5c42, lstm\u5c42, embedding\u5c42\u7b49, \u4e0d\u9700\u8981\u6211\u4eec\u518d\u91cd\u65b0\u9020\u8f6e\u5b50. import torch.nn as nn # \u6570\u5b66\u8ba1\u7b97\u5de5\u5177\u5305 import math # torch\u4e2d\u53d8\u91cf\u5c01\u88c5\u51fd\u6570Variable. from torch.autograd import Variable # Embeddings\u7c7b \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, d_model, vocab) # \u8bbe\u7f6e\u7c7b\u5c5e\u6027 \u5b9a\u4e49\u8bcd\u5d4c\u5165\u5c42 self.lut\u5c42 # 2 forward(x)\u51fd\u6570 # self.lut(x) * math.sqrt(self.d_model) class Embeddings ( nn . Module ): def __init__ ( self , d_model , vocab ): # \u53c2\u6570d_model \u6bcf\u4e2a\u8bcd\u6c47\u7684\u7279\u5f81\u5c3a\u5bf8 \u8bcd\u5d4c\u5165\u7ef4\u5ea6 # \u53c2\u6570vocab \u8bcd\u6c47\u8868\u5927\u5c0f super ( Embeddings , self ) . __init__ () self . d_model = d_model self . vocab = vocab # \u5b9a\u4e49\u8bcd\u5d4c\u5165\u5c42 self . lut = nn . Embedding ( self . vocab , self . d_model ) def forward ( self , x ): # \u5c06x\u4f20\u7ed9self.lut\u5e76\u4e0e\u6839\u53f7\u4e0bself.d_model\u76f8\u4e58\u4f5c\u4e3a\u7ed3\u679c\u8fd4\u56de # x\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u540e \u589e\u5927x\u7684\u503c, \u8bcd\u5d4c\u5165\u540e\u7684embedding_vector+\u4f4d\u7f6e\u7f16\u7801\u4fe1\u606f,\u503c\u91cf\u7eb2\u5dee\u5dee\u4e0d\u591a return self . lut ( x ) * math . sqrt ( self . d_model ) nn.Embedding\u6f14\u793a: >>> embedding = nn . Embedding ( 10 , 3 ) >>> input = torch . LongTensor ([[ 1 , 2 , 4 , 5 ],[ 4 , 3 , 2 , 9 ]]) >>> embedding ( input ) tensor ([[[ - 0.0251 , - 1.6902 , 0.7172 ], [ - 0.6431 , 0.0748 , 0.6969 ], [ 1.4970 , 1.3448 , - 0.9685 ], [ - 0.3677 , - 2.7265 , - 0.1685 ]], [[ 1.4970 , 1.3448 , - 0.9685 ], [ 0.4362 , - 0.4004 , 0.9400 ], [ - 0.6431 , 0.0748 , 0.6969 ], [ 0.9124 , - 2.3616 , 1.1151 ]]]) >>> embedding = nn . Embedding ( 10 , 3 , padding_idx = 0 ) >>> input = torch . LongTensor ([[ 0 , 2 , 0 , 5 ]]) >>> embedding ( input ) tensor ([[[ 0.0000 , 0.0000 , 0.0000 ], [ 0.1535 , - 2.0309 , 0.9315 ], [ 0.0000 , 0.0000 , 0.0000 ], [ - 0.1655 , 0.9897 , 0.0635 ]]]) \u8c03\u7528 def dm_test_Embeddings (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u5b9e\u4f8b\u5316\u8bcd\u5d4c\u5165\u5c42 my_embeddings = Embeddings ( d_model , vocab ) x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ],[ 491 , 998 , 1 , 221 ]])) embed = my_embeddings ( x ) print ( 'embed.shape' , embed . shape , ' \\n embed---> \\n ' , embed ) \u8f93\u51fa\u6548\u679c embed.shape torch.Size([2, 4, 512]) embed---> tensor([[[-19.0429, -44.2167, 2.6662, ..., -21.1199, -36.5275, -15.6872], [-25.4621, 25.6046, -45.5382, ..., 43.7159, 0.9437, -3.1733], [-15.7487, 8.1787, -20.6409, ..., -8.7201, -3.2585, -22.1298], [ 21.5044, 2.0660, -1.4059, ..., -6.3673, 3.4387, -22.4600]], [[ 15.7010, 2.6187, 14.1192, ..., -19.1751, 10.5954, 9.1155], [-21.5745, 9.6403, 17.9778, ..., 2.3668, 30.1526, -30.3724], [-17.6655, 33.6687, 19.3059, ..., -10.6276, -0.8653, 10.0715], [ 12.9400, -23.6355, -2.4750, ..., 19.1028, 6.6492, -45.1315]]], grad_fn=<MulBackward0>)","title":"2 \u6587\u672c\u5d4c\u5165\u5c42\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#3","text":"\u56e0\u4e3a\u5728Transformer\u7684\u7f16\u7801\u5668\u7ed3\u6784\u4e2d, \u5e76\u6ca1\u6709\u9488\u5bf9\u8bcd\u6c47\u4f4d\u7f6e\u4fe1\u606f\u7684\u5904\u7406\uff0c\u56e0\u6b64\u9700\u8981\u5728Embedding\u5c42\u540e\u52a0\u5165\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u5c06\u8bcd\u6c47\u4f4d\u7f6e\u4e0d\u540c\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e0d\u540c\u8bed\u4e49\u7684\u4fe1\u606f\u52a0\u5165\u5230\u8bcd\u5d4c\u5165\u5f20\u91cf\u4e2d, \u4ee5\u5f25\u8865\u4f4d\u7f6e\u4fe1\u606f\u7684\u7f3a\u5931.","title":"3 \u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#31","text":"# \u4f4d\u7f6e\u7f16\u7801\u5668\u7c7bPositionalEncoding \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, d_model, dropout, max_len=5000) # super()\u51fd\u6570 \u5b9a\u4e49\u5c42self.dropout # \u5b9a\u4e49\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635pe \u5b9a\u4e49\u4f4d\u7f6e\u5217-\u77e9\u9635position \u5b9a\u4e49\u53d8\u5316\u77e9\u9635div_term # \u5957\u516c\u5f0fdiv_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0)/d_model)) # \u4f4d\u7f6e\u5217-\u77e9\u9635 * \u53d8\u5316\u77e9\u9635 \u963f\u8fbe\u7801\u79efmy_matmulres # \u7ed9pe\u77e9\u9635\u5076\u6570\u5217\u5947\u6570\u5217\u8d4b\u503c pe[:, 0::2] pe[:, 1::2] # pe\u77e9\u9635\u6ce8\u518c\u5230\u6a21\u578b\u7f13\u51b2\u533a pe.unsqueeze(0)\u4e09\u7ef4 self.register_buffer('pe', pe) # 2 forward(self, x) \u8fd4\u56deself.dropout(x) # \u7ed9x\u6570\u636e\u6dfb\u52a0\u4f4d\u7f6e\u7279\u5f81\u4fe1\u606f x = x + Variable( self.pe[:,:x.size()[1]], requires_grad=False) class PositionalEncoding ( nn . Module ): def __init__ ( self , d_model , dropout , max_len = 5000 ): # \u53c2\u6570d_model \u8bcd\u5d4c\u5165\u7ef4\u5ea6 eg: 512\u4e2a\u7279\u5f81 # \u53c2\u6570max_len \u5355\u8bcdtoken\u4e2a\u6570 eg: 60\u4e2a\u5355\u8bcd super ( PositionalEncoding , self ) . __init__ () # \u5b9a\u4e49dropout\u5c42 self . dropout = nn . Dropout ( p = dropout ) # \u601d\u8def\uff1a\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635 + \u7279\u5f81\u77e9\u9635 \u76f8\u5f53\u4e8e\u7ed9\u7279\u5f81\u589e\u52a0\u4e86\u4f4d\u7f6e\u4fe1\u606f # \u5b9a\u4e49\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635PE eg pe[60, 512], \u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u548c\u7279\u5f81\u77e9\u9635\u5f62\u72b6\u662f\u4e00\u6837\u7684 pe = torch . zeros ( max_len , d_model ) # \u5b9a\u4e49\u4f4d\u7f6e\u5217-\u77e9\u9635position \u6570\u636e\u5f62\u72b6[max_len,1] eg: [0,1,2,3,4...60]^T position = torch . arange ( 0 , max_len ) . unsqueeze ( 1 ) # print('position--->', position.shape, position) # \u5b9a\u4e49\u53d8\u5316\u77e9\u9635div_term [1,256] # torch.arange(start=1, end=512, 2)\u7ed3\u679c\u5e76\u4e0d\u5305\u542bend\u3002\u5728start\u548cend\u4e4b\u95f4\u505a\u4e00\u4e2a\u7b49\u5dee\u6570\u7ec4 [0, 2, 4, 6 ... 510] div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * - ( math . log ( 10000.0 ) / d_model )) # \u4f4d\u7f6e\u5217-\u77e9\u9635 @ \u53d8\u5316\u77e9\u9635 \u505a\u77e9\u9635\u8fd0\u7b97 [60*1]@ [1*256] ==> 60 *256 # \u77e9\u9635\u76f8\u4e58\u4e5f\u5c31\u662f\u884c\u5217\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u4e58\u518d\u76f8\u52a0\uff0c\u5176\u542b\u4e49\uff0c\u7ed9\u6bcf\u4e00\u4e2a\u5217\u5c5e\u6027\uff08\u5217\u7279\u5f81\uff09\u589e\u52a0\u4f4d\u7f6e\u7f16\u7801\u4fe1\u606f my_matmulres = position * div_term # print('my_matmulres--->', my_matmulres.shape, my_matmulres) # \u7ed9\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u5947\u6570\u5217\uff0c\u8d4b\u503csin\u66f2\u7ebf\u7279\u5f81 pe [:, 0 :: 2 ] = torch . sin ( my_matmulres ) # \u7ed9\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u5076\u6570\u5217\uff0c\u8d4b\u503ccos\u66f2\u7ebf\u7279\u5f81 pe [:, 1 :: 2 ] = torch . cos ( my_matmulres ) # \u5f62\u72b6\u53d8\u5316 [60,512]-->[1,60,512] pe = pe . unsqueeze ( 0 ) # \u628ape\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635 \u6ce8\u518c\u6210\u6a21\u578b\u7684\u6301\u4e45\u7f13\u51b2\u533abuffer; \u6a21\u578b\u4fdd\u5b58\u518d\u52a0\u8f7d\u65f6\uff0c\u53ef\u4ee5\u6839\u6a21\u578b\u53c2\u6570\u4e00\u6837\uff0c\u4e00\u540c\u88ab\u52a0\u8f7d # \u4ec0\u4e48\u662fbuffer: \u5bf9\u6a21\u578b\u6548\u679c\u6709\u5e2e\u52a9\u7684\uff0c\u4f46\u662f\u5374\u4e0d\u662f\u6a21\u578b\u7ed3\u6784\u4e2d\u8d85\u53c2\u6570\u6216\u8005\u53c2\u6570\uff0c\u4e0d\u53c2\u4e0e\u6a21\u578b\u8bad\u7ec3 self . register_buffer ( 'pe' , pe ) def forward ( self , x ): # \u6ce8\u610f\uff1a\u8f93\u5165\u7684x\u5f62\u72b62*4*512 pe\u662f1*60*512 \u5f62\u72b6 \u5982\u4f55\u8fdb\u884c\u76f8\u52a0 # \u53ea\u9700\u6309\u7167x\u7684\u5355\u8bcd\u4e2a\u6570 \u7ed9\u7279\u5f81\u589e\u52a0\u4f4d\u7f6e\u4fe1\u606f x = x + Variable ( self . pe [:,: x . size ()[ 1 ]], requires_grad = False ) return self . dropout ( x ) nn.Dropout\u6f14\u793a >>> m = nn . Dropout ( p = 0.2 ) >>> input = torch . randn ( 4 , 5 ) >>> output = m ( input ) >>> output Variable containing : 0.0000 - 0.5856 - 1.4094 0.0000 - 1.0290 2.0591 - 1.3400 - 1.7247 - 0.9885 0.1286 0.5099 1.3715 0.0000 2.2079 - 0.5497 - 0.0000 - 0.7839 - 1.2434 - 0.1222 1.2815 [ torch . FloatTensor of size 4 x5 ] torch.unsqueeze\u6f14\u793a >>> x = torch . tensor ([ 1 , 2 , 3 , 4 ]) >>> torch . unsqueeze ( x , 0 ) tensor ([[ 1 , 2 , 3 , 4 ]]) >>> torch . unsqueeze ( x , 1 ) tensor ([[ 1 ], [ 2 ], [ 3 ], [ 4 ]]) \u8c03\u7528 def dm_test_PositionalEncoding (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # 1 \u5b9e\u4f8b\u5316\u8bcd\u5d4c\u5165\u5c42 my_embeddings = Embeddings ( d_model , vocab ) # 2 \u8ba9\u6570\u636e\u7ecf\u8fc7\u8bcd\u5d4c\u5165\u5c42 [2,4] --->[2,4,512] x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) embed = my_embeddings ( x ) # print('embed--->', embed.shape) # 3 \u521b\u5efape\u4f4d\u7f6e\u77e9\u9635 \u751f\u6210\u4f4d\u7f6e\u7279\u5f81\u6570\u636e[1,60,512] my_pe = PositionalEncoding ( d_model = d_model , dropout = 0.1 , max_len = 60 ) # 4 \u7ed9\u8bcd\u5d4c\u5165\u6570\u636eembed \u6dfb\u52a0\u4f4d\u7f6e\u7279\u5f81 [2,4,512] ---> [2,4,512] pe_result = my_pe ( embed ) print ( 'pe_result.shape--->' , pe_result . shape ) print ( 'pe_result--->' , pe_result ) \u8f93\u51fa\u6548\u679c pe_result . shape ---> torch . Size ([ 2 , 4 , 512 ]) pe_result ---> tensor ([[[ - 6.3490 , - 19.3785 , - 2.8700 , ... , - 23.4560 , - 31.7405 , 9.0657 ], [ - 27.7453 , 19.5398 , 62.4924 , ... , - 7.7443 , 12.3955 , - 29.1615 ], [ 80.8307 , 4.9565 , - 0.7523 , ... , 8.2715 , 26.7639 , - 6.9124 ], [ 13.3252 , - 21.8653 , 0.0000 , ... , - 8.4563 , 17.7678 , 9.6917 ]], [[ 5.2631 , 22.0867 , 15.3600 , ... , 80.5963 , 2.4491 , - 36.0901 ], [ - 19.0809 , 67.3568 , 10.3016 , ... , - 5.6103 , - 14.2998 , - 51.2010 ], [ - 31.1153 , 44.8199 , - 6.9740 , ... , 39.6247 , 33.6903 , 18.5471 ], [ 13.7074 , 26.4221 , - 27.3353 , ... , 24.1987 , 29.1897 , - 20.5858 ]]], grad_fn =< MulBackward0 > )","title":"3.1 \u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#32","text":"import matplotlib.pyplot as plt import numpy as np # \u7ed8\u5236PE\u4f4d\u7f6e\u7279\u5f81sin-cos\u66f2\u7ebf def dm_draw_PE_feature (): # 1 \u521b\u5efape\u4f4d\u7f6e\u77e9\u9635[1,5000,20]\uff0c\u6bcf\u4e00\u5217\u6570\u503c\u4fe1\u606f\uff1a\u5947\u6570\u5217sin\u66f2\u7ebf \u5076\u6570\u5217cos\u66f2\u7ebf my_pe = PositionalEncoding ( d_model = 20 , dropout = 0 ) print ( 'my_positionalencoding.shape--->' , my_pe . pe . shape ) # 2 \u521b\u5efa\u6570\u636ex[1,100,20], \u7ed9\u6570\u636ex\u6dfb\u52a0\u4f4d\u7f6e\u7279\u5f81 [1,100,20] ---> [1,100,20] y = my_pe ( Variable ( torch . zeros ( 1 , 100 , 20 ))) print ( 'y--->' , y . shape ) # 3 \u753b\u56fe \u7ed8\u5236pe\u4f4d\u7f6e\u77e9\u9635\u7684\u7b2c4-7\u5217\u7279\u5f81\u66f2\u7ebf plt . figure ( figsize = ( 20 , 20 )) # \u7b2c0\u4e2a\u53e5\u5b50\u7684\uff0c\u6240\u6709\u5355\u8bcd\u7684\uff0c\u7ed8\u52364\u52308\u7ef4\u5ea6\u7684\u7279\u5f81 \u770b\u770bsin-cos\u66f2\u7ebf\u53d8\u5316 plt . plot ( np . arange ( 100 ), y [ 0 , :, 4 : 8 ] . numpy ()) plt . legend ([ \"dim %d \" % p for p in [ 4 , 5 , 6 , 7 ]]) plt . show () # print('\u76f4\u63a5\u67e5\u770bpe\u6570\u636e\u5f62\u72b6--->', my_pe.pe.shape) # [1,5000,20] # \u76f4\u63a5\u7ed8\u5236pe\u6570\u636e\u4e5f\u662fok # plt.figure(figsize=(20, 20)) # # \u7b2c0\u4e2a\u53e5\u5b50\u7684\uff0c\u6240\u6709\u5355\u8bcd\u7684\uff0c\u7ed8\u52364\u52308\u7ef4\u5ea6\u7684\u7279\u5f81 \u770b\u770bsin-cos\u66f2\u7ebf\u53d8\u5316 # plt.plot(np.arange(100), my_pe.pe[0,0:100, 4:8]) # plt.legend([\"dim %d\" %p for p in [4,5,6,7]]) # plt.show() \u8f93\u51fa\u6548\u679c: \u6548\u679c\u5206\u6790 \u6bcf\u6761\u989c\u8272\u7684\u66f2\u7ebf\u4ee3\u8868\u67d0\u4e00\u4e2a\u8bcd\u6c47\u4e2d\u7684\u7279\u5f81\u5728\u4e0d\u540c\u4f4d\u7f6e\u7684\u542b\u4e49 \u4fdd\u8bc1\u540c\u4e00\u8bcd\u6c47\u968f\u7740\u6240\u5728\u4f4d\u7f6e\u4e0d\u540c\u5b83\u5bf9\u5e94\u4f4d\u7f6e\u5d4c\u5165\u5411\u91cf\u4f1a\u53d1\u751f\u53d8\u5316 \u6b63\u5f26\u6ce2\u548c\u4f59\u5f26\u6ce2\u7684\u503c\u57df\u8303\u56f4\u90fd\u662f1\u5230-1\u8fd9\u53c8\u5f88\u597d\u7684\u63a7\u5236\u4e86\u5d4c\u5165\u6570\u503c\u7684\u5927\u5c0f, \u6709\u52a9\u4e8e\u68af\u5ea6\u7684\u5feb\u901f\u8ba1\u7b97","title":"3.2 \u7ed8\u5236\u8bcd\u6c47\u5411\u91cf\u4e2d\u7279\u5f81\u7684\u5206\u5e03\u66f2\u7ebf"},{"location":"04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#4","text":"\u5b66\u4e60\u4e86\u6587\u672c\u5d4c\u5165\u5c42\u7684\u4f5c\u7528: \u65e0\u8bba\u662f\u6e90\u6587\u672c\u5d4c\u5165\u8fd8\u662f\u76ee\u6807\u6587\u672c\u5d4c\u5165\uff0c\u90fd\u662f\u4e3a\u4e86\u5c06\u6587\u672c\u4e2d\u8bcd\u6c47\u7684\u6570\u5b57\u8868\u793a\u8f6c\u53d8\u4e3a\u5411\u91cf\u8868\u793a, \u5e0c\u671b\u5728\u8fd9\u6837\u7684\u9ad8\u7ef4\u7a7a\u95f4\u6355\u6349\u8bcd\u6c47\u95f4\u7684\u5173\u7cfb. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6587\u672c\u5d4c\u5165\u5c42\u7684\u7c7b: Embeddings \u521d\u59cb\u5316\u51fd\u6570\u4ee5d_model, \u8bcd\u5d4c\u5165\u7ef4\u5ea6, \u548cvocab, \u8bcd\u6c47\u603b\u6570\u4e3a\u53c2\u6570, \u5185\u90e8\u4e3b\u8981\u4f7f\u7528\u4e86nn\u4e2d\u7684\u9884\u5b9a\u5c42Embedding\u8fdb\u884c\u8bcd\u5d4c\u5165. \u5728forward\u51fd\u6570\u4e2d, \u5c06\u8f93\u5165x\u4f20\u5165\u5230Embedding\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u4e2d, \u7136\u540e\u4e58\u4ee5\u4e00\u4e2a\u6839\u53f7\u4e0bd_model\u8fdb\u884c\u7f29\u653e, \u63a7\u5236\u6570\u503c\u5927\u5c0f. \u5b83\u7684\u8f93\u51fa\u662f\u6587\u672c\u5d4c\u5165\u540e\u7684\u7ed3\u679c. \u5b66\u4e60\u4e86\u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u4f5c\u7528: \u56e0\u4e3a\u5728Transformer\u7684\u7f16\u7801\u5668\u7ed3\u6784\u4e2d, \u5e76\u6ca1\u6709\u9488\u5bf9\u8bcd\u6c47\u4f4d\u7f6e\u4fe1\u606f\u7684\u5904\u7406\uff0c\u56e0\u6b64\u9700\u8981\u5728Embedding\u5c42\u540e\u52a0\u5165\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u5c06\u8bcd\u6c47\u4f4d\u7f6e\u4e0d\u540c\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e0d\u540c\u8bed\u4e49\u7684\u4fe1\u606f\u52a0\u5165\u5230\u8bcd\u5d4c\u5165\u5f20\u91cf\u4e2d, \u4ee5\u5f25\u8865\u4f4d\u7f6e\u4fe1\u606f\u7684\u7f3a\u5931. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u4f4d\u7f6e\u7f16\u7801\u5668\u7684\u7c7b: PositionalEncoding \u521d\u59cb\u5316\u51fd\u6570\u4ee5d_model, dropout, max_len\u4e3a\u53c2\u6570, \u5206\u522b\u4ee3\u8868d_model: \u8bcd\u5d4c\u5165\u7ef4\u5ea6, dropout: \u7f6e0\u6bd4\u7387, max_len: \u6bcf\u4e2a\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6. forward\u51fd\u6570\u4e2d\u7684\u8f93\u5165\u53c2\u6570\u4e3ax, \u662fEmbedding\u5c42\u7684\u8f93\u51fa. \u6700\u7ec8\u8f93\u51fa\u4e00\u4e2a\u52a0\u5165\u4e86\u4f4d\u7f6e\u7f16\u7801\u4fe1\u606f\u7684\u8bcd\u5d4c\u5165\u5f20\u91cf. \u5b9e\u73b0\u4e86\u7ed8\u5236\u8bcd\u6c47\u5411\u91cf\u4e2d\u7279\u5f81\u7684\u5206\u5e03\u66f2\u7ebf: \u4fdd\u8bc1\u540c\u4e00\u8bcd\u6c47\u968f\u7740\u6240\u5728\u4f4d\u7f6e\u4e0d\u540c\u5b83\u5bf9\u5e94\u4f4d\u7f6e\u5d4c\u5165\u5411\u91cf\u4f1a\u53d1\u751f\u53d8\u5316. \u6b63\u5f26\u6ce2\u548c\u4f59\u5f26\u6ce2\u7684\u503c\u57df\u8303\u56f4\u90fd\u662f1\u5230-1, \u8fd9\u53c8\u5f88\u597d\u7684\u63a7\u5236\u4e86\u5d4c\u5165\u6570\u503c\u7684\u5927\u5c0f, \u6709\u52a9\u4e8e\u68af\u5ea6\u7684\u5feb\u901f\u8ba1\u7b97.","title":"4 \u5c0f\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u7f16\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u4f5c\u7528. \u638c\u63e1\u7f16\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u5b9e\u73b0\u8fc7\u7a0b. 1 \u7f16\u7801\u5668\u4ecb\u7ecd \u00b6 \u7f16\u7801\u5668\u90e8\u5206: * \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 * \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u7531\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 * \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 * \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 2 \u63a9\u7801\u5f20\u91cf \u00b6 2.1 \u63a9\u7801\u5f20\u91cf\u4ecb\u7ecd \u00b6 \u63a9\u4ee3\u8868\u906e\u63a9\uff0c\u7801\u5c31\u662f\u6211\u4eec\u5f20\u91cf\u4e2d\u7684\u6570\u503c\uff0c\u5b83\u7684\u5c3a\u5bf8\u4e0d\u5b9a\uff0c\u91cc\u9762\u4e00\u822c\u53ea\u67091\u548c0\u7684\u5143\u7d20\uff0c\u4ee3\u8868\u4f4d\u7f6e\u88ab\u906e\u63a9\u6216\u8005\u4e0d\u88ab\u906e\u63a9\uff0c\u81f3\u4e8e\u662f0\u4f4d\u7f6e\u88ab\u906e\u63a9\u8fd8\u662f1\u4f4d\u7f6e\u88ab\u906e\u63a9\u53ef\u4ee5\u81ea\u5b9a\u4e49\uff0c\u56e0\u6b64\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u8ba9\u53e6\u5916\u4e00\u4e2a\u5f20\u91cf\u4e2d\u7684\u4e00\u4e9b\u6570\u503c\u88ab\u906e\u63a9\uff0c\u4e5f\u53ef\u4ee5\u8bf4\u88ab\u66ff\u6362, \u5b83\u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e00\u4e2a\u5f20\u91cf. 2.2 \u63a9\u7801\u5f20\u91cf\u7684\u4f5c\u7528 \u00b6 \u5728transformer\u4e2d, \u63a9\u7801\u5f20\u91cf\u7684\u4e3b\u8981\u4f5c\u7528\u5728\u5e94\u7528attention(\u5c06\u5728\u4e0b\u4e00\u5c0f\u8282\u8bb2\u89e3)\u65f6\uff0c\u6709\u4e00\u4e9b\u751f\u6210\u7684attention\u5f20\u91cf\u4e2d\u7684\u503c\u8ba1\u7b97\u6709\u53ef\u80fd\u5df2\u77e5\u4e86\u672a\u6765\u4fe1\u606f\u800c\u5f97\u5230\u7684\uff0c\u672a\u6765\u4fe1\u606f\u88ab\u770b\u5230\u662f\u56e0\u4e3a\u8bad\u7ec3\u65f6\u4f1a\u628a\u6574\u4e2a\u8f93\u51fa\u7ed3\u679c\u90fd\u4e00\u6b21\u6027\u8fdb\u884cEmbedding\uff0c\u4f46\u662f\u7406\u8bba\u4e0a\u89e3\u7801\u5668\u7684\u7684\u8f93\u51fa\u5374\u4e0d\u662f\u4e00\u6b21\u5c31\u80fd\u4ea7\u751f\u6700\u7ec8\u7ed3\u679c\u7684\uff0c\u800c\u662f\u4e00\u6b21\u6b21\u901a\u8fc7\u4e0a\u4e00\u6b21\u7ed3\u679c\u7efc\u5408\u5f97\u51fa\u7684\uff0c\u56e0\u6b64\uff0c\u672a\u6765\u7684\u4fe1\u606f\u53ef\u80fd\u88ab\u63d0\u524d\u5229\u7528. \u6240\u4ee5\uff0c\u6211\u4eec\u4f1a\u8fdb\u884c\u906e\u63a9. \u5173\u4e8e\u89e3\u7801\u5668\u7684\u6709\u5173\u77e5\u8bc6\u5c06\u5728\u540e\u9762\u7684\u7ae0\u8282\u4e2d\u8bb2\u89e3. 2.3 \u751f\u6210\u63a9\u7801\u5f20\u91cf\u7684\u4ee3\u7801\u5206\u6790 \u00b6 \u4e0a\u4e09\u89d2\u77e9\u9635\u548cnp.triu\u51fd\u6570\u6f14\u793a # \u4e0a\u4e09\u89d2\u77e9\u9635\uff1a\u4e0b\u9762\u77e9\u9635\u4e2d0\u7ec4\u6210\u7684\u5f62\u72b6\u4e3a\u4e0a\u4e09\u89d2\u77e9\u9635 ''' [[[0. 1. 1. 1. 1.] [0. 0. 1. 1. 1.] [0. 0. 0. 1. 1.] [0. 0. 0. 0. 1.] [0. 0. 0. 0. 0.]]] # nn.triu()\u51fd\u6570\u529f\u80fd\u4ecb\u7ecd # def triu\uff08m, k\uff09 # m\uff1a\u8868\u793a\u4e00\u4e2a\u77e9\u9635 # K\uff1a\u8868\u793a\u5bf9\u89d2\u7ebf\u7684\u8d77\u59cb\u4f4d\u7f6e\uff08k\u53d6\u503c\u9ed8\u8ba4\u4e3a0\uff09 # return: \u8fd4\u56de\u51fd\u6570\u7684\u4e0a\u4e09\u89d2\u77e9\u9635 ''' def dm_test_nptriu (): # \u6d4b\u8bd5\u4ea7\u751f\u4e0a\u4e09\u89d2\u77e9\u9635 print ( np . triu ([[ 1 , 1 , 1 , 1 , 1 ], [ 2 , 2 , 2 , 2 , 2 ], [ 3 , 3 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 4 , 4 ], [ 5 , 5 , 5 , 5 , 5 ]], k = 1 )) print ( np . triu ([[ 1 , 1 , 1 , 1 , 1 ], [ 2 , 2 , 2 , 2 , 2 ], [ 3 , 3 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 4 , 4 ], [ 5 , 5 , 5 , 5 , 5 ]], k = 0 )) print ( np . triu ([[ 1 , 1 , 1 , 1 , 1 ], [ 2 , 2 , 2 , 2 , 2 ], [ 3 , 3 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 4 , 4 ], [ 5 , 5 , 5 , 5 , 5 ]], k =- 1 )) # \u7ed3\u679c\u8f93\u51fa\uff1a [[ 0 1 1 1 1 ] [ 0 0 2 2 2 ] [ 0 0 0 3 3 ] [ 0 0 0 0 4 ] [ 0 0 0 0 0 ]] [[ 1 1 1 1 1 ] [ 0 2 2 2 2 ] [ 0 0 3 3 3 ] [ 0 0 0 4 4 ] [ 0 0 0 0 5 ]] [[ 1 1 1 1 1 ] [ 2 2 2 2 2 ] [ 0 3 3 3 3 ] [ 0 0 4 4 4 ] [ 0 0 0 5 5 ]] \u751f\u6210\u63a9\u7801\u51fd\u6570 # \u4e0b\u4e09\u89d2\u77e9\u9635\u4f5c\u7528: \u751f\u6210\u5b57\u7b26\u65f6,\u5e0c\u671b\u6a21\u578b\u4e0d\u8981\u4f7f\u7528\u5f53\u524d\u5b57\u7b26\u548c\u540e\u9762\u7684\u5b57\u7b26\u3002 # \u4f7f\u7528\u906e\u63a9mask\uff0c\u9632\u6b62\u672a\u6765\u7684\u4fe1\u606f\u53ef\u80fd\u88ab\u63d0\u524d\u5229\u7528 # \u5b9e\u73b0\u65b9\u6cd5\uff1a 1 - \u4e0a\u4e09\u89d2\u77e9\u9635 # \u51fd\u6570 subsequent_mask \u5b9e\u73b0\u5206\u6790 # \u4ea7\u751f\u4e0a\u4e09\u89d2\u77e9\u9635 np.triu(m=np.ones((1, size, size)), k=1).astype('uint8') # \u8fd4\u56de\u4e0b\u4e09\u89d2\u77e9\u9635 torch.from_numpy(1 - my_mask ) def subsequent_mask ( size ): # \u4ea7\u751f\u4e0a\u4e09\u89d2\u77e9\u9635 \u4ea7\u751f\u4e00\u4e2a\u65b9\u9635 subsequent_mask = np . triu ( m = np . ones (( 1 , size , size )), k = 1 ) . astype ( 'uint8' ) # \u8fd4\u56de\u4e0b\u4e09\u89d2\u77e9\u9635 return torch . from_numpy ( 1 - subsequent_mask ) \u8c03\u7528 def dm_test_subsequent_mask (): # \u4ea7\u751f5*5\u7684\u4e0b\u4e09\u89d2\u77e9\u9635 size = 5 sm = subsequent_mask ( size ) print ( '\u4e0b\u4e09\u89d2\u77e9\u9635---> \\n ' , sm ) \u8f93\u51fa\u6548\u679c \u4e0b\u4e09\u89d2\u77e9\u9635 ---> tensor ([[[ 1 , 0 , 0 , 0 , 0 ], [ 1 , 1 , 0 , 0 , 0 ], [ 1 , 1 , 1 , 0 , 0 ], [ 1 , 1 , 1 , 1 , 0 ], [ 1 , 1 , 1 , 1 , 1 ]]], dtype = torch . uint8 ) 2.4 \u63a9\u7801\u5f20\u91cf\u7684\u53ef\u89c6\u5316 \u00b6 plt . figure ( figsize = ( 5 , 5 )) plt . imshow ( subsequent_mask ( 20 )[ 0 ]) plt . show () \u8f93\u51fa\u6548\u679c: \u6548\u679c\u5206\u6790: \u901a\u8fc7\u89c2\u5bdf\u53ef\u89c6\u5316\u65b9\u9635, \u9ec4\u8272\u662f1\u7684\u90e8\u5206, \u8fd9\u91cc\u4ee3\u8868\u88ab\u906e\u63a9, \u7d2b\u8272\u4ee3\u8868\u6ca1\u6709\u88ab\u906e\u63a9\u7684\u4fe1\u606f, \u6a2a\u5750\u6807\u4ee3\u8868\u76ee\u6807\u8bcd\u6c47\u7684\u4f4d\u7f6e, \u7eb5\u5750\u6807\u4ee3\u8868\u53ef\u67e5\u770b\u7684\u4f4d\u7f6e; \u6211\u4eec\u770b\u5230, \u57280\u7684\u4f4d\u7f6e\u6211\u4eec\u4e00\u770b\u671b\u8fc7\u53bb\u90fd\u662f\u9ec4\u8272\u7684, \u90fd\u88ab\u906e\u4f4f\u4e86\uff0c1\u7684\u4f4d\u7f6e\u4e00\u773c\u671b\u8fc7\u53bb\u8fd8\u662f\u9ec4\u8272, \u8bf4\u660e\u7b2c\u4e00\u6b21\u8bcd\u8fd8\u6ca1\u6709\u4ea7\u751f, \u4ece\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u770b\u8fc7\u53bb, \u5c31\u80fd\u770b\u5230\u4f4d\u7f6e1\u7684\u8bcd, \u5176\u4ed6\u4f4d\u7f6e\u770b\u4e0d\u5230, \u4ee5\u6b64\u7c7b\u63a8. 2.5 \u63a9\u7801\u5f20\u91cf\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u63a9\u7801\u5f20\u91cf: \u63a9\u4ee3\u8868\u906e\u63a9\uff0c\u7801\u5c31\u662f\u6211\u4eec\u5f20\u91cf\u4e2d\u7684\u6570\u503c\uff0c\u5b83\u7684\u5c3a\u5bf8\u4e0d\u5b9a\uff0c\u91cc\u9762\u4e00\u822c\u53ea\u67091\u548c0\u7684\u5143\u7d20\uff0c\u4ee3\u8868\u4f4d\u7f6e\u88ab\u906e\u63a9\u6216\u8005\u4e0d\u88ab\u906e\u63a9\uff0c\u81f3\u4e8e\u662f0\u4f4d\u7f6e\u88ab\u906e\u63a9\u8fd8\u662f1\u4f4d\u7f6e\u88ab\u906e\u63a9\u53ef\u4ee5\u81ea\u5b9a\u4e49\uff0c\u56e0\u6b64\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u8ba9\u53e6\u5916\u4e00\u4e2a\u5f20\u91cf\u4e2d\u7684\u4e00\u4e9b\u6570\u503c\u88ab\u906e\u63a9, \u4e5f\u53ef\u4ee5\u8bf4\u88ab\u66ff\u6362, \u5b83\u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e00\u4e2a\u5f20\u91cf. \u5b66\u4e60\u4e86\u63a9\u7801\u5f20\u91cf\u7684\u4f5c\u7528: \u5728transformer\u4e2d, \u63a9\u7801\u5f20\u91cf\u7684\u4e3b\u8981\u4f5c\u7528\u5728\u5e94\u7528attention(\u5c06\u5728\u4e0b\u4e00\u5c0f\u8282\u8bb2\u89e3)\u65f6\uff0c\u6709\u4e00\u4e9b\u751f\u6210\u7684attetion\u5f20\u91cf\u4e2d\u7684\u503c\u8ba1\u7b97\u6709\u53ef\u80fd\u5df2\u77e5\u91cf\u672a\u6765\u4fe1\u606f\u800c\u5f97\u5230\u7684\uff0c\u672a\u6765\u4fe1\u606f\u88ab\u770b\u5230\u662f\u56e0\u4e3a\u8bad\u7ec3\u65f6\u4f1a\u628a\u6574\u4e2a\u8f93\u51fa\u7ed3\u679c\u90fd\u4e00\u6b21\u6027\u8fdb\u884cEmbedding\uff0c\u4f46\u662f\u7406\u8bba\u4e0a\u89e3\u7801\u5668\u7684\u7684\u8f93\u51fa\u5374\u4e0d\u662f\u4e00\u6b21\u5c31\u80fd\u4ea7\u751f\u6700\u7ec8\u7ed3\u679c\u7684\uff0c\u800c\u662f\u4e00\u6b21\u6b21\u901a\u8fc7\u4e0a\u4e00\u6b21\u7ed3\u679c\u7efc\u5408\u5f97\u51fa\u7684\uff0c\u56e0\u6b64\uff0c\u672a\u6765\u7684\u4fe1\u606f\u53ef\u80fd\u88ab\u63d0\u524d\u5229\u7528. \u6240\u4ee5\uff0c\u6211\u4eec\u4f1a\u8fdb\u884c\u906e\u63a9. \u5173\u4e8e\u89e3\u7801\u5668\u7684\u6709\u5173\u77e5\u8bc6\u5c06\u5728\u540e\u9762\u7684\u7ae0\u8282\u4e2d\u8bb2\u89e3. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u751f\u6210\u5411\u540e\u906e\u63a9\u7684\u63a9\u7801\u5f20\u91cf\u51fd\u6570: subsequent_mask \u5b83\u7684\u8f93\u5165\u662fsize, \u4ee3\u8868\u63a9\u7801\u5f20\u91cf\u7684\u5927\u5c0f. \u5b83\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u6700\u540e\u4e24\u7ef4\u5f62\u62101\u65b9\u9635\u7684\u4e0b\u4e09\u89d2\u9635. \u6700\u540e\u5bf9\u751f\u6210\u7684\u63a9\u7801\u5f20\u91cf\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u5206\u6790, \u66f4\u6df1\u4e00\u6b65\u7406\u89e3\u4e86\u5b83\u7684\u7528\u9014. 3 \u6ce8\u610f\u529b\u673a\u5236 \u00b6 \u6211\u4eec\u8fd9\u91cc\u4f7f\u7528\u7684\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u89c4\u5219: $$ Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V $$ 3.1 \u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u81ea\u6ce8\u610f\u529b\u673a\u5236\u51fd\u6570attention \u5b9e\u73b0\u601d\u8def\u5206\u6790 # attention(query, key, value, mask=None, dropout=None) # 1 \u6c42\u67e5\u8be2\u5f20\u91cf\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0f d_k # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03socres q@k^T /math.sqrt(d_k) # \u5f62\u72b6[2,4,512] @ [2,512,4] --->[2,4,4] # 3 \u662f\u5426\u5bf9\u6743\u91cd\u5206\u5e03scores\u8fdb\u884c scores.masked_fill(mask == 0, -1e9) # 4 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03 p_attn F.softmax() # 5 \u662f\u5426\u5bf9p_attn\u8fdb\u884cdropout if dropout is not None: # 6 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a [2,4,4]@[2,4,512] --->[2,4,512] # 7 \u8fd4\u56deq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a q\u7684\u6743\u91cd\u5206\u5e03 def attention ( query , key , value , mask = None , dropout = None ): # query, key, value\uff1a\u4ee3\u8868\u6ce8\u610f\u529b\u7684\u4e09\u4e2a\u8f93\u5165\u5f20\u91cf # mask\uff1a\u4ee3\u8868\u63a9\u7801\u5f20\u91cf # dropout\uff1a\u4f20\u5165\u7684dropout\u5b9e\u4f8b\u5316\u5bf9\u8c61 # 1 \u6c42\u67e5\u8be2\u5f20\u91cf\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0f d_k = query . size ()[ - 1 ] # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03socres q@k^T /math.sqrt(d_k) # [2,4,512] @ [2,512,4] --->[2,4,4] scores = torch . matmul ( query , key . transpose ( - 2 , - 1 ) ) / math . sqrt ( d_k ) # 3 \u662f\u5426\u5bf9\u6743\u91cd\u5206\u5e03scores \u8fdb\u884c masked_fill if mask is not None : # \u6839\u636emask\u77e9\u96350\u7684\u4f4d\u7f6e \u5bf9sorces\u77e9\u9635\u5bf9\u5e94\u4f4d\u7f6e\u8fdb\u884c\u63a9\u7801 scores = scores . masked_fill ( mask == 0 , - 1e9 ) # 4 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03 softmax p_attn = F . softmax ( scores , dim =- 1 ) # 5 \u662f\u5426\u5bf9p_attn\u8fdb\u884cdropout if dropout is not None : p_attn = dropout ( p_attn ) # \u8fd4\u56de \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm-matmul\u8fd0\u7b97, \u6ce8\u610f\u529b\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03p_attn # [2,4,4]*[2,4,512] --->[2,4,512] return torch . matmul ( p_attn , value ), p_attn tensor.masked_fill\u6f14\u793a: >>> input = Variable ( torch . randn ( 5 , 5 )) >>> input Variable containing : 2.0344 - 0.5450 0.3365 - 0.1888 - 2.1803 1.5221 - 0.3823 0.8414 0.7836 - 0.8481 - 0.0345 - 0.8643 0.6476 - 0.2713 1.5645 0.8788 - 2.2142 0.4022 0.1997 0.1474 2.9109 0.6006 - 0.6745 - 1.7262 0.6977 [ torch . FloatTensor of size 5 x5 ] >>> mask = Variable ( torch . zeros ( 5 , 5 )) >>> mask Variable containing : 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [ torch . FloatTensor of size 5 x5 ] >>> input . masked_fill ( mask == 0 , - 1e9 ) Variable containing : - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 [ torch . FloatTensor of size 5 x5 ] \u8c03\u7528 def dm_test_attention (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) my_embeddings = Embeddings ( d_model , vocab ) x = my_embeddings ( x ) dropout = 0.1 # \u7f6e0\u6bd4\u7387\u4e3a0.1 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 my_pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = my_pe ( x ) query = key = value = pe_result # torch.Size([2, 4, 512]) attn1 , p_attn1 = attention ( query , key , value ) print ( '\u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u4e0d\u505a\u63a9\u7801' ) print ( '\u6ce8\u610f\u529b\u6743\u91cd p_attn1--->' , p_attn1 . shape , ' \\n ' , p_attn1 ) # torch.Size([2, 4, 4]) print ( '\u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn1--->' , attn1 . shape , ' \\n ' , attn1 ) # torch.Size([2, 4, 512]) print ( '*' * 50 ) print ( '\u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u505a\u63a9\u7801' ) mask = Variable ( torch . zeros ( 2 , 4 , 4 )) attn2 , p_attn2 = attention ( query , key , value , mask = mask ) print ( \"\u6ce8\u610f\u529b\u6743\u91cd p_attn2--->\" , p_attn2 . shape , ' \\n ' , p_attn2 ) print ( \"\u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn2--->\" , attn2 . shape , ' \\n ' , attn2 ) \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03\u4e0d\u505a\u63a9\u7801 \u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u4e0d\u505a\u63a9\u7801 \u6ce8\u610f\u529b\u6743\u91cd p_attn1 ---> torch . Size ([ 2 , 4 , 4 ]) tensor ([[[ 1. , 0. , 0. , 0. ], [ 0. , 1. , 0. , 0. ], [ 0. , 0. , 1. , 0. ], [ 0. , 0. , 0. , 1. ]], [[ 1. , 0. , 0. , 0. ], [ 0. , 1. , 0. , 0. ], [ 0. , 0. , 1. , 0. ], [ 0. , 0. , 0. , 1. ]]], grad_fn =< SoftmaxBackward0 > ) \u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn1 ---> torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 44.7449 , 54.3616 , 26.8261 , ... , 19.0635 , - 18.6284 , 31.5430 ], [ - 15.6625 , 22.7993 , 0.8864 , ... , 5.7670 , - 13.6669 , - 24.4659 ], [ 7.5418 , 37.0576 , - 16.9318 , ... , 44.9160 , 14.9246 , 3.9773 ], [ 12.6941 , 7.1106 , - 16.8938 , ... , 41.8852 , - 1.2939 , - 23.8751 ]], [[ 35.8076 , - 28.2593 , 0.0000 , ... , - 18.0751 , - 7.6109 , - 18.9212 ], [ 0.0000 , 13.4511 , 60.3647 , ... , - 3.1866 , - 30.1779 , 22.9219 ], [ - 24.6156 , 31.9683 , - 2.5262 , ... , - 24.2111 , - 2.0382 , 6.7247 ], [ 33.4411 , - 20.6284 , - 4.9740 , ... , 11.4844 , 0.0000 , 7.1890 ]]], grad_fn =< UnsafeViewBackward0 > ) 3.2 \u5e26\u6709mask\u7684\u8f93\u5165\u53c2\u6570 \u00b6 \u5e26\u6709mask\u7684\u8f93\u51fa\u6548\u679c ************************************************** \u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u505a\u63a9\u7801 \u6ce8\u610f\u529b\u6743\u91cd p_attn2 ---> torch . Size ([ 2 , 4 , 4 ]) tensor ([[[ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ]], [[ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ]]], grad_fn =< SoftmaxBackward0 > ) \u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn2 ---> torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ], [ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ], [ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ], [ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ]], [[ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ], [ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ], [ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ], [ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ]]], grad_fn =< UnsafeViewBackward0 > ) 3.3 \u6ce8\u610f\u529b\u673a\u5236\u603b\u7ed3 \u00b6 \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u51fd\u6570: attention \u5b83\u7684\u8f93\u5165\u5c31\u662fQ\uff0cK\uff0cV\u4ee5\u53camask\u548cdropout, mask\u7528\u4e8e\u63a9\u7801, dropout\u7528\u4e8e\u968f\u673a\u7f6e0. \u5b83\u7684\u8f93\u51fa\u6709\u4e24\u4e2a, query\u7684\u6ce8\u610f\u529b\u8868\u793a\u4ee5\u53ca\u6ce8\u610f\u529b\u5f20\u91cf. 4 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 \u00b6 4.1 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u6982\u5ff5 \u00b6 \u4ece\u591a\u5934\u6ce8\u610f\u529b\u7684\u7ed3\u6784\u56fe\u4e2d\uff0c\u8c8c\u4f3c\u8fd9\u4e2a\u6240\u8c13\u7684\u591a\u4e2a\u5934\u5c31\u662f\u6307\u591a\u7ec4\u7ebf\u6027\u53d8\u6362\u5c42\uff0c\u5176\u5b9e\u5e76\u4e0d\u662f\uff0c\u6211\u53ea\u6709\u4f7f\u7528\u4e86\u4e00\u7ec4\u7ebf\u6027\u53d8\u5316\u5c42\uff0c\u5373\u4e09\u4e2a\u53d8\u6362\u5f20\u91cf\u5bf9Q\uff0cK\uff0cV\u5206\u522b\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u8fd9\u4e9b\u53d8\u6362\u4e0d\u4f1a\u6539\u53d8\u539f\u6709\u5f20\u91cf\u7684\u5c3a\u5bf8\uff0c\u56e0\u6b64\u6bcf\u4e2a\u53d8\u6362\u77e9\u9635\u90fd\u662f\u65b9\u9635\uff0c\u5f97\u5230\u8f93\u51fa\u7ed3\u679c\u540e\uff0c\u591a\u5934\u7684\u4f5c\u7528\u624d\u5f00\u59cb\u663e\u73b0\uff0c\u6bcf\u4e2a\u5934\u5f00\u59cb\u4ece\u8bcd\u4e49\u5c42\u9762\u5206\u5272\u8f93\u51fa\u7684\u5f20\u91cf\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u5934\u90fd\u60f3\u83b7\u5f97\u4e00\u7ec4Q\uff0cK\uff0cV\u8fdb\u884c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\uff0c\u4f46\u662f\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u7684\u8868\u793a\u53ea\u83b7\u5f97\u4e00\u90e8\u5206\uff0c\u4e5f\u5c31\u662f\u53ea\u5206\u5272\u4e86\u6700\u540e\u4e00\u7ef4\u7684\u8bcd\u5d4c\u5165\u5411\u91cf. \u8fd9\u5c31\u662f\u6240\u8c13\u7684\u591a\u5934\uff0c\u5c06\u6bcf\u4e2a\u5934\u7684\u83b7\u5f97\u7684\u8f93\u5165\u9001\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d, \u5c31\u5f62\u6210\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236. 4.2 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u6784\u56fe \u00b6 4.3 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528 \u00b6 \u8fd9\u79cd\u7ed3\u6784\u8bbe\u8ba1\u80fd\u8ba9\u6bcf\u4e2a\u6ce8\u610f\u529b\u673a\u5236\u53bb\u4f18\u5316\u6bcf\u4e2a\u8bcd\u6c47\u7684\u4e0d\u540c\u7279\u5f81\u90e8\u5206\uff0c\u4ece\u800c\u5747\u8861\u540c\u4e00\u79cd\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u4ea7\u751f\u7684\u504f\u5dee\uff0c\u8ba9\u8bcd\u4e49\u62e5\u6709\u6765\u81ea\u66f4\u591a\u5143\u7684\u8868\u8fbe\uff0c\u5b9e\u9a8c\u8868\u660e\u53ef\u4ee5\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6548\u679c. 4.4 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4ee3\u7801\u5b9e\u73b0 \u00b6 # \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7c7b MultiHeadedAttention \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, head, embedding_dim, dropout=0.1) # \u6bcf\u4e2a\u5934\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0fself.d_k \u591a\u5c11\u4e2a\u5934self.head \u7ebf\u6027\u5c42\u5217\u8868self.linears # self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4) # \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03self.attn=None dropout\u5c42self.dropout # 2 forward(self, query, key, value, mask=None) # 2-1 \u63a9\u7801\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6[8,4,4] -->[1,8,4,4] \u6c42\u591a\u5c11\u6279\u6b21batch_size # 2-2 \u6570\u636e\u7ecf\u8fc7\u7ebf\u6027\u5c42 \u5207\u62108\u4e2a\u5934,view(batch_size, -1, self.head, self.d_k), transpose(1,2)\u6570\u636e\u5f62\u72b6\u53d8\u5316 # \u6570\u636e\u5f62\u72b6\u53d8\u5316[2,4,512] ---> [2,4,8,64] ---> [2,8,4,64] # 2-3 24\u4e2a\u5934 \u4e00\u8d77\u9001\u5165\u5230attention\u51fd\u6570\u4e2d\u6c42 x, self.attn # attention([2,8,4,64],[2,8,4,64],[2,8,4,64],[1,8,4,4]) ==> x[2,8,4,64], self.attn[2,8,4,4]] # 2-4 \u6570\u636e\u5f62\u72b6\u518d\u53d8\u5316\u56de\u6765 x.transpose(1,2).contiguous().view(,,) # \u6570\u636e\u5f62\u72b6\u53d8\u5316 [2,8,4,64] ---> [2,4,8,64] ---> [2,4,512] # 2-5 \u8fd4\u56de\u6700\u540e\u7ebf\u6027\u5c42\u7ed3\u679c return self.linears[-1](x) # \u6df1\u5ea6copy\u6a21\u578b \u8f93\u5165\u6a21\u578b\u5bf9\u8c61\u548ccopy\u7684\u4e2a\u6570 \u5b58\u50a8\u5230\u6a21\u578b\u5217\u8868\u4e2d def clones ( module , N ): return nn . ModuleList ([ copy . deepcopy ( module ) for _ in range ( N )]) class MultiHeadedAttention ( nn . Module ): def __init__ ( self , head , embedding_dim , dropout = 0.1 ): super ( MultiHeadedAttention , self ) . __init__ () # \u786e\u8ba4\u6570\u636e\u7279\u5f81\u80fd\u5426\u88ab\u88ab\u6574\u9664 eg \u7279\u5f81\u5c3a\u5bf8256 % \u5934\u65708 assert embedding_dim % head == 0 # \u8ba1\u7b97\u6bcf\u4e2a\u5934\u7279\u5f81\u5c3a\u5bf8 \u7279\u5f81\u5c3a\u5bf8256 // \u5934\u65708 = 64 self . d_k = embedding_dim // head # \u591a\u5c11\u5934\u6570 self . head = head # \u56db\u4e2a\u7ebf\u6027\u5c42 self . linears = clones ( nn . Linear ( embedding_dim , embedding_dim ), 4 ) # \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 self . attn = None # dropout\u5c42 self . dropout = nn . Dropout ( p = dropout ) def forward ( self , query , key , value , mask = None ): # \u82e5\u4f7f\u7528\u63a9\u7801\uff0c\u5219\u63a9\u7801\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6[8,4,4] -->[1,8,4,4] if mask is not None : mask = mask . unsqueeze ( 0 ) # \u6c42\u6570\u636e\u591a\u5c11\u884c eg:[2,4,512] \u5219batch_size=2 batch_size = query . size ()[ 0 ] # \u6570\u636e\u5f62\u72b6\u53d8\u5316[2,4,512] ---> [2,4,8,64] ---> [2,8,4,64] # 4\u4ee3\u88684\u4e2a\u5355\u8bcd 8\u4ee3\u88688\u4e2a\u5934 \u8ba9\u53e5\u5b50\u957f\u5ea64\u548c\u53e5\u5b50\u7279\u5f8164\u9760\u5728\u4e00\u8d77 \u66f4\u6709\u5229\u6355\u6349\u53e5\u5b50\u7279\u5f81 query , key , value = [ model ( x ) . view ( batch_size , - 1 , self . head , self . d_k ) . transpose ( 1 , 2 ) for model , x in zip ( self . linears , ( query , key , value ) ) ] # myoutptlist_data = [] # for model, x in zip(self.linears, (query, key, value)): # print('x--->', x.shape) # [2,4,512] # myoutput = model(x) # print('myoutput--->', myoutput.shape) # [2,4,512] # # [2,4,512] --> [2,4,8,64] --> [2,8,4,64] # tmpmyoutput = myoutput.view(batch_size, -1, self.head, self.d_k).transpose(1, 2) # myoutptlist_data.append( tmpmyoutput ) # mylen = len(myoutptlist_data) # mylen:3 # query = myoutptlist_data[0] # [2,8,4,64] # key = myoutptlist_data[1] # [2,8,4,64] # value = myoutptlist_data[2] # [2,8,4,64] # \u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793ax\u5f62\u72b6 [2,8,4,64] \u6ce8\u610f\u529b\u6743\u91cdattn\u5f62\u72b6\uff1a[2,8,4,4] # attention([2,8,4,64],[2,8,4,64],[2,8,4,64],[1,8,4,4]) ==> x[2,8,4,64], self.attn[2,8,4,4]] x , self . attn = attention ( query , key , value , mask = mask , dropout = self . dropout ) # \u6570\u636e\u5f62\u72b6\u53d8\u5316 [2,8,4,64] ---> [2,4,8,64] ---> [2,4,512] x = x . transpose ( 1 , 2 ) . contiguous () . view ( batch_size , - 1 , self . head * self . d_k ) # \u8fd4\u56de\u6700\u540e\u53d8\u5316\u540e\u7684\u7ed3\u679c [2,4,512]---> [2,4,512] return self . linears [ - 1 ]( x ) tensor.view\u6f14\u793a: >>> x = torch . randn ( 4 , 4 ) >>> x . size () torch . Size ([ 4 , 4 ]) >>> y = x . view ( 16 ) >>> y . size () torch . Size ([ 16 ]) >>> z = x . view ( - 1 , 8 ) # the size -1 is inferred from other dimensions >>> z . size () torch . Size ([ 2 , 8 ]) >>> a = torch . randn ( 1 , 2 , 3 , 4 ) >>> a . size () torch . Size ([ 1 , 2 , 3 , 4 ]) >>> b = a . transpose ( 1 , 2 ) # Swaps 2nd and 3rd dimension >>> b . size () torch . Size ([ 1 , 3 , 2 , 4 ]) >>> c = a . view ( 1 , 3 , 2 , 4 ) # Does not change tensor layout in memory >>> c . size () torch . Size ([ 1 , 3 , 2 , 4 ]) >>> torch . equal ( b , c ) False torch.transpose\u6f14\u793a: >>> x = torch . randn ( 2 , 3 ) >>> x tensor ([[ 1.0028 , - 0.9893 , 0.5809 ], [ - 0.1669 , 0.7299 , 0.4942 ]]) >>> torch . transpose ( x , 0 , 1 ) tensor ([[ 1.0028 , - 0.1669 ], [ - 0.9893 , 0.7299 ], [ 0.5809 , 0.4942 ]]) \u51fd\u6570\u8c03\u7528 # \u6d4b\u8bd5\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 def dm_test_MultiHeadedAttention (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) my_embeddings = Embeddings ( d_model , vocab ) x = my_embeddings ( x ) dropout = 0.1 # \u7f6e0\u6bd4\u7387\u4e3a0.1 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 my_pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = my_pe ( x ) head = 8 # \u5934\u6570head query = key = value = pe_result # torch.Size([2, 4, 512]) # \u8f93\u5165\u7684\u63a9\u7801\u5f20\u91cfmask mask = Variable ( torch . zeros ( 8 , 4 , 4 )) my_mha = MultiHeadedAttention ( head , d_model , dropout ) x = my_mha ( query , key , value , mask ) print ( '\u591a\u5934\u6ce8\u610f\u673a\u5236\u540e\u7684x' , x . shape , ' \\n ' , x ) print ( '\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03' , my_mha . attn . shape ) \u8f93\u51fa\u6548\u679c \u591a\u5934\u6ce8\u610f\u673a\u5236\u540e\u7684x torch.Size([2, 4, 512]) tensor([[[-2.9384, 2.5006, -0.8888, ..., -6.1134, -6.5651, -5.7406], [-0.9007, 0.9144, -1.2935, ..., -6.6897, -6.7292, -6.2146], [-3.5213, 1.2106, -4.2973, ..., -5.6040, -7.7500, -2.3606], [-1.3711, 4.1226, -3.8623, ..., -6.0207, -8.6360, -4.6519]], [[ 6.1754, 3.4284, -5.4673, ..., -7.7355, -6.7766, -4.9681], [ 5.4382, 6.4217, -4.3761, ..., -8.3668, -3.1675, -6.6081], [ 9.0191, 3.2935, -4.4196, ..., -5.2750, -5.3374, -5.1187], [ 5.8635, 4.2653, -4.7956, ..., -9.4884, -8.6182, -4.5732]]], grad_fn=<AddBackward0>) \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 torch.Size([2, 8, 4, 4]) 4.5 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236: \u6bcf\u4e2a\u5934\u5f00\u59cb\u4ece\u8bcd\u4e49\u5c42\u9762\u5206\u5272\u8f93\u51fa\u7684\u5f20\u91cf\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u5934\u90fd\u60f3\u83b7\u5f97\u4e00\u7ec4Q\uff0cK\uff0cV\u8fdb\u884c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\uff0c\u4f46\u662f\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u7684\u8868\u793a\u53ea\u83b7\u5f97\u4e00\u90e8\u5206\uff0c\u4e5f\u5c31\u662f\u53ea\u5206\u5272\u4e86\u6700\u540e\u4e00\u7ef4\u7684\u8bcd\u5d4c\u5165\u5411\u91cf. \u8fd9\u5c31\u662f\u6240\u8c13\u7684\u591a\u5934.\u5c06\u6bcf\u4e2a\u5934\u7684\u83b7\u5f97\u7684\u8f93\u5165\u9001\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d, \u5c31\u5f62\u6210\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236. \u5b66\u4e60\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528: \u8fd9\u79cd\u7ed3\u6784\u8bbe\u8ba1\u80fd\u8ba9\u6bcf\u4e2a\u6ce8\u610f\u529b\u673a\u5236\u53bb\u4f18\u5316\u6bcf\u4e2a\u8bcd\u6c47\u7684\u4e0d\u540c\u7279\u5f81\u90e8\u5206\uff0c\u4ece\u800c\u5747\u8861\u540c\u4e00\u79cd\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u4ea7\u751f\u7684\u504f\u5dee\uff0c\u8ba9\u8bcd\u4e49\u62e5\u6709\u6765\u81ea\u66f4\u591a\u5143\u7684\u8868\u8fbe\uff0c\u5b9e\u9a8c\u8868\u660e\u53ef\u4ee5\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6548\u679c. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u7c7b: MultiHeadedAttention \u56e0\u4e3a\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u9700\u8981\u4f7f\u7528\u591a\u4e2a\u76f8\u540c\u7684\u7ebf\u6027\u5c42, \u9996\u5148\u5b9e\u73b0\u4e86\u514b\u9686\u51fd\u6570clones. clones\u51fd\u6570\u7684\u8f93\u5165\u662fmodule\uff0cN\uff0c\u5206\u522b\u4ee3\u8868\u514b\u9686\u7684\u76ee\u6807\u5c42\uff0c\u548c\u514b\u9686\u4e2a\u6570. clones\u51fd\u6570\u7684\u8f93\u51fa\u662f\u88c5\u6709N\u4e2a\u514b\u9686\u5c42\u7684Module\u5217\u8868. \u63a5\u7740\u5b9e\u73b0MultiHeadedAttention\u7c7b, \u5b83\u7684\u521d\u59cb\u5316\u51fd\u6570\u8f93\u5165\u662fh, d_model, dropout\u5206\u522b\u4ee3\u8868\u5934\u6570\uff0c\u8bcd\u5d4c\u5165\u7ef4\u5ea6\u548c\u7f6e\u96f6\u6bd4\u7387. \u5b83\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u8f93\u5165\u662fQ, K, V\u4ee5\u53ca\u63a9\u7801\u5f20\u91cfmask. \u5b83\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u8f93\u51fa\u662f\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u7684Q\u7684\u6ce8\u610f\u529b\u8868\u793a. 5 \u524d\u9988\u5168\u8fde\u63a5\u5c42 \u00b6 5.1 \u524d\u9988\u5168\u8fde\u63a5\u5c42 \u00b6 \u5728Transformer\u4e2d\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5c31\u662f\u5177\u6709\u4e24\u5c42\u7ebf\u6027\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u8003\u8651\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u5bf9\u590d\u6742\u8fc7\u7a0b\u7684\u62df\u5408\u7a0b\u5ea6\u4e0d\u591f, \u901a\u8fc7\u589e\u52a0\u4e24\u5c42\u7f51\u7edc\u6765\u589e\u5f3a\u6a21\u578b\u7684\u80fd\u529b. 5.2 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u524d\u9988\u5168\u8fde\u63a5\u5c42 PositionwiseFeedForward \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, d_model, d_ff, dropout=0.1): # \u5b9a\u4e49\u7ebf\u6027\u5c42self.w1 self.w2, self.dropout\u5c42 # 2 forward(self, x) # \u6570\u636e\u7ecf\u8fc7self.w1(x) -> F.relu() ->self.dropout() ->self.w2 \u8fd4\u56de class PositionwiseFeedForward ( nn . Module ): def __init__ ( self , d_model , d_ff , dropout = 0.1 ): # d_model \u7b2c1\u4e2a\u7ebf\u6027\u5c42\u8f93\u5165\u7ef4\u5ea6 # d_ff \u7b2c2\u4e2a\u7ebf\u6027\u5c42\u8f93\u51fa\u7ef4\u5ea6 super ( PositionwiseFeedForward , self ) . __init__ () # \u5b9a\u4e49\u7ebf\u6027\u5c42w1 w2 dropout self . w1 = nn . Linear ( d_model , d_ff ) self . w2 = nn . Linear ( d_ff , d_model ) self . dropout = nn . Dropout ( p = dropout ) def forward ( self , x ): # \u6570\u636e\u4f9d\u6b21\u7ecf\u8fc7\u7b2c1\u4e2a\u7ebf\u6027\u5c42 relu\u6fc0\u6d3b\u5c42 dropout\u5c42\uff0c\u7136\u540e\u662f\u7b2c2\u4e2a\u7ebf\u6027\u5c42 return self . w2 ( self . dropout ( F . relu ( self . w1 ( x )))) ReLU\u51fd\u6570\u516c\u5f0f: ReLU(x)=max(0, x) ReLU\u51fd\u6570\u56fe\u50cf: \u51fd\u6570\u8c03\u7528 def dm_test_PositionwiseFeedForward (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) my_embeddings = Embeddings ( d_model , vocab ) x = my_embeddings ( x ) dropout = 0.1 # \u7f6e0\u6bd4\u7387\u4e3a0.1 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 my_pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = my_pe ( x ) head = 8 # \u5934\u6570head query = key = value = pe_result # torch.Size([2, 4, 512]) # \u8f93\u5165\u7684\u63a9\u7801\u5f20\u91cfmask mask = Variable ( torch . zeros ( 8 , 4 , 4 )) my_mha = MultiHeadedAttention ( head , d_model , dropout ) x = my_mha ( query , key , value , mask ) # \u6d4b\u8bd5\u524d\u9988\u5168\u94fe\u63a5\u5c42 my_PFF = PositionwiseFeedForward ( d_model = 512 , d_ff = 64 , dropout = 0.1 ) ff_result = my_PFF ( x ) print ( 'x--->' , ff_result . shape , ff_result ) \u8f93\u51fa\u6548\u679c x ---> torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ - 0.1989 , 0.5191 , 1.3063 , ... , 0.1391 , - 0.8836 , 0.5450 ], [ - 0.2717 , 0.6541 , 0.9768 , ... , - 0.1452 , - 0.8929 , 0.9798 ], [ - 0.3297 , - 0.1791 , 0.8489 , ... , 0.6890 , - 1.0303 , 1.1638 ], [ 0.0308 , - 0.2209 , 1.3144 , ... , - 0.6433 , - 1.1207 , 0.6042 ]], [[ - 1.3265 , - 1.3563 , 0.6005 , ... , - 0.4166 , 0.1078 , - 0.0522 ], [ - 0.2736 , - 2.5544 , 1.3333 , ... , - 0.1704 , - 0.3514 , - 0.1901 ], [ - 0.0454 , - 1.1244 , 1.4875 , ... , - 0.5366 , - 0.0143 , 0.1453 ], [ - 1.2958 , - 1.6615 , 0.4268 , ... , - 0.5896 , 0.1486 , 0.1122 ]]], grad_fn =< AddBackward0 > ) 5.3 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42: \u5728Transformer\u4e2d\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5c31\u662f\u5177\u6709\u4e24\u5c42\u7ebf\u6027\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc. \u5b66\u4e60\u4e86\u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u8003\u8651\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u5bf9\u590d\u6742\u8fc7\u7a0b\u7684\u62df\u5408\u7a0b\u5ea6\u4e0d\u591f, \u901a\u8fc7\u589e\u52a0\u4e24\u5c42\u7f51\u7edc\u6765\u589e\u5f3a\u6a21\u578b\u7684\u80fd\u529b. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u7c7b: PositionwiseFeedForward \u5b83\u7684\u5b9e\u4f8b\u5316\u53c2\u6570\u4e3ad_model, d_ff, dropout, \u5206\u522b\u4ee3\u8868\u8bcd\u5d4c\u5165\u7ef4\u5ea6, \u7ebf\u6027\u53d8\u6362\u7ef4\u5ea6, \u548c\u7f6e\u96f6\u6bd4\u7387. \u5b83\u7684\u8f93\u5165\u53c2\u6570x, \u8868\u793a\u4e0a\u5c42\u7684\u8f93\u51fa. \u5b83\u7684\u8f93\u51fa\u662f\u7ecf\u8fc72\u5c42\u7ebf\u6027\u7f51\u7edc\u53d8\u6362\u7684\u7279\u5f81\u8868\u793a. 6 \u89c4\u8303\u5316\u5c42 \u00b6 6.1 \u89c4\u8303\u5316\u5c42\u7684\u4f5c\u7528 \u00b6 \u5b83\u662f\u6240\u6709\u6df1\u5c42\u7f51\u7edc\u6a21\u578b\u90fd\u9700\u8981\u7684\u6807\u51c6\u7f51\u7edc\u5c42\uff0c\u56e0\u4e3a\u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u589e\u52a0\uff0c\u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u5f00\u59cb\u51fa\u73b0\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684\u60c5\u51b5\uff0c\u8fd9\u6837\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38\uff0c\u6a21\u578b\u53ef\u80fd\u6536\u655b\u975e\u5e38\u7684\u6162. \u56e0\u6b64\u90fd\u4f1a\u5728\u4e00\u5b9a\u5c42\u6570\u540e\u63a5\u89c4\u8303\u5316\u5c42\u8fdb\u884c\u6570\u503c\u7684\u89c4\u8303\u5316\uff0c\u4f7f\u5176\u7279\u5f81\u6570\u503c\u5728\u5408\u7406\u8303\u56f4\u5185. 6.2 \u89c4\u8303\u5316\u5c42\u7684\u4ee3\u7801\u5b9e\u73b0 \u00b6 # \u89c4\u8303\u5316\u5c42 LayerNorm \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, features, eps=1e-6): # \u5b9a\u4e49\u7ebf\u6027\u5c42self.a2 self.b2, nn.Parameter(torch.ones(features)) # 2 forward(self, x) \u8fd4\u56de\u6807\u51c6\u5316\u540e\u7684\u7ed3\u679c # \u5bf9\u6570\u636e\u6c42\u5747\u503c \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 x.mean(-1, keepdims=True) # \u5bf9\u6570\u636e\u6c42\u65b9\u5dee \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 x.std(-1, keepdims=True) # \u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316\u53d8\u6362 \u53cd\u5411\u4f20\u64ad\u53ef\u5b66\u4e60\u53c2\u6570a2 b2 # eg self.a2 * (x-mean)/(std + self.eps) + self.b2 class LayerNorm ( nn . Module ): def __init__ ( self , features , eps = 1e-6 ): # \u53c2\u6570features \u5f85\u89c4\u8303\u5316\u7684\u6570\u636e # \u53c2\u6570 eps=1e-6 \u9632\u6b62\u5206\u6bcd\u4e3a\u96f6 super ( LayerNorm , self ) . __init__ () # \u5b9a\u4e49a2 \u89c4\u8303\u5316\u5c42\u7684\u7cfb\u6570 y=kx+b\u4e2d\u7684k self . a2 = nn . Parameter ( torch . ones ( features )) # \u5b9a\u4e49b2 \u89c4\u8303\u5316\u5c42\u7684\u7cfb\u6570 y=kx+b\u4e2d\u7684b self . b2 = nn . Parameter ( torch . zeros ( features )) self . eps = eps def forward ( self , x ): # \u5bf9\u6570\u636e\u6c42\u5747\u503c \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 # [2,4,512] -> [2,4,1] mean = x . mean ( - 1 , keepdims = True ) # \u5bf9\u6570\u636e\u6c42\u65b9\u5dee \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 # [2,4,512] -> [2,4,1] std = x . std ( - 1 , keepdims = True ) # \u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316\u53d8\u6362 \u53cd\u5411\u4f20\u64ad\u53ef\u5b66\u4e60\u53c2\u6570a2 b2 # \u6ce8\u610f * \u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u4e58 \u4e0d\u662f\u77e9\u9635\u8fd0\u7b97 y = self . a2 * ( x - mean ) / ( std + self . eps ) + self . b2 return y \u51fd\u6570\u8c03\u7528 # \u89c4\u8303\u5316\u5c42\u6d4b\u8bd5 def dm_test_LayerNorm (): embedding_dim = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( embedding_dim , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( embedding_dim , dropout , max_len ) pe_result = pe ( x ) query = key = value = pe_result # torch.Size([2, 4, 512]) # \u8c03\u7528\u9a8c\u8bc1 d_ff = 64 head = 8 # \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f93\u51fa \u4f5c\u4e3a\u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u5165 mask = Variable ( torch . zeros ( 8 , 4 , 4 )) mha = MultiHeadedAttention ( head , embedding_dim , dropout ) mha_result = mha ( query , key , value , mask ) x = mha_result ff = PositionwiseFeedForward ( embedding_dim , d_ff , dropout ) ff_result = ff ( x ) features = d_model = 512 eps = 1e-6 x = ff_result ln = LayerNorm ( features , eps ) ln_result = ln ( x ) print ( '\u89c4\u8303\u5316\u5c42:' , ln_result . shape , ln_result ) \u8f93\u51fa\u6548\u679c \u89c4\u8303\u5316\u5c42 : torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 1.1413 , - 0.0875 , 1.9878 , ... , 0.4824 , 1.2250 , - 0.5582 ], [ 0.3969 , 0.0417 , 0.6030 , ... , 0.6712 , 0.0858 , - 0.7419 ], [ 0.1618 , - 0.4729 , 1.1678 , ... , - 0.4206 , 0.2535 , 1.0424 ], [ 0.2952 , - 0.1489 , 0.7079 , ... , 0.5554 , 0.3931 , 0.4711 ]], [[ 0.8428 , 0.9732 , - 1.2423 , ... , - 1.1651 , - 1.3559 , 1.0449 ], [ 1.4975 , - 0.2760 , - 0.9415 , ... , - 0.2475 , - 1.1027 , 0.8396 ], [ 0.5669 , 1.0264 , - 0.6982 , ... , - 0.5022 , - 0.7629 , 0.7721 ], [ 1.2806 , - 0.3767 , - 0.0539 , ... , - 0.4042 , - 0.4116 , 0.3944 ]]], grad_fn =< AddBackward0 > ) 6.3 \u89c4\u8303\u5316\u5c42\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u89c4\u8303\u5316\u5c42\u7684\u4f5c\u7528: \u5b83\u662f\u6240\u6709\u6df1\u5c42\u7f51\u7edc\u6a21\u578b\u90fd\u9700\u8981\u7684\u6807\u51c6\u7f51\u7edc\u5c42\uff0c\u56e0\u4e3a\u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u589e\u52a0\uff0c\u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u5f00\u59cb\u51fa\u73b0\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684\u60c5\u51b5\uff0c\u8fd9\u6837\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38\uff0c\u6a21\u578b\u53ef\u80fd\u6536\u655b\u975e\u5e38\u7684\u6162. \u56e0\u6b64\u90fd\u4f1a\u5728\u4e00\u5b9a\u5c42\u6570\u540e\u63a5\u89c4\u8303\u5316\u5c42\u8fdb\u884c\u6570\u503c\u7684\u89c4\u8303\u5316\uff0c\u4f7f\u5176\u7279\u5f81\u6570\u503c\u5728\u5408\u7406\u8303\u56f4\u5185. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u89c4\u8303\u5316\u5c42\u7684\u7c7b: LayerNorm \u5b83\u7684\u5b9e\u4f8b\u5316\u53c2\u6570\u6709\u4e24\u4e2a, features\u548ceps\uff0c\u5206\u522b\u8868\u793a\u8bcd\u5d4c\u5165\u7279\u5f81\u5927\u5c0f\uff0c\u548c\u4e00\u4e2a\u8db3\u591f\u5c0f\u7684\u6570. \u5b83\u7684\u8f93\u5165\u53c2\u6570x\u4ee3\u8868\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa. \u5b83\u7684\u8f93\u51fa\u5c31\u662f\u7ecf\u8fc7\u89c4\u8303\u5316\u7684\u7279\u5f81\u8868\u793a. 7 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 \u00b6 7.1 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 \u00b6 \u5982\u56fe\u6240\u793a\uff0c\u8f93\u5165\u5230\u6bcf\u4e2a\u5b50\u5c42\u4ee5\u53ca\u89c4\u8303\u5316\u5c42\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u4f7f\u7528\u4e86\u6b8b\u5dee\u94fe\u63a5\uff08\u8df3\u8dc3\u8fde\u63a5\uff09\uff0c\u56e0\u6b64\u6211\u4eec\u628a\u8fd9\u4e00\u90e8\u5206\u7ed3\u6784\u6574\u4f53\u53eb\u505a\u5b50\u5c42\u8fde\u63a5\uff08\u4ee3\u8868\u5b50\u5c42\u53ca\u5176\u94fe\u63a5\u7ed3\u6784\uff09\uff0c\u5728\u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u4e2d\uff0c\u90fd\u6709\u4e24\u4e2a\u5b50\u5c42\uff0c\u8fd9\u4e24\u4e2a\u5b50\u5c42\u52a0\u4e0a\u5468\u56f4\u7684\u94fe\u63a5\u7ed3\u6784\u5c31\u5f62\u6210\u4e86\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784. \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u56fe: 7.2 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 \u5b50\u5c42(\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6216\u8005 \u6ce8\u610f\u529b\u673a\u5236\u5c42)+ norm\u5c42 + \u6b8b\u5dee\u8fde\u63a5 # SublayerConnection\u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, size, dropout=0.1): # \u5b9a\u4e49self.norm\u5c42 self.dropout\u5c42, \u5176\u4e2dLayerNorm(size) # 2 forward(self, x, sublayer) \u8fd4\u56de+\u4ee5\u540e\u7684\u7ed3\u679c # \u6570\u636eself.norm() -> sublayer()->self.dropout() + x class SublayerConnection ( nn . Module ): def __init__ ( self , size , dropout = 0.1 ): # \u53c2\u6570size \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u5c3a\u5bf8\u5927\u5c0f # \u53c2\u6570dropout \u7f6e\u96f6\u6bd4\u7387 super ( SublayerConnection , self ) . __init__ () # \u5b9a\u4e49norm\u5c42 self . norm = LayerNorm ( size ) # \u5b9a\u4e49dropout self . dropout = nn . Dropout ( dropout ) def forward ( self , x , sublayer ): # \u53c2\u6570x \u4ee3\u8868\u6570\u636e # sublayer \u51fd\u6570\u5165\u53e3\u5730\u5740 \u5b50\u5c42\u51fd\u6570(\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6216\u8005 \u6ce8\u610f\u529b\u673a\u5236\u5c42\u51fd\u6570\u7684\u5165\u53e3\u5730\u5740) # \u65b9\u5f0f1 # \u6570\u636eself.norm() -> sublayer()->self.dropout() + x myres = x + self . dropout ( sublayer ( self . norm ( x ))) # \u65b9\u5f0f2 # \u6570\u636esublayer() -> self.norm() ->self.dropout() + x # myres = x + self.dropout(self.norm(x.subtype(x))) return myres \u51fd\u6570\u8c03\u7528 def dm_test_SublayerConnection (): size = 512 head = 8 d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42 self_attn = MultiHeadedAttention ( head , d_model ) sublayer = lambda x : self_attn ( x , x , x , mask ) # \u5b50\u5c42\u94fe\u63a5\u7ed3\u6784 sc = SublayerConnection ( size , dropout ) sc_result = sc ( x , sublayer ) print ( 'sc_result.shape--->' , sc_result . shape ) print ( 'sc_result--->' , sc_result ) \u8f93\u51fa\u6548\u679c sc_result . shape ---> torch . Size ([ 2 , 4 , 512 ]) sc_result ---> tensor ([[[ - 30.8925 , 57.5868 , - 6.7073 , ... , 2.2304 , 0.0866 , - 25.0320 ], [ 19.7721 , - 0.2945 , - 10.9359 , ... , - 0.1355 , - 9.1049 , 35.7419 ], [ 0.1608 , 3.0822 , 0.1203 , ... , 2.9998 , 40.5865 , 12.3813 ], [ 0.0765 , 14.6370 , - 22.0670 , ... , 6.8273 , 0.2928 , 26.7776 ]], [[ - 0.2359 , - 0.0000 , - 26.8415 , ... , 10.3175 , - 25.3874 , 20.8764 ], [ 23.7864 , - 0.2481 , 51.0186 , ... , - 7.8931 , 9.0427 , - 2.3697 ], [ - 21.1101 , - 0.4014 , 37.0955 , ... , - 26.1717 , 35.2731 , - 37.8626 ], [ 7.5792 , 21.9032 , - 18.7778 , ... , 4.6249 , - 33.6907 , 22.5649 ]]], grad_fn =< AddBackward0 > ) 7.3 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u603b\u7ed3 \u00b6 \u4ec0\u4e48\u662f\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784: \u5982\u56fe\u6240\u793a\uff0c\u8f93\u5165\u5230\u6bcf\u4e2a\u5b50\u5c42\u4ee5\u53ca\u89c4\u8303\u5316\u5c42\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u4f7f\u7528\u4e86\u6b8b\u5dee\u94fe\u63a5\uff08\u8df3\u8dc3\u8fde\u63a5\uff09\uff0c\u56e0\u6b64\u6211\u4eec\u628a\u8fd9\u4e00\u90e8\u5206\u7ed3\u6784\u6574\u4f53\u53eb\u505a\u5b50\u5c42\u8fde\u63a5\uff08\u4ee3\u8868\u5b50\u5c42\u53ca\u5176\u94fe\u63a5\u7ed3\u6784\uff09, \u5728\u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u4e2d\uff0c\u90fd\u6709\u4e24\u4e2a\u5b50\u5c42\uff0c\u8fd9\u4e24\u4e2a\u5b50\u5c42\u52a0\u4e0a\u5468\u56f4\u7684\u94fe\u63a5\u7ed3\u6784\u5c31\u5f62\u6210\u4e86\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7684\u7c7b: SublayerConnection \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u8f93\u5165\u53c2\u6570\u662fsize, dropout, \u5206\u522b\u4ee3\u8868\u8bcd\u5d4c\u5165\u5927\u5c0f\u548c\u7f6e\u96f6\u6bd4\u7387. \u5b83\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u8f93\u5165\u53c2\u6570\u662fx, sublayer, \u5206\u522b\u4ee3\u8868\u4e0a\u4e00\u5c42\u8f93\u51fa\u4ee5\u53ca\u5b50\u5c42\u7684\u51fd\u6570\u8868\u793a. \u5b83\u7684\u8f93\u51fa\u5c31\u662f\u901a\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5904\u7406\u7684\u8f93\u51fa. 8 \u7f16\u7801\u5668\u5c42 \u00b6 8.1 \u7f16\u7801\u5668\u5c42\u7684\u4f5c\u7528 \u00b6 \u4f5c\u4e3a\u7f16\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u5b8c\u6210\u4e00\u6b21\u5bf9\u8f93\u5165\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5373\u7f16\u7801\u8fc7\u7a0b. \u7f16\u7801\u5668\u5c42\u7684\u6784\u6210\u56fe: 8.2 \u7f16\u7801\u5668\u5c42\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u7f16\u7801\u5668\u5c42\u7c7b EncoderLayer \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, size, self_attn, feed_forward, dropout): # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61self_attn # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61feed_forward size\u8bcd\u5d4c\u5165\u7ef4\u5ea6512 # clones\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self.sublayer = clones(SublayerConnection(size,dropout),2) # forward\u51fd\u6570 (self, x, mask) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67841 self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67842 self.sublayer[1](x, self.feed_forward) class EncoderLayer ( nn . Module ): def __init__ ( self , size , self_atten , feed_forward , dropout ): super ( EncoderLayer , self ) . __init__ () # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61 self . self_attn = self_atten # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61feed_forward self . feed_forward = feed_forward # size\u8bcd\u5d4c\u5165\u7ef4\u5ea6512 self . size = size # clones\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self.sublayer = clones(SublayerConnection(size,dropout),2) self . sublayer = clones ( SublayerConnection ( size , dropout ) , 2 ) def forward ( self , x , mask ): # \u6570\u636e\u7ecf\u8fc7\u7b2c1\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 # \u53c2\u6570x\uff1a\u4f20\u5165\u7684\u6570\u636e \u53c2\u6570lambda x... : \u5b50\u51fd\u6570\u5165\u53e3\u5730\u5740 x = self . sublayer [ 0 ]( x , lambda x : self . self_attn ( x , x , x , mask )) # \u6570\u636e\u7ecf\u8fc7\u7b2c2\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 # \u53c2\u6570x\uff1a\u4f20\u5165\u7684\u6570\u636e self.feed_forward\u5b50\u51fd\u6570\u5165\u53e3\u5730\u5740 x = self . sublayer [ 1 ]( x , self . feed_forward ) return x \u51fd\u6570\u8c03\u7528 def dm_test_EncoderLayer (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result size = 512 head = 8 d_ff = 64 # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7c7b\u5bf9\u8c61 self_attn = MultiHeadedAttention ( head , d_model ) # \u5b9e\u4f8b\u5316\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61 ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) # mask\u6570\u636e mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u5c42\u5bf9\u8c61 my_encoderlayer = EncoderLayer ( size , self_attn , ff , dropout ) # \u6570\u636e\u901a\u8fc7\u7f16\u7801\u5c42\u7f16\u7801 el_result = my_encoderlayer ( x , mask ) print ( 'el_result.shape' , el_result . shape , el_result ) \u8f93\u51fa\u6548\u679c el_result . shape torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 27.1315 , 64.8418 , - 10.6292 , ... , - 23.3170 , - 30.5543 , 13.2727 ], [ - 0.1474 , 54.1129 , 0.0000 , ... , - 0.1820 , - 35.7688 , - 15.1666 ], [ - 0.0691 , 8.3125 , 7.3380 , ... , 40.2273 , - 10.4544 , - 14.1511 ], [ 34.2015 , - 25.0465 , - 31.5629 , ... , - 42.4037 , - 35.9813 , 44.9897 ]], [[ - 8.8238 , 0.0935 , - 13.7027 , ... , - 20.9247 , - 19.9678 , - 0.1526 ], [ - 18.8739 , 0.3252 , 28.1221 , ... , 34.7250 , - 0.7414 , 8.1599 ], [ 52.2108 , - 0.6148 , - 16.3005 , ... , 3.1570 , - 15.0894 , 0.9009 ], [ - 22.5749 , - 54.0201 , 3.9647 , ... , 12.6702 , - 0.2983 , 13.6588 ]]], grad_fn =< AddBackward0 > ) 8.3 \u7f16\u7801\u5668\u5c42\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u7f16\u7801\u5668\u5c42\u7684\u4f5c\u7528: \u4f5c\u4e3a\u7f16\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u5b8c\u6210\u4e00\u6b21\u5bf9\u8f93\u5165\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5373\u7f16\u7801\u8fc7\u7a0b. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7f16\u7801\u5668\u5c42\u7684\u7c7b: EncoderLayer \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u5171\u67094\u4e2a, \u522b\u662fsize\uff0c\u5176\u5b9e\u5c31\u662f\u6211\u4eec\u8bcd\u5d4c\u5165\u7ef4\u5ea6\u7684\u5927\u5c0f. \u7b2c\u4e8c\u4e2aself_attn\uff0c\u4e4b\u540e\u6211\u4eec\u5c06\u4f20\u5165\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u5b9e\u4f8b\u5316\u5bf9\u8c61, \u5e76\u4e14\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236. \u7b2c\u4e09\u4e2a\u662ffeed_froward, \u4e4b\u540e\u6211\u4eec\u5c06\u4f20\u5165\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5b9e\u4f8b\u5316\u5bf9\u8c61. \u6700\u540e\u4e00\u4e2a\u662f\u7f6e0\u6bd4\u7387dropout. \u5b9e\u4f8b\u5316\u5bf9\u8c61\u7684\u8f93\u5165\u53c2\u6570\u67092\u4e2a\uff0cx\u4ee3\u8868\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa, mask\u4ee3\u8868\u63a9\u7801\u5f20\u91cf. \u5b83\u7684\u8f93\u51fa\u4ee3\u8868\u7ecf\u8fc7\u6574\u4e2a\u7f16\u7801\u5c42\u7684\u7279\u5f81\u8868\u793a. 9 \u7f16\u7801\u5668 \u00b6 9.1 \u7f16\u7801\u5668\u7684\u4f5c\u7528 \u00b6 \u7f16\u7801\u5668\u7528\u4e8e\u5bf9\u8f93\u5165\u8fdb\u884c\u6307\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u4e5f\u79f0\u4e3a\u7f16\u7801, \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210. \u7f16\u7801\u5668\u7684\u7ed3\u6784\u56fe: 9.2 \u7f16\u7801\u5668\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u7f16\u7801\u5668\u7c7b Encoder \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, layer, N) # \u5b9e\u4f8b\u5316\u591a\u4e2a\u7f16\u7801\u5668\u5c42\u5bf9\u8c61self.layers \u901a\u8fc7\u65b9\u6cd5clones(layer, N) # \u5b9e\u4f8b\u5316\u89c4\u8303\u5316\u5c42 self.norm = LayerNorm(layer.size) # forward\u51fd\u6570 (self, x, mask) # \u6570\u636e\u7ecf\u8fc7N\u4e2a\u5c42 x = layer(x, mask) # \u8fd4\u56de\u89c4\u8303\u5316\u540e\u7684\u6570\u636e return self.norm(x) class Encoder ( nn . Module ): def __init__ ( self , layer , N ): # \u53c2\u6570layer 1\u4e2a\u7f16\u7801\u5668\u5c42 # \u53c2\u6570 \u7f16\u7801\u5668\u5c42\u7684\u4e2a\u6570 super ( Encoder , self ) . __init__ () # \u5b9e\u4f8b\u5316\u591a\u4e2a\u7f16\u7801\u5668\u5c42\u5bf9\u8c61 self . layers = clones ( layer , N ) # \u5b9e\u4f8b\u5316\u89c4\u8303\u5316\u5c42 self . norm = LayerNorm ( layer . size ) def forward ( self , x , mask ): # \u6570\u636e\u7ecf\u8fc7N\u4e2a\u5c42 x = layer(x, mask) for layer in self . layers : x = layer ( x , mask ) # \u8fd4\u56de\u89c4\u8303\u5316\u540e\u7684\u6570\u636e return self.norm(x) return self . norm ( x ) \u51fd\u6570\u8c03\u7528 def dm_test_Encoder (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) # writeFile(\"dafdsafds\") emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result # \u83b7\u53d6\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42 \u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c size = 512 head = 8 d_model = 512 d_ff = 64 c = copy . deepcopy attn = MultiHeadedAttention ( head , d_model ) dropout = 0.2 ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) layer = EncoderLayer ( size , c ( attn ), c ( ff ), dropout ) # \u7f16\u7801\u5668\u4e2d\u7f16\u7801\u5668\u5c42\u7684\u4e2a\u6570N N = 6 mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u5bf9\u8c61 en = Encoder ( layer , N ) en_result = en ( x , mask ) print ( 'en_result.shape--->' , en_result . shape ) print ( 'en_result--->' , en_result ) \u8f93\u51fa\u6548\u679c en_result.shape---> torch.Size([2, 4, 512]) en_result---> tensor([[[-0.2184, 0.0614, -0.6718, ..., -0.3551, 1.0668, 1.4026], [ 0.7157, -0.0899, 0.0247, ..., -0.0708, 0.4524, 0.2722], [ 0.0519, 1.5825, 1.0757, ..., -0.8435, -0.0662, 0.6865], [-0.0924, 0.0881, -0.1037, ..., 1.4178, -0.0214, 0.5966]], [[-1.4012, 2.1713, 1.6771, ..., -0.0964, 0.7202, 0.0828], [ 0.1039, 1.8749, 0.0414, ..., 0.5602, 2.9122, 0.0356], [-0.1112, -0.5311, 0.4800, ..., -0.0533, -0.8752, 0.5790], [ 0.6887, -0.9975, 0.0244, ..., -0.2390, -0.9284, 0.8737]]], grad_fn=<AddBackward0>) 9.3 \u7f16\u7801\u5668\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u7f16\u7801\u5668\u7684\u4f5c\u7528: \u7f16\u7801\u5668\u7528\u4e8e\u5bf9\u8f93\u5165\u8fdb\u884c\u6307\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u4e5f\u79f0\u4e3a\u7f16\u7801, \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7f16\u7801\u5668\u7684\u7c7b: Encoder \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u53c2\u6570\u6709\u4e24\u4e2a\uff0c\u5206\u522b\u662flayer\u548cN\uff0c\u4ee3\u8868\u7f16\u7801\u5668\u5c42\u548c\u7f16\u7801\u5668\u5c42\u7684\u4e2a\u6570. forward\u51fd\u6570\u7684\u8f93\u5165\u53c2\u6570\u4e5f\u6709\u4e24\u4e2a, \u548c\u7f16\u7801\u5668\u5c42\u7684forward\u76f8\u540c, x\u4ee3\u8868\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa, mask\u4ee3\u7801\u63a9\u7801\u5f20\u91cf. \u7f16\u7801\u5668\u7c7b\u7684\u8f93\u51fa\u5c31\u662fTransformer\u4e2d\u7f16\u7801\u5668\u7684\u7279\u5f81\u63d0\u53d6\u8868\u793a, \u5b83\u5c06\u6210\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u7684\u4e00\u90e8\u5206.","title":"4 \u7f16\u7801\u5668\u90e8\u5206\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#_1","text":"\u4e86\u89e3\u7f16\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u4f5c\u7528. \u638c\u63e1\u7f16\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u5b9e\u73b0\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#1","text":"\u7f16\u7801\u5668\u90e8\u5206: * \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 * \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u7531\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 * \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 * \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5","title":"1 \u7f16\u7801\u5668\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#2","text":"","title":"2 \u63a9\u7801\u5f20\u91cf"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#21","text":"\u63a9\u4ee3\u8868\u906e\u63a9\uff0c\u7801\u5c31\u662f\u6211\u4eec\u5f20\u91cf\u4e2d\u7684\u6570\u503c\uff0c\u5b83\u7684\u5c3a\u5bf8\u4e0d\u5b9a\uff0c\u91cc\u9762\u4e00\u822c\u53ea\u67091\u548c0\u7684\u5143\u7d20\uff0c\u4ee3\u8868\u4f4d\u7f6e\u88ab\u906e\u63a9\u6216\u8005\u4e0d\u88ab\u906e\u63a9\uff0c\u81f3\u4e8e\u662f0\u4f4d\u7f6e\u88ab\u906e\u63a9\u8fd8\u662f1\u4f4d\u7f6e\u88ab\u906e\u63a9\u53ef\u4ee5\u81ea\u5b9a\u4e49\uff0c\u56e0\u6b64\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u8ba9\u53e6\u5916\u4e00\u4e2a\u5f20\u91cf\u4e2d\u7684\u4e00\u4e9b\u6570\u503c\u88ab\u906e\u63a9\uff0c\u4e5f\u53ef\u4ee5\u8bf4\u88ab\u66ff\u6362, \u5b83\u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e00\u4e2a\u5f20\u91cf.","title":"2.1 \u63a9\u7801\u5f20\u91cf\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#22","text":"\u5728transformer\u4e2d, \u63a9\u7801\u5f20\u91cf\u7684\u4e3b\u8981\u4f5c\u7528\u5728\u5e94\u7528attention(\u5c06\u5728\u4e0b\u4e00\u5c0f\u8282\u8bb2\u89e3)\u65f6\uff0c\u6709\u4e00\u4e9b\u751f\u6210\u7684attention\u5f20\u91cf\u4e2d\u7684\u503c\u8ba1\u7b97\u6709\u53ef\u80fd\u5df2\u77e5\u4e86\u672a\u6765\u4fe1\u606f\u800c\u5f97\u5230\u7684\uff0c\u672a\u6765\u4fe1\u606f\u88ab\u770b\u5230\u662f\u56e0\u4e3a\u8bad\u7ec3\u65f6\u4f1a\u628a\u6574\u4e2a\u8f93\u51fa\u7ed3\u679c\u90fd\u4e00\u6b21\u6027\u8fdb\u884cEmbedding\uff0c\u4f46\u662f\u7406\u8bba\u4e0a\u89e3\u7801\u5668\u7684\u7684\u8f93\u51fa\u5374\u4e0d\u662f\u4e00\u6b21\u5c31\u80fd\u4ea7\u751f\u6700\u7ec8\u7ed3\u679c\u7684\uff0c\u800c\u662f\u4e00\u6b21\u6b21\u901a\u8fc7\u4e0a\u4e00\u6b21\u7ed3\u679c\u7efc\u5408\u5f97\u51fa\u7684\uff0c\u56e0\u6b64\uff0c\u672a\u6765\u7684\u4fe1\u606f\u53ef\u80fd\u88ab\u63d0\u524d\u5229\u7528. \u6240\u4ee5\uff0c\u6211\u4eec\u4f1a\u8fdb\u884c\u906e\u63a9. \u5173\u4e8e\u89e3\u7801\u5668\u7684\u6709\u5173\u77e5\u8bc6\u5c06\u5728\u540e\u9762\u7684\u7ae0\u8282\u4e2d\u8bb2\u89e3.","title":"2.2 \u63a9\u7801\u5f20\u91cf\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#23","text":"\u4e0a\u4e09\u89d2\u77e9\u9635\u548cnp.triu\u51fd\u6570\u6f14\u793a # \u4e0a\u4e09\u89d2\u77e9\u9635\uff1a\u4e0b\u9762\u77e9\u9635\u4e2d0\u7ec4\u6210\u7684\u5f62\u72b6\u4e3a\u4e0a\u4e09\u89d2\u77e9\u9635 ''' [[[0. 1. 1. 1. 1.] [0. 0. 1. 1. 1.] [0. 0. 0. 1. 1.] [0. 0. 0. 0. 1.] [0. 0. 0. 0. 0.]]] # nn.triu()\u51fd\u6570\u529f\u80fd\u4ecb\u7ecd # def triu\uff08m, k\uff09 # m\uff1a\u8868\u793a\u4e00\u4e2a\u77e9\u9635 # K\uff1a\u8868\u793a\u5bf9\u89d2\u7ebf\u7684\u8d77\u59cb\u4f4d\u7f6e\uff08k\u53d6\u503c\u9ed8\u8ba4\u4e3a0\uff09 # return: \u8fd4\u56de\u51fd\u6570\u7684\u4e0a\u4e09\u89d2\u77e9\u9635 ''' def dm_test_nptriu (): # \u6d4b\u8bd5\u4ea7\u751f\u4e0a\u4e09\u89d2\u77e9\u9635 print ( np . triu ([[ 1 , 1 , 1 , 1 , 1 ], [ 2 , 2 , 2 , 2 , 2 ], [ 3 , 3 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 4 , 4 ], [ 5 , 5 , 5 , 5 , 5 ]], k = 1 )) print ( np . triu ([[ 1 , 1 , 1 , 1 , 1 ], [ 2 , 2 , 2 , 2 , 2 ], [ 3 , 3 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 4 , 4 ], [ 5 , 5 , 5 , 5 , 5 ]], k = 0 )) print ( np . triu ([[ 1 , 1 , 1 , 1 , 1 ], [ 2 , 2 , 2 , 2 , 2 ], [ 3 , 3 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 4 , 4 ], [ 5 , 5 , 5 , 5 , 5 ]], k =- 1 )) # \u7ed3\u679c\u8f93\u51fa\uff1a [[ 0 1 1 1 1 ] [ 0 0 2 2 2 ] [ 0 0 0 3 3 ] [ 0 0 0 0 4 ] [ 0 0 0 0 0 ]] [[ 1 1 1 1 1 ] [ 0 2 2 2 2 ] [ 0 0 3 3 3 ] [ 0 0 0 4 4 ] [ 0 0 0 0 5 ]] [[ 1 1 1 1 1 ] [ 2 2 2 2 2 ] [ 0 3 3 3 3 ] [ 0 0 4 4 4 ] [ 0 0 0 5 5 ]] \u751f\u6210\u63a9\u7801\u51fd\u6570 # \u4e0b\u4e09\u89d2\u77e9\u9635\u4f5c\u7528: \u751f\u6210\u5b57\u7b26\u65f6,\u5e0c\u671b\u6a21\u578b\u4e0d\u8981\u4f7f\u7528\u5f53\u524d\u5b57\u7b26\u548c\u540e\u9762\u7684\u5b57\u7b26\u3002 # \u4f7f\u7528\u906e\u63a9mask\uff0c\u9632\u6b62\u672a\u6765\u7684\u4fe1\u606f\u53ef\u80fd\u88ab\u63d0\u524d\u5229\u7528 # \u5b9e\u73b0\u65b9\u6cd5\uff1a 1 - \u4e0a\u4e09\u89d2\u77e9\u9635 # \u51fd\u6570 subsequent_mask \u5b9e\u73b0\u5206\u6790 # \u4ea7\u751f\u4e0a\u4e09\u89d2\u77e9\u9635 np.triu(m=np.ones((1, size, size)), k=1).astype('uint8') # \u8fd4\u56de\u4e0b\u4e09\u89d2\u77e9\u9635 torch.from_numpy(1 - my_mask ) def subsequent_mask ( size ): # \u4ea7\u751f\u4e0a\u4e09\u89d2\u77e9\u9635 \u4ea7\u751f\u4e00\u4e2a\u65b9\u9635 subsequent_mask = np . triu ( m = np . ones (( 1 , size , size )), k = 1 ) . astype ( 'uint8' ) # \u8fd4\u56de\u4e0b\u4e09\u89d2\u77e9\u9635 return torch . from_numpy ( 1 - subsequent_mask ) \u8c03\u7528 def dm_test_subsequent_mask (): # \u4ea7\u751f5*5\u7684\u4e0b\u4e09\u89d2\u77e9\u9635 size = 5 sm = subsequent_mask ( size ) print ( '\u4e0b\u4e09\u89d2\u77e9\u9635---> \\n ' , sm ) \u8f93\u51fa\u6548\u679c \u4e0b\u4e09\u89d2\u77e9\u9635 ---> tensor ([[[ 1 , 0 , 0 , 0 , 0 ], [ 1 , 1 , 0 , 0 , 0 ], [ 1 , 1 , 1 , 0 , 0 ], [ 1 , 1 , 1 , 1 , 0 ], [ 1 , 1 , 1 , 1 , 1 ]]], dtype = torch . uint8 )","title":"2.3 \u751f\u6210\u63a9\u7801\u5f20\u91cf\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#24","text":"plt . figure ( figsize = ( 5 , 5 )) plt . imshow ( subsequent_mask ( 20 )[ 0 ]) plt . show () \u8f93\u51fa\u6548\u679c: \u6548\u679c\u5206\u6790: \u901a\u8fc7\u89c2\u5bdf\u53ef\u89c6\u5316\u65b9\u9635, \u9ec4\u8272\u662f1\u7684\u90e8\u5206, \u8fd9\u91cc\u4ee3\u8868\u88ab\u906e\u63a9, \u7d2b\u8272\u4ee3\u8868\u6ca1\u6709\u88ab\u906e\u63a9\u7684\u4fe1\u606f, \u6a2a\u5750\u6807\u4ee3\u8868\u76ee\u6807\u8bcd\u6c47\u7684\u4f4d\u7f6e, \u7eb5\u5750\u6807\u4ee3\u8868\u53ef\u67e5\u770b\u7684\u4f4d\u7f6e; \u6211\u4eec\u770b\u5230, \u57280\u7684\u4f4d\u7f6e\u6211\u4eec\u4e00\u770b\u671b\u8fc7\u53bb\u90fd\u662f\u9ec4\u8272\u7684, \u90fd\u88ab\u906e\u4f4f\u4e86\uff0c1\u7684\u4f4d\u7f6e\u4e00\u773c\u671b\u8fc7\u53bb\u8fd8\u662f\u9ec4\u8272, \u8bf4\u660e\u7b2c\u4e00\u6b21\u8bcd\u8fd8\u6ca1\u6709\u4ea7\u751f, \u4ece\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u770b\u8fc7\u53bb, \u5c31\u80fd\u770b\u5230\u4f4d\u7f6e1\u7684\u8bcd, \u5176\u4ed6\u4f4d\u7f6e\u770b\u4e0d\u5230, \u4ee5\u6b64\u7c7b\u63a8.","title":"2.4 \u63a9\u7801\u5f20\u91cf\u7684\u53ef\u89c6\u5316"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#25","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u63a9\u7801\u5f20\u91cf: \u63a9\u4ee3\u8868\u906e\u63a9\uff0c\u7801\u5c31\u662f\u6211\u4eec\u5f20\u91cf\u4e2d\u7684\u6570\u503c\uff0c\u5b83\u7684\u5c3a\u5bf8\u4e0d\u5b9a\uff0c\u91cc\u9762\u4e00\u822c\u53ea\u67091\u548c0\u7684\u5143\u7d20\uff0c\u4ee3\u8868\u4f4d\u7f6e\u88ab\u906e\u63a9\u6216\u8005\u4e0d\u88ab\u906e\u63a9\uff0c\u81f3\u4e8e\u662f0\u4f4d\u7f6e\u88ab\u906e\u63a9\u8fd8\u662f1\u4f4d\u7f6e\u88ab\u906e\u63a9\u53ef\u4ee5\u81ea\u5b9a\u4e49\uff0c\u56e0\u6b64\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u8ba9\u53e6\u5916\u4e00\u4e2a\u5f20\u91cf\u4e2d\u7684\u4e00\u4e9b\u6570\u503c\u88ab\u906e\u63a9, \u4e5f\u53ef\u4ee5\u8bf4\u88ab\u66ff\u6362, \u5b83\u7684\u8868\u73b0\u5f62\u5f0f\u662f\u4e00\u4e2a\u5f20\u91cf. \u5b66\u4e60\u4e86\u63a9\u7801\u5f20\u91cf\u7684\u4f5c\u7528: \u5728transformer\u4e2d, \u63a9\u7801\u5f20\u91cf\u7684\u4e3b\u8981\u4f5c\u7528\u5728\u5e94\u7528attention(\u5c06\u5728\u4e0b\u4e00\u5c0f\u8282\u8bb2\u89e3)\u65f6\uff0c\u6709\u4e00\u4e9b\u751f\u6210\u7684attetion\u5f20\u91cf\u4e2d\u7684\u503c\u8ba1\u7b97\u6709\u53ef\u80fd\u5df2\u77e5\u91cf\u672a\u6765\u4fe1\u606f\u800c\u5f97\u5230\u7684\uff0c\u672a\u6765\u4fe1\u606f\u88ab\u770b\u5230\u662f\u56e0\u4e3a\u8bad\u7ec3\u65f6\u4f1a\u628a\u6574\u4e2a\u8f93\u51fa\u7ed3\u679c\u90fd\u4e00\u6b21\u6027\u8fdb\u884cEmbedding\uff0c\u4f46\u662f\u7406\u8bba\u4e0a\u89e3\u7801\u5668\u7684\u7684\u8f93\u51fa\u5374\u4e0d\u662f\u4e00\u6b21\u5c31\u80fd\u4ea7\u751f\u6700\u7ec8\u7ed3\u679c\u7684\uff0c\u800c\u662f\u4e00\u6b21\u6b21\u901a\u8fc7\u4e0a\u4e00\u6b21\u7ed3\u679c\u7efc\u5408\u5f97\u51fa\u7684\uff0c\u56e0\u6b64\uff0c\u672a\u6765\u7684\u4fe1\u606f\u53ef\u80fd\u88ab\u63d0\u524d\u5229\u7528. \u6240\u4ee5\uff0c\u6211\u4eec\u4f1a\u8fdb\u884c\u906e\u63a9. \u5173\u4e8e\u89e3\u7801\u5668\u7684\u6709\u5173\u77e5\u8bc6\u5c06\u5728\u540e\u9762\u7684\u7ae0\u8282\u4e2d\u8bb2\u89e3. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u751f\u6210\u5411\u540e\u906e\u63a9\u7684\u63a9\u7801\u5f20\u91cf\u51fd\u6570: subsequent_mask \u5b83\u7684\u8f93\u5165\u662fsize, \u4ee3\u8868\u63a9\u7801\u5f20\u91cf\u7684\u5927\u5c0f. \u5b83\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u6700\u540e\u4e24\u7ef4\u5f62\u62101\u65b9\u9635\u7684\u4e0b\u4e09\u89d2\u9635. \u6700\u540e\u5bf9\u751f\u6210\u7684\u63a9\u7801\u5f20\u91cf\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u5206\u6790, \u66f4\u6df1\u4e00\u6b65\u7406\u89e3\u4e86\u5b83\u7684\u7528\u9014.","title":"2.5 \u63a9\u7801\u5f20\u91cf\u603b\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#3","text":"\u6211\u4eec\u8fd9\u91cc\u4f7f\u7528\u7684\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u89c4\u5219: $$ Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V $$","title":"3 \u6ce8\u610f\u529b\u673a\u5236"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#31","text":"# \u81ea\u6ce8\u610f\u529b\u673a\u5236\u51fd\u6570attention \u5b9e\u73b0\u601d\u8def\u5206\u6790 # attention(query, key, value, mask=None, dropout=None) # 1 \u6c42\u67e5\u8be2\u5f20\u91cf\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0f d_k # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03socres q@k^T /math.sqrt(d_k) # \u5f62\u72b6[2,4,512] @ [2,512,4] --->[2,4,4] # 3 \u662f\u5426\u5bf9\u6743\u91cd\u5206\u5e03scores\u8fdb\u884c scores.masked_fill(mask == 0, -1e9) # 4 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03 p_attn F.softmax() # 5 \u662f\u5426\u5bf9p_attn\u8fdb\u884cdropout if dropout is not None: # 6 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a [2,4,4]@[2,4,512] --->[2,4,512] # 7 \u8fd4\u56deq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a q\u7684\u6743\u91cd\u5206\u5e03 def attention ( query , key , value , mask = None , dropout = None ): # query, key, value\uff1a\u4ee3\u8868\u6ce8\u610f\u529b\u7684\u4e09\u4e2a\u8f93\u5165\u5f20\u91cf # mask\uff1a\u4ee3\u8868\u63a9\u7801\u5f20\u91cf # dropout\uff1a\u4f20\u5165\u7684dropout\u5b9e\u4f8b\u5316\u5bf9\u8c61 # 1 \u6c42\u67e5\u8be2\u5f20\u91cf\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0f d_k = query . size ()[ - 1 ] # 2 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03socres q@k^T /math.sqrt(d_k) # [2,4,512] @ [2,512,4] --->[2,4,4] scores = torch . matmul ( query , key . transpose ( - 2 , - 1 ) ) / math . sqrt ( d_k ) # 3 \u662f\u5426\u5bf9\u6743\u91cd\u5206\u5e03scores \u8fdb\u884c masked_fill if mask is not None : # \u6839\u636emask\u77e9\u96350\u7684\u4f4d\u7f6e \u5bf9sorces\u77e9\u9635\u5bf9\u5e94\u4f4d\u7f6e\u8fdb\u884c\u63a9\u7801 scores = scores . masked_fill ( mask == 0 , - 1e9 ) # 4 \u6c42\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03 softmax p_attn = F . softmax ( scores , dim =- 1 ) # 5 \u662f\u5426\u5bf9p_attn\u8fdb\u884cdropout if dropout is not None : p_attn = dropout ( p_attn ) # \u8fd4\u56de \u67e5\u8be2\u5f20\u91cfq\u7684\u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793a bmm-matmul\u8fd0\u7b97, \u6ce8\u610f\u529b\u67e5\u8be2\u5f20\u91cfq\u7684\u6743\u91cd\u5206\u5e03p_attn # [2,4,4]*[2,4,512] --->[2,4,512] return torch . matmul ( p_attn , value ), p_attn tensor.masked_fill\u6f14\u793a: >>> input = Variable ( torch . randn ( 5 , 5 )) >>> input Variable containing : 2.0344 - 0.5450 0.3365 - 0.1888 - 2.1803 1.5221 - 0.3823 0.8414 0.7836 - 0.8481 - 0.0345 - 0.8643 0.6476 - 0.2713 1.5645 0.8788 - 2.2142 0.4022 0.1997 0.1474 2.9109 0.6006 - 0.6745 - 1.7262 0.6977 [ torch . FloatTensor of size 5 x5 ] >>> mask = Variable ( torch . zeros ( 5 , 5 )) >>> mask Variable containing : 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [ torch . FloatTensor of size 5 x5 ] >>> input . masked_fill ( mask == 0 , - 1e9 ) Variable containing : - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 - 1.0000e+09 [ torch . FloatTensor of size 5 x5 ] \u8c03\u7528 def dm_test_attention (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) my_embeddings = Embeddings ( d_model , vocab ) x = my_embeddings ( x ) dropout = 0.1 # \u7f6e0\u6bd4\u7387\u4e3a0.1 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 my_pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = my_pe ( x ) query = key = value = pe_result # torch.Size([2, 4, 512]) attn1 , p_attn1 = attention ( query , key , value ) print ( '\u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u4e0d\u505a\u63a9\u7801' ) print ( '\u6ce8\u610f\u529b\u6743\u91cd p_attn1--->' , p_attn1 . shape , ' \\n ' , p_attn1 ) # torch.Size([2, 4, 4]) print ( '\u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn1--->' , attn1 . shape , ' \\n ' , attn1 ) # torch.Size([2, 4, 512]) print ( '*' * 50 ) print ( '\u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u505a\u63a9\u7801' ) mask = Variable ( torch . zeros ( 2 , 4 , 4 )) attn2 , p_attn2 = attention ( query , key , value , mask = mask ) print ( \"\u6ce8\u610f\u529b\u6743\u91cd p_attn2--->\" , p_attn2 . shape , ' \\n ' , p_attn2 ) print ( \"\u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn2--->\" , attn2 . shape , ' \\n ' , attn2 ) \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03\u4e0d\u505a\u63a9\u7801 \u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u4e0d\u505a\u63a9\u7801 \u6ce8\u610f\u529b\u6743\u91cd p_attn1 ---> torch . Size ([ 2 , 4 , 4 ]) tensor ([[[ 1. , 0. , 0. , 0. ], [ 0. , 1. , 0. , 0. ], [ 0. , 0. , 1. , 0. ], [ 0. , 0. , 0. , 1. ]], [[ 1. , 0. , 0. , 0. ], [ 0. , 1. , 0. , 0. ], [ 0. , 0. , 1. , 0. ], [ 0. , 0. , 0. , 1. ]]], grad_fn =< SoftmaxBackward0 > ) \u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn1 ---> torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 44.7449 , 54.3616 , 26.8261 , ... , 19.0635 , - 18.6284 , 31.5430 ], [ - 15.6625 , 22.7993 , 0.8864 , ... , 5.7670 , - 13.6669 , - 24.4659 ], [ 7.5418 , 37.0576 , - 16.9318 , ... , 44.9160 , 14.9246 , 3.9773 ], [ 12.6941 , 7.1106 , - 16.8938 , ... , 41.8852 , - 1.2939 , - 23.8751 ]], [[ 35.8076 , - 28.2593 , 0.0000 , ... , - 18.0751 , - 7.6109 , - 18.9212 ], [ 0.0000 , 13.4511 , 60.3647 , ... , - 3.1866 , - 30.1779 , 22.9219 ], [ - 24.6156 , 31.9683 , - 2.5262 , ... , - 24.2111 , - 2.0382 , 6.7247 ], [ 33.4411 , - 20.6284 , - 4.9740 , ... , 11.4844 , 0.0000 , 7.1890 ]]], grad_fn =< UnsafeViewBackward0 > )","title":"3.1 \u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#32-mask","text":"\u5e26\u6709mask\u7684\u8f93\u51fa\u6548\u679c ************************************************** \u7f16\u7801\u9636\u6bb5 \u5bf9\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 \u505a\u63a9\u7801 \u6ce8\u610f\u529b\u6743\u91cd p_attn2 ---> torch . Size ([ 2 , 4 , 4 ]) tensor ([[[ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ]], [[ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ], [ 0.2500 , 0.2500 , 0.2500 , 0.2500 ]]], grad_fn =< SoftmaxBackward0 > ) \u6ce8\u610f\u529b\u8868\u793a\u7ed3\u679c attn2 ---> torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ], [ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ], [ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ], [ 12.3296 , 30.3323 , - 1.5283 , ... , 27.9079 , - 4.6661 , - 3.2052 ]], [[ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ], [ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ], [ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ], [ 11.1583 , - 0.8671 , 13.2161 , ... , - 8.4971 , - 9.9567 , 4.4786 ]]], grad_fn =< UnsafeViewBackward0 > )","title":"3.2 \u5e26\u6709mask\u7684\u8f93\u5165\u53c2\u6570"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#33","text":"\u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u7684\u51fd\u6570: attention \u5b83\u7684\u8f93\u5165\u5c31\u662fQ\uff0cK\uff0cV\u4ee5\u53camask\u548cdropout, mask\u7528\u4e8e\u63a9\u7801, dropout\u7528\u4e8e\u968f\u673a\u7f6e0. \u5b83\u7684\u8f93\u51fa\u6709\u4e24\u4e2a, query\u7684\u6ce8\u610f\u529b\u8868\u793a\u4ee5\u53ca\u6ce8\u610f\u529b\u5f20\u91cf.","title":"3.3 \u6ce8\u610f\u529b\u673a\u5236\u603b\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#4","text":"","title":"4 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#41","text":"\u4ece\u591a\u5934\u6ce8\u610f\u529b\u7684\u7ed3\u6784\u56fe\u4e2d\uff0c\u8c8c\u4f3c\u8fd9\u4e2a\u6240\u8c13\u7684\u591a\u4e2a\u5934\u5c31\u662f\u6307\u591a\u7ec4\u7ebf\u6027\u53d8\u6362\u5c42\uff0c\u5176\u5b9e\u5e76\u4e0d\u662f\uff0c\u6211\u53ea\u6709\u4f7f\u7528\u4e86\u4e00\u7ec4\u7ebf\u6027\u53d8\u5316\u5c42\uff0c\u5373\u4e09\u4e2a\u53d8\u6362\u5f20\u91cf\u5bf9Q\uff0cK\uff0cV\u5206\u522b\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u8fd9\u4e9b\u53d8\u6362\u4e0d\u4f1a\u6539\u53d8\u539f\u6709\u5f20\u91cf\u7684\u5c3a\u5bf8\uff0c\u56e0\u6b64\u6bcf\u4e2a\u53d8\u6362\u77e9\u9635\u90fd\u662f\u65b9\u9635\uff0c\u5f97\u5230\u8f93\u51fa\u7ed3\u679c\u540e\uff0c\u591a\u5934\u7684\u4f5c\u7528\u624d\u5f00\u59cb\u663e\u73b0\uff0c\u6bcf\u4e2a\u5934\u5f00\u59cb\u4ece\u8bcd\u4e49\u5c42\u9762\u5206\u5272\u8f93\u51fa\u7684\u5f20\u91cf\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u5934\u90fd\u60f3\u83b7\u5f97\u4e00\u7ec4Q\uff0cK\uff0cV\u8fdb\u884c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\uff0c\u4f46\u662f\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u7684\u8868\u793a\u53ea\u83b7\u5f97\u4e00\u90e8\u5206\uff0c\u4e5f\u5c31\u662f\u53ea\u5206\u5272\u4e86\u6700\u540e\u4e00\u7ef4\u7684\u8bcd\u5d4c\u5165\u5411\u91cf. \u8fd9\u5c31\u662f\u6240\u8c13\u7684\u591a\u5934\uff0c\u5c06\u6bcf\u4e2a\u5934\u7684\u83b7\u5f97\u7684\u8f93\u5165\u9001\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d, \u5c31\u5f62\u6210\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236.","title":"4.1 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u6982\u5ff5"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#42","text":"","title":"4.2 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u6784\u56fe"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#43","text":"\u8fd9\u79cd\u7ed3\u6784\u8bbe\u8ba1\u80fd\u8ba9\u6bcf\u4e2a\u6ce8\u610f\u529b\u673a\u5236\u53bb\u4f18\u5316\u6bcf\u4e2a\u8bcd\u6c47\u7684\u4e0d\u540c\u7279\u5f81\u90e8\u5206\uff0c\u4ece\u800c\u5747\u8861\u540c\u4e00\u79cd\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u4ea7\u751f\u7684\u504f\u5dee\uff0c\u8ba9\u8bcd\u4e49\u62e5\u6709\u6765\u81ea\u66f4\u591a\u5143\u7684\u8868\u8fbe\uff0c\u5b9e\u9a8c\u8868\u660e\u53ef\u4ee5\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6548\u679c.","title":"4.3 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#44","text":"# \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7c7b MultiHeadedAttention \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, head, embedding_dim, dropout=0.1) # \u6bcf\u4e2a\u5934\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0fself.d_k \u591a\u5c11\u4e2a\u5934self.head \u7ebf\u6027\u5c42\u5217\u8868self.linears # self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4) # \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03self.attn=None dropout\u5c42self.dropout # 2 forward(self, query, key, value, mask=None) # 2-1 \u63a9\u7801\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6[8,4,4] -->[1,8,4,4] \u6c42\u591a\u5c11\u6279\u6b21batch_size # 2-2 \u6570\u636e\u7ecf\u8fc7\u7ebf\u6027\u5c42 \u5207\u62108\u4e2a\u5934,view(batch_size, -1, self.head, self.d_k), transpose(1,2)\u6570\u636e\u5f62\u72b6\u53d8\u5316 # \u6570\u636e\u5f62\u72b6\u53d8\u5316[2,4,512] ---> [2,4,8,64] ---> [2,8,4,64] # 2-3 24\u4e2a\u5934 \u4e00\u8d77\u9001\u5165\u5230attention\u51fd\u6570\u4e2d\u6c42 x, self.attn # attention([2,8,4,64],[2,8,4,64],[2,8,4,64],[1,8,4,4]) ==> x[2,8,4,64], self.attn[2,8,4,4]] # 2-4 \u6570\u636e\u5f62\u72b6\u518d\u53d8\u5316\u56de\u6765 x.transpose(1,2).contiguous().view(,,) # \u6570\u636e\u5f62\u72b6\u53d8\u5316 [2,8,4,64] ---> [2,4,8,64] ---> [2,4,512] # 2-5 \u8fd4\u56de\u6700\u540e\u7ebf\u6027\u5c42\u7ed3\u679c return self.linears[-1](x) # \u6df1\u5ea6copy\u6a21\u578b \u8f93\u5165\u6a21\u578b\u5bf9\u8c61\u548ccopy\u7684\u4e2a\u6570 \u5b58\u50a8\u5230\u6a21\u578b\u5217\u8868\u4e2d def clones ( module , N ): return nn . ModuleList ([ copy . deepcopy ( module ) for _ in range ( N )]) class MultiHeadedAttention ( nn . Module ): def __init__ ( self , head , embedding_dim , dropout = 0.1 ): super ( MultiHeadedAttention , self ) . __init__ () # \u786e\u8ba4\u6570\u636e\u7279\u5f81\u80fd\u5426\u88ab\u88ab\u6574\u9664 eg \u7279\u5f81\u5c3a\u5bf8256 % \u5934\u65708 assert embedding_dim % head == 0 # \u8ba1\u7b97\u6bcf\u4e2a\u5934\u7279\u5f81\u5c3a\u5bf8 \u7279\u5f81\u5c3a\u5bf8256 // \u5934\u65708 = 64 self . d_k = embedding_dim // head # \u591a\u5c11\u5934\u6570 self . head = head # \u56db\u4e2a\u7ebf\u6027\u5c42 self . linears = clones ( nn . Linear ( embedding_dim , embedding_dim ), 4 ) # \u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 self . attn = None # dropout\u5c42 self . dropout = nn . Dropout ( p = dropout ) def forward ( self , query , key , value , mask = None ): # \u82e5\u4f7f\u7528\u63a9\u7801\uff0c\u5219\u63a9\u7801\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6[8,4,4] -->[1,8,4,4] if mask is not None : mask = mask . unsqueeze ( 0 ) # \u6c42\u6570\u636e\u591a\u5c11\u884c eg:[2,4,512] \u5219batch_size=2 batch_size = query . size ()[ 0 ] # \u6570\u636e\u5f62\u72b6\u53d8\u5316[2,4,512] ---> [2,4,8,64] ---> [2,8,4,64] # 4\u4ee3\u88684\u4e2a\u5355\u8bcd 8\u4ee3\u88688\u4e2a\u5934 \u8ba9\u53e5\u5b50\u957f\u5ea64\u548c\u53e5\u5b50\u7279\u5f8164\u9760\u5728\u4e00\u8d77 \u66f4\u6709\u5229\u6355\u6349\u53e5\u5b50\u7279\u5f81 query , key , value = [ model ( x ) . view ( batch_size , - 1 , self . head , self . d_k ) . transpose ( 1 , 2 ) for model , x in zip ( self . linears , ( query , key , value ) ) ] # myoutptlist_data = [] # for model, x in zip(self.linears, (query, key, value)): # print('x--->', x.shape) # [2,4,512] # myoutput = model(x) # print('myoutput--->', myoutput.shape) # [2,4,512] # # [2,4,512] --> [2,4,8,64] --> [2,8,4,64] # tmpmyoutput = myoutput.view(batch_size, -1, self.head, self.d_k).transpose(1, 2) # myoutptlist_data.append( tmpmyoutput ) # mylen = len(myoutptlist_data) # mylen:3 # query = myoutptlist_data[0] # [2,8,4,64] # key = myoutptlist_data[1] # [2,8,4,64] # value = myoutptlist_data[2] # [2,8,4,64] # \u6ce8\u610f\u529b\u7ed3\u679c\u8868\u793ax\u5f62\u72b6 [2,8,4,64] \u6ce8\u610f\u529b\u6743\u91cdattn\u5f62\u72b6\uff1a[2,8,4,4] # attention([2,8,4,64],[2,8,4,64],[2,8,4,64],[1,8,4,4]) ==> x[2,8,4,64], self.attn[2,8,4,4]] x , self . attn = attention ( query , key , value , mask = mask , dropout = self . dropout ) # \u6570\u636e\u5f62\u72b6\u53d8\u5316 [2,8,4,64] ---> [2,4,8,64] ---> [2,4,512] x = x . transpose ( 1 , 2 ) . contiguous () . view ( batch_size , - 1 , self . head * self . d_k ) # \u8fd4\u56de\u6700\u540e\u53d8\u5316\u540e\u7684\u7ed3\u679c [2,4,512]---> [2,4,512] return self . linears [ - 1 ]( x ) tensor.view\u6f14\u793a: >>> x = torch . randn ( 4 , 4 ) >>> x . size () torch . Size ([ 4 , 4 ]) >>> y = x . view ( 16 ) >>> y . size () torch . Size ([ 16 ]) >>> z = x . view ( - 1 , 8 ) # the size -1 is inferred from other dimensions >>> z . size () torch . Size ([ 2 , 8 ]) >>> a = torch . randn ( 1 , 2 , 3 , 4 ) >>> a . size () torch . Size ([ 1 , 2 , 3 , 4 ]) >>> b = a . transpose ( 1 , 2 ) # Swaps 2nd and 3rd dimension >>> b . size () torch . Size ([ 1 , 3 , 2 , 4 ]) >>> c = a . view ( 1 , 3 , 2 , 4 ) # Does not change tensor layout in memory >>> c . size () torch . Size ([ 1 , 3 , 2 , 4 ]) >>> torch . equal ( b , c ) False torch.transpose\u6f14\u793a: >>> x = torch . randn ( 2 , 3 ) >>> x tensor ([[ 1.0028 , - 0.9893 , 0.5809 ], [ - 0.1669 , 0.7299 , 0.4942 ]]) >>> torch . transpose ( x , 0 , 1 ) tensor ([[ 1.0028 , - 0.1669 ], [ - 0.9893 , 0.7299 ], [ 0.5809 , 0.4942 ]]) \u51fd\u6570\u8c03\u7528 # \u6d4b\u8bd5\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 def dm_test_MultiHeadedAttention (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) my_embeddings = Embeddings ( d_model , vocab ) x = my_embeddings ( x ) dropout = 0.1 # \u7f6e0\u6bd4\u7387\u4e3a0.1 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 my_pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = my_pe ( x ) head = 8 # \u5934\u6570head query = key = value = pe_result # torch.Size([2, 4, 512]) # \u8f93\u5165\u7684\u63a9\u7801\u5f20\u91cfmask mask = Variable ( torch . zeros ( 8 , 4 , 4 )) my_mha = MultiHeadedAttention ( head , d_model , dropout ) x = my_mha ( query , key , value , mask ) print ( '\u591a\u5934\u6ce8\u610f\u673a\u5236\u540e\u7684x' , x . shape , ' \\n ' , x ) print ( '\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03' , my_mha . attn . shape ) \u8f93\u51fa\u6548\u679c \u591a\u5934\u6ce8\u610f\u673a\u5236\u540e\u7684x torch.Size([2, 4, 512]) tensor([[[-2.9384, 2.5006, -0.8888, ..., -6.1134, -6.5651, -5.7406], [-0.9007, 0.9144, -1.2935, ..., -6.6897, -6.7292, -6.2146], [-3.5213, 1.2106, -4.2973, ..., -5.6040, -7.7500, -2.3606], [-1.3711, 4.1226, -3.8623, ..., -6.0207, -8.6360, -4.6519]], [[ 6.1754, 3.4284, -5.4673, ..., -7.7355, -6.7766, -4.9681], [ 5.4382, 6.4217, -4.3761, ..., -8.3668, -3.1675, -6.6081], [ 9.0191, 3.2935, -4.4196, ..., -5.2750, -5.3374, -5.1187], [ 5.8635, 4.2653, -4.7956, ..., -9.4884, -8.6182, -4.5732]]], grad_fn=<AddBackward0>) \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03 torch.Size([2, 8, 4, 4])","title":"4.4 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4ee3\u7801\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#45","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236: \u6bcf\u4e2a\u5934\u5f00\u59cb\u4ece\u8bcd\u4e49\u5c42\u9762\u5206\u5272\u8f93\u51fa\u7684\u5f20\u91cf\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u5934\u90fd\u60f3\u83b7\u5f97\u4e00\u7ec4Q\uff0cK\uff0cV\u8fdb\u884c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\uff0c\u4f46\u662f\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u7684\u8868\u793a\u53ea\u83b7\u5f97\u4e00\u90e8\u5206\uff0c\u4e5f\u5c31\u662f\u53ea\u5206\u5272\u4e86\u6700\u540e\u4e00\u7ef4\u7684\u8bcd\u5d4c\u5165\u5411\u91cf. \u8fd9\u5c31\u662f\u6240\u8c13\u7684\u591a\u5934.\u5c06\u6bcf\u4e2a\u5934\u7684\u83b7\u5f97\u7684\u8f93\u5165\u9001\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d, \u5c31\u5f62\u6210\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236. \u5b66\u4e60\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f5c\u7528: \u8fd9\u79cd\u7ed3\u6784\u8bbe\u8ba1\u80fd\u8ba9\u6bcf\u4e2a\u6ce8\u610f\u529b\u673a\u5236\u53bb\u4f18\u5316\u6bcf\u4e2a\u8bcd\u6c47\u7684\u4e0d\u540c\u7279\u5f81\u90e8\u5206\uff0c\u4ece\u800c\u5747\u8861\u540c\u4e00\u79cd\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u4ea7\u751f\u7684\u504f\u5dee\uff0c\u8ba9\u8bcd\u4e49\u62e5\u6709\u6765\u81ea\u66f4\u591a\u5143\u7684\u8868\u8fbe\uff0c\u5b9e\u9a8c\u8868\u660e\u53ef\u4ee5\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6548\u679c. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u7c7b: MultiHeadedAttention \u56e0\u4e3a\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u9700\u8981\u4f7f\u7528\u591a\u4e2a\u76f8\u540c\u7684\u7ebf\u6027\u5c42, \u9996\u5148\u5b9e\u73b0\u4e86\u514b\u9686\u51fd\u6570clones. clones\u51fd\u6570\u7684\u8f93\u5165\u662fmodule\uff0cN\uff0c\u5206\u522b\u4ee3\u8868\u514b\u9686\u7684\u76ee\u6807\u5c42\uff0c\u548c\u514b\u9686\u4e2a\u6570. clones\u51fd\u6570\u7684\u8f93\u51fa\u662f\u88c5\u6709N\u4e2a\u514b\u9686\u5c42\u7684Module\u5217\u8868. \u63a5\u7740\u5b9e\u73b0MultiHeadedAttention\u7c7b, \u5b83\u7684\u521d\u59cb\u5316\u51fd\u6570\u8f93\u5165\u662fh, d_model, dropout\u5206\u522b\u4ee3\u8868\u5934\u6570\uff0c\u8bcd\u5d4c\u5165\u7ef4\u5ea6\u548c\u7f6e\u96f6\u6bd4\u7387. \u5b83\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u8f93\u5165\u662fQ, K, V\u4ee5\u53ca\u63a9\u7801\u5f20\u91cfmask. \u5b83\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u8f93\u51fa\u662f\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u7684Q\u7684\u6ce8\u610f\u529b\u8868\u793a.","title":"4.5 \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u603b\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#5","text":"","title":"5 \u524d\u9988\u5168\u8fde\u63a5\u5c42"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#51","text":"\u5728Transformer\u4e2d\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5c31\u662f\u5177\u6709\u4e24\u5c42\u7ebf\u6027\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u8003\u8651\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u5bf9\u590d\u6742\u8fc7\u7a0b\u7684\u62df\u5408\u7a0b\u5ea6\u4e0d\u591f, \u901a\u8fc7\u589e\u52a0\u4e24\u5c42\u7f51\u7edc\u6765\u589e\u5f3a\u6a21\u578b\u7684\u80fd\u529b.","title":"5.1 \u524d\u9988\u5168\u8fde\u63a5\u5c42"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#52","text":"# \u524d\u9988\u5168\u8fde\u63a5\u5c42 PositionwiseFeedForward \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, d_model, d_ff, dropout=0.1): # \u5b9a\u4e49\u7ebf\u6027\u5c42self.w1 self.w2, self.dropout\u5c42 # 2 forward(self, x) # \u6570\u636e\u7ecf\u8fc7self.w1(x) -> F.relu() ->self.dropout() ->self.w2 \u8fd4\u56de class PositionwiseFeedForward ( nn . Module ): def __init__ ( self , d_model , d_ff , dropout = 0.1 ): # d_model \u7b2c1\u4e2a\u7ebf\u6027\u5c42\u8f93\u5165\u7ef4\u5ea6 # d_ff \u7b2c2\u4e2a\u7ebf\u6027\u5c42\u8f93\u51fa\u7ef4\u5ea6 super ( PositionwiseFeedForward , self ) . __init__ () # \u5b9a\u4e49\u7ebf\u6027\u5c42w1 w2 dropout self . w1 = nn . Linear ( d_model , d_ff ) self . w2 = nn . Linear ( d_ff , d_model ) self . dropout = nn . Dropout ( p = dropout ) def forward ( self , x ): # \u6570\u636e\u4f9d\u6b21\u7ecf\u8fc7\u7b2c1\u4e2a\u7ebf\u6027\u5c42 relu\u6fc0\u6d3b\u5c42 dropout\u5c42\uff0c\u7136\u540e\u662f\u7b2c2\u4e2a\u7ebf\u6027\u5c42 return self . w2 ( self . dropout ( F . relu ( self . w1 ( x )))) ReLU\u51fd\u6570\u516c\u5f0f: ReLU(x)=max(0, x) ReLU\u51fd\u6570\u56fe\u50cf: \u51fd\u6570\u8c03\u7528 def dm_test_PositionwiseFeedForward (): d_model = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) my_embeddings = Embeddings ( d_model , vocab ) x = my_embeddings ( x ) dropout = 0.1 # \u7f6e0\u6bd4\u7387\u4e3a0.1 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 my_pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = my_pe ( x ) head = 8 # \u5934\u6570head query = key = value = pe_result # torch.Size([2, 4, 512]) # \u8f93\u5165\u7684\u63a9\u7801\u5f20\u91cfmask mask = Variable ( torch . zeros ( 8 , 4 , 4 )) my_mha = MultiHeadedAttention ( head , d_model , dropout ) x = my_mha ( query , key , value , mask ) # \u6d4b\u8bd5\u524d\u9988\u5168\u94fe\u63a5\u5c42 my_PFF = PositionwiseFeedForward ( d_model = 512 , d_ff = 64 , dropout = 0.1 ) ff_result = my_PFF ( x ) print ( 'x--->' , ff_result . shape , ff_result ) \u8f93\u51fa\u6548\u679c x ---> torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ - 0.1989 , 0.5191 , 1.3063 , ... , 0.1391 , - 0.8836 , 0.5450 ], [ - 0.2717 , 0.6541 , 0.9768 , ... , - 0.1452 , - 0.8929 , 0.9798 ], [ - 0.3297 , - 0.1791 , 0.8489 , ... , 0.6890 , - 1.0303 , 1.1638 ], [ 0.0308 , - 0.2209 , 1.3144 , ... , - 0.6433 , - 1.1207 , 0.6042 ]], [[ - 1.3265 , - 1.3563 , 0.6005 , ... , - 0.4166 , 0.1078 , - 0.0522 ], [ - 0.2736 , - 2.5544 , 1.3333 , ... , - 0.1704 , - 0.3514 , - 0.1901 ], [ - 0.0454 , - 1.1244 , 1.4875 , ... , - 0.5366 , - 0.0143 , 0.1453 ], [ - 1.2958 , - 1.6615 , 0.4268 , ... , - 0.5896 , 0.1486 , 0.1122 ]]], grad_fn =< AddBackward0 > )","title":"5.2 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#53","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42: \u5728Transformer\u4e2d\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5c31\u662f\u5177\u6709\u4e24\u5c42\u7ebf\u6027\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc. \u5b66\u4e60\u4e86\u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u8003\u8651\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u5bf9\u590d\u6742\u8fc7\u7a0b\u7684\u62df\u5408\u7a0b\u5ea6\u4e0d\u591f, \u901a\u8fc7\u589e\u52a0\u4e24\u5c42\u7f51\u7edc\u6765\u589e\u5f3a\u6a21\u578b\u7684\u80fd\u529b. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u7c7b: PositionwiseFeedForward \u5b83\u7684\u5b9e\u4f8b\u5316\u53c2\u6570\u4e3ad_model, d_ff, dropout, \u5206\u522b\u4ee3\u8868\u8bcd\u5d4c\u5165\u7ef4\u5ea6, \u7ebf\u6027\u53d8\u6362\u7ef4\u5ea6, \u548c\u7f6e\u96f6\u6bd4\u7387. \u5b83\u7684\u8f93\u5165\u53c2\u6570x, \u8868\u793a\u4e0a\u5c42\u7684\u8f93\u51fa. \u5b83\u7684\u8f93\u51fa\u662f\u7ecf\u8fc72\u5c42\u7ebf\u6027\u7f51\u7edc\u53d8\u6362\u7684\u7279\u5f81\u8868\u793a.","title":"5.3 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u603b\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#6","text":"","title":"6 \u89c4\u8303\u5316\u5c42"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#61","text":"\u5b83\u662f\u6240\u6709\u6df1\u5c42\u7f51\u7edc\u6a21\u578b\u90fd\u9700\u8981\u7684\u6807\u51c6\u7f51\u7edc\u5c42\uff0c\u56e0\u4e3a\u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u589e\u52a0\uff0c\u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u5f00\u59cb\u51fa\u73b0\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684\u60c5\u51b5\uff0c\u8fd9\u6837\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38\uff0c\u6a21\u578b\u53ef\u80fd\u6536\u655b\u975e\u5e38\u7684\u6162. \u56e0\u6b64\u90fd\u4f1a\u5728\u4e00\u5b9a\u5c42\u6570\u540e\u63a5\u89c4\u8303\u5316\u5c42\u8fdb\u884c\u6570\u503c\u7684\u89c4\u8303\u5316\uff0c\u4f7f\u5176\u7279\u5f81\u6570\u503c\u5728\u5408\u7406\u8303\u56f4\u5185.","title":"6.1 \u89c4\u8303\u5316\u5c42\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#62","text":"# \u89c4\u8303\u5316\u5c42 LayerNorm \u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, features, eps=1e-6): # \u5b9a\u4e49\u7ebf\u6027\u5c42self.a2 self.b2, nn.Parameter(torch.ones(features)) # 2 forward(self, x) \u8fd4\u56de\u6807\u51c6\u5316\u540e\u7684\u7ed3\u679c # \u5bf9\u6570\u636e\u6c42\u5747\u503c \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 x.mean(-1, keepdims=True) # \u5bf9\u6570\u636e\u6c42\u65b9\u5dee \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 x.std(-1, keepdims=True) # \u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316\u53d8\u6362 \u53cd\u5411\u4f20\u64ad\u53ef\u5b66\u4e60\u53c2\u6570a2 b2 # eg self.a2 * (x-mean)/(std + self.eps) + self.b2 class LayerNorm ( nn . Module ): def __init__ ( self , features , eps = 1e-6 ): # \u53c2\u6570features \u5f85\u89c4\u8303\u5316\u7684\u6570\u636e # \u53c2\u6570 eps=1e-6 \u9632\u6b62\u5206\u6bcd\u4e3a\u96f6 super ( LayerNorm , self ) . __init__ () # \u5b9a\u4e49a2 \u89c4\u8303\u5316\u5c42\u7684\u7cfb\u6570 y=kx+b\u4e2d\u7684k self . a2 = nn . Parameter ( torch . ones ( features )) # \u5b9a\u4e49b2 \u89c4\u8303\u5316\u5c42\u7684\u7cfb\u6570 y=kx+b\u4e2d\u7684b self . b2 = nn . Parameter ( torch . zeros ( features )) self . eps = eps def forward ( self , x ): # \u5bf9\u6570\u636e\u6c42\u5747\u503c \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 # [2,4,512] -> [2,4,1] mean = x . mean ( - 1 , keepdims = True ) # \u5bf9\u6570\u636e\u6c42\u65b9\u5dee \u4fdd\u6301\u5f62\u72b6\u4e0d\u53d8 # [2,4,512] -> [2,4,1] std = x . std ( - 1 , keepdims = True ) # \u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316\u53d8\u6362 \u53cd\u5411\u4f20\u64ad\u53ef\u5b66\u4e60\u53c2\u6570a2 b2 # \u6ce8\u610f * \u8868\u793a\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u4e58 \u4e0d\u662f\u77e9\u9635\u8fd0\u7b97 y = self . a2 * ( x - mean ) / ( std + self . eps ) + self . b2 return y \u51fd\u6570\u8c03\u7528 # \u89c4\u8303\u5316\u5c42\u6d4b\u8bd5 def dm_test_LayerNorm (): embedding_dim = 512 # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u662f512\u7ef4 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( embedding_dim , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( embedding_dim , dropout , max_len ) pe_result = pe ( x ) query = key = value = pe_result # torch.Size([2, 4, 512]) # \u8c03\u7528\u9a8c\u8bc1 d_ff = 64 head = 8 # \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f93\u51fa \u4f5c\u4e3a\u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u5165 mask = Variable ( torch . zeros ( 8 , 4 , 4 )) mha = MultiHeadedAttention ( head , embedding_dim , dropout ) mha_result = mha ( query , key , value , mask ) x = mha_result ff = PositionwiseFeedForward ( embedding_dim , d_ff , dropout ) ff_result = ff ( x ) features = d_model = 512 eps = 1e-6 x = ff_result ln = LayerNorm ( features , eps ) ln_result = ln ( x ) print ( '\u89c4\u8303\u5316\u5c42:' , ln_result . shape , ln_result ) \u8f93\u51fa\u6548\u679c \u89c4\u8303\u5316\u5c42 : torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 1.1413 , - 0.0875 , 1.9878 , ... , 0.4824 , 1.2250 , - 0.5582 ], [ 0.3969 , 0.0417 , 0.6030 , ... , 0.6712 , 0.0858 , - 0.7419 ], [ 0.1618 , - 0.4729 , 1.1678 , ... , - 0.4206 , 0.2535 , 1.0424 ], [ 0.2952 , - 0.1489 , 0.7079 , ... , 0.5554 , 0.3931 , 0.4711 ]], [[ 0.8428 , 0.9732 , - 1.2423 , ... , - 1.1651 , - 1.3559 , 1.0449 ], [ 1.4975 , - 0.2760 , - 0.9415 , ... , - 0.2475 , - 1.1027 , 0.8396 ], [ 0.5669 , 1.0264 , - 0.6982 , ... , - 0.5022 , - 0.7629 , 0.7721 ], [ 1.2806 , - 0.3767 , - 0.0539 , ... , - 0.4042 , - 0.4116 , 0.3944 ]]], grad_fn =< AddBackward0 > )","title":"6.2 \u89c4\u8303\u5316\u5c42\u7684\u4ee3\u7801\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#63","text":"\u5b66\u4e60\u4e86\u89c4\u8303\u5316\u5c42\u7684\u4f5c\u7528: \u5b83\u662f\u6240\u6709\u6df1\u5c42\u7f51\u7edc\u6a21\u578b\u90fd\u9700\u8981\u7684\u6807\u51c6\u7f51\u7edc\u5c42\uff0c\u56e0\u4e3a\u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u589e\u52a0\uff0c\u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u5f00\u59cb\u51fa\u73b0\u8fc7\u5927\u6216\u8fc7\u5c0f\u7684\u60c5\u51b5\uff0c\u8fd9\u6837\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38\uff0c\u6a21\u578b\u53ef\u80fd\u6536\u655b\u975e\u5e38\u7684\u6162. \u56e0\u6b64\u90fd\u4f1a\u5728\u4e00\u5b9a\u5c42\u6570\u540e\u63a5\u89c4\u8303\u5316\u5c42\u8fdb\u884c\u6570\u503c\u7684\u89c4\u8303\u5316\uff0c\u4f7f\u5176\u7279\u5f81\u6570\u503c\u5728\u5408\u7406\u8303\u56f4\u5185. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u89c4\u8303\u5316\u5c42\u7684\u7c7b: LayerNorm \u5b83\u7684\u5b9e\u4f8b\u5316\u53c2\u6570\u6709\u4e24\u4e2a, features\u548ceps\uff0c\u5206\u522b\u8868\u793a\u8bcd\u5d4c\u5165\u7279\u5f81\u5927\u5c0f\uff0c\u548c\u4e00\u4e2a\u8db3\u591f\u5c0f\u7684\u6570. \u5b83\u7684\u8f93\u5165\u53c2\u6570x\u4ee3\u8868\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa. \u5b83\u7684\u8f93\u51fa\u5c31\u662f\u7ecf\u8fc7\u89c4\u8303\u5316\u7684\u7279\u5f81\u8868\u793a.","title":"6.3 \u89c4\u8303\u5316\u5c42\u603b\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#7","text":"","title":"7 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#71","text":"\u5982\u56fe\u6240\u793a\uff0c\u8f93\u5165\u5230\u6bcf\u4e2a\u5b50\u5c42\u4ee5\u53ca\u89c4\u8303\u5316\u5c42\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u4f7f\u7528\u4e86\u6b8b\u5dee\u94fe\u63a5\uff08\u8df3\u8dc3\u8fde\u63a5\uff09\uff0c\u56e0\u6b64\u6211\u4eec\u628a\u8fd9\u4e00\u90e8\u5206\u7ed3\u6784\u6574\u4f53\u53eb\u505a\u5b50\u5c42\u8fde\u63a5\uff08\u4ee3\u8868\u5b50\u5c42\u53ca\u5176\u94fe\u63a5\u7ed3\u6784\uff09\uff0c\u5728\u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u4e2d\uff0c\u90fd\u6709\u4e24\u4e2a\u5b50\u5c42\uff0c\u8fd9\u4e24\u4e2a\u5b50\u5c42\u52a0\u4e0a\u5468\u56f4\u7684\u94fe\u63a5\u7ed3\u6784\u5c31\u5f62\u6210\u4e86\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784. \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u56fe:","title":"7.1 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#72","text":"# \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 \u5b50\u5c42(\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6216\u8005 \u6ce8\u610f\u529b\u673a\u5236\u5c42)+ norm\u5c42 + \u6b8b\u5dee\u8fde\u63a5 # SublayerConnection\u5b9e\u73b0\u601d\u8def\u5206\u6790 # 1 init\u51fd\u6570 (self, size, dropout=0.1): # \u5b9a\u4e49self.norm\u5c42 self.dropout\u5c42, \u5176\u4e2dLayerNorm(size) # 2 forward(self, x, sublayer) \u8fd4\u56de+\u4ee5\u540e\u7684\u7ed3\u679c # \u6570\u636eself.norm() -> sublayer()->self.dropout() + x class SublayerConnection ( nn . Module ): def __init__ ( self , size , dropout = 0.1 ): # \u53c2\u6570size \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u5c3a\u5bf8\u5927\u5c0f # \u53c2\u6570dropout \u7f6e\u96f6\u6bd4\u7387 super ( SublayerConnection , self ) . __init__ () # \u5b9a\u4e49norm\u5c42 self . norm = LayerNorm ( size ) # \u5b9a\u4e49dropout self . dropout = nn . Dropout ( dropout ) def forward ( self , x , sublayer ): # \u53c2\u6570x \u4ee3\u8868\u6570\u636e # sublayer \u51fd\u6570\u5165\u53e3\u5730\u5740 \u5b50\u5c42\u51fd\u6570(\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6216\u8005 \u6ce8\u610f\u529b\u673a\u5236\u5c42\u51fd\u6570\u7684\u5165\u53e3\u5730\u5740) # \u65b9\u5f0f1 # \u6570\u636eself.norm() -> sublayer()->self.dropout() + x myres = x + self . dropout ( sublayer ( self . norm ( x ))) # \u65b9\u5f0f2 # \u6570\u636esublayer() -> self.norm() ->self.dropout() + x # myres = x + self.dropout(self.norm(x.subtype(x))) return myres \u51fd\u6570\u8c03\u7528 def dm_test_SublayerConnection (): size = 512 head = 8 d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42 self_attn = MultiHeadedAttention ( head , d_model ) sublayer = lambda x : self_attn ( x , x , x , mask ) # \u5b50\u5c42\u94fe\u63a5\u7ed3\u6784 sc = SublayerConnection ( size , dropout ) sc_result = sc ( x , sublayer ) print ( 'sc_result.shape--->' , sc_result . shape ) print ( 'sc_result--->' , sc_result ) \u8f93\u51fa\u6548\u679c sc_result . shape ---> torch . Size ([ 2 , 4 , 512 ]) sc_result ---> tensor ([[[ - 30.8925 , 57.5868 , - 6.7073 , ... , 2.2304 , 0.0866 , - 25.0320 ], [ 19.7721 , - 0.2945 , - 10.9359 , ... , - 0.1355 , - 9.1049 , 35.7419 ], [ 0.1608 , 3.0822 , 0.1203 , ... , 2.9998 , 40.5865 , 12.3813 ], [ 0.0765 , 14.6370 , - 22.0670 , ... , 6.8273 , 0.2928 , 26.7776 ]], [[ - 0.2359 , - 0.0000 , - 26.8415 , ... , 10.3175 , - 25.3874 , 20.8764 ], [ 23.7864 , - 0.2481 , 51.0186 , ... , - 7.8931 , 9.0427 , - 2.3697 ], [ - 21.1101 , - 0.4014 , 37.0955 , ... , - 26.1717 , 35.2731 , - 37.8626 ], [ 7.5792 , 21.9032 , - 18.7778 , ... , 4.6249 , - 33.6907 , 22.5649 ]]], grad_fn =< AddBackward0 > )","title":"7.2 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#73","text":"\u4ec0\u4e48\u662f\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784: \u5982\u56fe\u6240\u793a\uff0c\u8f93\u5165\u5230\u6bcf\u4e2a\u5b50\u5c42\u4ee5\u53ca\u89c4\u8303\u5316\u5c42\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u4f7f\u7528\u4e86\u6b8b\u5dee\u94fe\u63a5\uff08\u8df3\u8dc3\u8fde\u63a5\uff09\uff0c\u56e0\u6b64\u6211\u4eec\u628a\u8fd9\u4e00\u90e8\u5206\u7ed3\u6784\u6574\u4f53\u53eb\u505a\u5b50\u5c42\u8fde\u63a5\uff08\u4ee3\u8868\u5b50\u5c42\u53ca\u5176\u94fe\u63a5\u7ed3\u6784\uff09, \u5728\u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u4e2d\uff0c\u90fd\u6709\u4e24\u4e2a\u5b50\u5c42\uff0c\u8fd9\u4e24\u4e2a\u5b50\u5c42\u52a0\u4e0a\u5468\u56f4\u7684\u94fe\u63a5\u7ed3\u6784\u5c31\u5f62\u6210\u4e86\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7684\u7c7b: SublayerConnection \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u8f93\u5165\u53c2\u6570\u662fsize, dropout, \u5206\u522b\u4ee3\u8868\u8bcd\u5d4c\u5165\u5927\u5c0f\u548c\u7f6e\u96f6\u6bd4\u7387. \u5b83\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u8f93\u5165\u53c2\u6570\u662fx, sublayer, \u5206\u522b\u4ee3\u8868\u4e0a\u4e00\u5c42\u8f93\u51fa\u4ee5\u53ca\u5b50\u5c42\u7684\u51fd\u6570\u8868\u793a. \u5b83\u7684\u8f93\u51fa\u5c31\u662f\u901a\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5904\u7406\u7684\u8f93\u51fa.","title":"7.3 \u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u603b\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#8","text":"","title":"8 \u7f16\u7801\u5668\u5c42"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#81","text":"\u4f5c\u4e3a\u7f16\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u5b8c\u6210\u4e00\u6b21\u5bf9\u8f93\u5165\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5373\u7f16\u7801\u8fc7\u7a0b. \u7f16\u7801\u5668\u5c42\u7684\u6784\u6210\u56fe:","title":"8.1 \u7f16\u7801\u5668\u5c42\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#82","text":"# \u7f16\u7801\u5668\u5c42\u7c7b EncoderLayer \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, size, self_attn, feed_forward, dropout): # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61self_attn # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61feed_forward size\u8bcd\u5d4c\u5165\u7ef4\u5ea6512 # clones\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self.sublayer = clones(SublayerConnection(size,dropout),2) # forward\u51fd\u6570 (self, x, mask) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67841 self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67842 self.sublayer[1](x, self.feed_forward) class EncoderLayer ( nn . Module ): def __init__ ( self , size , self_atten , feed_forward , dropout ): super ( EncoderLayer , self ) . __init__ () # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61 self . self_attn = self_atten # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61feed_forward self . feed_forward = feed_forward # size\u8bcd\u5d4c\u5165\u7ef4\u5ea6512 self . size = size # clones\u4e24\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self.sublayer = clones(SublayerConnection(size,dropout),2) self . sublayer = clones ( SublayerConnection ( size , dropout ) , 2 ) def forward ( self , x , mask ): # \u6570\u636e\u7ecf\u8fc7\u7b2c1\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 # \u53c2\u6570x\uff1a\u4f20\u5165\u7684\u6570\u636e \u53c2\u6570lambda x... : \u5b50\u51fd\u6570\u5165\u53e3\u5730\u5740 x = self . sublayer [ 0 ]( x , lambda x : self . self_attn ( x , x , x , mask )) # \u6570\u636e\u7ecf\u8fc7\u7b2c2\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 # \u53c2\u6570x\uff1a\u4f20\u5165\u7684\u6570\u636e self.feed_forward\u5b50\u51fd\u6570\u5165\u53e3\u5730\u5740 x = self . sublayer [ 1 ]( x , self . feed_forward ) return x \u51fd\u6570\u8c03\u7528 def dm_test_EncoderLayer (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result size = 512 head = 8 d_ff = 64 # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7c7b\u5bf9\u8c61 self_attn = MultiHeadedAttention ( head , d_model ) # \u5b9e\u4f8b\u5316\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61 ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) # mask\u6570\u636e mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u5c42\u5bf9\u8c61 my_encoderlayer = EncoderLayer ( size , self_attn , ff , dropout ) # \u6570\u636e\u901a\u8fc7\u7f16\u7801\u5c42\u7f16\u7801 el_result = my_encoderlayer ( x , mask ) print ( 'el_result.shape' , el_result . shape , el_result ) \u8f93\u51fa\u6548\u679c el_result . shape torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ 27.1315 , 64.8418 , - 10.6292 , ... , - 23.3170 , - 30.5543 , 13.2727 ], [ - 0.1474 , 54.1129 , 0.0000 , ... , - 0.1820 , - 35.7688 , - 15.1666 ], [ - 0.0691 , 8.3125 , 7.3380 , ... , 40.2273 , - 10.4544 , - 14.1511 ], [ 34.2015 , - 25.0465 , - 31.5629 , ... , - 42.4037 , - 35.9813 , 44.9897 ]], [[ - 8.8238 , 0.0935 , - 13.7027 , ... , - 20.9247 , - 19.9678 , - 0.1526 ], [ - 18.8739 , 0.3252 , 28.1221 , ... , 34.7250 , - 0.7414 , 8.1599 ], [ 52.2108 , - 0.6148 , - 16.3005 , ... , 3.1570 , - 15.0894 , 0.9009 ], [ - 22.5749 , - 54.0201 , 3.9647 , ... , 12.6702 , - 0.2983 , 13.6588 ]]], grad_fn =< AddBackward0 > )","title":"8.2 \u7f16\u7801\u5668\u5c42\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#83","text":"\u5b66\u4e60\u4e86\u7f16\u7801\u5668\u5c42\u7684\u4f5c\u7528: \u4f5c\u4e3a\u7f16\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u5b8c\u6210\u4e00\u6b21\u5bf9\u8f93\u5165\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u5373\u7f16\u7801\u8fc7\u7a0b. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7f16\u7801\u5668\u5c42\u7684\u7c7b: EncoderLayer \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u5171\u67094\u4e2a, \u522b\u662fsize\uff0c\u5176\u5b9e\u5c31\u662f\u6211\u4eec\u8bcd\u5d4c\u5165\u7ef4\u5ea6\u7684\u5927\u5c0f. \u7b2c\u4e8c\u4e2aself_attn\uff0c\u4e4b\u540e\u6211\u4eec\u5c06\u4f20\u5165\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u5b9e\u4f8b\u5316\u5bf9\u8c61, \u5e76\u4e14\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236. \u7b2c\u4e09\u4e2a\u662ffeed_froward, \u4e4b\u540e\u6211\u4eec\u5c06\u4f20\u5165\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5b9e\u4f8b\u5316\u5bf9\u8c61. \u6700\u540e\u4e00\u4e2a\u662f\u7f6e0\u6bd4\u7387dropout. \u5b9e\u4f8b\u5316\u5bf9\u8c61\u7684\u8f93\u5165\u53c2\u6570\u67092\u4e2a\uff0cx\u4ee3\u8868\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa, mask\u4ee3\u8868\u63a9\u7801\u5f20\u91cf. \u5b83\u7684\u8f93\u51fa\u4ee3\u8868\u7ecf\u8fc7\u6574\u4e2a\u7f16\u7801\u5c42\u7684\u7279\u5f81\u8868\u793a.","title":"8.3 \u7f16\u7801\u5668\u5c42\u603b\u7ed3"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#9","text":"","title":"9 \u7f16\u7801\u5668"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#91","text":"\u7f16\u7801\u5668\u7528\u4e8e\u5bf9\u8f93\u5165\u8fdb\u884c\u6307\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u4e5f\u79f0\u4e3a\u7f16\u7801, \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210. \u7f16\u7801\u5668\u7684\u7ed3\u6784\u56fe:","title":"9.1 \u7f16\u7801\u5668\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#92","text":"# \u7f16\u7801\u5668\u7c7b Encoder \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, layer, N) # \u5b9e\u4f8b\u5316\u591a\u4e2a\u7f16\u7801\u5668\u5c42\u5bf9\u8c61self.layers \u901a\u8fc7\u65b9\u6cd5clones(layer, N) # \u5b9e\u4f8b\u5316\u89c4\u8303\u5316\u5c42 self.norm = LayerNorm(layer.size) # forward\u51fd\u6570 (self, x, mask) # \u6570\u636e\u7ecf\u8fc7N\u4e2a\u5c42 x = layer(x, mask) # \u8fd4\u56de\u89c4\u8303\u5316\u540e\u7684\u6570\u636e return self.norm(x) class Encoder ( nn . Module ): def __init__ ( self , layer , N ): # \u53c2\u6570layer 1\u4e2a\u7f16\u7801\u5668\u5c42 # \u53c2\u6570 \u7f16\u7801\u5668\u5c42\u7684\u4e2a\u6570 super ( Encoder , self ) . __init__ () # \u5b9e\u4f8b\u5316\u591a\u4e2a\u7f16\u7801\u5668\u5c42\u5bf9\u8c61 self . layers = clones ( layer , N ) # \u5b9e\u4f8b\u5316\u89c4\u8303\u5316\u5c42 self . norm = LayerNorm ( layer . size ) def forward ( self , x , mask ): # \u6570\u636e\u7ecf\u8fc7N\u4e2a\u5c42 x = layer(x, mask) for layer in self . layers : x = layer ( x , mask ) # \u8fd4\u56de\u89c4\u8303\u5316\u540e\u7684\u6570\u636e return self.norm(x) return self . norm ( x ) \u51fd\u6570\u8c03\u7528 def dm_test_Encoder (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) # writeFile(\"dafdsafds\") emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result # \u83b7\u53d6\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42 \u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c size = 512 head = 8 d_model = 512 d_ff = 64 c = copy . deepcopy attn = MultiHeadedAttention ( head , d_model ) dropout = 0.2 ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) layer = EncoderLayer ( size , c ( attn ), c ( ff ), dropout ) # \u7f16\u7801\u5668\u4e2d\u7f16\u7801\u5668\u5c42\u7684\u4e2a\u6570N N = 6 mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # \u5b9e\u4f8b\u5316\u7f16\u7801\u5668\u5bf9\u8c61 en = Encoder ( layer , N ) en_result = en ( x , mask ) print ( 'en_result.shape--->' , en_result . shape ) print ( 'en_result--->' , en_result ) \u8f93\u51fa\u6548\u679c en_result.shape---> torch.Size([2, 4, 512]) en_result---> tensor([[[-0.2184, 0.0614, -0.6718, ..., -0.3551, 1.0668, 1.4026], [ 0.7157, -0.0899, 0.0247, ..., -0.0708, 0.4524, 0.2722], [ 0.0519, 1.5825, 1.0757, ..., -0.8435, -0.0662, 0.6865], [-0.0924, 0.0881, -0.1037, ..., 1.4178, -0.0214, 0.5966]], [[-1.4012, 2.1713, 1.6771, ..., -0.0964, 0.7202, 0.0828], [ 0.1039, 1.8749, 0.0414, ..., 0.5602, 2.9122, 0.0356], [-0.1112, -0.5311, 0.4800, ..., -0.0533, -0.8752, 0.5790], [ 0.6887, -0.9975, 0.0244, ..., -0.2390, -0.9284, 0.8737]]], grad_fn=<AddBackward0>)","title":"9.2 \u7f16\u7801\u5668\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#93","text":"\u5b66\u4e60\u4e86\u7f16\u7801\u5668\u7684\u4f5c\u7528: \u7f16\u7801\u5668\u7528\u4e8e\u5bf9\u8f93\u5165\u8fdb\u884c\u6307\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b, \u4e5f\u79f0\u4e3a\u7f16\u7801, \u7531N\u4e2a\u7f16\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7f16\u7801\u5668\u7684\u7c7b: Encoder \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u53c2\u6570\u6709\u4e24\u4e2a\uff0c\u5206\u522b\u662flayer\u548cN\uff0c\u4ee3\u8868\u7f16\u7801\u5668\u5c42\u548c\u7f16\u7801\u5668\u5c42\u7684\u4e2a\u6570. forward\u51fd\u6570\u7684\u8f93\u5165\u53c2\u6570\u4e5f\u6709\u4e24\u4e2a, \u548c\u7f16\u7801\u5668\u5c42\u7684forward\u76f8\u540c, x\u4ee3\u8868\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa, mask\u4ee3\u7801\u63a9\u7801\u5f20\u91cf. \u7f16\u7801\u5668\u7c7b\u7684\u8f93\u51fa\u5c31\u662fTransformer\u4e2d\u7f16\u7801\u5668\u7684\u7279\u5f81\u63d0\u53d6\u8868\u793a, \u5b83\u5c06\u6210\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u7684\u4e00\u90e8\u5206.","title":"9.3 \u7f16\u7801\u5668\u603b\u7ed3"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u89e3\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u4f5c\u7528. \u638c\u63e1\u89e3\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u5b9e\u73b0\u8fc7\u7a0b. 1 \u89e3\u7801\u5668\u4ecb\u7ecd \u00b6 \u89e3\u7801\u5668\u90e8\u5206: \u7531N\u4e2a\u89e3\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u7531\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u8bf4\u660e: \u89e3\u7801\u5668\u5c42\u4e2d\u7684\u5404\u4e2a\u90e8\u5206\uff0c\u5982\uff0c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89c4\u8303\u5316\u5c42\uff0c\u524d\u9988\u5168\u8fde\u63a5\u7f51\u7edc\uff0c\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u90fd\u4e0e\u7f16\u7801\u5668\u4e2d\u7684\u5b9e\u73b0\u76f8\u540c. \u56e0\u6b64\u8fd9\u91cc\u53ef\u4ee5\u76f4\u63a5\u62ff\u6765\u6784\u5efa\u89e3\u7801\u5668\u5c42. 2 \u89e3\u7801\u5668\u5c42 \u00b6 2.1 \u89e3\u7801\u5668\u5c42\u7684\u4f5c\u7528 \u00b6 \u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u6839\u636e\u7ed9\u5b9a\u7684\u8f93\u5165\u5411\u76ee\u6807\u65b9\u5411\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u64cd\u4f5c\uff0c\u5373\u89e3\u7801\u8fc7\u7a0b. 2.2 \u89e3\u7801\u5668\u5c42\u7684\u4ee3\u7801\u5b9e\u73b0 \u00b6 # \u89e3\u7801\u5668\u5c42\u7c7b DecoderLayer \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, size, self_attn, src_attn, feed_forward, dropout) # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u5c3a\u5bf8\u5927\u5c0fsize \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5c42\u5bf9\u8c61self_attn \u4e00\u822c\u6ce8\u610f\u529b\u673a\u5236\u5c42\u5bf9\u8c61src_attn \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61feed_forward # clones3\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self.sublayer = clones(SublayerConnection(size,dropout),3) # forward\u51fd\u6570 (self, x, memory, source_mask, target_mask) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67841 self.sublayer[0](x, lambda x:self.self_attn(x, x, x, target_mask)) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67842 self.sublayer[1](x, lambda x:self.src_attn(x, m, m, source_mask)) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67843 self.sublayer[2](x, self.feed_forward) class DecoderLayer ( nn . Module ): def __init__ ( self , size , self_attn , src_attn , feed_forward , dropout ): super ( DecoderLayer , self ) . __init__ () # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u5c3a\u5bf8\u5927\u5c0f self . size = size # \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5c42\u5bf9\u8c61 q=k=v self . self_attn = self_attn # \u4e00\u904d\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u8c61 q!=k=v self . src_attn = src_attn # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61 self . feed_forward = feed_forward # clones3\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self . sublayer = clones ( SublayerConnection ( size , dropout ), 3 ) def forward ( self , x , memory , source_mask , target_mask ): m = memory # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67841 x = self . sublayer [ 0 ]( x , lambda x : self . self_attn ( x , x , x , target_mask )) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67842 x = self . sublayer [ 1 ]( x , lambda x : self . src_attn ( x , m , m , source_mask )) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67843 x = self . sublayer [ 2 ]( x , self . feed_forward ) return x \u51fd\u6570\u8c03\u7528 def dm_test_DecoderLayer (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result # \u83b7\u53d6\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42 \u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c # \u7c7b\u7684\u5b9e\u4f8b\u5316\u53c2\u6570\u4e0e\u89e3\u7801\u5668\u5c42\u7c7b\u4f3c, \u76f8\u6bd4\u591a\u51fa\u4e86src_attn, \u4f46\u662f\u548cself_attn\u662f\u540c\u4e00\u4e2a\u7c7b. head = 8 d_ff = 64 size = 512 self_attn = src_attn = MultiHeadedAttention ( head , d_model , dropout ) # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u4e5f\u548c\u4e4b\u524d\u76f8\u540c ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) x = pe_result # \u4ea7\u751f\u7f16\u7801\u5668\u7ed3\u679c # \u6ce8\u610f\u6b64\u51fd\u6570\u8fd4\u56de\u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c \u8981\u6709\u8fd4\u56de\u503c en_result = dm_test_Encoder () memory = en_result mask = Variable ( torch . zeros ( 8 , 4 , 4 )) source_mask = target_mask = mask # \u5b9e\u4f8b\u5316\u89e3\u7801\u5668\u5c42 \u5bf9\u8c61 dl = DecoderLayer ( size , self_attn , src_attn , ff , dropout ) # \u5bf9\u8c61\u8c03\u7528 dl_result = dl ( x , memory , source_mask , target_mask ) print ( dl_result . shape ) print ( dl_result ) \u8f93\u51fa\u6548\u679c torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ - 27.4382 , 0.6516 , 6.6735 , ... , - 42.2930 , - 44.9728 , 0.1264 ], [ - 28.7835 , 26.4919 , - 0.5608 , ... , 0.5652 , - 2.9634 , 9.7438 ], [ - 19.6998 , 13.5164 , 45.8216 , ... , 23.9127 , 22.0259 , 34.0195 ], [ - 0.1647 , 0.2331 , - 36.4173 , ... , - 20.0557 , 29.4576 , 2.5048 ]], [[ 29.1466 , 50.7677 , 26.4624 , ... , - 39.1015 , - 27.9200 , 19.6819 ], [ - 10.7069 , 28.0897 , - 0.4107 , ... , - 35.7795 , 9.6881 , 0.3228 ], [ - 6.9027 , - 16.0590 , - 0.8897 , ... , 4.0253 , 2.5961 , 37.4659 ], [ 9.8892 , 32.7008 , - 6.6772 , ... , - 11.4273 , - 21.4676 , 32.5692 ]]], grad_fn =< AddBackward0 > ) 2.3 \u89e3\u7801\u5668\u5c42\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u89e3\u7801\u5668\u5c42\u7684\u4f5c\u7528: \u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u6839\u636e\u7ed9\u5b9a\u7684\u8f93\u5165\u5411\u76ee\u6807\u65b9\u5411\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u64cd\u4f5c\uff0c\u5373\u89e3\u7801\u8fc7\u7a0b. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u89e3\u7801\u5668\u5c42\u7684\u7c7b: DecoderLayer \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u7684\u53c2\u6570\u67095\u4e2a, \u5206\u522b\u662fsize\uff0c\u4ee3\u8868\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5927\u5c0f, \u540c\u65f6\u4e5f\u4ee3\u8868\u89e3\u7801\u5668\u5c42\u7684\u5c3a\u5bf8\uff0c\u7b2c\u4e8c\u4e2a\u662fself_attn\uff0c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5bf9\u8c61\uff0c\u4e5f\u5c31\u662f\u8bf4\u8fd9\u4e2a\u6ce8\u610f\u529b\u673a\u5236\u9700\u8981Q=K=V\uff0c\u7b2c\u4e09\u4e2a\u662fsrc_attn\uff0c\u591a\u5934\u6ce8\u610f\u529b\u5bf9\u8c61\uff0c\u8fd9\u91ccQ!=K=V\uff0c \u7b2c\u56db\u4e2a\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61\uff0c\u6700\u540e\u5c31\u662fdroupout\u7f6e0\u6bd4\u7387. forward\u51fd\u6570\u7684\u53c2\u6570\u67094\u4e2a\uff0c\u5206\u522b\u662f\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u5165x\uff0c\u6765\u81ea\u7f16\u7801\u5668\u5c42\u7684\u8bed\u4e49\u5b58\u50a8\u53d8\u91cfmermory\uff0c \u4ee5\u53ca\u6e90\u6570\u636e\u63a9\u7801\u5f20\u91cf\u548c\u76ee\u6807\u6570\u636e\u63a9\u7801\u5f20\u91cf. \u6700\u7ec8\u8f93\u51fa\u4e86\u7531\u7f16\u7801\u5668\u8f93\u5165\u548c\u76ee\u6807\u6570\u636e\u4e00\u540c\u4f5c\u7528\u7684\u7279\u5f81\u63d0\u53d6\u7ed3\u679c. 3 \u89e3\u7801\u5668 \u00b6 3.1 \u89e3\u7801\u5668\u7684\u4f5c\u7528 \u00b6 \u6839\u636e\u7f16\u7801\u5668\u7684\u7ed3\u679c\u4ee5\u53ca\u4e0a\u4e00\u6b21\u9884\u6d4b\u7684\u7ed3\u679c, \u5bf9\u4e0b\u4e00\u6b21\u53ef\u80fd\u51fa\u73b0\u7684'\u503c'\u8fdb\u884c\u7279\u5f81\u8868\u793a. 3.2 \u89e3\u7801\u5668\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u89e3\u7801\u5668\u7c7b Decoder \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, layer, N): # self.layers clones N\u4e2a\u89e3\u7801\u5668\u5c42clones(layer, N) # self.norm \u5b9a\u4e49\u89c4\u8303\u5316\u5c42 LayerNorm(layer.size) # forward\u51fd\u6570 (self, x, memory, source_mask, target_mask) # \u6570\u636e\u4ee5\u6b64\u7ecf\u8fc7\u5404\u4e2a\u5b50\u5c42 x = layer(x, memory, source_mask, target_mask) # \u6570\u636e\u6700\u540e\u7ecf\u8fc7\u89c4\u8303\u5316\u5c42 return self.norm(x) # \u8fd4\u56de\u5904\u7406\u597d\u7684\u6570\u636e class Decoder ( nn . Module ): def __init__ ( self , layer , N ): # \u53c2\u6570layer \u89e3\u7801\u5668\u5c42\u5bf9\u8c61 # \u53c2\u6570N \u89e3\u7801\u5668\u5c42\u5bf9\u8c61\u7684\u4e2a\u6570 super ( Decoder , self ) . __init__ () # clones N\u4e2a\u89e3\u7801\u5668\u5c42 self . layers = clones ( layer , N ) # \u5b9a\u4e49\u89c4\u8303\u5316\u5c42 self . norm = LayerNorm ( layer . size ) def forward ( self , x , memory , source_mask , target_mask ): # \u6570\u636e\u4ee5\u6b64\u7ecf\u8fc7\u5404\u4e2a\u5b50\u5c42 for layer in self . layers : x = layer ( x , memory , source_mask , target_mask ) # \u6570\u636e\u6700\u540e\u7ecf\u8fc7\u89c4\u8303\u5316\u5c42 return self . norm ( x ) \u51fd\u6570\u8c03\u7528 # \u6d4b\u8bd5 \u89e3\u7801\u5668 def dm_test_Decoder (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result # \u83b7\u53d6\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42 \u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c # \u5206\u522b\u662f\u89e3\u7801\u5668\u5c42layer\u548c\u89e3\u7801\u5668\u5c42\u7684\u4e2a\u6570N size = 512 d_model = 512 head = 8 d_ff = 64 dropout = 0.2 c = copy . deepcopy # \u591a\u5934\u6ce8\u610f\u529b\u5bf9\u8c61 attn = MultiHeadedAttention ( head , d_model ) # \u524d\u9988\u5168\u8fde\u63a5\u5c42 ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) # \u89e3\u7801\u5668\u5c42 layer = DecoderLayer ( d_model , c ( attn ), c ( attn ), c ( ff ), dropout ) N = 6 # \u8f93\u5165\u53c2\u6570\u4e0e\u89e3\u7801\u5668\u5c42\u7684\u8f93\u5165\u53c2\u6570\u76f8\u540c x = pe_result # \u4ea7\u751f\u7f16\u7801\u5668\u7ed3\u679c en_result = demo238_test_Encoder () memory = en_result # \u63a9\u7801\u5bf9\u8c61 mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # sorce\u63a9\u7801 target\u63a9\u7801 source_mask = target_mask = mask # \u521b\u5efa \u89e3\u7801\u5668 \u5bf9\u8c61 de = Decoder ( layer , N ) # \u89e3\u7801\u5668\u5bf9\u8c61 \u89e3\u7801 de_result = de ( x , memory , source_mask , target_mask ) print ( de_result ) print ( de_result . shape ) \u8f93\u51fa\u7ed3\u679c tensor ([[[ 0.1853 , - 0.8858 , - 0.0393 , ... , - 1.4989 , - 1.4008 , 0.8456 ], [ - 1.0841 , - 0.0777 , 0.0836 , ... , - 1.5568 , 1.4074 , - 0.0848 ], [ - 0.4107 , - 0.1306 , - 0.0069 , ... , - 0.2370 , - 0.1259 , 0.7591 ], [ 1.2895 , 0.2655 , 1.1799 , ... , - 0.2413 , 0.9087 , 0.4055 ]], [[ 0.3645 , - 0.3991 , - 1.2862 , ... , - 0.7078 , - 0.1457 , - 1.0457 ], [ 0.0146 , - 0.0639 , - 1.2143 , ... , - 0.7865 , - 0.1270 , 0.5623 ], [ 0.0685 , - 0.1465 , - 0.1354 , ... , 0.0738 , - 0.9769 , - 1.4295 ], [ 0.3168 , 0.6305 , - 0.1549 , ... , 1.0969 , 1.8775 , - 0.5154 ]]], grad_fn =< AddBackward0 > ) torch . Size ([ 2 , 4 , 512 ]) 3.3 \u89e3\u7801\u5668\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u89e3\u7801\u5668\u7684\u4f5c\u7528: \u6839\u636e\u7f16\u7801\u5668\u7684\u7ed3\u679c\u4ee5\u53ca\u4e0a\u4e00\u6b21\u9884\u6d4b\u7684\u7ed3\u679c, \u5bf9\u4e0b\u4e00\u6b21\u53ef\u80fd\u51fa\u73b0\u7684'\u503c'\u8fdb\u884c\u7279\u5f81\u8868\u793a. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u89e3\u7801\u5668\u7684\u7c7b: Decoder \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u7684\u53c2\u6570\u6709\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u5c31\u662f\u89e3\u7801\u5668\u5c42layer\uff0c\u7b2c\u4e8c\u4e2a\u662f\u89e3\u7801\u5668\u5c42\u7684\u4e2a\u6570N. forward\u51fd\u6570\u4e2d\u7684\u53c2\u6570\u67094\u4e2a\uff0cx\u4ee3\u8868\u76ee\u6807\u6570\u636e\u7684\u5d4c\u5165\u8868\u793a\uff0cmemory\u662f\u7f16\u7801\u5668\u5c42\u7684\u8f93\u51fa\uff0csrc_mask, tgt_mask\u4ee3\u8868\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u7684\u63a9\u7801\u5f20\u91cf. \u8f93\u51fa\u89e3\u7801\u8fc7\u7a0b\u7684\u6700\u7ec8\u7279\u5f81\u8868\u793a.","title":"5 \u89e3\u7801\u5668\u90e8\u5206\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#_1","text":"\u4e86\u89e3\u89e3\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u4f5c\u7528. \u638c\u63e1\u89e3\u7801\u5668\u4e2d\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u5b9e\u73b0\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#1","text":"\u89e3\u7801\u5668\u90e8\u5206: \u7531N\u4e2a\u89e3\u7801\u5668\u5c42\u5806\u53e0\u800c\u6210 \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u7531\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u7ec4\u6210 \u7b2c\u4e00\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e8c\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u7b2c\u4e09\u4e2a\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u5305\u62ec\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5b50\u5c42\u548c\u89c4\u8303\u5316\u5c42\u4ee5\u53ca\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5 \u8bf4\u660e: \u89e3\u7801\u5668\u5c42\u4e2d\u7684\u5404\u4e2a\u90e8\u5206\uff0c\u5982\uff0c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89c4\u8303\u5316\u5c42\uff0c\u524d\u9988\u5168\u8fde\u63a5\u7f51\u7edc\uff0c\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784\u90fd\u4e0e\u7f16\u7801\u5668\u4e2d\u7684\u5b9e\u73b0\u76f8\u540c. \u56e0\u6b64\u8fd9\u91cc\u53ef\u4ee5\u76f4\u63a5\u62ff\u6765\u6784\u5efa\u89e3\u7801\u5668\u5c42.","title":"1 \u89e3\u7801\u5668\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#2","text":"","title":"2 \u89e3\u7801\u5668\u5c42"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#21","text":"\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u6839\u636e\u7ed9\u5b9a\u7684\u8f93\u5165\u5411\u76ee\u6807\u65b9\u5411\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u64cd\u4f5c\uff0c\u5373\u89e3\u7801\u8fc7\u7a0b.","title":"2.1 \u89e3\u7801\u5668\u5c42\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#22","text":"# \u89e3\u7801\u5668\u5c42\u7c7b DecoderLayer \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, size, self_attn, src_attn, feed_forward, dropout) # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u5c3a\u5bf8\u5927\u5c0fsize \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5c42\u5bf9\u8c61self_attn \u4e00\u822c\u6ce8\u610f\u529b\u673a\u5236\u5c42\u5bf9\u8c61src_attn \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61feed_forward # clones3\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self.sublayer = clones(SublayerConnection(size,dropout),3) # forward\u51fd\u6570 (self, x, memory, source_mask, target_mask) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67841 self.sublayer[0](x, lambda x:self.self_attn(x, x, x, target_mask)) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67842 self.sublayer[1](x, lambda x:self.src_attn(x, m, m, source_mask)) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67843 self.sublayer[2](x, self.feed_forward) class DecoderLayer ( nn . Module ): def __init__ ( self , size , self_attn , src_attn , feed_forward , dropout ): super ( DecoderLayer , self ) . __init__ () # \u8bcd\u5d4c\u5165\u7ef4\u5ea6\u5c3a\u5bf8\u5927\u5c0f self . size = size # \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5c42\u5bf9\u8c61 q=k=v self . self_attn = self_attn # \u4e00\u904d\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u8c61 q!=k=v self . src_attn = src_attn # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61 self . feed_forward = feed_forward # clones3\u5b50\u5c42\u8fde\u63a5\u7ed3\u6784 self . sublayer = clones ( SublayerConnection ( size , dropout ), 3 ) def forward ( self , x , memory , source_mask , target_mask ): m = memory # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67841 x = self . sublayer [ 0 ]( x , lambda x : self . self_attn ( x , x , x , target_mask )) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67842 x = self . sublayer [ 1 ]( x , lambda x : self . src_attn ( x , m , m , source_mask )) # \u6570\u636e\u7ecf\u8fc7\u5b50\u5c42\u8fde\u63a5\u7ed3\u67843 x = self . sublayer [ 2 ]( x , self . feed_forward ) return x \u51fd\u6570\u8c03\u7528 def dm_test_DecoderLayer (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result # \u83b7\u53d6\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42 \u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c # \u7c7b\u7684\u5b9e\u4f8b\u5316\u53c2\u6570\u4e0e\u89e3\u7801\u5668\u5c42\u7c7b\u4f3c, \u76f8\u6bd4\u591a\u51fa\u4e86src_attn, \u4f46\u662f\u548cself_attn\u662f\u540c\u4e00\u4e2a\u7c7b. head = 8 d_ff = 64 size = 512 self_attn = src_attn = MultiHeadedAttention ( head , d_model , dropout ) # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u4e5f\u548c\u4e4b\u524d\u76f8\u540c ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) x = pe_result # \u4ea7\u751f\u7f16\u7801\u5668\u7ed3\u679c # \u6ce8\u610f\u6b64\u51fd\u6570\u8fd4\u56de\u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c \u8981\u6709\u8fd4\u56de\u503c en_result = dm_test_Encoder () memory = en_result mask = Variable ( torch . zeros ( 8 , 4 , 4 )) source_mask = target_mask = mask # \u5b9e\u4f8b\u5316\u89e3\u7801\u5668\u5c42 \u5bf9\u8c61 dl = DecoderLayer ( size , self_attn , src_attn , ff , dropout ) # \u5bf9\u8c61\u8c03\u7528 dl_result = dl ( x , memory , source_mask , target_mask ) print ( dl_result . shape ) print ( dl_result ) \u8f93\u51fa\u6548\u679c torch . Size ([ 2 , 4 , 512 ]) tensor ([[[ - 27.4382 , 0.6516 , 6.6735 , ... , - 42.2930 , - 44.9728 , 0.1264 ], [ - 28.7835 , 26.4919 , - 0.5608 , ... , 0.5652 , - 2.9634 , 9.7438 ], [ - 19.6998 , 13.5164 , 45.8216 , ... , 23.9127 , 22.0259 , 34.0195 ], [ - 0.1647 , 0.2331 , - 36.4173 , ... , - 20.0557 , 29.4576 , 2.5048 ]], [[ 29.1466 , 50.7677 , 26.4624 , ... , - 39.1015 , - 27.9200 , 19.6819 ], [ - 10.7069 , 28.0897 , - 0.4107 , ... , - 35.7795 , 9.6881 , 0.3228 ], [ - 6.9027 , - 16.0590 , - 0.8897 , ... , 4.0253 , 2.5961 , 37.4659 ], [ 9.8892 , 32.7008 , - 6.6772 , ... , - 11.4273 , - 21.4676 , 32.5692 ]]], grad_fn =< AddBackward0 > )","title":"2.2 \u89e3\u7801\u5668\u5c42\u7684\u4ee3\u7801\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#23","text":"\u5b66\u4e60\u4e86\u89e3\u7801\u5668\u5c42\u7684\u4f5c\u7528: \u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u7ec4\u6210\u5355\u5143, \u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u6839\u636e\u7ed9\u5b9a\u7684\u8f93\u5165\u5411\u76ee\u6807\u65b9\u5411\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u64cd\u4f5c\uff0c\u5373\u89e3\u7801\u8fc7\u7a0b. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u89e3\u7801\u5668\u5c42\u7684\u7c7b: DecoderLayer \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u7684\u53c2\u6570\u67095\u4e2a, \u5206\u522b\u662fsize\uff0c\u4ee3\u8868\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5927\u5c0f, \u540c\u65f6\u4e5f\u4ee3\u8868\u89e3\u7801\u5668\u5c42\u7684\u5c3a\u5bf8\uff0c\u7b2c\u4e8c\u4e2a\u662fself_attn\uff0c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5bf9\u8c61\uff0c\u4e5f\u5c31\u662f\u8bf4\u8fd9\u4e2a\u6ce8\u610f\u529b\u673a\u5236\u9700\u8981Q=K=V\uff0c\u7b2c\u4e09\u4e2a\u662fsrc_attn\uff0c\u591a\u5934\u6ce8\u610f\u529b\u5bf9\u8c61\uff0c\u8fd9\u91ccQ!=K=V\uff0c \u7b2c\u56db\u4e2a\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42\u5bf9\u8c61\uff0c\u6700\u540e\u5c31\u662fdroupout\u7f6e0\u6bd4\u7387. forward\u51fd\u6570\u7684\u53c2\u6570\u67094\u4e2a\uff0c\u5206\u522b\u662f\u6765\u81ea\u4e0a\u4e00\u5c42\u7684\u8f93\u5165x\uff0c\u6765\u81ea\u7f16\u7801\u5668\u5c42\u7684\u8bed\u4e49\u5b58\u50a8\u53d8\u91cfmermory\uff0c \u4ee5\u53ca\u6e90\u6570\u636e\u63a9\u7801\u5f20\u91cf\u548c\u76ee\u6807\u6570\u636e\u63a9\u7801\u5f20\u91cf. \u6700\u7ec8\u8f93\u51fa\u4e86\u7531\u7f16\u7801\u5668\u8f93\u5165\u548c\u76ee\u6807\u6570\u636e\u4e00\u540c\u4f5c\u7528\u7684\u7279\u5f81\u63d0\u53d6\u7ed3\u679c.","title":"2.3 \u89e3\u7801\u5668\u5c42\u603b\u7ed3"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#3","text":"","title":"3 \u89e3\u7801\u5668"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#31","text":"\u6839\u636e\u7f16\u7801\u5668\u7684\u7ed3\u679c\u4ee5\u53ca\u4e0a\u4e00\u6b21\u9884\u6d4b\u7684\u7ed3\u679c, \u5bf9\u4e0b\u4e00\u6b21\u53ef\u80fd\u51fa\u73b0\u7684'\u503c'\u8fdb\u884c\u7279\u5f81\u8868\u793a.","title":"3.1 \u89e3\u7801\u5668\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#32","text":"# \u89e3\u7801\u5668\u7c7b Decoder \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, layer, N): # self.layers clones N\u4e2a\u89e3\u7801\u5668\u5c42clones(layer, N) # self.norm \u5b9a\u4e49\u89c4\u8303\u5316\u5c42 LayerNorm(layer.size) # forward\u51fd\u6570 (self, x, memory, source_mask, target_mask) # \u6570\u636e\u4ee5\u6b64\u7ecf\u8fc7\u5404\u4e2a\u5b50\u5c42 x = layer(x, memory, source_mask, target_mask) # \u6570\u636e\u6700\u540e\u7ecf\u8fc7\u89c4\u8303\u5316\u5c42 return self.norm(x) # \u8fd4\u56de\u5904\u7406\u597d\u7684\u6570\u636e class Decoder ( nn . Module ): def __init__ ( self , layer , N ): # \u53c2\u6570layer \u89e3\u7801\u5668\u5c42\u5bf9\u8c61 # \u53c2\u6570N \u89e3\u7801\u5668\u5c42\u5bf9\u8c61\u7684\u4e2a\u6570 super ( Decoder , self ) . __init__ () # clones N\u4e2a\u89e3\u7801\u5668\u5c42 self . layers = clones ( layer , N ) # \u5b9a\u4e49\u89c4\u8303\u5316\u5c42 self . norm = LayerNorm ( layer . size ) def forward ( self , x , memory , source_mask , target_mask ): # \u6570\u636e\u4ee5\u6b64\u7ecf\u8fc7\u5404\u4e2a\u5b50\u5c42 for layer in self . layers : x = layer ( x , memory , source_mask , target_mask ) # \u6570\u636e\u6700\u540e\u7ecf\u8fc7\u89c4\u8303\u5316\u5c42 return self . norm ( x ) \u51fd\u6570\u8c03\u7528 # \u6d4b\u8bd5 \u89e3\u7801\u5668 def dm_test_Decoder (): d_model = 512 vocab = 1000 # \u8bcd\u8868\u5927\u5c0f\u662f1000 # \u8f93\u5165x \u662f\u4e00\u4e2a\u4f7f\u7528Variable\u5c01\u88c5\u7684\u957f\u6574\u578b\u5f20\u91cf, \u5f62\u72b6\u662f2 x 4 x = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) emb = Embeddings ( d_model , vocab ) embr = emb ( x ) dropout = 0.2 max_len = 60 # \u53e5\u5b50\u6700\u5927\u957f\u5ea6 x = embr # [2, 4, 512] pe = PositionalEncoding ( d_model , dropout , max_len ) pe_result = pe ( x ) x = pe_result # \u83b7\u53d6\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42 \u7f16\u7801\u4ee5\u540e\u7684\u7ed3\u679c # \u5206\u522b\u662f\u89e3\u7801\u5668\u5c42layer\u548c\u89e3\u7801\u5668\u5c42\u7684\u4e2a\u6570N size = 512 d_model = 512 head = 8 d_ff = 64 dropout = 0.2 c = copy . deepcopy # \u591a\u5934\u6ce8\u610f\u529b\u5bf9\u8c61 attn = MultiHeadedAttention ( head , d_model ) # \u524d\u9988\u5168\u8fde\u63a5\u5c42 ff = PositionwiseFeedForward ( d_model , d_ff , dropout ) # \u89e3\u7801\u5668\u5c42 layer = DecoderLayer ( d_model , c ( attn ), c ( attn ), c ( ff ), dropout ) N = 6 # \u8f93\u5165\u53c2\u6570\u4e0e\u89e3\u7801\u5668\u5c42\u7684\u8f93\u5165\u53c2\u6570\u76f8\u540c x = pe_result # \u4ea7\u751f\u7f16\u7801\u5668\u7ed3\u679c en_result = demo238_test_Encoder () memory = en_result # \u63a9\u7801\u5bf9\u8c61 mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # sorce\u63a9\u7801 target\u63a9\u7801 source_mask = target_mask = mask # \u521b\u5efa \u89e3\u7801\u5668 \u5bf9\u8c61 de = Decoder ( layer , N ) # \u89e3\u7801\u5668\u5bf9\u8c61 \u89e3\u7801 de_result = de ( x , memory , source_mask , target_mask ) print ( de_result ) print ( de_result . shape ) \u8f93\u51fa\u7ed3\u679c tensor ([[[ 0.1853 , - 0.8858 , - 0.0393 , ... , - 1.4989 , - 1.4008 , 0.8456 ], [ - 1.0841 , - 0.0777 , 0.0836 , ... , - 1.5568 , 1.4074 , - 0.0848 ], [ - 0.4107 , - 0.1306 , - 0.0069 , ... , - 0.2370 , - 0.1259 , 0.7591 ], [ 1.2895 , 0.2655 , 1.1799 , ... , - 0.2413 , 0.9087 , 0.4055 ]], [[ 0.3645 , - 0.3991 , - 1.2862 , ... , - 0.7078 , - 0.1457 , - 1.0457 ], [ 0.0146 , - 0.0639 , - 1.2143 , ... , - 0.7865 , - 0.1270 , 0.5623 ], [ 0.0685 , - 0.1465 , - 0.1354 , ... , 0.0738 , - 0.9769 , - 1.4295 ], [ 0.3168 , 0.6305 , - 0.1549 , ... , 1.0969 , 1.8775 , - 0.5154 ]]], grad_fn =< AddBackward0 > ) torch . Size ([ 2 , 4 , 512 ])","title":"3.2 \u89e3\u7801\u5668\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#33","text":"\u5b66\u4e60\u4e86\u89e3\u7801\u5668\u7684\u4f5c\u7528: \u6839\u636e\u7f16\u7801\u5668\u7684\u7ed3\u679c\u4ee5\u53ca\u4e0a\u4e00\u6b21\u9884\u6d4b\u7684\u7ed3\u679c, \u5bf9\u4e0b\u4e00\u6b21\u53ef\u80fd\u51fa\u73b0\u7684'\u503c'\u8fdb\u884c\u7279\u5f81\u8868\u793a. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u89e3\u7801\u5668\u7684\u7c7b: Decoder \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u7684\u53c2\u6570\u6709\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u5c31\u662f\u89e3\u7801\u5668\u5c42layer\uff0c\u7b2c\u4e8c\u4e2a\u662f\u89e3\u7801\u5668\u5c42\u7684\u4e2a\u6570N. forward\u51fd\u6570\u4e2d\u7684\u53c2\u6570\u67094\u4e2a\uff0cx\u4ee3\u8868\u76ee\u6807\u6570\u636e\u7684\u5d4c\u5165\u8868\u793a\uff0cmemory\u662f\u7f16\u7801\u5668\u5c42\u7684\u8f93\u51fa\uff0csrc_mask, tgt_mask\u4ee3\u8868\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e\u7684\u63a9\u7801\u5f20\u91cf. \u8f93\u51fa\u89e3\u7801\u8fc7\u7a0b\u7684\u6700\u7ec8\u7279\u5f81\u8868\u793a.","title":"3.3 \u89e3\u7801\u5668\u603b\u7ed3"},{"location":"04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u7ebf\u6027\u5c42\u548csoftmax\u7684\u4f5c\u7528. \u638c\u63e1\u7ebf\u6027\u5c42\u548csoftmax\u7684\u5b9e\u73b0\u8fc7\u7a0b. 1 \u8f93\u51fa\u90e8\u5206\u4ecb\u7ecd \u00b6 \u8f93\u51fa\u90e8\u5206\u5305\u542b: \u7ebf\u6027\u5c42 softmax\u5c42 2 \u7ebf\u6027\u5c42\u7684\u4f5c\u7528 \u00b6 \u901a\u8fc7\u5bf9\u4e0a\u4e00\u6b65\u7684\u7ebf\u6027\u53d8\u5316\u5f97\u5230\u6307\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa, \u4e5f\u5c31\u662f\u8f6c\u6362\u7ef4\u5ea6\u7684\u4f5c\u7528. 3 softmax\u5c42\u7684\u4f5c\u7528 \u00b6 \u4f7f\u6700\u540e\u4e00\u7ef4\u7684\u5411\u91cf\u4e2d\u7684\u6570\u5b57\u7f29\u653e\u52300-1\u7684\u6982\u7387\u503c\u57df\u5185, \u5e76\u6ee1\u8db3\u4ed6\u4eec\u7684\u548c\u4e3a1. 3.1 \u7ebf\u6027\u5c42\u548csoftmax\u5c42\u7684\u4ee3\u7801\u5206\u6790 \u00b6 # \u89e3\u7801\u5668\u7c7b Generator \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, d_model, vocab_size) # \u5b9a\u4e49\u7ebf\u6027\u5c42self.project # forward\u51fd\u6570 (self, x) # \u6570\u636e F.log_softmax(self.project(x), dim=-1) class Generator ( nn . Module ): def __init__ ( self , d_model , vocab_size ): # \u53c2\u6570d_model \u7ebf\u6027\u5c42\u8f93\u5165\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0f # \u53c2\u6570vocab_size \u7ebf\u5c42\u8f93\u51fa\u5c3a\u5bf8\u5927\u5c0f super ( Generator , self ) . __init__ () # \u5b9a\u4e49\u7ebf\u6027\u5c42 self . project = nn . Linear ( d_model , vocab_size ) def forward ( self , x ): # \u6570\u636e\u7ecf\u8fc7\u7ebf\u6027\u5c42 \u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u5f52\u4e00\u5316 log\u65b9\u5f0f x = F . log_softmax ( self . project ( x ), dim =- 1 ) return x nn.Linear\u6f14\u793a: >>> m = nn . Linear ( 20 , 30 ) >>> input = torch . randn ( 128 , 20 ) >>> output = m ( input ) >>> print ( output . size ()) torch . Size ([ 128 , 30 ]) \u51fd\u6570\u8c03\u7528 if __name__ == '__main__' : # \u5b9e\u4f8b\u5316output\u5c42\u5bf9\u8c61 d_model = 512 vocab_size = 1000 my_generator = Generator ( d_model , vocab_size ) # \u51c6\u5907\u6a21\u578b\u6570\u636e x = torch . randn ( 2 , 4 , 512 ) # \u6570\u636e\u7ecf\u8fc7out\u5c42 gen_result = my_generator ( x ) print ( 'gen_result--->' , gen_result . shape , ' \\n ' , gen_result ) \u8f93\u51fa\u6548\u679c gen_result ---> torch . Size ([ 2 , 4 , 1000 ]) tensor ([[[ - 6.5949 , - 7.0295 , - 6.5928 , ... , - 7.4317 , - 7.5488 , - 6.4871 ], [ - 7.0481 , - 6.2352 , - 7.2797 , ... , - 6.1491 , - 6.1621 , - 7.1798 ], [ - 8.1724 , - 7.0675 , - 8.2814 , ... , - 6.0033 , - 7.1100 , - 7.6844 ], [ - 6.2466 , - 6.6074 , - 6.1852 , ... , - 6.8373 , - 7.6600 , - 6.8578 ]], [[ - 7.7598 , - 7.4174 , - 6.2134 , ... , - 7.8000 , - 6.9862 , - 6.9261 ], [ - 6.4790 , - 7.5458 , - 6.2342 , ... , - 6.8340 , - 6.6827 , - 7.0287 ], [ - 7.2524 , - 7.2598 , - 7.0600 , ... , - 7.5680 , - 6.9492 , - 6.7689 ], [ - 6.6260 , - 6.1928 , - 6.7045 , ... , - 6.6323 , - 7.9005 , - 7.5397 ]]], grad_fn =< LogSoftmaxBackward0 > ) 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u8f93\u51fa\u90e8\u5206\u5305\u542b: \u7ebf\u6027\u5c42 softmax\u5c42 \u7ebf\u6027\u5c42\u7684\u4f5c\u7528: \u901a\u8fc7\u5bf9\u4e0a\u4e00\u6b65\u7684\u7ebf\u6027\u53d8\u5316\u5f97\u5230\u6307\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa, \u4e5f\u5c31\u662f\u8f6c\u6362\u7ef4\u5ea6\u7684\u4f5c\u7528. softmax\u5c42\u7684\u4f5c\u7528: \u4f7f\u6700\u540e\u4e00\u7ef4\u7684\u5411\u91cf\u4e2d\u7684\u6570\u5b57\u7f29\u653e\u52300-1\u7684\u6982\u7387\u503c\u57df\u5185, \u5e76\u6ee1\u8db3\u4ed6\u4eec\u7684\u548c\u4e3a1. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7ebf\u6027\u5c42\u548csoftmax\u5c42\u7684\u7c7b: Generator \u521d\u59cb\u5316\u51fd\u6570\u7684\u8f93\u5165\u53c2\u6570\u6709\u4e24\u4e2a, d_model\u4ee3\u8868\u8bcd\u5d4c\u5165\u7ef4\u5ea6, vocab_size\u4ee3\u8868\u8bcd\u8868\u5927\u5c0f. forward\u51fd\u6570\u63a5\u53d7\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa. \u6700\u7ec8\u83b7\u5f97\u7ecf\u8fc7\u7ebf\u6027\u5c42\u548csoftmax\u5c42\u5904\u7406\u7684\u7ed3\u679c.","title":"6 \u8f93\u51fa\u90e8\u5206\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#_1","text":"\u4e86\u89e3\u7ebf\u6027\u5c42\u548csoftmax\u7684\u4f5c\u7528. \u638c\u63e1\u7ebf\u6027\u5c42\u548csoftmax\u7684\u5b9e\u73b0\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#1","text":"\u8f93\u51fa\u90e8\u5206\u5305\u542b: \u7ebf\u6027\u5c42 softmax\u5c42","title":"1 \u8f93\u51fa\u90e8\u5206\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#2","text":"\u901a\u8fc7\u5bf9\u4e0a\u4e00\u6b65\u7684\u7ebf\u6027\u53d8\u5316\u5f97\u5230\u6307\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa, \u4e5f\u5c31\u662f\u8f6c\u6362\u7ef4\u5ea6\u7684\u4f5c\u7528.","title":"2 \u7ebf\u6027\u5c42\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#3-softmax","text":"\u4f7f\u6700\u540e\u4e00\u7ef4\u7684\u5411\u91cf\u4e2d\u7684\u6570\u5b57\u7f29\u653e\u52300-1\u7684\u6982\u7387\u503c\u57df\u5185, \u5e76\u6ee1\u8db3\u4ed6\u4eec\u7684\u548c\u4e3a1.","title":"3 softmax\u5c42\u7684\u4f5c\u7528"},{"location":"04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#31-softmax","text":"# \u89e3\u7801\u5668\u7c7b Generator \u5b9e\u73b0\u601d\u8def\u5206\u6790 # init\u51fd\u6570 (self, d_model, vocab_size) # \u5b9a\u4e49\u7ebf\u6027\u5c42self.project # forward\u51fd\u6570 (self, x) # \u6570\u636e F.log_softmax(self.project(x), dim=-1) class Generator ( nn . Module ): def __init__ ( self , d_model , vocab_size ): # \u53c2\u6570d_model \u7ebf\u6027\u5c42\u8f93\u5165\u7279\u5f81\u5c3a\u5bf8\u5927\u5c0f # \u53c2\u6570vocab_size \u7ebf\u5c42\u8f93\u51fa\u5c3a\u5bf8\u5927\u5c0f super ( Generator , self ) . __init__ () # \u5b9a\u4e49\u7ebf\u6027\u5c42 self . project = nn . Linear ( d_model , vocab_size ) def forward ( self , x ): # \u6570\u636e\u7ecf\u8fc7\u7ebf\u6027\u5c42 \u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u5f52\u4e00\u5316 log\u65b9\u5f0f x = F . log_softmax ( self . project ( x ), dim =- 1 ) return x nn.Linear\u6f14\u793a: >>> m = nn . Linear ( 20 , 30 ) >>> input = torch . randn ( 128 , 20 ) >>> output = m ( input ) >>> print ( output . size ()) torch . Size ([ 128 , 30 ]) \u51fd\u6570\u8c03\u7528 if __name__ == '__main__' : # \u5b9e\u4f8b\u5316output\u5c42\u5bf9\u8c61 d_model = 512 vocab_size = 1000 my_generator = Generator ( d_model , vocab_size ) # \u51c6\u5907\u6a21\u578b\u6570\u636e x = torch . randn ( 2 , 4 , 512 ) # \u6570\u636e\u7ecf\u8fc7out\u5c42 gen_result = my_generator ( x ) print ( 'gen_result--->' , gen_result . shape , ' \\n ' , gen_result ) \u8f93\u51fa\u6548\u679c gen_result ---> torch . Size ([ 2 , 4 , 1000 ]) tensor ([[[ - 6.5949 , - 7.0295 , - 6.5928 , ... , - 7.4317 , - 7.5488 , - 6.4871 ], [ - 7.0481 , - 6.2352 , - 7.2797 , ... , - 6.1491 , - 6.1621 , - 7.1798 ], [ - 8.1724 , - 7.0675 , - 8.2814 , ... , - 6.0033 , - 7.1100 , - 7.6844 ], [ - 6.2466 , - 6.6074 , - 6.1852 , ... , - 6.8373 , - 7.6600 , - 6.8578 ]], [[ - 7.7598 , - 7.4174 , - 6.2134 , ... , - 7.8000 , - 6.9862 , - 6.9261 ], [ - 6.4790 , - 7.5458 , - 6.2342 , ... , - 6.8340 , - 6.6827 , - 7.0287 ], [ - 7.2524 , - 7.2598 , - 7.0600 , ... , - 7.5680 , - 6.9492 , - 6.7689 ], [ - 6.6260 , - 6.1928 , - 6.7045 , ... , - 6.6323 , - 7.9005 , - 7.5397 ]]], grad_fn =< LogSoftmaxBackward0 > )","title":"3.1 \u7ebf\u6027\u5c42\u548csoftmax\u5c42\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html#4","text":"\u5b66\u4e60\u4e86\u8f93\u51fa\u90e8\u5206\u5305\u542b: \u7ebf\u6027\u5c42 softmax\u5c42 \u7ebf\u6027\u5c42\u7684\u4f5c\u7528: \u901a\u8fc7\u5bf9\u4e0a\u4e00\u6b65\u7684\u7ebf\u6027\u53d8\u5316\u5f97\u5230\u6307\u5b9a\u7ef4\u5ea6\u7684\u8f93\u51fa, \u4e5f\u5c31\u662f\u8f6c\u6362\u7ef4\u5ea6\u7684\u4f5c\u7528. softmax\u5c42\u7684\u4f5c\u7528: \u4f7f\u6700\u540e\u4e00\u7ef4\u7684\u5411\u91cf\u4e2d\u7684\u6570\u5b57\u7f29\u653e\u52300-1\u7684\u6982\u7387\u503c\u57df\u5185, \u5e76\u6ee1\u8db3\u4ed6\u4eec\u7684\u548c\u4e3a1. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7ebf\u6027\u5c42\u548csoftmax\u5c42\u7684\u7c7b: Generator \u521d\u59cb\u5316\u51fd\u6570\u7684\u8f93\u5165\u53c2\u6570\u6709\u4e24\u4e2a, d_model\u4ee3\u8868\u8bcd\u5d4c\u5165\u7ef4\u5ea6, vocab_size\u4ee3\u8868\u8bcd\u8868\u5927\u5c0f. forward\u51fd\u6570\u63a5\u53d7\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa. \u6700\u7ec8\u83b7\u5f97\u7ecf\u8fc7\u7ebf\u6027\u5c42\u548csoftmax\u5c42\u5904\u7406\u7684\u7ed3\u679c.","title":"4 \u5c0f\u7ed3"},{"location":"04_mkdocs_transformer/7%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u5b9e\u73b0\u8fc7\u7a0b. \u638c\u63e1Transformer\u6a21\u578b\u7684\u6784\u5efa\u8fc7\u7a0b. 1 \u6a21\u578b\u6784\u5efa\u4ecb\u7ecd \u00b6 \u901a\u8fc7\u4e0a\u9762\u7684\u5c0f\u8282, \u6211\u4eec\u5df2\u7ecf\u5b8c\u6210\u4e86\u6240\u6709\u7ec4\u6210\u90e8\u5206\u7684\u5b9e\u73b0, \u63a5\u4e0b\u6765\u5c31\u6765\u5b9e\u73b0\u5b8c\u6574\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784. Transformer\u603b\u4f53\u67b6\u6784\u56fe: 2 \u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u4ee3\u7801\u5b9e\u73b0 \u00b6 EncoderDecoder\u51fd\u6570\u5b8c\u6210\u7f16\u7801\u89e3\u7801\u7684\u5b50\u4efb\u52a1\uff0c\u5c31\u662f\u628a\u7f16\u7801\u548c\u89e3\u7801\u7684\u6d41\u7a0b\u8fdb\u884c\u5c01\u88c5\u5b9e\u73b0\u3002 # \u7f16\u7801\u89e3\u7801\u5185\u90e8\u51fd\u6570\u7c7b EncoderDecoder \u5b9e\u73b0\u5206\u6790 # init\u51fd\u6570 (self, encoder, decoder, source_embed, target_embed, generator) # 5\u4e2a\u6210\u5458\u5c5e\u6027\u8d4b\u503c encoder \u7f16\u7801\u5668\u5bf9\u8c61 decoder \u89e3\u7801\u5668\u5bf9\u8c61 source_embed source\u7aef\u8bcd\u5d4c\u5165\u5c42\u5bf9\u8c61 # target_embed target\u7aef\u8bcd\u5d4c\u5165\u5c42\u5bf9\u8c61 generator \u8f93\u51fa\u5c42\u5bf9\u8c61 # forward\u51fd\u6570 (self, source, target, source_mask, target_mask) # 1 \u7f16\u7801 s.encoder(self.src_embed(source), source_mask) # 2 \u89e3\u7801 s.decoder(self.tgt_embed(target), memory, source_mask, target_mask) # 3 \u8f93\u51fa s.generator() # \u4f7f\u7528EncoderDecoder\u7c7b\u6765\u5b9e\u73b0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784 class EncoderDecoder ( nn . Module ): def __init__ ( self , encoder , decoder , source_embed , target_embed , generator ): \"\"\"\u521d\u59cb\u5316\u51fd\u6570\u4e2d\u67095\u4e2a\u53c2\u6570, \u5206\u522b\u662f\u7f16\u7801\u5668\u5bf9\u8c61, \u89e3\u7801\u5668\u5bf9\u8c61, \u6e90\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u76ee\u6807\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u4ee5\u53ca\u8f93\u51fa\u90e8\u5206\u7684\u7c7b\u522b\u751f\u6210\u5668\u5bf9\u8c61 \"\"\" super ( EncoderDecoder , self ) . __init__ () # \u5c06\u53c2\u6570\u4f20\u5165\u5230\u7c7b\u4e2d self . encoder = encoder self . decoder = decoder self . src_embed = source_embed self . tgt_embed = target_embed self . generator = generator def forward ( self , source , target , source_mask , target_mask ): \"\"\"\u5728forward\u51fd\u6570\u4e2d\uff0c\u6709\u56db\u4e2a\u53c2\u6570, source\u4ee3\u8868\u6e90\u6570\u636e, target\u4ee3\u8868\u76ee\u6807\u6570\u636e, source_mask\u548ctarget_mask\u4ee3\u8868\u5bf9\u5e94\u7684\u63a9\u7801\u5f20\u91cf\"\"\" # \u5728\u51fd\u6570\u4e2d, \u5c06source, source_mask\u4f20\u5165\u7f16\u7801\u51fd\u6570, \u5f97\u5230\u7ed3\u679c\u540e, # \u4e0esource_mask\uff0ctarget\uff0c\u548ctarget_mask\u4e00\u540c\u4f20\u7ed9\u89e3\u7801\u51fd\u6570 return self . generator ( self . decode ( self . encode ( source , source_mask ), source_mask , target , target_mask )) def encode ( self , source , source_mask ): \"\"\"\u7f16\u7801\u51fd\u6570, \u4ee5source\u548csource_mask\u4e3a\u53c2\u6570\"\"\" # \u4f7f\u7528src_embed\u5bf9source\u505a\u5904\u7406, \u7136\u540e\u548csource_mask\u4e00\u8d77\u4f20\u7ed9self.encoder return self . encoder ( self . src_embed ( source ), source_mask ) def decode ( self , memory , source_mask , target , target_mask ): \"\"\"\u89e3\u7801\u51fd\u6570, \u4ee5memory\u5373\u7f16\u7801\u5668\u7684\u8f93\u51fa, source_mask, target, target_mask\u4e3a\u53c2\u6570\"\"\" # \u4f7f\u7528tgt_embed\u5bf9target\u505a\u5904\u7406, \u7136\u540e\u548csource_mask, target_mask, memory\u4e00\u8d77\u4f20\u7ed9self.decoder return self . decoder ( self . tgt_embed ( target ), memory , source_mask , target_mask ) \u5b9e\u4f8b\u5316\u53c2\u6570 vocab_size = 1000 d_model = 512 encoder = en decoder = de source_embed = nn . Embedding ( vocab_size , d_model ) target_embed = nn . Embedding ( vocab_size , d_model ) generator = gen \u8f93\u5165\u53c2\u6570: # \u5047\u8bbe\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e\u76f8\u540c, \u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source = target = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) # \u5047\u8bbesrc_mask\u4e0etgt_mask\u76f8\u540c\uff0c\u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source_mask = target_mask = Variable ( torch . zeros ( 8 , 4 , 4 )) \u8c03\u7528: ed = EncoderDecoder ( encoder , decoder , source_embed , target_embed , generator ) ed_result = ed ( source , target , source_mask , target_mask ) print ( ed_result ) print ( ed_result . shape ) \u8f93\u51fa\u6548\u679c: tensor([[[ 0.2102, -0.0826, -0.0550, ..., 1.5555, 1.3025, -0.6296], [ 0.8270, -0.5372, -0.9559, ..., 0.3665, 0.4338, -0.7505], [ 0.4956, -0.5133, -0.9323, ..., 1.0773, 1.1913, -0.6240], [ 0.5770, -0.6258, -0.4833, ..., 0.1171, 1.0069, -1.9030]], [[-0.4355, -1.7115, -1.5685, ..., -0.6941, -0.1878, -0.1137], [-0.8867, -1.2207, -1.4151, ..., -0.9618, 0.1722, -0.9562], [-0.0946, -0.9012, -1.6388, ..., -0.2604, -0.3357, -0.6436], [-1.1204, -1.4481, -1.5888, ..., -0.8816, -0.6497, 0.0606]]], grad_fn=<AddBackward0>) torch.Size([2, 4, 512]) \u63a5\u7740\u5c06\u57fa\u4e8e\u4ee5\u4e0a\u7ed3\u6784\u6784\u5efa\u7528\u4e8e\u8bad\u7ec3\u7684\u6a21\u578b. 3 Tansformer\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u7684\u4ee3\u7801\u5206\u6790 \u00b6 make_model\u51fd\u6570\u521d\u59cb\u5316\u4e00\u4e2a\u4e00\u4e2a\u7ec4\u4ef6\u5bf9\u8c61\uff08\u8f6e\u5b50\u5bf9\u8c61\uff09\uff0c\u8c03\u7528EncoderDecoder()\u51fd\u6570 # make_model\u51fd\u6570\u5b9e\u73b0\u601d\u8def\u5206\u6790 # \u51fd\u6570\u539f\u578b (source_vocab, target_vocab, N=6, d_model=512, d_ff=2048, head=8, dropout=0.1) # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61 attn # \u5b9e\u4f8b\u5316\u524d\u9988\u5168\u8fde\u63a5\u5bf9\u8c61ff # \u5b9e\u4f8b\u5316\u4f4d\u7f6e\u7f16\u7801\u5668\u5bf9\u8c61position # \u6784\u5efa EncoderDecoder\u5bf9\u8c61(Encoder\u5bf9\u8c61, Decoder\u5bf9\u8c61, # source\u7aef\u8f93\u5165\u90e8\u5206nn.Sequential(), # target\u7aef\u8f93\u5165\u90e8\u5206nn.Sequential(), # \u7ebf\u6027\u5c42\u8f93\u51faGenerator) # \u5bf9\u6a21\u578b\u53c2\u6570\u521d\u59cb\u5316 nn.init.xavier_uniform_(p) # \u6ce8\u610f\u4f7f\u7528 c = copy.deepcopy # \u8fd4\u56demodel def make_model ( source_vocab , target_vocab , N = 6 , d_model = 512 , d_ff = 2048 , head = 8 , dropout = 0.1 ): c = copy . deepcopy # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61 attn = MultiHeadedAttention ( head = 8 , embedding_dim = 512 , dropout = dropout ) # \u5b9e\u4f8b\u5316\u524d\u9988\u5168\u8fde\u63a5\u5bf9\u8c61ff ff = PositionwiseFeedForward ( d_model = d_model , d_ff = d_ff , dropout = dropout ) # \u5b9e\u4f8b\u5316 \u4f4d\u7f6e\u7f16\u7801\u5668\u5bf9\u8c61position position = PositionalEncoding ( d_model = d_model , dropout = dropout ) # \u6784\u5efa EncoderDecoder\u5bf9\u8c61 model = EncoderDecoder ( # \u7f16\u7801\u5668\u5bf9\u8c61 Encoder ( EncoderLayer ( d_model , c ( attn ), c ( ff ), dropout ), N ), # \u89e3\u7801\u5668\u5bf9\u8c61 Decoder ( DecoderLayer ( d_model , c ( attn ), c ( attn ), c ( ff ), dropout ), N ), # \u8bcd\u5d4c\u5165\u5c42 \u4f4d\u7f6e\u7f16\u7801\u5668\u5c42\u5bb9\u5668 nn . Sequential ( Embeddings ( d_model , source_vocab ), c ( position )), # \u8bcd\u5d4c\u5165\u5c42 \u4f4d\u7f6e\u7f16\u7801\u5668\u5c42\u5bb9\u5668 nn . Sequential ( Embeddings ( d_model , target_vocab ), c ( position )), # \u8f93\u51fa\u5c42\u5bf9\u8c61 Generator ( d_model , target_vocab )) for p in model . parameters (): if p . dim () > 1 : nn . init . xavier_uniform_ ( p ) return model nn.init.xavier_uniform\u6f14\u793a: # \u7ed3\u679c\u670d\u4ece\u5747\u5300\u5206\u5e03U(-a, a) >>> w = torch . empty ( 3 , 5 ) >>> w = nn . init . xavier_uniform_ ( w , gain = nn . init . calculate_gain ( 'relu' )) >>> w tensor ([[ - 0.7742 , 0.5413 , 0.5478 , - 0.4806 , - 0.2555 ], [ - 0.8358 , 0.4673 , 0.3012 , 0.3882 , - 0.6375 ], [ 0.4622 , - 0.0794 , 0.1851 , 0.8462 , - 0.3591 ]]) \u51fd\u6570\u8c03\u7528 def dm_test_make_model (): source_vocab = 500 target_vocab = 1000 N = 6 my_transform_modelobj = make_model ( source_vocab , target_vocab , N = 6 , d_model = 512 , d_ff = 2048 , head = 8 , dropout = 0.1 ) print ( my_transform_modelobj ) # \u5047\u8bbe\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e\u76f8\u540c, \u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source = target = Variable ( torch . LongTensor ([[ 1 , 2 , 3 , 8 ], [ 3 , 4 , 1 , 8 ]])) # \u5047\u8bbesrc_mask\u4e0etgt_mask\u76f8\u540c\uff0c\u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source_mask = target_mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # mydata = my_transform_modelobj ( source , target , source_mask , target_mask ) print ( 'mydata.shape--->' , mydata . shape ) print ( 'mydata--->' , mydata ) \u8f93\u51fa\u6548\u679c1 mydata . shape ---> torch . Size ([ 2 , 4 , 1000 ]) mydata ---> tensor ([[[ - 5.7188 , - 7.4484 , - 8.0710 , ... , - 9.1009 , - 7.7561 , - 6.1054 ], [ - 6.7604 , - 8.1813 , - 8.1233 , ... , - 7.7539 , - 8.1921 , - 7.0365 ], [ - 6.6139 , - 8.4309 , - 8.0176 , ... , - 8.9429 , - 8.2295 , - 6.8527 ], [ - 6.6079 , - 8.4657 , - 8.2147 , ... , - 8.8127 , - 6.9746 , - 6.1084 ]], [[ - 6.7538 , - 7.9822 , - 7.6833 , ... , - 8.8334 , - 7.0283 , - 7.4291 ], [ - 6.7661 , - 7.6868 , - 8.0763 , ... , - 8.6204 , - 7.7191 , - 7.6031 ], [ - 5.9538 , - 7.0344 , - 7.3635 , ... , - 8.5833 , - 7.5199 , - 6.9852 ], [ - 6.6039 , - 8.2063 , - 8.2185 , ... , - 8.5063 , - 6.9020 , - 7.1619 ]]], grad_fn =< LogSoftmaxBackward0 > ) \u8f93\u51fa\u6548\u679c2 # \u6839\u636eTransformer\u7ed3\u6784\u56fe\u6784\u5efa\u7684\u6700\u7ec8\u6a21\u578b\u7ed3\u6784 EncoderDecoder ( ( encoder ): Encoder ( ( layers ): ModuleList ( ( 0 ): EncoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ( 1 ): EncoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ) ( norm ): LayerNorm ( ) ) ( decoder ): Decoder ( ( layers ): ModuleList ( ( 0 ): DecoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( src_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 2 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ( 1 ): DecoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( src_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 2 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ) ( norm ): LayerNorm ( ) ) ( src_embed ): Sequential ( ( 0 ): Embeddings ( ( lut ): Embedding ( 11 , 512 ) ) ( 1 ): PositionalEncoding ( ( dropout ): Dropout ( p = 0.1 ) ) ) ( tgt_embed ): Sequential ( ( 0 ): Embeddings ( ( lut ): Embedding ( 11 , 512 ) ) ( 1 ): PositionalEncoding ( ( dropout ): Dropout ( p = 0.1 ) ) ) ( generator ): Generator ( ( proj ): Linear ( in_features = 512 , out_features = 11 ) ) ) 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u7c7b: EncoderDecoder \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u4f20\u51655\u4e2a\u53c2\u6570, \u5206\u522b\u662f\u7f16\u7801\u5668\u5bf9\u8c61, \u89e3\u7801\u5668\u5bf9\u8c61, \u6e90\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u76ee\u6807\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u4ee5\u53ca\u8f93\u51fa\u90e8\u5206\u7684\u7c7b\u522b\u751f\u6210\u5668\u5bf9\u8c61. \u7c7b\u4e2d\u5171\u5b9e\u73b0\u4e09\u4e2a\u51fd\u6570, forward, encode, decode forward\u662f\u4e3b\u8981\u903b\u8f91\u51fd\u6570, \u6709\u56db\u4e2a\u53c2\u6570, source\u4ee3\u8868\u6e90\u6570\u636e, target\u4ee3\u8868\u76ee\u6807\u6570\u636e, source_mask\u548ctarget_mask\u4ee3\u8868\u5bf9\u5e94\u7684\u63a9\u7801\u5f20\u91cf. encode\u662f\u7f16\u7801\u51fd\u6570, \u4ee5source\u548csource_mask\u4e3a\u53c2\u6570. decode\u662f\u89e3\u7801\u51fd\u6570, \u4ee5memory\u5373\u7f16\u7801\u5668\u7684\u8f93\u51fa, source_mask, target, target_mask\u4e3a\u53c2\u6570 \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6a21\u578b\u6784\u5efa\u51fd\u6570: make_model \u67097\u4e2a\u53c2\u6570\uff0c\u5206\u522b\u662f\u6e90\u6570\u636e\u7279\u5f81(\u8bcd\u6c47)\u603b\u6570\uff0c\u76ee\u6807\u6570\u636e\u7279\u5f81(\u8bcd\u6c47)\u603b\u6570\uff0c\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5806\u53e0\u6570\uff0c\u8bcd\u5411\u91cf\u6620\u5c04\u7ef4\u5ea6\uff0c\u524d\u9988\u5168\u8fde\u63a5\u7f51\u7edc\u4e2d\u53d8\u6362\u77e9\u9635\u7684\u7ef4\u5ea6\uff0c\u591a\u5934\u6ce8\u610f\u529b\u7ed3\u6784\u4e2d\u7684\u591a\u5934\u6570\uff0c\u4ee5\u53ca\u7f6e\u96f6\u6bd4\u7387dropout. \u8be5\u51fd\u6570\u6700\u540e\u8fd4\u56de\u4e00\u4e2a\u6784\u5efa\u597d\u7684\u6a21\u578b\u5bf9\u8c61.","title":"7 \u6a21\u578b\u6784\u5efa"},{"location":"04_mkdocs_transformer/7%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html#_1","text":"\u638c\u63e1\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u5b9e\u73b0\u8fc7\u7a0b. \u638c\u63e1Transformer\u6a21\u578b\u7684\u6784\u5efa\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/7%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html#1","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u5c0f\u8282, \u6211\u4eec\u5df2\u7ecf\u5b8c\u6210\u4e86\u6240\u6709\u7ec4\u6210\u90e8\u5206\u7684\u5b9e\u73b0, \u63a5\u4e0b\u6765\u5c31\u6765\u5b9e\u73b0\u5b8c\u6574\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784. Transformer\u603b\u4f53\u67b6\u6784\u56fe:","title":"1 \u6a21\u578b\u6784\u5efa\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/7%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html#2-","text":"EncoderDecoder\u51fd\u6570\u5b8c\u6210\u7f16\u7801\u89e3\u7801\u7684\u5b50\u4efb\u52a1\uff0c\u5c31\u662f\u628a\u7f16\u7801\u548c\u89e3\u7801\u7684\u6d41\u7a0b\u8fdb\u884c\u5c01\u88c5\u5b9e\u73b0\u3002 # \u7f16\u7801\u89e3\u7801\u5185\u90e8\u51fd\u6570\u7c7b EncoderDecoder \u5b9e\u73b0\u5206\u6790 # init\u51fd\u6570 (self, encoder, decoder, source_embed, target_embed, generator) # 5\u4e2a\u6210\u5458\u5c5e\u6027\u8d4b\u503c encoder \u7f16\u7801\u5668\u5bf9\u8c61 decoder \u89e3\u7801\u5668\u5bf9\u8c61 source_embed source\u7aef\u8bcd\u5d4c\u5165\u5c42\u5bf9\u8c61 # target_embed target\u7aef\u8bcd\u5d4c\u5165\u5c42\u5bf9\u8c61 generator \u8f93\u51fa\u5c42\u5bf9\u8c61 # forward\u51fd\u6570 (self, source, target, source_mask, target_mask) # 1 \u7f16\u7801 s.encoder(self.src_embed(source), source_mask) # 2 \u89e3\u7801 s.decoder(self.tgt_embed(target), memory, source_mask, target_mask) # 3 \u8f93\u51fa s.generator() # \u4f7f\u7528EncoderDecoder\u7c7b\u6765\u5b9e\u73b0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784 class EncoderDecoder ( nn . Module ): def __init__ ( self , encoder , decoder , source_embed , target_embed , generator ): \"\"\"\u521d\u59cb\u5316\u51fd\u6570\u4e2d\u67095\u4e2a\u53c2\u6570, \u5206\u522b\u662f\u7f16\u7801\u5668\u5bf9\u8c61, \u89e3\u7801\u5668\u5bf9\u8c61, \u6e90\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u76ee\u6807\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u4ee5\u53ca\u8f93\u51fa\u90e8\u5206\u7684\u7c7b\u522b\u751f\u6210\u5668\u5bf9\u8c61 \"\"\" super ( EncoderDecoder , self ) . __init__ () # \u5c06\u53c2\u6570\u4f20\u5165\u5230\u7c7b\u4e2d self . encoder = encoder self . decoder = decoder self . src_embed = source_embed self . tgt_embed = target_embed self . generator = generator def forward ( self , source , target , source_mask , target_mask ): \"\"\"\u5728forward\u51fd\u6570\u4e2d\uff0c\u6709\u56db\u4e2a\u53c2\u6570, source\u4ee3\u8868\u6e90\u6570\u636e, target\u4ee3\u8868\u76ee\u6807\u6570\u636e, source_mask\u548ctarget_mask\u4ee3\u8868\u5bf9\u5e94\u7684\u63a9\u7801\u5f20\u91cf\"\"\" # \u5728\u51fd\u6570\u4e2d, \u5c06source, source_mask\u4f20\u5165\u7f16\u7801\u51fd\u6570, \u5f97\u5230\u7ed3\u679c\u540e, # \u4e0esource_mask\uff0ctarget\uff0c\u548ctarget_mask\u4e00\u540c\u4f20\u7ed9\u89e3\u7801\u51fd\u6570 return self . generator ( self . decode ( self . encode ( source , source_mask ), source_mask , target , target_mask )) def encode ( self , source , source_mask ): \"\"\"\u7f16\u7801\u51fd\u6570, \u4ee5source\u548csource_mask\u4e3a\u53c2\u6570\"\"\" # \u4f7f\u7528src_embed\u5bf9source\u505a\u5904\u7406, \u7136\u540e\u548csource_mask\u4e00\u8d77\u4f20\u7ed9self.encoder return self . encoder ( self . src_embed ( source ), source_mask ) def decode ( self , memory , source_mask , target , target_mask ): \"\"\"\u89e3\u7801\u51fd\u6570, \u4ee5memory\u5373\u7f16\u7801\u5668\u7684\u8f93\u51fa, source_mask, target, target_mask\u4e3a\u53c2\u6570\"\"\" # \u4f7f\u7528tgt_embed\u5bf9target\u505a\u5904\u7406, \u7136\u540e\u548csource_mask, target_mask, memory\u4e00\u8d77\u4f20\u7ed9self.decoder return self . decoder ( self . tgt_embed ( target ), memory , source_mask , target_mask ) \u5b9e\u4f8b\u5316\u53c2\u6570 vocab_size = 1000 d_model = 512 encoder = en decoder = de source_embed = nn . Embedding ( vocab_size , d_model ) target_embed = nn . Embedding ( vocab_size , d_model ) generator = gen \u8f93\u5165\u53c2\u6570: # \u5047\u8bbe\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e\u76f8\u540c, \u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source = target = Variable ( torch . LongTensor ([[ 100 , 2 , 421 , 508 ], [ 491 , 998 , 1 , 221 ]])) # \u5047\u8bbesrc_mask\u4e0etgt_mask\u76f8\u540c\uff0c\u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source_mask = target_mask = Variable ( torch . zeros ( 8 , 4 , 4 )) \u8c03\u7528: ed = EncoderDecoder ( encoder , decoder , source_embed , target_embed , generator ) ed_result = ed ( source , target , source_mask , target_mask ) print ( ed_result ) print ( ed_result . shape ) \u8f93\u51fa\u6548\u679c: tensor([[[ 0.2102, -0.0826, -0.0550, ..., 1.5555, 1.3025, -0.6296], [ 0.8270, -0.5372, -0.9559, ..., 0.3665, 0.4338, -0.7505], [ 0.4956, -0.5133, -0.9323, ..., 1.0773, 1.1913, -0.6240], [ 0.5770, -0.6258, -0.4833, ..., 0.1171, 1.0069, -1.9030]], [[-0.4355, -1.7115, -1.5685, ..., -0.6941, -0.1878, -0.1137], [-0.8867, -1.2207, -1.4151, ..., -0.9618, 0.1722, -0.9562], [-0.0946, -0.9012, -1.6388, ..., -0.2604, -0.3357, -0.6436], [-1.1204, -1.4481, -1.5888, ..., -0.8816, -0.6497, 0.0606]]], grad_fn=<AddBackward0>) torch.Size([2, 4, 512]) \u63a5\u7740\u5c06\u57fa\u4e8e\u4ee5\u4e0a\u7ed3\u6784\u6784\u5efa\u7528\u4e8e\u8bad\u7ec3\u7684\u6a21\u578b.","title":"2 \u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u4ee3\u7801\u5b9e\u73b0"},{"location":"04_mkdocs_transformer/7%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html#3-tansformer","text":"make_model\u51fd\u6570\u521d\u59cb\u5316\u4e00\u4e2a\u4e00\u4e2a\u7ec4\u4ef6\u5bf9\u8c61\uff08\u8f6e\u5b50\u5bf9\u8c61\uff09\uff0c\u8c03\u7528EncoderDecoder()\u51fd\u6570 # make_model\u51fd\u6570\u5b9e\u73b0\u601d\u8def\u5206\u6790 # \u51fd\u6570\u539f\u578b (source_vocab, target_vocab, N=6, d_model=512, d_ff=2048, head=8, dropout=0.1) # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61 attn # \u5b9e\u4f8b\u5316\u524d\u9988\u5168\u8fde\u63a5\u5bf9\u8c61ff # \u5b9e\u4f8b\u5316\u4f4d\u7f6e\u7f16\u7801\u5668\u5bf9\u8c61position # \u6784\u5efa EncoderDecoder\u5bf9\u8c61(Encoder\u5bf9\u8c61, Decoder\u5bf9\u8c61, # source\u7aef\u8f93\u5165\u90e8\u5206nn.Sequential(), # target\u7aef\u8f93\u5165\u90e8\u5206nn.Sequential(), # \u7ebf\u6027\u5c42\u8f93\u51faGenerator) # \u5bf9\u6a21\u578b\u53c2\u6570\u521d\u59cb\u5316 nn.init.xavier_uniform_(p) # \u6ce8\u610f\u4f7f\u7528 c = copy.deepcopy # \u8fd4\u56demodel def make_model ( source_vocab , target_vocab , N = 6 , d_model = 512 , d_ff = 2048 , head = 8 , dropout = 0.1 ): c = copy . deepcopy # \u5b9e\u4f8b\u5316\u591a\u5934\u6ce8\u610f\u529b\u5c42\u5bf9\u8c61 attn = MultiHeadedAttention ( head = 8 , embedding_dim = 512 , dropout = dropout ) # \u5b9e\u4f8b\u5316\u524d\u9988\u5168\u8fde\u63a5\u5bf9\u8c61ff ff = PositionwiseFeedForward ( d_model = d_model , d_ff = d_ff , dropout = dropout ) # \u5b9e\u4f8b\u5316 \u4f4d\u7f6e\u7f16\u7801\u5668\u5bf9\u8c61position position = PositionalEncoding ( d_model = d_model , dropout = dropout ) # \u6784\u5efa EncoderDecoder\u5bf9\u8c61 model = EncoderDecoder ( # \u7f16\u7801\u5668\u5bf9\u8c61 Encoder ( EncoderLayer ( d_model , c ( attn ), c ( ff ), dropout ), N ), # \u89e3\u7801\u5668\u5bf9\u8c61 Decoder ( DecoderLayer ( d_model , c ( attn ), c ( attn ), c ( ff ), dropout ), N ), # \u8bcd\u5d4c\u5165\u5c42 \u4f4d\u7f6e\u7f16\u7801\u5668\u5c42\u5bb9\u5668 nn . Sequential ( Embeddings ( d_model , source_vocab ), c ( position )), # \u8bcd\u5d4c\u5165\u5c42 \u4f4d\u7f6e\u7f16\u7801\u5668\u5c42\u5bb9\u5668 nn . Sequential ( Embeddings ( d_model , target_vocab ), c ( position )), # \u8f93\u51fa\u5c42\u5bf9\u8c61 Generator ( d_model , target_vocab )) for p in model . parameters (): if p . dim () > 1 : nn . init . xavier_uniform_ ( p ) return model nn.init.xavier_uniform\u6f14\u793a: # \u7ed3\u679c\u670d\u4ece\u5747\u5300\u5206\u5e03U(-a, a) >>> w = torch . empty ( 3 , 5 ) >>> w = nn . init . xavier_uniform_ ( w , gain = nn . init . calculate_gain ( 'relu' )) >>> w tensor ([[ - 0.7742 , 0.5413 , 0.5478 , - 0.4806 , - 0.2555 ], [ - 0.8358 , 0.4673 , 0.3012 , 0.3882 , - 0.6375 ], [ 0.4622 , - 0.0794 , 0.1851 , 0.8462 , - 0.3591 ]]) \u51fd\u6570\u8c03\u7528 def dm_test_make_model (): source_vocab = 500 target_vocab = 1000 N = 6 my_transform_modelobj = make_model ( source_vocab , target_vocab , N = 6 , d_model = 512 , d_ff = 2048 , head = 8 , dropout = 0.1 ) print ( my_transform_modelobj ) # \u5047\u8bbe\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e\u76f8\u540c, \u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source = target = Variable ( torch . LongTensor ([[ 1 , 2 , 3 , 8 ], [ 3 , 4 , 1 , 8 ]])) # \u5047\u8bbesrc_mask\u4e0etgt_mask\u76f8\u540c\uff0c\u5b9e\u9645\u4e2d\u5e76\u4e0d\u76f8\u540c source_mask = target_mask = Variable ( torch . zeros ( 8 , 4 , 4 )) # mydata = my_transform_modelobj ( source , target , source_mask , target_mask ) print ( 'mydata.shape--->' , mydata . shape ) print ( 'mydata--->' , mydata ) \u8f93\u51fa\u6548\u679c1 mydata . shape ---> torch . Size ([ 2 , 4 , 1000 ]) mydata ---> tensor ([[[ - 5.7188 , - 7.4484 , - 8.0710 , ... , - 9.1009 , - 7.7561 , - 6.1054 ], [ - 6.7604 , - 8.1813 , - 8.1233 , ... , - 7.7539 , - 8.1921 , - 7.0365 ], [ - 6.6139 , - 8.4309 , - 8.0176 , ... , - 8.9429 , - 8.2295 , - 6.8527 ], [ - 6.6079 , - 8.4657 , - 8.2147 , ... , - 8.8127 , - 6.9746 , - 6.1084 ]], [[ - 6.7538 , - 7.9822 , - 7.6833 , ... , - 8.8334 , - 7.0283 , - 7.4291 ], [ - 6.7661 , - 7.6868 , - 8.0763 , ... , - 8.6204 , - 7.7191 , - 7.6031 ], [ - 5.9538 , - 7.0344 , - 7.3635 , ... , - 8.5833 , - 7.5199 , - 6.9852 ], [ - 6.6039 , - 8.2063 , - 8.2185 , ... , - 8.5063 , - 6.9020 , - 7.1619 ]]], grad_fn =< LogSoftmaxBackward0 > ) \u8f93\u51fa\u6548\u679c2 # \u6839\u636eTransformer\u7ed3\u6784\u56fe\u6784\u5efa\u7684\u6700\u7ec8\u6a21\u578b\u7ed3\u6784 EncoderDecoder ( ( encoder ): Encoder ( ( layers ): ModuleList ( ( 0 ): EncoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ( 1 ): EncoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ) ( norm ): LayerNorm ( ) ) ( decoder ): Decoder ( ( layers ): ModuleList ( ( 0 ): DecoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( src_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 2 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ( 1 ): DecoderLayer ( ( self_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( src_attn ): MultiHeadedAttention ( ( linears ): ModuleList ( ( 0 ): Linear ( in_features = 512 , out_features = 512 ) ( 1 ): Linear ( in_features = 512 , out_features = 512 ) ( 2 ): Linear ( in_features = 512 , out_features = 512 ) ( 3 ): Linear ( in_features = 512 , out_features = 512 ) ) ( dropout ): Dropout ( p = 0.1 ) ) ( feed_forward ): PositionwiseFeedForward ( ( w_1 ): Linear ( in_features = 512 , out_features = 2048 ) ( w_2 ): Linear ( in_features = 2048 , out_features = 512 ) ( dropout ): Dropout ( p = 0.1 ) ) ( sublayer ): ModuleList ( ( 0 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 1 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ( 2 ): SublayerConnection ( ( norm ): LayerNorm ( ) ( dropout ): Dropout ( p = 0.1 ) ) ) ) ) ( norm ): LayerNorm ( ) ) ( src_embed ): Sequential ( ( 0 ): Embeddings ( ( lut ): Embedding ( 11 , 512 ) ) ( 1 ): PositionalEncoding ( ( dropout ): Dropout ( p = 0.1 ) ) ) ( tgt_embed ): Sequential ( ( 0 ): Embeddings ( ( lut ): Embedding ( 11 , 512 ) ) ( 1 ): PositionalEncoding ( ( dropout ): Dropout ( p = 0.1 ) ) ) ( generator ): Generator ( ( proj ): Linear ( in_features = 512 , out_features = 11 ) ) )","title":"3 Tansformer\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u7684\u4ee3\u7801\u5206\u6790"},{"location":"04_mkdocs_transformer/7%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html#4","text":"\u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684\u7c7b: EncoderDecoder \u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\u4f20\u51655\u4e2a\u53c2\u6570, \u5206\u522b\u662f\u7f16\u7801\u5668\u5bf9\u8c61, \u89e3\u7801\u5668\u5bf9\u8c61, \u6e90\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u76ee\u6807\u6570\u636e\u5d4c\u5165\u51fd\u6570, \u4ee5\u53ca\u8f93\u51fa\u90e8\u5206\u7684\u7c7b\u522b\u751f\u6210\u5668\u5bf9\u8c61. \u7c7b\u4e2d\u5171\u5b9e\u73b0\u4e09\u4e2a\u51fd\u6570, forward, encode, decode forward\u662f\u4e3b\u8981\u903b\u8f91\u51fd\u6570, \u6709\u56db\u4e2a\u53c2\u6570, source\u4ee3\u8868\u6e90\u6570\u636e, target\u4ee3\u8868\u76ee\u6807\u6570\u636e, source_mask\u548ctarget_mask\u4ee3\u8868\u5bf9\u5e94\u7684\u63a9\u7801\u5f20\u91cf. encode\u662f\u7f16\u7801\u51fd\u6570, \u4ee5source\u548csource_mask\u4e3a\u53c2\u6570. decode\u662f\u89e3\u7801\u51fd\u6570, \u4ee5memory\u5373\u7f16\u7801\u5668\u7684\u8f93\u51fa, source_mask, target, target_mask\u4e3a\u53c2\u6570 \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6a21\u578b\u6784\u5efa\u51fd\u6570: make_model \u67097\u4e2a\u53c2\u6570\uff0c\u5206\u522b\u662f\u6e90\u6570\u636e\u7279\u5f81(\u8bcd\u6c47)\u603b\u6570\uff0c\u76ee\u6807\u6570\u636e\u7279\u5f81(\u8bcd\u6c47)\u603b\u6570\uff0c\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5806\u53e0\u6570\uff0c\u8bcd\u5411\u91cf\u6620\u5c04\u7ef4\u5ea6\uff0c\u524d\u9988\u5168\u8fde\u63a5\u7f51\u7edc\u4e2d\u53d8\u6362\u77e9\u9635\u7684\u7ef4\u5ea6\uff0c\u591a\u5934\u6ce8\u610f\u529b\u7ed3\u6784\u4e2d\u7684\u591a\u5934\u6570\uff0c\u4ee5\u53ca\u7f6e\u96f6\u6bd4\u7387dropout. \u8be5\u51fd\u6570\u6700\u540e\u8fd4\u56de\u4e00\u4e2a\u6784\u5efa\u597d\u7684\u6a21\u578b\u5bf9\u8c61.","title":"4 \u5c0f\u7ed3"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6709\u5173\u673a\u5668\u7ffb\u8bd1\u7684\u77e5\u8bc6 \u4e86\u89e3seq2seq\u67b6\u6784 \u638c\u63e1\u4f7f\u7528Transformer\u6784\u5efa\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u7684\u5b9e\u73b0\u8fc7\u7a0b 1 Transformer\u67b6\u6784 \u00b6 Transformer\u6a21\u578b\u67b6\u6784\u5206\u6790 \u4ece\u56fe\u4e2d\u53ef\u77e5, Transformer\u6a21\u578b\u67b6\u6784, \u5927\u8303\u56f4\u5185\u5305\u62ec\u4e24\u90e8\u5206\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u548cdecoder(\u89e3\u7801\u5668), \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5185\u90e8\u5b9e\u73b0\u90fd\u4f7f\u7528\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0, \u8fd9\u91cc\u5b83\u8981\u5b8c\u6210\u7684\u662f\u4e00\u4e2a\u5fb7\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1: Willkommen in peking \u2192 welcome to BeiJing. \u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"Willkommen in peking\", \u901a\u8fc7Transformer\u5185\u90e8\u7684\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u4fe1\u606f\u4e4b\u540e\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u5c31\u662f\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc, \u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u76ee\u6807\u8bed\u8a00\u7684\u8bed\u4e49\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00. 2 \u6848\u4f8b\u4ecb\u7ecd \u00b6 2.1 \u6570\u636e\u96c6: \u00b6 \u4f7f\u7528\u7684\u662ftorchtext\u4e2d\u81ea\u5e26\u7684\u6570\u636e\u96c6Multi30k, \u76f4\u63a5\u53ef\u4ee5\u4f7f\u7528\u5185\u7f6e\u7684API\u51fd\u6570\u5373\u53ef\u4e0b\u8f7d # \u9ed8\u8ba4\u4e0b\u8f7d\u7684\u8def\u5f84\u4e3a: /root/.torchtext/cache/Multi30k \u2514\u2500\u2500 Multi30k \u251c\u2500\u2500 mmt16_task1_test . tar . gz \u251c\u2500\u2500 test . de \u251c\u2500\u2500 test . en \u251c\u2500\u2500 train . de \u251c\u2500\u2500 train . en \u251c\u2500\u2500 training . tar . gz \u251c\u2500\u2500 val . de \u251c\u2500\u2500 val . en \u2514\u2500\u2500 validation . tar . gz 2.2 \u673a\u5668\u7ffb\u8bd1\u8fc7\u7a0b \u00b6 \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bfc\u5165Multi30k\u6570\u636e\u96c6\u5e76\u505a\u57fa\u672c\u5904\u7406 \u7b2c\u4e09\u6b65: \u6784\u5efaTransformer\u6a21\u578b \u7b2c\u56db\u6b65: \u5b9a\u4e49mask\u7684\u51fd\u6570, \u521b\u5efa\u5bf9\u5e94\u7684\u4e0d\u540c\u7684mask \u7b2c\u4e94\u6b65: \u5b9a\u4e49\u6279\u6b21\u6570\u636e\u5904\u7406\u7684\u56de\u8c03\u51fd\u6570 \u7b2c\u516d\u6b65: \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u548c\u8bc4\u4f30\u51fd\u6570 \u7b2c\u4e03\u6b65: \u8bad\u7ec3Transformer\u6a21\u578b \u7b2c\u516b\u6b65: \u8fdb\u884c\u89e3\u7801\u751f\u6210\u76ee\u6807\u8bed\u8a00\u8bed\u53e5 \u7b2c\u4e5d\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u548c\u91cd\u52a0\u8f7d 3 \u6848\u4f8b\u5b9e\u73b0\u6b65\u9aa4 \u00b6 1 \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u00b6 import torch import torch.nn as nn import math from torchtext.data.utils import get_tokenizer from torchtext.vocab import build_vocab_from_iterator from torchtext.datasets import Multi30k from typing import Iterable , List from torch import Tensor from torch.nn import Transformer from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader from timeit import default_timer as timer DEVICE = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) 2 \u5bfc\u5165Multi30k\u6570\u636e\u96c6\u5e76\u505a\u57fa\u672c\u5904\u7406 \u00b6 2.1 \u52a0\u8f7d\u5bf9\u5e94\u7684tokenizer \u00b6 # \u6e90\u8bed\u8a00\u662f\u5fb7\u8bed SRC_LANGUAGE = 'de' # \u76ee\u6807\u8bed\u8a00\u662f\u82f1\u8bed TGT_LANGUAGE = 'en' # \u5b9a\u4e49token\u7684\u5b57\u5178, \u5b9a\u4e49vocab\u5b57\u5178 token_transform = {} vocab_transform = {} # \u521b\u5efa\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684kokenizer, \u786e\u4fdd\u4f9d\u8d56\u5173\u7cfb\u5df2\u7ecf\u5b89\u88c5 # pip install -U spacy # python -m spacy download en_core_web_sm # python -m spacy download de_core_news_sm # get_tokenizer\u662f\u5206\u8bcd\u51fd\u6570, \u5982\u679c\u6ca1\u6709\u7279\u6b8a\u7684\u5219\u6309\u7167\u82f1\u8bed\u7684\u7a7a\u683c\u5206\u5272, \u5982\u679c\u6709\u8fd9\u6309\u7167\u5bf9\u5e94\u7684\u5206\u8bcd\u5e93\u8fd4\u56de. \u6bd4\u5982spacy, \u8fd4\u56de\u5bf9\u5e94\u7684\u5206\u8bcd\u5e93 token_transform [ SRC_LANGUAGE ] = get_tokenizer ( 'spacy' , language = 'de_core_news_sm' ) token_transform [ TGT_LANGUAGE ] = get_tokenizer ( 'spacy' , language = 'en_core_web_sm' ) 2.2 \u6784\u5efa\u751f\u6210\u5206\u8bcd\u7684\u8fed\u4ee3\u5668 \u00b6 def yield_tokens ( data_iter : Iterable , language : str ) -> List [ str ]: # data_iter: \u5bf9\u8c61\u7684\u8fed\u4ee3\u5bf9\u8c61 Multi30k\u5bf9\u8c61 # language: \u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00 {'de': 0, 'en': 1} language_index = { SRC_LANGUAGE : 0 , TGT_LANGUAGE : 1 } # \u8fd4\u56de\u5bf9\u5e94\u7684\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61 for data_sample in data_iter : # data_sample:(\u5fb7\u6587, \u82f1\u6587) # data_sample:('Zwei junge wei\u00dfe M\u00e4nner sind im Freien in der N\u00e4he vieler B\u00fcsche.\\n', 'Two young, White males are outside near many bushes.\\n') # token_transform['de']()=['Zwei', 'junge', 'wei\u00dfe', 'M\u00e4nner', 'sind', 'im', 'Freien', 'in', 'der', 'N\u00e4he', 'vieler', 'B\u00fcsche', '.', '\\n'] # or token_transform['en']\u5206\u522b\u8fdb\u884c\u6784\u9020\u5bf9\u5e94\u7684\u5b57\u5178 yield token_transform [ language ]( data_sample [ language_index [ language ]]) 2.3 \u5b9a\u4e49\u7279\u6b8a\u5b57\u7b26\u5e76\u4e0b\u8f7d\u6570\u636e\u8bbe\u7f6e\u9ed8\u8ba4\u7d22\u5f15 \u00b6 # \u5b9a\u4e49\u7279\u6b8a\u5b57\u7b26\u53ca\u5176\u5bf9\u5e94\u7684\u7d22\u5f15\u503c UNK_IDX , PAD_IDX , BOS_IDX , EOS_IDX = 0 , 1 , 2 , 3 # \u786e\u4fdd\u6807\u8bb0\u6309\u5176\u7d22\u5f15\u7684\u987a\u5e8f\u6b63\u786e\u63d2\u5165\u5230\u8bcd\u6c47\u8868\u4e2d special_symbols = [ '<unk>' , '<pad>' , '<bos>' , '<eos>' ] for ln in [ SRC_LANGUAGE , TGT_LANGUAGE ]: # \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u8fed\u4ee3\u5668, # \u6570\u636e\u96c6\u662f\u7528\u82f1\u6587\u63cf\u8ff0\u56fe\u50cf\u7684\u82f1\u6587\u8bed\u53e5, \u7136\u540e\u4eba\u5de5\u5c06\u5176\u7ffb\u8bd1\u4e3a\u5fb7\u6587\u7684\u8bed\u53e5,\u6709\u4e24\u4e2a\u6587\u4ef6, \u4e00\u4e2a\u662ftrain.de \u4e00\u4e2a\u662ftrain.en\u6587\u4ef6, # \u7136\u540e\u5c06\u5176\u6784\u5efa\u4e3a(\u5fb7\u6587, \u82f1\u6587)\u7684\u5f62\u5f0f train_iter = Multi30k ( split = 'train' , language_pair = ( SRC_LANGUAGE , TGT_LANGUAGE )) # \u521b\u5efatorchtext\u7684vocab\u5bf9\u8c61, \u5373\u8bcd\u6c47\u8868 vocab_transform [ ln ] = build_vocab_from_iterator ( yield_tokens ( train_iter , ln ), # \u7528\u4e8e\u6784\u5efa Vocab \u7684\u8fed\u4ee3\u5668\u3002\u5fc5\u987b\u4ea7\u751f\u4ee4\u724c\u5217\u8868\u6216\u8fed\u4ee3\u5668 min_freq = 1 , #\u5728\u8bcd\u6c47\u8868\u4e2d\u5305\u542b\u4e00\u4e2a\u6807\u8bb0\u6240\u9700\u7684\u6700\u4f4e\u9891\u7387 specials = special_symbols , # \u7528\u4e8e\u6dfb\u52a0\u7684\u7279\u6b8a\u5b57\u7b26 special_first = True ) # \u6307\u793a\u662f\u5728\u5f00\u5934\u8fd8\u662f\u7ed3\u5c3e\u63d2\u5165\u7b26\u53f7 # \u5c06 UNK_IDX \u8bbe\u7f6e\u4e3a\u9ed8\u8ba4\u7d22\u5f15\u3002\u672a\u627e\u5230\u4ee4\u724c\u65f6\u8fd4\u56de\u6b64\u7d22\u5f15 # \u5982\u679c\u672a\u8bbe\u7f6e\uff0c\u5219\u5728 Vocabulary \u4e2d\u627e\u4e0d\u5230\u67e5\u8be2\u7684\u6807\u8bb0\u65f6\u629b\u51fa RuntimeError for ln in [ SRC_LANGUAGE , TGT_LANGUAGE ]: vocab_transform [ ln ] . set_default_index ( UNK_IDX ) 3 \u6784\u5efaTransformer\u6a21\u578b \u00b6 3.1 \u5b9a\u4e49\u4f4d\u7f6e\u7f16\u7801\u5668\u7c7b \u00b6 class PositionalEncoding ( nn . Module ): def __init__ ( self , emb_size : int , dropout : float , maxlen : int = 5000 ): ''' emb_size: \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5927\u5c0f dropout: \u6b63\u5219\u5316\u7684\u5927\u5c0f maxlen: \u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6 ''' super ( PositionalEncoding , self ) . __init__ () # \u5c061000\u76842i/d_model\u53d8\u578b\u4e3ae\u7684\u6307\u6570\u5f62\u5f0f den = torch . exp ( - torch . arange ( 0 , emb_size , 2 ) * math . log ( 10000 ) / emb_size ) # \u6548\u679c\u7b49\u4ef7\u4e0etorch.arange(0, maxlen).unsqueeze(1) pos = torch . arange ( 0 , maxlen ) . reshape ( maxlen , 1 ) # \u6784\u5efa\u4e00\u4e2a(maxlen, emb_size)\u5927\u5c0f\u7684\u5168\u96f6\u77e9\u9635 pos_embedding = torch . zeros (( maxlen , emb_size )) # \u5076\u6570\u5217\u662f\u6b63\u5f26\u51fd\u6570\u586b\u5145 pos_embedding [:, 0 :: 2 ] = torch . sin ( pos * den ) # \u5947\u6570\u5217\u662f\u4f59\u5f26\u51fd\u6570\u586b\u5145 pos_embedding [:, 1 :: 2 ] = torch . cos ( pos * den ) # \u5c06\u5176\u7ef4\u5ea6\u53d8\u6210\u4e09\u7ef4, \u4e3a\u4e86\u540e\u671f\u65b9\u4fbf\u8ba1\u7b97 pos_embedding = pos_embedding . unsqueeze ( - 2 ) # \u6dfb\u52a0dropout\u5c42, \u9632\u6b62\u8fc7\u62df\u5408 self . dropout = nn . Dropout ( dropout ) ''' \u5411\u6a21\u5757\u6dfb\u52a0\u6301\u4e45\u7f13\u51b2\u533a\u3002 \u8fd9\u901a\u5e38\u7528\u4e8e\u6ce8\u518c\u4e0d\u5e94\u88ab\u89c6\u4e3a\u6a21\u578b\u53c2\u6570\u7684\u7f13\u51b2\u533a\u3002\u4f8b\u5982\uff0cpos_embedding\u4e0d\u662f\u4e00\u4e2a\u53c2\u6570\uff0c\u800c\u662f\u6301\u4e45\u72b6\u6001\u7684\u4e00\u90e8\u5206\u3002 \u7f13\u51b2\u533a\u53ef\u4ee5\u4f7f\u7528\u7ed9\u5b9a\u7684\u540d\u79f0\u4f5c\u4e3a\u5c5e\u6027\u8bbf\u95ee\u3002 \u8bf4\u660e\uff1a \u5e94\u8be5\u5c31\u662f\u5728\u5185\u5b58\u4e2d\u5b9a\u4e49\u4e00\u4e2a\u5e38\u91cf\uff0c\u540c\u65f6\uff0c\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d\u7684\u65f6\u5019\u53ef\u4ee5\u5199\u5165\u548c\u8bfb\u51fa ''' self . register_buffer ( 'pos_embedding' , pos_embedding ) def forward ( self , token_embedding : Tensor ): # \u5c06token_embedding\u548c\u4f4d\u7f6e\u7f16\u7801\u76f8\u878d\u5408 return self . dropout ( token_embedding + self . pos_embedding [: token_embedding . size ( 0 ), :]) 3.2 \u5b9a\u4e49\u8bcd\u5d4c\u5165\u5c42\u7c7b \u00b6 class TokenEmbedding ( nn . Module ): def __init__ ( self , vocab_size : int , emb_size ): ''' vocab_size:\u8bcd\u8868\u7684\u5927\u5c0f emb_size:\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6 ''' super ( TokenEmbedding , self ) . __init__ () # \u8c03\u7528nn\u4e2d\u7684\u9884\u5b9a\u4e49\u5c42Embedding, \u83b7\u53d6\u4e00\u4e2a\u8bcd\u5d4c\u5165\u5bf9\u8c61self.embedding self . embedding = nn . Embedding ( vocab_size , emb_size ) # \u5c06emb_size\u4f20\u5165\u7c7b\u5185, \u53d8\u6210\u7c7b\u5185\u7684\u53d8\u91cf self . emb_size = emb_size def forward ( self , tokens : Tensor ): # \u8ba9 embeddings vector \u5728\u589e\u52a0 \u4e4b\u540e\u7684 postion encoing \u4e4b\u524d\u76f8\u5bf9\u5927\u4e00\u4e9b\u7684\u64cd\u4f5c\uff0c # \u4e3b\u8981\u662f\u4e3a\u4e86\u8ba9position encoding \u76f8\u5bf9\u7684\u5c0f\uff0c\u8fd9\u6837\u4f1a\u8ba9\u539f\u6765\u7684 embedding vector \u4e2d\u7684\u4fe1\u606f\u5728\u548c position encoding \u7684\u4fe1\u606f\u76f8\u52a0\u65f6\u4e0d\u81f3\u4e8e\u4e22\u5931\u6389 # \u8ba9 embeddings vector \u76f8\u5bf9\u5927\u4e00\u4e9b return self . embedding ( tokens . long ()) * math . sqrt ( self . emb_size ) 3.3 \u6784\u5efaSeq2SeqTransformer\u6a21\u578b \u00b6 class Seq2SeqTransformer ( nn . Module ): def __init__ ( self , num_encoder_layers , num_decoder_layers , emb_size , nhead , src_vocab_size , tgt_vocab_size , dim_feedforward = 512 , dropout = 0.1 ): ''' num_encoder_layers: \u7f16\u7801\u5668\u7684\u5c42\u6570 num_decoder_layers: \u89e3\u7801\u5668\u7684\u5c42\u6570 emb_size: \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6 nhead: \u5934\u6570 src_vocab_size: \u6e90\u8bed\u8a00\u7684\u8bcd\u8868\u5927\u5c0f tgt_vocab_size: \u76ee\u6807\u8bed\u8a00\u7684\u8bcd\u8868\u5927\u5c0f dim_feedforward: \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u7ef4\u5ea6 dropout: \u6b63\u5219\u5316\u7684\u5927\u5c0f ''' # \u7ee7\u627fnn.Module\u7c7b, \u4e00\u822c\u7ee7\u627f\u4e60\u60ef\u884c\u7684\u5199\u6cd5 super ( Seq2SeqTransformer , self ) . __init__ () # \u521b\u5efaTransformer\u5bf9\u8c61 self . transformer = Transformer ( d_model = emb_size , nhead = nhead , num_encoder_layers = num_encoder_layers , num_decoder_layers = num_decoder_layers , dim_feedforward = dim_feedforward , dropout = dropout ) # \u521b\u5efa\u5168\u8fde\u63a5\u7ebf\u6027\u5c42 self . generator = nn . Linear ( emb_size , tgt_vocab_size ) # \u521b\u5efa\u6e90\u8bed\u8a00\u7684embedding\u5c42 self . src_tok_emb = TokenEmbedding ( src_vocab_size , emb_size ) # \u521b\u5efa\u76ee\u6807\u8bed\u8a00\u7684embedding\u5c42 self . tgt_tok_emb = TokenEmbedding ( tgt_vocab_size , emb_size ) # \u521b\u5efa\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42\u5bf9\u8c61 self . positional_encoding = PositionalEncoding ( emb_size , dropout = dropout ) def forward ( self , src , trg , src_mask , tgt_mask , src_padding_mask , tgt_padding_mask , memory_key_padding_mask ): ''' src: \u6e90\u8bed\u8a00 trg: \u76ee\u6807\u8bed\u8a00 src_mask: \u6e90\u8bed\u8a00\u63a9\u7801 tgt_mask: \u76ee\u6807\u8bed\u8a00\u63a9\u7801 src_padding_mask: \u6e90\u8bed\u8a00\u7684padding_mask tgt_padding_mask: \u76ee\u6807\u8bed\u8a00\u7684padding_mask memory_key_padding_mask: \u4e2d\u95f4\u8bed\u4e49\u5f20\u91cf\u7684padding_mask ''' # \u83b7\u53d6\u6e90\u8bed\u8a00\u7684embedding\u5f20\u91cf\u878d\u5408\u4e86\u4f4d\u7f6e\u7f16\u7801 src_emb = self . positional_encoding ( self . src_tok_emb ( src )) # \u83b7\u53d6\u76ee\u6807\u8bed\u8a00\u7684embedding\u5f20\u91cf\u878d\u5408\u4e86\u4f4d\u7f6e\u7f16\u7801 tgt_emb = self . positional_encoding ( self . tgt_tok_emb ( trg )) # \u7ecf\u8fc7Transformer\u8fdb\u884c\u7f16\u89e3\u7801\u4e4b\u540e\u8f93\u51faout\u503c outs = self . transformer ( src_emb , tgt_emb , src_mask , tgt_mask , None , src_padding_mask , tgt_padding_mask , memory_key_padding_mask ) # outs\u503c\u7ecf\u8fc7\u8f93\u51fa\u5c42\u5f97\u5230\u6700\u540e\u7684\u8f93\u51fa\u5206\u5e03\u503c return self . generator ( outs ) # \u5b9a\u4e49Transformer\u7684\u7f16\u7801\u5668 def encode ( self , src , src_mask ): ''' src:\u6e90\u8bed\u8a00 src_mask:\u6e90\u8bed\u8a00\u63a9\u7801 ''' return self . transformer . encoder ( self . positional_encoding ( self . src_tok_emb ( src )), src_mask ) # \u5b9a\u4e49Transformer\u7684\u89e3\u7801\u5668 def decode ( self , tgt , memory , tgt_mask ): ''' tgt:\u76ee\u6807\u8bed\u8a00 memory:\u4e2d\u95f4\u8bed\u8a00\u5f20\u91cf\u8f93\u51fa tgt_mask: \u76ee\u6807\u8bed\u8a00\u7684\u63a9\u7801 ''' return self . transformer . decoder ( self . positional_encoding ( self . tgt_tok_emb ( tgt )), memory , tgt_mask ) 4 \u5b9a\u4e49mask\u7684\u51fd\u6570, \u521b\u5efa\u5bf9\u5e94\u7684\u4e0d\u540c\u7684mask \u00b6 4.1 \u5b9a\u4e49\u63a9\u7801 \u00b6 \u4f5c\u7528\u662f\u9632\u6b62\u6a21\u578b\u5728\u8fdb\u884c\u9884\u6d4b\u7684\u8fc7\u7a0b\u4e2d\u67e5\u770b\u5230\u672a\u6765\u7684\u5355\u8bcd. \u540c\u65f6\u9700\u8981\u63a9\u7801\u6765\u9690\u85cf\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684padding tokens def generate_square_subsequent_mask ( sz ): # sz: \u53e5\u5b50\u7684\u957f\u5ea6 # triu\u751f\u6210\u7684\u662f\u4e0a\u4e09\u89d2, \u7ecf\u8fc7transpose\u4e4b\u540e\u53d8\u6210\u4e86\u4e0b\u4e09\u89d2\u77e9\u9635 mask = ( torch . triu ( torch . ones (( sz , sz ), device = DEVICE )) == 1 ) . transpose ( 0 , 1 ) # \u5c060\u7684\u4f4d\u7f6e\u586b\u5145\u8d1f\u65e0\u7a77\u5c0f, \u5c061\u7684\u4f4d\u7f6e\u586b\u5145\u4e3a0 mask = mask . float () . masked_fill ( mask == 0 , float ( '-inf' )) . masked_fill ( mask == 1 , float ( 0.0 )) return mask def create_mask ( src , tgt ): ''' src: \u6e90\u8bed\u8a00\u5f20\u91cf\u5f62\u72b6\u4e3a: [seq_length , batch_size] tgt: \u76ee\u6807\u8bed\u8a00\u5f20\u91cf\u5f62\u72b6\u4e3a: [seq_length , batch_size] ''' # \u83b7\u53d6\u6e90\u8bed\u8a00\u7684\u53e5\u5b50\u957f\u5ea6 src_seq_len = src . shape [ 0 ] # \u83b7\u53d6\u76ee\u6807\u8bed\u8a00\u7684\u53e5\u5b50\u957f\u5ea6 tgt_seq_len = tgt . shape [ 0 ] # \u4ea7\u751f\u76ee\u6807\u8bed\u8a00\u7684\u63a9\u7801\u5f20\u91cf tgt_mask = generate_square_subsequent_mask ( tgt_seq_len ) # \u4ea7\u751f\u6e90\u8bed\u8a00\u7684\u63a9\u7801\u5f20\u91cf src_mask = torch . zeros (( src_seq_len , src_seq_len ), device = DEVICE ) . type ( torch . bool ) # \u6784\u5efa\u6e90\u8bed\u8a00\u7684padding_mask src_padding_mask==> [batch_size, seq_len] src_padding_mask = ( src == PAD_IDX ) . transpose ( 0 , 1 ) # \u6784\u5efa\u76ee\u6807\u8bed\u8a00\u7684padding_mask tgt_paddig_mask ==> [batch_size, seq_len-1] tgt_padding_mask = ( tgt == PAD_IDX ) . transpose ( 0 , 1 ) return src_mask , tgt_mask , src_padding_mask , tgt_padding_mask 5 \u5b9a\u4e49\u6279\u6b21\u6570\u636e\u5904\u7406\u7684\u56de\u8c03\u51fd\u6570 \u00b6 5.1 \u5c06\u5b57\u7b26\u4e32\u8f6c\u5316\u4e3a\u6574\u6570\u7684tensor\u5f20\u91cf \u00b6 # \u5c06\u53e5\u5b50\u5b57\u7b26\u8f6c\u5316\u4e3a\u5bf9\u5e94\u7684tensor\u5f20\u91cf def sequential_transforms ( * transforms ): ''' Transformers\u4e2d\u4f1a\u4f20\u5165\u4e09\u4e2a\u8fed\u4ee3\u5668: \u7b2c\u4e00\u4e2a\u662fTokenization\u7684, \u7b2c\u4e8c\u4e2a\u662fNumericalization, \u7b2c\u4e09\u4e2a\u662fAdd BOS/EOS and create tensor ''' def func ( txt_input ): # \u5faa\u73af\u4e09\u4e2a\u8fed\u4ee3\u5668, \u7b2c\u4e00\u4e2a\u8fdb\u884c\u8bed\u53e5\u7684\u5206\u5272, \u7b2c\u4e8c\u4e2a\u5c06\u5bf9\u5e94\u7684\u8bcd\u8bed\u6620\u5c04\u4e3a\u5bf9\u5e94\u7684\u5f20\u91cf\u8868\u793a, \u7b2c\u4e09\u4e2a\u662f\u5728\u6574\u4e2a\u53e5\u5b50\u7684\u9996\u5c3e\u90e8\u5206\u6dfb\u52a0\u8d77\u59cb\u548c\u7ed3\u675f\u6807\u5fd7. for transform in transforms : txt_input = transform ( txt_input ) return txt_input return func 5.2 \u5728\u53e5\u5b50\u9996\u5c3e\u6dfb\u52a0\u8d77\u59cb\u548c\u7ed3\u675f\u7b26\u53f7 \u00b6 # \u8f85\u52a9\u51fd\u6570, \u5b8c\u6210\u53e5\u5b50\u9996\u5c3eBOS/EOS\u7684\u6dfb\u52a0\u8fc7\u7a0b def tensor_transform ( token_ids : List [ int ]): # \u6dfb\u52a0\u7684\u662f\u5217\u8868\u5f62\u5f0f\u7684\u6570\u636e, \u5c06BOS\u548cEOS\u6dfb\u52a0\u5230\u53e5\u5b50\u7684\u9996\u5c3e\u90e8\u5206 return torch . cat (( torch . tensor ([ BOS_IDX ]), torch . tensor ( token_ids ), torch . tensor ([ EOS_IDX ]))) text_transform = {} # \u5faa\u73af\u6dfb\u52a0\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00 for ln in [ SRC_LANGUAGE , TGT_LANGUAGE ]: text_transform [ ln ] = sequential_transforms ( token_transform [ ln ], #Tokenization vocab_transform [ ln ], #Numericalization tensor_transform ) # Add BOS/EOS and create tensor 5.3 \u6570\u636e\u8fdb\u884c\u6279\u6b21\u5316\u5904\u7406 \u00b6 # \u6309\u7167\u6279\u6b21\u8fdb\u884c\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684\u7ec4\u88c5 def collate_fn ( batch ): # \u5b9a\u4e49\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684\u6279\u6b21\u5217\u8868 src_batch , tgt_batch = [], [] # \u5faa\u73af\u6279\u6b21\u6837\u672c for src_sample , tgt_sample in batch : # \u6dfb\u52a0\u6e90\u8bed\u8a00\u53e5\u5b50\u5230\u5217\u8868\u4e2d src_batch . append ( text_transform [ SRC_LANGUAGE ]( src_sample . rstrip ( \" \\n \" ))) # \u6dfb\u52a0\u76ee\u6807\u8bed\u8a00\u53e5\u5b50\u5230\u5217\u8868\u4e2d tgt_batch . append ( text_transform [ TGT_LANGUAGE ]( tgt_sample . rstrip ( \" \\n \" ))) # \u5c06\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u8fdb\u884c\u622a\u65ad\u8865\u9f50 PAD_IDX=1 # src_batch\u7684\u5f62\u72b6\u4e3a: [seq_length, batch] seq_length\u662f\u6700\u957f\u7684\u53e5\u5b50\u957f\u5ea6 src_batch = pad_sequence ( src_batch , padding_value = PAD_IDX ) # tgt_batch\u7684\u5f62\u72b6\u4e3a: [seq_length, batch] seq_length\u662f\u6700\u957f\u7684\u53e5\u5b50\u957f\u5ea6 tgt_batch = pad_sequence ( tgt_batch , padding_value = PAD_IDX ) return src_batch , tgt_batch 6 \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u548c\u8bc4\u4f30\u51fd\u6570 \u00b6 6.1 \u5b9e\u4f8b\u5316\u6a21\u578b\u5e76\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668 \u00b6 # \u8bbe\u7f6e\u79cd\u5b50\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\uff0c\u4ee5\u4f7f\u5f97\u7ed3\u679c\u662f\u786e\u5b9a\u7684 torch . manual_seed ( 0 ) # \u8bbe\u7f6e\u8c03\u7528\u65f6\u5019\u4f7f\u7528\u7684\u53c2\u6570 SRC_VOCAB_SIZE = len ( vocab_transform [ SRC_LANGUAGE ]) TGT_VOCAB_SIZE = len ( vocab_transform [ TGT_LANGUAGE ]) EMB_SIZE = 512 NHEAD = 8 FFN_HID_DIM = 512 BATCH_SIZE = 128 NUM_ENCODER_LAYERS = 3 NUM_DECODER_LAYERS = 3 # \u5b9e\u4f8b\u5316Transformer\u5bf9\u8c61 transformer = Seq2SeqTransformer ( NUM_ENCODER_LAYERS , NUM_DECODER_LAYERS , EMB_SIZE , NHEAD , SRC_VOCAB_SIZE , TGT_VOCAB_SIZE , FFN_HID_DIM ) # \u4e3a\u4e86\u4fdd\u8bc1\u6bcf\u5c42\u7684\u8f93\u5165\u548c\u8f93\u51fa\u7684\u65b9\u5dee\u76f8\u540c, \u9632\u6b62\u68af\u5ea6\u6d88\u5931\u95ee\u9898 for p in transformer . parameters (): if p . dim () > 1 : # \u6b64\u5904\u4f7f\u7528\u7684\u662fxavier\u7684\u5747\u5300\u5206\u5e03 nn . init . xavier_uniform_ ( p ) # \u5982\u679c\u6709GPU\u5219\u5c06\u6a21\u578b\u79fb\u52a8\u5230GPU\u4e0a transformer = transformer . to ( DEVICE ) # \u5b9a\u4e49\u635f\u5931\u51fd\u6570 loss_fn = torch . nn . CrossEntropyLoss ( ignore_index = PAD_IDX ) # \u5b9a\u4e49\u4f18\u5316\u5668 betas: \u7528\u4e8e\u8ba1\u7b97\u68af\u5ea6\u53ca\u5176\u5e73\u65b9\u7684\u8fd0\u884c\u5e73\u5747\u503c\u7684\u7cfb\u6570 eps:\u6dfb\u52a0\u5230\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027 optimizer = torch . optim . Adam ( transformer . parameters (), lr = 0.0001 , betas = ( 0.9 , 0.98 ), eps = 1e-9 ) 6.2 \u5b9a\u4e49\u6279\u6b21\u8bad\u7ec3\u51fd\u6570 \u00b6 def train_epoch ( model , optimizer ): # \u5f00\u542f\u8bad\u7ec3\u6a21\u5f0f model . train () # \u5b9a\u4e49\u5176\u5b9e\u7684\u635f\u5931\u503c\u4e3a0 losses = 0 # \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u8fed\u4ee3\u5668, \u8bed\u8a00\u5bf9\u4e3a(de, en) train_iter = Multi30k ( split = 'train' , language_pair = ( SRC_LANGUAGE , TGT_LANGUAGE )) # \u52a0\u8f7d\u6570\u636e, \u6309\u7167\u4e00\u4e2a\u6279\u6b21\u4e00\u4e2a\u6279\u6b21\u8fdb\u884c\u52a0\u8f7d, \u8fd4\u56de\u4e00\u4e2a\u8fed\u4ee3\u5668 train_dataloader = DataLoader ( train_iter , batch_size = BATCH_SIZE , collate_fn = collate_fn ) # \u5faa\u73af\u6570\u636e\u8fed\u4ee3\u5668 for src , tgt in train_dataloader : # \u5c06\u6e90\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a\u53bb src = src . to ( DEVICE ) # \u5c06\u76ee\u6807\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u8bbe\u5907\u4e0a\u53bb tgt = tgt . to ( DEVICE ) # \u83b7\u53d6\u8f93\u5165\u771f\u5b9e\u7684\u5f20\u91cf \u7b2c\u4e00\u4e2a\u5355\u8bcd\u5230\u5012\u6570\u7b2c\u4e8c\u4e2a\u5355\u8bcd tgt_input = tgt [: - 1 , :] # \u8c03\u7528mask\u51fd\u6570, \u751f\u6210\u5bf9\u5e94\u7684\u56db\u4e2amask src_mask , tgt_mask , src_padding_mask , tgt_padding_mask = create_mask ( src , tgt_input ) # \u8c03\u7528\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3, \u5f97\u5230\u6700\u540e\u7684\u5f20\u91cf\u5206\u5e03 logits = model ( src , tgt_input , src_mask , tgt_mask , src_padding_mask , tgt_padding_mask , src_padding_mask ) # \u68af\u5ea6\u6e05\u96f6 optimizer . zero_grad () # \u83b7\u53d6\u8f93\u51fa\u771f\u5b9e\u7684\u6807\u7b7e\u6570\u636e \u7b2c\u4e8c\u4e2a\u5355\u8bcd\u5230\u6700\u540e\u4e00\u4e2a\u5355\u8bcd tgt_out = tgt [ 1 :, :] # \u8ba1\u7b97\u635f\u5931 loss = loss_fn ( logits . reshape ( - 1 , logits . shape [ - 1 ]), tgt_out . reshape ( - 1 )) # \u53cd\u5411\u4f20\u64ad loss . backward () # \u68af\u5ea6\u66f4\u65b0 optimizer . step () # \u635f\u5931\u503c\u7d2f\u52a0\u6c42\u548c losses += loss . item () # \u8fd4\u56de\u5e73\u5747\u635f\u5931\u503c return losses / len ( train_dataloader ) 6.3 \u5b9a\u4e49\u6279\u6b21\u8bc4\u4f30\u51fd\u6570 \u00b6 def evaluate ( model ): # \u5f00\u542f\u6a21\u578b\u8bc4\u4f30\u6a21\u5f0f model . eval () # \u5b9a\u4e49\u8d77\u59cb\u635f\u5931\u503c losses = 0 # \u52a0\u8f7d\u9a8c\u8bc1\u6570\u636e\u96c6, \u8bed\u8a00\u5bf9\u4e3a(de, en) val_iter = Multi30k ( split = 'valid' , language_pair = ( SRC_LANGUAGE , TGT_LANGUAGE )) # \u8fd4\u56de\u9a8c\u8bc1\u96c6\u7684\u6570\u636e\u52a0\u8f7d\u5668 val_dataloader = DataLoader ( val_iter , batch_size = BATCH_SIZE , collate_fn = collate_fn ) # \u5faa\u73af\u9a8c\u8bc1\u96c6 for src , tgt in val_dataloader : # \u6e90\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a src = src . to ( DEVICE ) # \u76ee\u6807\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a tgt = tgt . to ( DEVICE ) # \u83b7\u53d6\u8f93\u5165\u7684\u771f\u5b9e\u7684\u5f20\u91cf tgt_input = tgt [: - 1 , :] # \u8c03\u7528mask\u51fd\u6570, \u4ea7\u751f\u5bf9\u5e94\u7684\u56db\u4e2amask\u503c src_mask , tgt_mask , src_padding_mask , tgt_padding_mask = create_mask ( src , tgt_input ) # \u8c03\u7528\u6a21\u578b, \u5f97\u5230\u5bf9\u5e94\u7684\u8f93\u51fa\u5206\u5e03\u503c logits = model ( src , tgt_input , src_mask , tgt_mask , src_padding_mask , tgt_padding_mask , src_padding_mask ) # \u83b7\u53d6\u8f93\u51fa\u7684\u771f\u5b9e\u5f20\u91cf tgt_out = tgt [ 1 :, :] # \u8ba1\u7b97\u635f\u5931\u503c loss = loss_fn ( logits . reshape ( - 1 , logits . shape [ - 1 ]), tgt_out . reshape ( - 1 )) # \u635f\u5931\u503c\u7d2f\u52a0, \u6c42\u548c losses += loss . item () # \u6c42\u5f97\u5bf9\u5e94\u7684\u5e73\u5747\u635f\u5931 return losses / len ( val_dataloader ) 7 \u8bad\u7ec3Transformer\u6a21\u578b \u00b6 7.1 \u5229\u7528\u5faa\u73af\u8bad\u7ec3Transformer\u6a21\u578b \u00b6 # \u5b9a\u4e49epoch\u7684\u6b21\u6570 NUM_EPOCHS = 18 # \u5faa\u73af\u6574\u4e2a\u6570\u636e\u96c6num_epochs\u6b21 for epoch in range ( 1 , NUM_EPOCHS + 1 ): # \u83b7\u53d6\u5f00\u59cb\u65f6\u95f4 start_time = timer () # \u5c06\u6574\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3 train_loss = train_epoch ( transformer , optimizer ) # \u83b7\u53d6\u7ed3\u675f\u65f6\u95f4 end_time = timer () # \u5c06\u6574\u4e2a\u9a8c\u8bc1\u96c6\u8fdb\u884c\u8bc4\u4f30 val_loss = evaluate ( transformer ) # \u6253\u5370\u6bcf\u4e2aepoch\u7684\u8bad\u7ec3\u635f\u5931, \u9a8c\u8bc1\u635f\u5931, \u548c\u8bad\u7ec3\u65f6\u95f4. print (( f \"Epoch: { epoch } , Train loss: { train_loss : .3f } , Val loss: { val_loss : .3f } , \" f \"Epoch time = { ( end_time - start_time ) : .3f } s\" )) \u8f93\u51fa\u6548\u679c\u5c55\u793a Epoch : 1 , Train loss : 5.342 , Val loss : 4.138 , Epoch time = 653.749 s Epoch : 2 , Train loss : 3.799 , Val loss : 3.370 , Epoch time = 649.536 s Epoch : 3 , Train loss : 3.184 , Val loss : 2.921 , Epoch time = 644.899 s Epoch : 4 , Train loss : 2.782 , Val loss : 2.642 , Epoch time = 648.685 s Epoch : 5 , Train loss : 2.490 , Val loss : 2.453 , Epoch time = 650.243 s Epoch : 6 , Train loss : 2.256 , Val loss : 2.321 , Epoch time = 647.609 s Epoch : 7 , Train loss : 2.064 , Val loss : 2.210 , Epoch time = 654.674 s Epoch : 8 , Train loss : 1.905 , Val loss : 2.132 , Epoch time = 659.779 s Epoch : 9 , Train loss : 1.761 , Val loss : 2.070 , Epoch time = 652.363 s Epoch : 10 , Train loss : 1.637 , Val loss : 2.016 , Epoch time = 646.682 s Epoch : 11 , Train loss : 1.527 , Val loss : 1.977 , Epoch time = 643.913 s Epoch : 12 , Train loss : 1.427 , Val loss : 1.970 , Epoch time = 640.084 s Epoch : 13 , Train loss : 1.335 , Val loss : 1.964 , Epoch time = 639.331 s Epoch : 14 , Train loss : 1.253 , Val loss : 1.936 , Epoch time = 639.232 s Epoch : 15 , Train loss : 1.173 , Val loss : 1.928 , Epoch time = 649.990 s Epoch : 16 , Train loss : 1.106 , Val loss : 1.909 , Epoch time = 636.465 s Epoch : 17 , Train loss : 1.038 , Val loss : 1.905 , Epoch time = 644.609 s Epoch : 18 , Train loss : 0.976 , Val loss : 1.914 , Epoch time = 644.115 s \u6ce8\u610f: \u8fd9\u4e2a\u8bad\u7ec3\u7684\u8fc7\u7a0b\u662f4\u68388G\u5185\u5b58\u7684CPU\u670d\u52a1\u5668,\u5927\u5bb6\u53ef\u4ee5\u66f4\u6362\u4e3aGPU\u670d\u52a1\u5668, \u901f\u5ea6\u4f1a\u66f4\u5feb. 8 \u8fdb\u884c\u89e3\u7801\u751f\u6210\u76ee\u6807\u8bed\u8a00\u8bed\u53e5 \u00b6 8.1 \u4f7f\u7528\u8d2a\u5fc3\u7b97\u6cd5\u6784\u5efa\u751f\u6210\u5e8f\u5217\u51fd\u6570 \u00b6 def greedy_decode ( model , src , src_mask , max_len , start_symbol ): # \u5c06\u5bf9\u5e94\u7684\u6e90\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u7684\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a src = src . to ( DEVICE ) # \u5c06\u5bf9\u5e94\u7684\u6e90\u8bed\u8a00\u7684mask\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a src_mask = src_mask . to ( DEVICE ) # \u5c06\u6e90\u8bed\u8a00\u4f7f\u7528\u6a21\u578b\u7684\u7f16\u7801\u5668, \u5f97\u5230\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cf memory\u7684\u5f62\u72b6\u4e3a: [seq_len, batch_size, dim] memory = model . encode ( src , src_mask ) # \u6784\u5efa\u4e00\u4e2a\u8d77\u59cb\u7684\u4e8c\u7ef4\u77e9\u9635, \u7136\u540e\u51c6\u5907\u5f00\u59cb\u53e5\u5b50\u7684\u89e3\u7801\u8fc7\u7a0b. ys\u5f62\u72b6\u4e3a[1, 1]\u4e8c\u7ef4\u7684 ys = torch . ones ( 1 , 1 ) . fill_ ( start_symbol ) . type ( torch . long ) . to ( DEVICE ) for i in range ( max_len - 1 ): # \u5c06\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cf\u7684\u6570\u636e\u4e00\u5b9a\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a memory = memory . to ( DEVICE ) # \u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684mask\u503c tgt_mask = ( generate_square_subsequent_mask ( ys . size ( 0 )) . type ( torch . bool )) . to ( DEVICE ) # \u8c03\u7528\u6a21\u578b\u7684\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801 out\u5f62\u72b6\u4e3a:[seq_len, 1, 512]==> [seq_len, batch_size, emb_size] out = model . decode ( ys , memory , tgt_mask ) # \u8f93\u51fa\u5f20\u91cf\u8fdb\u884c\u5f62\u72b6\u7684\u8f6c\u6362 out = out . transpose ( 0 , 1 ) # \u7ecf\u8fc7\u6700\u540e\u8f93\u51fa\u5c42, \u83b7\u53d6\u6700\u540e\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03 out[:, -1]\u5f62\u72b6\u4e3a: [1, 512] --> [seq_len, emb_size] # prob\u7684\u5f62\u72b6\u4e3a: [1, tgt_vocab_size] prob = model . generator ( out [:, - 1 ]) # \u57281\u7ef4\u5ea6\u4e0a, \u83b7\u53d6\u6982\u7387\u6700\u5927\u7684\u90a3\u4e2a\u5c31\u662f\u6700\u540e\u9884\u6d4b\u7684\u90a3\u4e2a\u503c max\u8fd4\u56de\u4e24\u4e2a\u503c, \u7b2c\u4e00\u4e2a\u662f\u8fd4\u56de\u7684\u6700\u5927\u503c\u7684\u6982\u7387, \u7b2c\u4e8c\u4e2a\u662f\u8fd4\u56de\u6700\u5927\u6982\u7387\u7684\u4e0b\u6807\u503c. _ , next_word = torch . max ( prob , dim = 1 ) # \u83b7\u53d6\u5bf9\u5e94\u7684\u90a3\u4e2a\u4e0b\u6807\u503c next_word = next_word . item () # \u62fc\u63a5\u4e0a\u4e00\u6b65\u548c\u8fd9\u4e00\u6b65\u4ea7\u751f\u7684\u5355\u8bcd, \u4f5c\u4e3a\u4e0b\u4e00\u6b65\u4f7f\u7528\u7684ys fill_()\u8868\u793a\u7528\u62ec\u53f7\u4e2d\u7684\u6570\u5b57\u53bb\u586b\u5145\u6574\u4e2a\u77e9\u9635 ys = torch . cat ([ ys , torch . ones ( 1 , 1 ) . type_as ( src . data ) . fill_ ( next_word )], dim = 0 ) if next_word == EOS_IDX : break return ys 8.2 \u5b9a\u4e49\u6700\u7ec8\u7684\u7ffb\u8bd1\u8f6c\u5316\u51fd\u6570 \u00b6 def translate ( model : torch . nn . Module , src_sentence : str ): ''' model: \u8f93\u5165\u6574\u4e2aTransformer\u6a21\u578b src_sentence:\u8981\u7ffb\u8bd1\u7684\u8bed\u53e5 ''' # \u5f00\u542f\u6a21\u578b\u7684\u8bc4\u4f30\u6a21\u5f0f model . eval () # \u5c06\u6e90\u8bed\u53e5\u8f6c\u5316\u4e3a\u5bf9\u5e94\u7684\u5f20\u91cf\u8868\u793a \u8d77\u521d\u662f\u4e00\u7ef4\u7684(seq_len, ), \u540e\u7ecf\u8fc7view(-1, 1)\u8f6c\u5316\u4e3a[seq_len, 1]\u4e8c\u7ef4\u7684\u5f62\u72b6. src = text_transform [ SRC_LANGUAGE ]( src_sentence ) . view ( - 1 , 1 ) # src.shape==> [seq_len, 1] num_tokens = src . shape [ 0 ] # \u521b\u5efa\u4e00\u4e2a\u5168\u96f6\u7684\u77e9\u9635\u4f5c\u4e3asrc_mask\u7684\u8d77\u59cb\u77e9\u9635 src_mask = ( torch . zeros ( num_tokens , num_tokens )) . type ( torch . bool ) # \u4f7f\u7528\u8d2a\u5fc3\u7b97\u6cd5\u8fdb\u884c\u89e3\u7801 tgt_tokens = greedy_decode ( model , src , src_mask , max_len = num_tokens + 5 , start_symbol = BOS_IDX ) . flatten () # \u73b0\u5c06\u6570\u636e\u4eceGPU\u4e0a\u8fc1\u79fb\u5230CPU\u4e0a, \u7136\u540e\u8fdb\u884ctensor\u7c7b\u578b\u8f6c\u5316\u4e3anumpy.ndarray\u7c7b\u578b\u7684\u6574\u6570\u503c # \u4f7f\u7528lookup_tokens\u8fdb\u884c\u7d22\u5f15\u5230\u5bf9\u5e94\u5b57\u7b26\u7684\u67e5\u627e, \u53cd\u8f6c\u4e3a\u5bf9\u5e94\u7684\u5b57\u7b26, \u7136\u540e\u5c06\u53e5\u5b50\u7684\u9996\u5c3e\u7684bos\u548ceos\u66ff\u6362\u6389, \u5373\u4e3a\u89e3\u7801\u4e4b\u540e\u7684\u8bed\u53e5. return \" \" . join ( vocab_transform [ TGT_LANGUAGE ] . lookup_tokens ( list ( tgt_tokens . cpu () . numpy ()))) . replace ( \"<bos>\" , \"\" ) . replace ( \"<eos>\" , \"\" ) \u9a8c\u8bc1 print ( translate ( transformer , \"Eine Gruppe von Menschen steht vor einem Iglu .\" )) \u8f93\u51fa\u6548\u679c A group of people stand in front of an aquarium . 9 \u6a21\u578b\u7684\u4fdd\u5b58\u548c\u91cd\u52a0\u8f7d \u00b6 9.1 \u6a21\u578b\u7684\u4fdd\u5b58 \u00b6 path = './model/transformer_translation_18.pth' torch . save ( transformer . state_dict (), path ) 9.2 \u6a21\u578b\u7684\u91cd\u52a0\u8f7d \u00b6 transformer = Seq2SeqTransformer ( NUM_ENCODER_LAYERS , NUM_DECODER_LAYERS , EMB_SIZE , NHEAD , SRC_VOCAB_SIZE , TGT_VOCAB_SIZE , FFN_HID_DIM ) transformer . load_state_dict ( torch . load ( path ))","title":"8 \u6848\u4f8bTransformer\u673a\u5668\u7ffb\u8bd1\u6a21\u578b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#_1","text":"\u4e86\u89e3\u6709\u5173\u673a\u5668\u7ffb\u8bd1\u7684\u77e5\u8bc6 \u4e86\u89e3seq2seq\u67b6\u6784 \u638c\u63e1\u4f7f\u7528Transformer\u6784\u5efa\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u7684\u5b9e\u73b0\u8fc7\u7a0b","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#1-transformer","text":"Transformer\u6a21\u578b\u67b6\u6784\u5206\u6790 \u4ece\u56fe\u4e2d\u53ef\u77e5, Transformer\u6a21\u578b\u67b6\u6784, \u5927\u8303\u56f4\u5185\u5305\u62ec\u4e24\u90e8\u5206\u5206\u522b\u662fencoder(\u7f16\u7801\u5668)\u548cdecoder(\u89e3\u7801\u5668), \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u5185\u90e8\u5b9e\u73b0\u90fd\u4f7f\u7528\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0, \u8fd9\u91cc\u5b83\u8981\u5b8c\u6210\u7684\u662f\u4e00\u4e2a\u5fb7\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1: Willkommen in peking \u2192 welcome to BeiJing. \u7f16\u7801\u5668\u9996\u5148\u5904\u7406\u4e2d\u6587\u8f93\u5165\"Willkommen in peking\", \u901a\u8fc7Transformer\u5185\u90e8\u7684\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u4fe1\u606f\u4e4b\u540e\u7684\u8f93\u51fa\u5f20\u91cf\uff0c\u5c31\u662f\u4e00\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc, \u63a5\u7740\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e2a\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cfc\u4ee5\u53ca\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u76ee\u6807\u8bed\u8a00\u7684\u8bed\u4e49\u5f20\u91cf, \u9010\u4e2a\u751f\u6210\u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00.","title":"1 Transformer\u67b6\u6784"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#2","text":"","title":"2 \u6848\u4f8b\u4ecb\u7ecd"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#21","text":"\u4f7f\u7528\u7684\u662ftorchtext\u4e2d\u81ea\u5e26\u7684\u6570\u636e\u96c6Multi30k, \u76f4\u63a5\u53ef\u4ee5\u4f7f\u7528\u5185\u7f6e\u7684API\u51fd\u6570\u5373\u53ef\u4e0b\u8f7d # \u9ed8\u8ba4\u4e0b\u8f7d\u7684\u8def\u5f84\u4e3a: /root/.torchtext/cache/Multi30k \u2514\u2500\u2500 Multi30k \u251c\u2500\u2500 mmt16_task1_test . tar . gz \u251c\u2500\u2500 test . de \u251c\u2500\u2500 test . en \u251c\u2500\u2500 train . de \u251c\u2500\u2500 train . en \u251c\u2500\u2500 training . tar . gz \u251c\u2500\u2500 val . de \u251c\u2500\u2500 val . en \u2514\u2500\u2500 validation . tar . gz","title":"2.1 \u6570\u636e\u96c6:"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#22","text":"\u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bfc\u5165Multi30k\u6570\u636e\u96c6\u5e76\u505a\u57fa\u672c\u5904\u7406 \u7b2c\u4e09\u6b65: \u6784\u5efaTransformer\u6a21\u578b \u7b2c\u56db\u6b65: \u5b9a\u4e49mask\u7684\u51fd\u6570, \u521b\u5efa\u5bf9\u5e94\u7684\u4e0d\u540c\u7684mask \u7b2c\u4e94\u6b65: \u5b9a\u4e49\u6279\u6b21\u6570\u636e\u5904\u7406\u7684\u56de\u8c03\u51fd\u6570 \u7b2c\u516d\u6b65: \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u548c\u8bc4\u4f30\u51fd\u6570 \u7b2c\u4e03\u6b65: \u8bad\u7ec3Transformer\u6a21\u578b \u7b2c\u516b\u6b65: \u8fdb\u884c\u89e3\u7801\u751f\u6210\u76ee\u6807\u8bed\u8a00\u8bed\u53e5 \u7b2c\u4e5d\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u548c\u91cd\u52a0\u8f7d","title":"2.2 \u673a\u5668\u7ffb\u8bd1\u8fc7\u7a0b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#3","text":"","title":"3 \u6848\u4f8b\u5b9e\u73b0\u6b65\u9aa4"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#1","text":"import torch import torch.nn as nn import math from torchtext.data.utils import get_tokenizer from torchtext.vocab import build_vocab_from_iterator from torchtext.datasets import Multi30k from typing import Iterable , List from torch import Tensor from torch.nn import Transformer from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader from timeit import default_timer as timer DEVICE = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )","title":"1 \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#2-multi30k","text":"","title":"2 \u5bfc\u5165Multi30k\u6570\u636e\u96c6\u5e76\u505a\u57fa\u672c\u5904\u7406"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#21-tokenizer","text":"# \u6e90\u8bed\u8a00\u662f\u5fb7\u8bed SRC_LANGUAGE = 'de' # \u76ee\u6807\u8bed\u8a00\u662f\u82f1\u8bed TGT_LANGUAGE = 'en' # \u5b9a\u4e49token\u7684\u5b57\u5178, \u5b9a\u4e49vocab\u5b57\u5178 token_transform = {} vocab_transform = {} # \u521b\u5efa\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684kokenizer, \u786e\u4fdd\u4f9d\u8d56\u5173\u7cfb\u5df2\u7ecf\u5b89\u88c5 # pip install -U spacy # python -m spacy download en_core_web_sm # python -m spacy download de_core_news_sm # get_tokenizer\u662f\u5206\u8bcd\u51fd\u6570, \u5982\u679c\u6ca1\u6709\u7279\u6b8a\u7684\u5219\u6309\u7167\u82f1\u8bed\u7684\u7a7a\u683c\u5206\u5272, \u5982\u679c\u6709\u8fd9\u6309\u7167\u5bf9\u5e94\u7684\u5206\u8bcd\u5e93\u8fd4\u56de. \u6bd4\u5982spacy, \u8fd4\u56de\u5bf9\u5e94\u7684\u5206\u8bcd\u5e93 token_transform [ SRC_LANGUAGE ] = get_tokenizer ( 'spacy' , language = 'de_core_news_sm' ) token_transform [ TGT_LANGUAGE ] = get_tokenizer ( 'spacy' , language = 'en_core_web_sm' )","title":"2.1 \u52a0\u8f7d\u5bf9\u5e94\u7684tokenizer"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#22_1","text":"def yield_tokens ( data_iter : Iterable , language : str ) -> List [ str ]: # data_iter: \u5bf9\u8c61\u7684\u8fed\u4ee3\u5bf9\u8c61 Multi30k\u5bf9\u8c61 # language: \u5bf9\u5e94\u7684\u7ffb\u8bd1\u8bed\u8a00 {'de': 0, 'en': 1} language_index = { SRC_LANGUAGE : 0 , TGT_LANGUAGE : 1 } # \u8fd4\u56de\u5bf9\u5e94\u7684\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61 for data_sample in data_iter : # data_sample:(\u5fb7\u6587, \u82f1\u6587) # data_sample:('Zwei junge wei\u00dfe M\u00e4nner sind im Freien in der N\u00e4he vieler B\u00fcsche.\\n', 'Two young, White males are outside near many bushes.\\n') # token_transform['de']()=['Zwei', 'junge', 'wei\u00dfe', 'M\u00e4nner', 'sind', 'im', 'Freien', 'in', 'der', 'N\u00e4he', 'vieler', 'B\u00fcsche', '.', '\\n'] # or token_transform['en']\u5206\u522b\u8fdb\u884c\u6784\u9020\u5bf9\u5e94\u7684\u5b57\u5178 yield token_transform [ language ]( data_sample [ language_index [ language ]])","title":"2.2 \u6784\u5efa\u751f\u6210\u5206\u8bcd\u7684\u8fed\u4ee3\u5668"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#23","text":"# \u5b9a\u4e49\u7279\u6b8a\u5b57\u7b26\u53ca\u5176\u5bf9\u5e94\u7684\u7d22\u5f15\u503c UNK_IDX , PAD_IDX , BOS_IDX , EOS_IDX = 0 , 1 , 2 , 3 # \u786e\u4fdd\u6807\u8bb0\u6309\u5176\u7d22\u5f15\u7684\u987a\u5e8f\u6b63\u786e\u63d2\u5165\u5230\u8bcd\u6c47\u8868\u4e2d special_symbols = [ '<unk>' , '<pad>' , '<bos>' , '<eos>' ] for ln in [ SRC_LANGUAGE , TGT_LANGUAGE ]: # \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u8fed\u4ee3\u5668, # \u6570\u636e\u96c6\u662f\u7528\u82f1\u6587\u63cf\u8ff0\u56fe\u50cf\u7684\u82f1\u6587\u8bed\u53e5, \u7136\u540e\u4eba\u5de5\u5c06\u5176\u7ffb\u8bd1\u4e3a\u5fb7\u6587\u7684\u8bed\u53e5,\u6709\u4e24\u4e2a\u6587\u4ef6, \u4e00\u4e2a\u662ftrain.de \u4e00\u4e2a\u662ftrain.en\u6587\u4ef6, # \u7136\u540e\u5c06\u5176\u6784\u5efa\u4e3a(\u5fb7\u6587, \u82f1\u6587)\u7684\u5f62\u5f0f train_iter = Multi30k ( split = 'train' , language_pair = ( SRC_LANGUAGE , TGT_LANGUAGE )) # \u521b\u5efatorchtext\u7684vocab\u5bf9\u8c61, \u5373\u8bcd\u6c47\u8868 vocab_transform [ ln ] = build_vocab_from_iterator ( yield_tokens ( train_iter , ln ), # \u7528\u4e8e\u6784\u5efa Vocab \u7684\u8fed\u4ee3\u5668\u3002\u5fc5\u987b\u4ea7\u751f\u4ee4\u724c\u5217\u8868\u6216\u8fed\u4ee3\u5668 min_freq = 1 , #\u5728\u8bcd\u6c47\u8868\u4e2d\u5305\u542b\u4e00\u4e2a\u6807\u8bb0\u6240\u9700\u7684\u6700\u4f4e\u9891\u7387 specials = special_symbols , # \u7528\u4e8e\u6dfb\u52a0\u7684\u7279\u6b8a\u5b57\u7b26 special_first = True ) # \u6307\u793a\u662f\u5728\u5f00\u5934\u8fd8\u662f\u7ed3\u5c3e\u63d2\u5165\u7b26\u53f7 # \u5c06 UNK_IDX \u8bbe\u7f6e\u4e3a\u9ed8\u8ba4\u7d22\u5f15\u3002\u672a\u627e\u5230\u4ee4\u724c\u65f6\u8fd4\u56de\u6b64\u7d22\u5f15 # \u5982\u679c\u672a\u8bbe\u7f6e\uff0c\u5219\u5728 Vocabulary \u4e2d\u627e\u4e0d\u5230\u67e5\u8be2\u7684\u6807\u8bb0\u65f6\u629b\u51fa RuntimeError for ln in [ SRC_LANGUAGE , TGT_LANGUAGE ]: vocab_transform [ ln ] . set_default_index ( UNK_IDX )","title":"2.3 \u5b9a\u4e49\u7279\u6b8a\u5b57\u7b26\u5e76\u4e0b\u8f7d\u6570\u636e\u8bbe\u7f6e\u9ed8\u8ba4\u7d22\u5f15"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#3-transformer","text":"","title":"3 \u6784\u5efaTransformer\u6a21\u578b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#31","text":"class PositionalEncoding ( nn . Module ): def __init__ ( self , emb_size : int , dropout : float , maxlen : int = 5000 ): ''' emb_size: \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5927\u5c0f dropout: \u6b63\u5219\u5316\u7684\u5927\u5c0f maxlen: \u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6 ''' super ( PositionalEncoding , self ) . __init__ () # \u5c061000\u76842i/d_model\u53d8\u578b\u4e3ae\u7684\u6307\u6570\u5f62\u5f0f den = torch . exp ( - torch . arange ( 0 , emb_size , 2 ) * math . log ( 10000 ) / emb_size ) # \u6548\u679c\u7b49\u4ef7\u4e0etorch.arange(0, maxlen).unsqueeze(1) pos = torch . arange ( 0 , maxlen ) . reshape ( maxlen , 1 ) # \u6784\u5efa\u4e00\u4e2a(maxlen, emb_size)\u5927\u5c0f\u7684\u5168\u96f6\u77e9\u9635 pos_embedding = torch . zeros (( maxlen , emb_size )) # \u5076\u6570\u5217\u662f\u6b63\u5f26\u51fd\u6570\u586b\u5145 pos_embedding [:, 0 :: 2 ] = torch . sin ( pos * den ) # \u5947\u6570\u5217\u662f\u4f59\u5f26\u51fd\u6570\u586b\u5145 pos_embedding [:, 1 :: 2 ] = torch . cos ( pos * den ) # \u5c06\u5176\u7ef4\u5ea6\u53d8\u6210\u4e09\u7ef4, \u4e3a\u4e86\u540e\u671f\u65b9\u4fbf\u8ba1\u7b97 pos_embedding = pos_embedding . unsqueeze ( - 2 ) # \u6dfb\u52a0dropout\u5c42, \u9632\u6b62\u8fc7\u62df\u5408 self . dropout = nn . Dropout ( dropout ) ''' \u5411\u6a21\u5757\u6dfb\u52a0\u6301\u4e45\u7f13\u51b2\u533a\u3002 \u8fd9\u901a\u5e38\u7528\u4e8e\u6ce8\u518c\u4e0d\u5e94\u88ab\u89c6\u4e3a\u6a21\u578b\u53c2\u6570\u7684\u7f13\u51b2\u533a\u3002\u4f8b\u5982\uff0cpos_embedding\u4e0d\u662f\u4e00\u4e2a\u53c2\u6570\uff0c\u800c\u662f\u6301\u4e45\u72b6\u6001\u7684\u4e00\u90e8\u5206\u3002 \u7f13\u51b2\u533a\u53ef\u4ee5\u4f7f\u7528\u7ed9\u5b9a\u7684\u540d\u79f0\u4f5c\u4e3a\u5c5e\u6027\u8bbf\u95ee\u3002 \u8bf4\u660e\uff1a \u5e94\u8be5\u5c31\u662f\u5728\u5185\u5b58\u4e2d\u5b9a\u4e49\u4e00\u4e2a\u5e38\u91cf\uff0c\u540c\u65f6\uff0c\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d\u7684\u65f6\u5019\u53ef\u4ee5\u5199\u5165\u548c\u8bfb\u51fa ''' self . register_buffer ( 'pos_embedding' , pos_embedding ) def forward ( self , token_embedding : Tensor ): # \u5c06token_embedding\u548c\u4f4d\u7f6e\u7f16\u7801\u76f8\u878d\u5408 return self . dropout ( token_embedding + self . pos_embedding [: token_embedding . size ( 0 ), :])","title":"3.1 \u5b9a\u4e49\u4f4d\u7f6e\u7f16\u7801\u5668\u7c7b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#32","text":"class TokenEmbedding ( nn . Module ): def __init__ ( self , vocab_size : int , emb_size ): ''' vocab_size:\u8bcd\u8868\u7684\u5927\u5c0f emb_size:\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6 ''' super ( TokenEmbedding , self ) . __init__ () # \u8c03\u7528nn\u4e2d\u7684\u9884\u5b9a\u4e49\u5c42Embedding, \u83b7\u53d6\u4e00\u4e2a\u8bcd\u5d4c\u5165\u5bf9\u8c61self.embedding self . embedding = nn . Embedding ( vocab_size , emb_size ) # \u5c06emb_size\u4f20\u5165\u7c7b\u5185, \u53d8\u6210\u7c7b\u5185\u7684\u53d8\u91cf self . emb_size = emb_size def forward ( self , tokens : Tensor ): # \u8ba9 embeddings vector \u5728\u589e\u52a0 \u4e4b\u540e\u7684 postion encoing \u4e4b\u524d\u76f8\u5bf9\u5927\u4e00\u4e9b\u7684\u64cd\u4f5c\uff0c # \u4e3b\u8981\u662f\u4e3a\u4e86\u8ba9position encoding \u76f8\u5bf9\u7684\u5c0f\uff0c\u8fd9\u6837\u4f1a\u8ba9\u539f\u6765\u7684 embedding vector \u4e2d\u7684\u4fe1\u606f\u5728\u548c position encoding \u7684\u4fe1\u606f\u76f8\u52a0\u65f6\u4e0d\u81f3\u4e8e\u4e22\u5931\u6389 # \u8ba9 embeddings vector \u76f8\u5bf9\u5927\u4e00\u4e9b return self . embedding ( tokens . long ()) * math . sqrt ( self . emb_size )","title":"3.2 \u5b9a\u4e49\u8bcd\u5d4c\u5165\u5c42\u7c7b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#33-seq2seqtransformer","text":"class Seq2SeqTransformer ( nn . Module ): def __init__ ( self , num_encoder_layers , num_decoder_layers , emb_size , nhead , src_vocab_size , tgt_vocab_size , dim_feedforward = 512 , dropout = 0.1 ): ''' num_encoder_layers: \u7f16\u7801\u5668\u7684\u5c42\u6570 num_decoder_layers: \u89e3\u7801\u5668\u7684\u5c42\u6570 emb_size: \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6 nhead: \u5934\u6570 src_vocab_size: \u6e90\u8bed\u8a00\u7684\u8bcd\u8868\u5927\u5c0f tgt_vocab_size: \u76ee\u6807\u8bed\u8a00\u7684\u8bcd\u8868\u5927\u5c0f dim_feedforward: \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u7ef4\u5ea6 dropout: \u6b63\u5219\u5316\u7684\u5927\u5c0f ''' # \u7ee7\u627fnn.Module\u7c7b, \u4e00\u822c\u7ee7\u627f\u4e60\u60ef\u884c\u7684\u5199\u6cd5 super ( Seq2SeqTransformer , self ) . __init__ () # \u521b\u5efaTransformer\u5bf9\u8c61 self . transformer = Transformer ( d_model = emb_size , nhead = nhead , num_encoder_layers = num_encoder_layers , num_decoder_layers = num_decoder_layers , dim_feedforward = dim_feedforward , dropout = dropout ) # \u521b\u5efa\u5168\u8fde\u63a5\u7ebf\u6027\u5c42 self . generator = nn . Linear ( emb_size , tgt_vocab_size ) # \u521b\u5efa\u6e90\u8bed\u8a00\u7684embedding\u5c42 self . src_tok_emb = TokenEmbedding ( src_vocab_size , emb_size ) # \u521b\u5efa\u76ee\u6807\u8bed\u8a00\u7684embedding\u5c42 self . tgt_tok_emb = TokenEmbedding ( tgt_vocab_size , emb_size ) # \u521b\u5efa\u4f4d\u7f6e\u7f16\u7801\u5668\u5c42\u5bf9\u8c61 self . positional_encoding = PositionalEncoding ( emb_size , dropout = dropout ) def forward ( self , src , trg , src_mask , tgt_mask , src_padding_mask , tgt_padding_mask , memory_key_padding_mask ): ''' src: \u6e90\u8bed\u8a00 trg: \u76ee\u6807\u8bed\u8a00 src_mask: \u6e90\u8bed\u8a00\u63a9\u7801 tgt_mask: \u76ee\u6807\u8bed\u8a00\u63a9\u7801 src_padding_mask: \u6e90\u8bed\u8a00\u7684padding_mask tgt_padding_mask: \u76ee\u6807\u8bed\u8a00\u7684padding_mask memory_key_padding_mask: \u4e2d\u95f4\u8bed\u4e49\u5f20\u91cf\u7684padding_mask ''' # \u83b7\u53d6\u6e90\u8bed\u8a00\u7684embedding\u5f20\u91cf\u878d\u5408\u4e86\u4f4d\u7f6e\u7f16\u7801 src_emb = self . positional_encoding ( self . src_tok_emb ( src )) # \u83b7\u53d6\u76ee\u6807\u8bed\u8a00\u7684embedding\u5f20\u91cf\u878d\u5408\u4e86\u4f4d\u7f6e\u7f16\u7801 tgt_emb = self . positional_encoding ( self . tgt_tok_emb ( trg )) # \u7ecf\u8fc7Transformer\u8fdb\u884c\u7f16\u89e3\u7801\u4e4b\u540e\u8f93\u51faout\u503c outs = self . transformer ( src_emb , tgt_emb , src_mask , tgt_mask , None , src_padding_mask , tgt_padding_mask , memory_key_padding_mask ) # outs\u503c\u7ecf\u8fc7\u8f93\u51fa\u5c42\u5f97\u5230\u6700\u540e\u7684\u8f93\u51fa\u5206\u5e03\u503c return self . generator ( outs ) # \u5b9a\u4e49Transformer\u7684\u7f16\u7801\u5668 def encode ( self , src , src_mask ): ''' src:\u6e90\u8bed\u8a00 src_mask:\u6e90\u8bed\u8a00\u63a9\u7801 ''' return self . transformer . encoder ( self . positional_encoding ( self . src_tok_emb ( src )), src_mask ) # \u5b9a\u4e49Transformer\u7684\u89e3\u7801\u5668 def decode ( self , tgt , memory , tgt_mask ): ''' tgt:\u76ee\u6807\u8bed\u8a00 memory:\u4e2d\u95f4\u8bed\u8a00\u5f20\u91cf\u8f93\u51fa tgt_mask: \u76ee\u6807\u8bed\u8a00\u7684\u63a9\u7801 ''' return self . transformer . decoder ( self . positional_encoding ( self . tgt_tok_emb ( tgt )), memory , tgt_mask )","title":"3.3 \u6784\u5efaSeq2SeqTransformer\u6a21\u578b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#4-mask-mask","text":"","title":"4 \u5b9a\u4e49mask\u7684\u51fd\u6570, \u521b\u5efa\u5bf9\u5e94\u7684\u4e0d\u540c\u7684mask"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#41","text":"\u4f5c\u7528\u662f\u9632\u6b62\u6a21\u578b\u5728\u8fdb\u884c\u9884\u6d4b\u7684\u8fc7\u7a0b\u4e2d\u67e5\u770b\u5230\u672a\u6765\u7684\u5355\u8bcd. \u540c\u65f6\u9700\u8981\u63a9\u7801\u6765\u9690\u85cf\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684padding tokens def generate_square_subsequent_mask ( sz ): # sz: \u53e5\u5b50\u7684\u957f\u5ea6 # triu\u751f\u6210\u7684\u662f\u4e0a\u4e09\u89d2, \u7ecf\u8fc7transpose\u4e4b\u540e\u53d8\u6210\u4e86\u4e0b\u4e09\u89d2\u77e9\u9635 mask = ( torch . triu ( torch . ones (( sz , sz ), device = DEVICE )) == 1 ) . transpose ( 0 , 1 ) # \u5c060\u7684\u4f4d\u7f6e\u586b\u5145\u8d1f\u65e0\u7a77\u5c0f, \u5c061\u7684\u4f4d\u7f6e\u586b\u5145\u4e3a0 mask = mask . float () . masked_fill ( mask == 0 , float ( '-inf' )) . masked_fill ( mask == 1 , float ( 0.0 )) return mask def create_mask ( src , tgt ): ''' src: \u6e90\u8bed\u8a00\u5f20\u91cf\u5f62\u72b6\u4e3a: [seq_length , batch_size] tgt: \u76ee\u6807\u8bed\u8a00\u5f20\u91cf\u5f62\u72b6\u4e3a: [seq_length , batch_size] ''' # \u83b7\u53d6\u6e90\u8bed\u8a00\u7684\u53e5\u5b50\u957f\u5ea6 src_seq_len = src . shape [ 0 ] # \u83b7\u53d6\u76ee\u6807\u8bed\u8a00\u7684\u53e5\u5b50\u957f\u5ea6 tgt_seq_len = tgt . shape [ 0 ] # \u4ea7\u751f\u76ee\u6807\u8bed\u8a00\u7684\u63a9\u7801\u5f20\u91cf tgt_mask = generate_square_subsequent_mask ( tgt_seq_len ) # \u4ea7\u751f\u6e90\u8bed\u8a00\u7684\u63a9\u7801\u5f20\u91cf src_mask = torch . zeros (( src_seq_len , src_seq_len ), device = DEVICE ) . type ( torch . bool ) # \u6784\u5efa\u6e90\u8bed\u8a00\u7684padding_mask src_padding_mask==> [batch_size, seq_len] src_padding_mask = ( src == PAD_IDX ) . transpose ( 0 , 1 ) # \u6784\u5efa\u76ee\u6807\u8bed\u8a00\u7684padding_mask tgt_paddig_mask ==> [batch_size, seq_len-1] tgt_padding_mask = ( tgt == PAD_IDX ) . transpose ( 0 , 1 ) return src_mask , tgt_mask , src_padding_mask , tgt_padding_mask","title":"4.1 \u5b9a\u4e49\u63a9\u7801"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#5","text":"","title":"5 \u5b9a\u4e49\u6279\u6b21\u6570\u636e\u5904\u7406\u7684\u56de\u8c03\u51fd\u6570"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#51-tensor","text":"# \u5c06\u53e5\u5b50\u5b57\u7b26\u8f6c\u5316\u4e3a\u5bf9\u5e94\u7684tensor\u5f20\u91cf def sequential_transforms ( * transforms ): ''' Transformers\u4e2d\u4f1a\u4f20\u5165\u4e09\u4e2a\u8fed\u4ee3\u5668: \u7b2c\u4e00\u4e2a\u662fTokenization\u7684, \u7b2c\u4e8c\u4e2a\u662fNumericalization, \u7b2c\u4e09\u4e2a\u662fAdd BOS/EOS and create tensor ''' def func ( txt_input ): # \u5faa\u73af\u4e09\u4e2a\u8fed\u4ee3\u5668, \u7b2c\u4e00\u4e2a\u8fdb\u884c\u8bed\u53e5\u7684\u5206\u5272, \u7b2c\u4e8c\u4e2a\u5c06\u5bf9\u5e94\u7684\u8bcd\u8bed\u6620\u5c04\u4e3a\u5bf9\u5e94\u7684\u5f20\u91cf\u8868\u793a, \u7b2c\u4e09\u4e2a\u662f\u5728\u6574\u4e2a\u53e5\u5b50\u7684\u9996\u5c3e\u90e8\u5206\u6dfb\u52a0\u8d77\u59cb\u548c\u7ed3\u675f\u6807\u5fd7. for transform in transforms : txt_input = transform ( txt_input ) return txt_input return func","title":"5.1 \u5c06\u5b57\u7b26\u4e32\u8f6c\u5316\u4e3a\u6574\u6570\u7684tensor\u5f20\u91cf"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#52","text":"# \u8f85\u52a9\u51fd\u6570, \u5b8c\u6210\u53e5\u5b50\u9996\u5c3eBOS/EOS\u7684\u6dfb\u52a0\u8fc7\u7a0b def tensor_transform ( token_ids : List [ int ]): # \u6dfb\u52a0\u7684\u662f\u5217\u8868\u5f62\u5f0f\u7684\u6570\u636e, \u5c06BOS\u548cEOS\u6dfb\u52a0\u5230\u53e5\u5b50\u7684\u9996\u5c3e\u90e8\u5206 return torch . cat (( torch . tensor ([ BOS_IDX ]), torch . tensor ( token_ids ), torch . tensor ([ EOS_IDX ]))) text_transform = {} # \u5faa\u73af\u6dfb\u52a0\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00 for ln in [ SRC_LANGUAGE , TGT_LANGUAGE ]: text_transform [ ln ] = sequential_transforms ( token_transform [ ln ], #Tokenization vocab_transform [ ln ], #Numericalization tensor_transform ) # Add BOS/EOS and create tensor","title":"5.2 \u5728\u53e5\u5b50\u9996\u5c3e\u6dfb\u52a0\u8d77\u59cb\u548c\u7ed3\u675f\u7b26\u53f7"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#53","text":"# \u6309\u7167\u6279\u6b21\u8fdb\u884c\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684\u7ec4\u88c5 def collate_fn ( batch ): # \u5b9a\u4e49\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u7684\u6279\u6b21\u5217\u8868 src_batch , tgt_batch = [], [] # \u5faa\u73af\u6279\u6b21\u6837\u672c for src_sample , tgt_sample in batch : # \u6dfb\u52a0\u6e90\u8bed\u8a00\u53e5\u5b50\u5230\u5217\u8868\u4e2d src_batch . append ( text_transform [ SRC_LANGUAGE ]( src_sample . rstrip ( \" \\n \" ))) # \u6dfb\u52a0\u76ee\u6807\u8bed\u8a00\u53e5\u5b50\u5230\u5217\u8868\u4e2d tgt_batch . append ( text_transform [ TGT_LANGUAGE ]( tgt_sample . rstrip ( \" \\n \" ))) # \u5c06\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u8fdb\u884c\u622a\u65ad\u8865\u9f50 PAD_IDX=1 # src_batch\u7684\u5f62\u72b6\u4e3a: [seq_length, batch] seq_length\u662f\u6700\u957f\u7684\u53e5\u5b50\u957f\u5ea6 src_batch = pad_sequence ( src_batch , padding_value = PAD_IDX ) # tgt_batch\u7684\u5f62\u72b6\u4e3a: [seq_length, batch] seq_length\u662f\u6700\u957f\u7684\u53e5\u5b50\u957f\u5ea6 tgt_batch = pad_sequence ( tgt_batch , padding_value = PAD_IDX ) return src_batch , tgt_batch","title":"5.3 \u6570\u636e\u8fdb\u884c\u6279\u6b21\u5316\u5904\u7406"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#6","text":"","title":"6 \u6784\u5efa\u8bad\u7ec3\u51fd\u6570\u548c\u8bc4\u4f30\u51fd\u6570"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#61","text":"# \u8bbe\u7f6e\u79cd\u5b50\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\uff0c\u4ee5\u4f7f\u5f97\u7ed3\u679c\u662f\u786e\u5b9a\u7684 torch . manual_seed ( 0 ) # \u8bbe\u7f6e\u8c03\u7528\u65f6\u5019\u4f7f\u7528\u7684\u53c2\u6570 SRC_VOCAB_SIZE = len ( vocab_transform [ SRC_LANGUAGE ]) TGT_VOCAB_SIZE = len ( vocab_transform [ TGT_LANGUAGE ]) EMB_SIZE = 512 NHEAD = 8 FFN_HID_DIM = 512 BATCH_SIZE = 128 NUM_ENCODER_LAYERS = 3 NUM_DECODER_LAYERS = 3 # \u5b9e\u4f8b\u5316Transformer\u5bf9\u8c61 transformer = Seq2SeqTransformer ( NUM_ENCODER_LAYERS , NUM_DECODER_LAYERS , EMB_SIZE , NHEAD , SRC_VOCAB_SIZE , TGT_VOCAB_SIZE , FFN_HID_DIM ) # \u4e3a\u4e86\u4fdd\u8bc1\u6bcf\u5c42\u7684\u8f93\u5165\u548c\u8f93\u51fa\u7684\u65b9\u5dee\u76f8\u540c, \u9632\u6b62\u68af\u5ea6\u6d88\u5931\u95ee\u9898 for p in transformer . parameters (): if p . dim () > 1 : # \u6b64\u5904\u4f7f\u7528\u7684\u662fxavier\u7684\u5747\u5300\u5206\u5e03 nn . init . xavier_uniform_ ( p ) # \u5982\u679c\u6709GPU\u5219\u5c06\u6a21\u578b\u79fb\u52a8\u5230GPU\u4e0a transformer = transformer . to ( DEVICE ) # \u5b9a\u4e49\u635f\u5931\u51fd\u6570 loss_fn = torch . nn . CrossEntropyLoss ( ignore_index = PAD_IDX ) # \u5b9a\u4e49\u4f18\u5316\u5668 betas: \u7528\u4e8e\u8ba1\u7b97\u68af\u5ea6\u53ca\u5176\u5e73\u65b9\u7684\u8fd0\u884c\u5e73\u5747\u503c\u7684\u7cfb\u6570 eps:\u6dfb\u52a0\u5230\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027 optimizer = torch . optim . Adam ( transformer . parameters (), lr = 0.0001 , betas = ( 0.9 , 0.98 ), eps = 1e-9 )","title":"6.1 \u5b9e\u4f8b\u5316\u6a21\u578b\u5e76\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#62","text":"def train_epoch ( model , optimizer ): # \u5f00\u542f\u8bad\u7ec3\u6a21\u5f0f model . train () # \u5b9a\u4e49\u5176\u5b9e\u7684\u635f\u5931\u503c\u4e3a0 losses = 0 # \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u8fed\u4ee3\u5668, \u8bed\u8a00\u5bf9\u4e3a(de, en) train_iter = Multi30k ( split = 'train' , language_pair = ( SRC_LANGUAGE , TGT_LANGUAGE )) # \u52a0\u8f7d\u6570\u636e, \u6309\u7167\u4e00\u4e2a\u6279\u6b21\u4e00\u4e2a\u6279\u6b21\u8fdb\u884c\u52a0\u8f7d, \u8fd4\u56de\u4e00\u4e2a\u8fed\u4ee3\u5668 train_dataloader = DataLoader ( train_iter , batch_size = BATCH_SIZE , collate_fn = collate_fn ) # \u5faa\u73af\u6570\u636e\u8fed\u4ee3\u5668 for src , tgt in train_dataloader : # \u5c06\u6e90\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a\u53bb src = src . to ( DEVICE ) # \u5c06\u76ee\u6807\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u8bbe\u5907\u4e0a\u53bb tgt = tgt . to ( DEVICE ) # \u83b7\u53d6\u8f93\u5165\u771f\u5b9e\u7684\u5f20\u91cf \u7b2c\u4e00\u4e2a\u5355\u8bcd\u5230\u5012\u6570\u7b2c\u4e8c\u4e2a\u5355\u8bcd tgt_input = tgt [: - 1 , :] # \u8c03\u7528mask\u51fd\u6570, \u751f\u6210\u5bf9\u5e94\u7684\u56db\u4e2amask src_mask , tgt_mask , src_padding_mask , tgt_padding_mask = create_mask ( src , tgt_input ) # \u8c03\u7528\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3, \u5f97\u5230\u6700\u540e\u7684\u5f20\u91cf\u5206\u5e03 logits = model ( src , tgt_input , src_mask , tgt_mask , src_padding_mask , tgt_padding_mask , src_padding_mask ) # \u68af\u5ea6\u6e05\u96f6 optimizer . zero_grad () # \u83b7\u53d6\u8f93\u51fa\u771f\u5b9e\u7684\u6807\u7b7e\u6570\u636e \u7b2c\u4e8c\u4e2a\u5355\u8bcd\u5230\u6700\u540e\u4e00\u4e2a\u5355\u8bcd tgt_out = tgt [ 1 :, :] # \u8ba1\u7b97\u635f\u5931 loss = loss_fn ( logits . reshape ( - 1 , logits . shape [ - 1 ]), tgt_out . reshape ( - 1 )) # \u53cd\u5411\u4f20\u64ad loss . backward () # \u68af\u5ea6\u66f4\u65b0 optimizer . step () # \u635f\u5931\u503c\u7d2f\u52a0\u6c42\u548c losses += loss . item () # \u8fd4\u56de\u5e73\u5747\u635f\u5931\u503c return losses / len ( train_dataloader )","title":"6.2 \u5b9a\u4e49\u6279\u6b21\u8bad\u7ec3\u51fd\u6570"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#63","text":"def evaluate ( model ): # \u5f00\u542f\u6a21\u578b\u8bc4\u4f30\u6a21\u5f0f model . eval () # \u5b9a\u4e49\u8d77\u59cb\u635f\u5931\u503c losses = 0 # \u52a0\u8f7d\u9a8c\u8bc1\u6570\u636e\u96c6, \u8bed\u8a00\u5bf9\u4e3a(de, en) val_iter = Multi30k ( split = 'valid' , language_pair = ( SRC_LANGUAGE , TGT_LANGUAGE )) # \u8fd4\u56de\u9a8c\u8bc1\u96c6\u7684\u6570\u636e\u52a0\u8f7d\u5668 val_dataloader = DataLoader ( val_iter , batch_size = BATCH_SIZE , collate_fn = collate_fn ) # \u5faa\u73af\u9a8c\u8bc1\u96c6 for src , tgt in val_dataloader : # \u6e90\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a src = src . to ( DEVICE ) # \u76ee\u6807\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a tgt = tgt . to ( DEVICE ) # \u83b7\u53d6\u8f93\u5165\u7684\u771f\u5b9e\u7684\u5f20\u91cf tgt_input = tgt [: - 1 , :] # \u8c03\u7528mask\u51fd\u6570, \u4ea7\u751f\u5bf9\u5e94\u7684\u56db\u4e2amask\u503c src_mask , tgt_mask , src_padding_mask , tgt_padding_mask = create_mask ( src , tgt_input ) # \u8c03\u7528\u6a21\u578b, \u5f97\u5230\u5bf9\u5e94\u7684\u8f93\u51fa\u5206\u5e03\u503c logits = model ( src , tgt_input , src_mask , tgt_mask , src_padding_mask , tgt_padding_mask , src_padding_mask ) # \u83b7\u53d6\u8f93\u51fa\u7684\u771f\u5b9e\u5f20\u91cf tgt_out = tgt [ 1 :, :] # \u8ba1\u7b97\u635f\u5931\u503c loss = loss_fn ( logits . reshape ( - 1 , logits . shape [ - 1 ]), tgt_out . reshape ( - 1 )) # \u635f\u5931\u503c\u7d2f\u52a0, \u6c42\u548c losses += loss . item () # \u6c42\u5f97\u5bf9\u5e94\u7684\u5e73\u5747\u635f\u5931 return losses / len ( val_dataloader )","title":"6.3 \u5b9a\u4e49\u6279\u6b21\u8bc4\u4f30\u51fd\u6570"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#7-transformer","text":"","title":"7 \u8bad\u7ec3Transformer\u6a21\u578b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#71-transformer","text":"# \u5b9a\u4e49epoch\u7684\u6b21\u6570 NUM_EPOCHS = 18 # \u5faa\u73af\u6574\u4e2a\u6570\u636e\u96c6num_epochs\u6b21 for epoch in range ( 1 , NUM_EPOCHS + 1 ): # \u83b7\u53d6\u5f00\u59cb\u65f6\u95f4 start_time = timer () # \u5c06\u6574\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3 train_loss = train_epoch ( transformer , optimizer ) # \u83b7\u53d6\u7ed3\u675f\u65f6\u95f4 end_time = timer () # \u5c06\u6574\u4e2a\u9a8c\u8bc1\u96c6\u8fdb\u884c\u8bc4\u4f30 val_loss = evaluate ( transformer ) # \u6253\u5370\u6bcf\u4e2aepoch\u7684\u8bad\u7ec3\u635f\u5931, \u9a8c\u8bc1\u635f\u5931, \u548c\u8bad\u7ec3\u65f6\u95f4. print (( f \"Epoch: { epoch } , Train loss: { train_loss : .3f } , Val loss: { val_loss : .3f } , \" f \"Epoch time = { ( end_time - start_time ) : .3f } s\" )) \u8f93\u51fa\u6548\u679c\u5c55\u793a Epoch : 1 , Train loss : 5.342 , Val loss : 4.138 , Epoch time = 653.749 s Epoch : 2 , Train loss : 3.799 , Val loss : 3.370 , Epoch time = 649.536 s Epoch : 3 , Train loss : 3.184 , Val loss : 2.921 , Epoch time = 644.899 s Epoch : 4 , Train loss : 2.782 , Val loss : 2.642 , Epoch time = 648.685 s Epoch : 5 , Train loss : 2.490 , Val loss : 2.453 , Epoch time = 650.243 s Epoch : 6 , Train loss : 2.256 , Val loss : 2.321 , Epoch time = 647.609 s Epoch : 7 , Train loss : 2.064 , Val loss : 2.210 , Epoch time = 654.674 s Epoch : 8 , Train loss : 1.905 , Val loss : 2.132 , Epoch time = 659.779 s Epoch : 9 , Train loss : 1.761 , Val loss : 2.070 , Epoch time = 652.363 s Epoch : 10 , Train loss : 1.637 , Val loss : 2.016 , Epoch time = 646.682 s Epoch : 11 , Train loss : 1.527 , Val loss : 1.977 , Epoch time = 643.913 s Epoch : 12 , Train loss : 1.427 , Val loss : 1.970 , Epoch time = 640.084 s Epoch : 13 , Train loss : 1.335 , Val loss : 1.964 , Epoch time = 639.331 s Epoch : 14 , Train loss : 1.253 , Val loss : 1.936 , Epoch time = 639.232 s Epoch : 15 , Train loss : 1.173 , Val loss : 1.928 , Epoch time = 649.990 s Epoch : 16 , Train loss : 1.106 , Val loss : 1.909 , Epoch time = 636.465 s Epoch : 17 , Train loss : 1.038 , Val loss : 1.905 , Epoch time = 644.609 s Epoch : 18 , Train loss : 0.976 , Val loss : 1.914 , Epoch time = 644.115 s \u6ce8\u610f: \u8fd9\u4e2a\u8bad\u7ec3\u7684\u8fc7\u7a0b\u662f4\u68388G\u5185\u5b58\u7684CPU\u670d\u52a1\u5668,\u5927\u5bb6\u53ef\u4ee5\u66f4\u6362\u4e3aGPU\u670d\u52a1\u5668, \u901f\u5ea6\u4f1a\u66f4\u5feb.","title":"7.1 \u5229\u7528\u5faa\u73af\u8bad\u7ec3Transformer\u6a21\u578b"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#8","text":"","title":"8 \u8fdb\u884c\u89e3\u7801\u751f\u6210\u76ee\u6807\u8bed\u8a00\u8bed\u53e5"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#81","text":"def greedy_decode ( model , src , src_mask , max_len , start_symbol ): # \u5c06\u5bf9\u5e94\u7684\u6e90\u8bed\u8a00\u6570\u636e\u79fb\u52a8\u7684\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a src = src . to ( DEVICE ) # \u5c06\u5bf9\u5e94\u7684\u6e90\u8bed\u8a00\u7684mask\u79fb\u52a8\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a src_mask = src_mask . to ( DEVICE ) # \u5c06\u6e90\u8bed\u8a00\u4f7f\u7528\u6a21\u578b\u7684\u7f16\u7801\u5668, \u5f97\u5230\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cf memory\u7684\u5f62\u72b6\u4e3a: [seq_len, batch_size, dim] memory = model . encode ( src , src_mask ) # \u6784\u5efa\u4e00\u4e2a\u8d77\u59cb\u7684\u4e8c\u7ef4\u77e9\u9635, \u7136\u540e\u51c6\u5907\u5f00\u59cb\u53e5\u5b50\u7684\u89e3\u7801\u8fc7\u7a0b. ys\u5f62\u72b6\u4e3a[1, 1]\u4e8c\u7ef4\u7684 ys = torch . ones ( 1 , 1 ) . fill_ ( start_symbol ) . type ( torch . long ) . to ( DEVICE ) for i in range ( max_len - 1 ): # \u5c06\u4e2d\u95f4\u8bed\u4e49\u5f20\u91cf\u7684\u6570\u636e\u4e00\u5b9a\u5230\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a memory = memory . to ( DEVICE ) # \u751f\u6210\u76ee\u6807\u8bed\u8a00\u7684mask\u503c tgt_mask = ( generate_square_subsequent_mask ( ys . size ( 0 )) . type ( torch . bool )) . to ( DEVICE ) # \u8c03\u7528\u6a21\u578b\u7684\u89e3\u7801\u5668\u8fdb\u884c\u89e3\u7801 out\u5f62\u72b6\u4e3a:[seq_len, 1, 512]==> [seq_len, batch_size, emb_size] out = model . decode ( ys , memory , tgt_mask ) # \u8f93\u51fa\u5f20\u91cf\u8fdb\u884c\u5f62\u72b6\u7684\u8f6c\u6362 out = out . transpose ( 0 , 1 ) # \u7ecf\u8fc7\u6700\u540e\u8f93\u51fa\u5c42, \u83b7\u53d6\u6700\u540e\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03 out[:, -1]\u5f62\u72b6\u4e3a: [1, 512] --> [seq_len, emb_size] # prob\u7684\u5f62\u72b6\u4e3a: [1, tgt_vocab_size] prob = model . generator ( out [:, - 1 ]) # \u57281\u7ef4\u5ea6\u4e0a, \u83b7\u53d6\u6982\u7387\u6700\u5927\u7684\u90a3\u4e2a\u5c31\u662f\u6700\u540e\u9884\u6d4b\u7684\u90a3\u4e2a\u503c max\u8fd4\u56de\u4e24\u4e2a\u503c, \u7b2c\u4e00\u4e2a\u662f\u8fd4\u56de\u7684\u6700\u5927\u503c\u7684\u6982\u7387, \u7b2c\u4e8c\u4e2a\u662f\u8fd4\u56de\u6700\u5927\u6982\u7387\u7684\u4e0b\u6807\u503c. _ , next_word = torch . max ( prob , dim = 1 ) # \u83b7\u53d6\u5bf9\u5e94\u7684\u90a3\u4e2a\u4e0b\u6807\u503c next_word = next_word . item () # \u62fc\u63a5\u4e0a\u4e00\u6b65\u548c\u8fd9\u4e00\u6b65\u4ea7\u751f\u7684\u5355\u8bcd, \u4f5c\u4e3a\u4e0b\u4e00\u6b65\u4f7f\u7528\u7684ys fill_()\u8868\u793a\u7528\u62ec\u53f7\u4e2d\u7684\u6570\u5b57\u53bb\u586b\u5145\u6574\u4e2a\u77e9\u9635 ys = torch . cat ([ ys , torch . ones ( 1 , 1 ) . type_as ( src . data ) . fill_ ( next_word )], dim = 0 ) if next_word == EOS_IDX : break return ys","title":"8.1 \u4f7f\u7528\u8d2a\u5fc3\u7b97\u6cd5\u6784\u5efa\u751f\u6210\u5e8f\u5217\u51fd\u6570"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#82","text":"def translate ( model : torch . nn . Module , src_sentence : str ): ''' model: \u8f93\u5165\u6574\u4e2aTransformer\u6a21\u578b src_sentence:\u8981\u7ffb\u8bd1\u7684\u8bed\u53e5 ''' # \u5f00\u542f\u6a21\u578b\u7684\u8bc4\u4f30\u6a21\u5f0f model . eval () # \u5c06\u6e90\u8bed\u53e5\u8f6c\u5316\u4e3a\u5bf9\u5e94\u7684\u5f20\u91cf\u8868\u793a \u8d77\u521d\u662f\u4e00\u7ef4\u7684(seq_len, ), \u540e\u7ecf\u8fc7view(-1, 1)\u8f6c\u5316\u4e3a[seq_len, 1]\u4e8c\u7ef4\u7684\u5f62\u72b6. src = text_transform [ SRC_LANGUAGE ]( src_sentence ) . view ( - 1 , 1 ) # src.shape==> [seq_len, 1] num_tokens = src . shape [ 0 ] # \u521b\u5efa\u4e00\u4e2a\u5168\u96f6\u7684\u77e9\u9635\u4f5c\u4e3asrc_mask\u7684\u8d77\u59cb\u77e9\u9635 src_mask = ( torch . zeros ( num_tokens , num_tokens )) . type ( torch . bool ) # \u4f7f\u7528\u8d2a\u5fc3\u7b97\u6cd5\u8fdb\u884c\u89e3\u7801 tgt_tokens = greedy_decode ( model , src , src_mask , max_len = num_tokens + 5 , start_symbol = BOS_IDX ) . flatten () # \u73b0\u5c06\u6570\u636e\u4eceGPU\u4e0a\u8fc1\u79fb\u5230CPU\u4e0a, \u7136\u540e\u8fdb\u884ctensor\u7c7b\u578b\u8f6c\u5316\u4e3anumpy.ndarray\u7c7b\u578b\u7684\u6574\u6570\u503c # \u4f7f\u7528lookup_tokens\u8fdb\u884c\u7d22\u5f15\u5230\u5bf9\u5e94\u5b57\u7b26\u7684\u67e5\u627e, \u53cd\u8f6c\u4e3a\u5bf9\u5e94\u7684\u5b57\u7b26, \u7136\u540e\u5c06\u53e5\u5b50\u7684\u9996\u5c3e\u7684bos\u548ceos\u66ff\u6362\u6389, \u5373\u4e3a\u89e3\u7801\u4e4b\u540e\u7684\u8bed\u53e5. return \" \" . join ( vocab_transform [ TGT_LANGUAGE ] . lookup_tokens ( list ( tgt_tokens . cpu () . numpy ()))) . replace ( \"<bos>\" , \"\" ) . replace ( \"<eos>\" , \"\" ) \u9a8c\u8bc1 print ( translate ( transformer , \"Eine Gruppe von Menschen steht vor einem Iglu .\" )) \u8f93\u51fa\u6548\u679c A group of people stand in front of an aquarium .","title":"8.2 \u5b9a\u4e49\u6700\u7ec8\u7684\u7ffb\u8bd1\u8f6c\u5316\u51fd\u6570"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#9","text":"","title":"9 \u6a21\u578b\u7684\u4fdd\u5b58\u548c\u91cd\u52a0\u8f7d"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#91","text":"path = './model/transformer_translation_18.pth' torch . save ( transformer . state_dict (), path )","title":"9.1 \u6a21\u578b\u7684\u4fdd\u5b58"},{"location":"04_mkdocs_transformer/8%20%E6%A1%88%E4%BE%8BTransformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.html#92","text":"transformer = Seq2SeqTransformer ( NUM_ENCODER_LAYERS , NUM_DECODER_LAYERS , EMB_SIZE , NHEAD , SRC_VOCAB_SIZE , TGT_VOCAB_SIZE , FFN_HID_DIM ) transformer . load_state_dict ( torch . load ( path ))","title":"9.2 \u6a21\u578b\u7684\u91cd\u52a0\u8f7d"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html","text":"3.1 \u4f7f\u7528Transformer\u6784\u5efa\u8bed\u8a00\u6a21\u578b \u00b6 \u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u6709\u5173\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6. \u638c\u63e1\u4f7f\u7528Transformer\u6784\u5efa\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u73b0\u8fc7\u7a0b. \u4ec0\u4e48\u662f\u8bed\u8a00\u6a21\u578b: \u4ee5\u4e00\u4e2a\u7b26\u5408\u8bed\u8a00\u89c4\u5f8b\u7684\u5e8f\u5217\u4e3a\u8f93\u5165\uff0c\u6a21\u578b\u5c06\u5229\u7528\u5e8f\u5217\u95f4\u5173\u7cfb\u7b49\u7279\u5f81\uff0c\u8f93\u51fa\u4e00\u4e2a\u5728\u6240\u6709\u8bcd\u6c47\u4e0a\u7684\u6982\u7387\u5206\u5e03.\u8fd9\u6837\u7684\u6a21\u578b\u79f0\u4e3a\u8bed\u8a00\u6a21\u578b. # \u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u8bed\u6599\u4e00\u822c\u6765\u81ea\u4e8e\u6587\u7ae0\uff0c\u5bf9\u5e94\u7684\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u5f62\u5982: src1 = \"I can do\" tgt1 = \"can do it\" src2 = \"can do it\", tgt2 = \"do it <eos>\" \u8bed\u8a00\u6a21\u578b\u80fd\u89e3\u51b3\u54ea\u4e9b\u95ee\u9898: 1, \u6839\u636e\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u5728\u5b83\u7684\u57fa\u7840\u4e0a\u5b8c\u6210\u673a\u5668\u7ffb\u8bd1\uff0c\u6587\u672c\u751f\u6210\u7b49\u4efb\u52a1\uff0c\u56e0\u4e3a\u6211\u4eec\u901a\u8fc7\u6700\u540e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u6c47\u662f\u4ec0\u4e48. 2, \u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5224\u65ad\u8f93\u5165\u7684\u5e8f\u5217\u662f\u5426\u4e3a\u4e00\u53e5\u5b8c\u6574\u7684\u8bdd\uff0c\u56e0\u4e3a\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u67e5\u770b\u6700\u5927\u6982\u7387\u662f\u5426\u843d\u5728\u53e5\u5b50\u7ed3\u675f\u7b26\u4e0a\uff0c\u6765\u5224\u65ad\u5b8c\u6574\u6027. 3, \u8bed\u8a00\u6a21\u578b\u672c\u8eab\u7684\u8bad\u7ec3\u76ee\u6807\u662f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\uff0c\u56e0\u4e3a\u5b83\u7684\u7279\u5f81\u63d0\u53d6\u90e8\u5206\u4f1a\u62bd\u8c61\u5f88\u591a\u8bed\u8a00\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5173\u7cfb\u53ef\u80fd\u540c\u6837\u5bf9\u5176\u4ed6\u8bed\u8a00\u7c7b\u4efb\u52a1\u6709\u6548\u679c.\u56e0\u6b64\u53ef\u4ee5\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60. \u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406 \u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570 \u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5) \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u00b6 pytorch\u7248\u672c\u5fc5\u987b\u4f7f\u75281.3.1, python\u7248\u672c\u4f7f\u75283.6.x pip install torch==1.3.1 # \u6570\u5b66\u8ba1\u7b97\u5de5\u5177\u5305math import math # torch\u4ee5\u53catorch.nn, torch.nn.functional import torch import torch.nn as nn import torch.nn.functional as F # torch\u4e2d\u7ecf\u5178\u6587\u672c\u6570\u636e\u96c6\u6709\u5173\u7684\u5de5\u5177\u5305 # \u5177\u4f53\u8be6\u60c5\u53c2\u8003\u4e0b\u65b9torchtext\u4ecb\u7ecd import torchtext # torchtext\u4e2d\u7684\u6570\u636e\u5904\u7406\u5de5\u5177, get_tokenizer\u7528\u4e8e\u82f1\u6587\u5206\u8bcd from torchtext.data.utils import get_tokenizer # \u5df2\u7ecf\u6784\u5efa\u5b8c\u6210\u7684TransformerModel from pyitcast.transformer import TransformerModel torchtext\u4ecb\u7ecd: \u5b83\u662ftorch\u5de5\u5177\u4e2d\u5904\u7406NLP\u95ee\u9898\u7684\u5e38\u7528\u6570\u636e\u5904\u7406\u5305. torchtext\u7684\u91cd\u8981\u529f\u80fd: \u5bf9\u6587\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406, \u6bd4\u5982\u6587\u672c\u8bed\u6599\u52a0\u8f7d, \u6587\u672c\u8fed\u4ee3\u5668\u6784\u5efa\u7b49. \u5305\u542b\u5f88\u591a\u7ecf\u5178\u6587\u672c\u8bed\u6599\u7684\u9884\u52a0\u8f7d\u65b9\u6cd5. \u5176\u4e2d\u5305\u62ec\u7684\u8bed\u6599\u6709\uff1a\u7528\u4e8e\u60c5\u611f\u5206\u6790\u7684SST\u548cIMDB, \u7528\u4e8e\u95ee\u9898\u5206\u7c7b\u7684TREC, \u7528\u4e8e\u53ca\u5176\u7ffb\u8bd1\u7684 WMT14\uff0c IWSLT\uff0c\u4ee5\u53ca\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u4efb\u52a1wikiText-2, WikiText103, PennTreebank. \u6211\u4eec\u8fd9\u91cc\u4f7f\u7528wikiText-2\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b, \u4e0b\u9762\u6709\u5173\u8be5\u6570\u636e\u96c6\u7684\u76f8\u5173\u8be6\u60c5: wikiText-2\u6570\u636e\u96c6\u7684\u4f53\u91cf\u4e2d\u7b49, \u8bad\u7ec3\u96c6\u5171\u6709600\u7bc7\u77ed\u6587, \u5171208\u4e07\u5de6\u53f3\u7684\u8bcd\u6c47, 33278\u4e2a\u4e0d\u91cd\u590d\u8bcd\u6c47, OoV\uff08\u6709\u591a\u5c11\u6b63\u5e38\u82f1\u6587\u8bcd\u6c47\u4e0d\u5728\u8be5\u6570\u636e\u96c6\u4e2d\u7684\u5360\u6bd4\uff09\u4e3a2.6%\uff0c\u6570\u636e\u96c6\u4e2d\u7684\u77ed\u6587\u90fd\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5bf9\u4e00\u4e9b\u6982\u5ff5\u7684\u4ecb\u7ecd\u548c\u63cf\u8ff0. \u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406 \u00b6 # \u521b\u5efa\u8bed\u6599\u57df, \u8bed\u6599\u57df\u662f\u5b58\u653e\u8bed\u6599\u7684\u6570\u636e\u7ed3\u6784, # \u5b83\u7684\u56db\u4e2a\u53c2\u6570\u4ee3\u8868\u7ed9\u5b58\u653e\u8bed\u6599\uff08\u6216\u79f0\u4f5c\u6587\u672c\uff09\u65bd\u52a0\u7684\u4f5c\u7528. # \u5206\u522b\u4e3a tokenize,\u4f7f\u7528get_tokenizer(\"basic_english\")\u83b7\u5f97\u4e00\u4e2a\u5206\u5272\u5668\u5bf9\u8c61, # \u5206\u5272\u65b9\u5f0f\u6309\u7167\u6587\u672c\u4e3a\u57fa\u7840\u82f1\u6587\u8fdb\u884c\u5206\u5272. # init_token\u4e3a\u7ed9\u6587\u672c\u65bd\u52a0\u7684\u8d77\u59cb\u7b26 <sos>\u7ed9\u6587\u672c\u65bd\u52a0\u7684\u7ec8\u6b62\u7b26<eos>, # \u6700\u540e\u4e00\u4e2alower\u4e3aTrue, \u5b58\u653e\u7684\u6587\u672c\u5b57\u6bcd\u5168\u90e8\u5c0f\u5199. TEXT = torchtext . data . Field ( tokenize = get_tokenizer ( \"basic_english\" ), init_token = '<sos>' , eos_token = '<eos>' , lower = True ) # \u6700\u7ec8\u83b7\u5f97\u4e00\u4e2aField\u5bf9\u8c61. # <torchtext.data.field.Field object at 0x7fc42a02e7f0> # \u7136\u540e\u4f7f\u7528torchtext\u7684\u6570\u636e\u96c6\u65b9\u6cd5\u5bfc\u5165WikiText2\u6570\u636e, # \u5e76\u5207\u5206\u4e3a\u5bf9\u5e94\u8bad\u7ec3\u6587\u672c, \u9a8c\u8bc1\u6587\u672c\uff0c\u6d4b\u8bd5\u6587\u672c, \u5e76\u5bf9\u8fd9\u4e9b\u6587\u672c\u65bd\u52a0\u521a\u521a\u521b\u5efa\u7684\u8bed\u6599\u57df. train_txt , val_txt , test_txt = torchtext . datasets . WikiText2 . splits ( TEXT ) # \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7examples[0].text\u53d6\u51fa\u6587\u672c\u5bf9\u8c61\u8fdb\u884c\u67e5\u770b. # >>> test_txt.examples[0].text[:10] # ['<eos>', '=', 'robert', '<unk>', '=', '<eos>', '<eos>', 'robert', '<unk>', 'is'] # \u5c06\u8bad\u7ec3\u96c6\u6587\u672c\u6570\u636e\u6784\u5efa\u4e00\u4e2avocab\u5bf9\u8c61, # \u8fd9\u6837\u53ef\u4ee5\u4f7f\u7528vocab\u5bf9\u8c61\u7684stoi\u65b9\u6cd5\u7edf\u8ba1\u6587\u672c\u5171\u5305\u542b\u7684\u4e0d\u91cd\u590d\u8bcd\u6c47\u603b\u6570. TEXT . build_vocab ( train_txt ) # \u7136\u540e\u9009\u62e9\u8bbe\u5907cuda\u6216\u8005cpu device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) \u8be5\u6848\u4f8b\u7684\u6240\u6709\u4ee3\u7801\u90fd\u5c06\u5b9e\u73b0\u5728\u4e00\u4e2atransformer_lm.py\u6587\u4ef6\u4e2d. \u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e \u00b6 \u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e00\u4e2a\u51fd\u6570batchify\u4ee3\u7801\u5206\u6790: def batchify ( data , bsz ): \"\"\"batchify\u51fd\u6570\u7528\u4e8e\u5c06\u6587\u672c\u6570\u636e\u6620\u5c04\u6210\u8fde\u7eed\u6570\u5b57, \u5e76\u8f6c\u6362\u6210\u6307\u5b9a\u7684\u6837\u5f0f, \u6307\u5b9a\u7684\u6837\u5f0f\u53ef\u53c2\u8003\u4e0b\u56fe. \u5b83\u6709\u4e24\u4e2a\u8f93\u5165\u53c2\u6570, data\u5c31\u662f\u6211\u4eec\u4e4b\u524d\u5f97\u5230\u7684\u6587\u672c\u6570\u636e(train_txt, val_txt, test_txt), bsz\u662f\u5c31\u662fbatch_size, \u6bcf\u6b21\u6a21\u578b\u66f4\u65b0\u53c2\u6570\u7684\u6570\u636e\u91cf\"\"\" # \u4f7f\u7528TEXT\u7684numericalize\u65b9\u6cd5\u5c06\u5355\u8bcd\u6620\u5c04\u6210\u5bf9\u5e94\u7684\u8fde\u7eed\u6570\u5b57. data = TEXT . numericalize ([ data . examples [ 0 ] . text ]) # >>> data # tensor([[ 3], # [ 12], # [3852], # ..., # [ 6], # [ 3], # [ 3]]) # \u63a5\u7740\u7528\u6570\u636e\u8bcd\u6c47\u603b\u6570\u9664\u4ee5bsz, # \u53d6\u6574\u6570\u5f97\u5230\u4e00\u4e2anbatch\u4ee3\u8868\u9700\u8981\u591a\u5c11\u6b21batch\u540e\u80fd\u591f\u904d\u5386\u5b8c\u6240\u6709\u6570\u636e nbatch = data . size ( 0 ) // bsz # \u4e4b\u540e\u4f7f\u7528narrow\u65b9\u6cd5\u5bf9\u4e0d\u89c4\u6574\u7684\u5269\u4f59\u6570\u636e\u8fdb\u884c\u5220\u9664, # \u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u4ee3\u8868\u6a2a\u8f74\u5220\u9664\u8fd8\u662f\u7eb5\u8f74\u5220\u9664, 0\u4e3a\u6a2a\u8f74\uff0c1\u4e3a\u7eb5\u8f74 # \u7b2c\u4e8c\u4e2a\u548c\u7b2c\u4e09\u4e2a\u53c2\u6570\u4ee3\u8868\u4fdd\u7559\u5f00\u59cb\u8f74\u5230\u7ed3\u675f\u8f74\u7684\u6570\u503c.\u7c7b\u4f3c\u4e8e\u5207\u7247 # \u53ef\u53c2\u8003\u4e0b\u65b9\u6f14\u793a\u793a\u4f8b\u8fdb\u884c\u66f4\u6df1\u7406\u89e3. data = data . narrow ( 0 , 0 , nbatch * bsz ) # >>> data # tensor([[ 3], # [ 12], # [3852], # ..., # [ 78], # [ 299], # [ 36]]) # \u540e\u9762\u4e0d\u80fd\u5f62\u6210bsz\u4e2a\u7684\u4e00\u7ec4\u6570\u636e\u88ab\u5220\u9664 # \u63a5\u7740\u6211\u4eec\u4f7f\u7528view\u65b9\u6cd5\u5bf9data\u8fdb\u884c\u77e9\u9635\u53d8\u6362, \u4f7f\u5176\u6210\u4e3a\u5982\u4e0b\u6837\u5f0f: # tensor([[ 3, 25, 1849, ..., 5, 65, 30], # [ 12, 66, 13, ..., 35, 2438, 4064], # [ 3852, 13667, 2962, ..., 902, 33, 20], # ..., # [ 154, 7, 10, ..., 5, 1076, 78], # [ 25, 4, 4135, ..., 4, 56, 299], # [ 6, 57, 385, ..., 3168, 737, 36]]) # \u56e0\u4e3a\u4f1a\u505a\u8f6c\u7f6e\u64cd\u4f5c, \u56e0\u6b64\u8fd9\u4e2a\u77e9\u9635\u7684\u5f62\u72b6\u662f[None, bsz], # \u5982\u679c\u8f93\u5165\u662f\u8bad\u7ec3\u6570\u636e\u7684\u8bdd\uff0c\u5f62\u72b6\u4e3a[104335, 20], \u53ef\u4ee5\u901a\u8fc7\u6253\u5370data.shape\u83b7\u5f97. # \u4e5f\u5c31\u662fdata\u7684\u5217\u6570\u662f\u7b49\u4e8ebsz\u7684\u503c\u7684. data = data . view ( bsz , - 1 ) . t () . contiguous () # \u6700\u540e\u5c06\u6570\u636e\u5206\u914d\u5728\u6307\u5b9a\u7684\u8bbe\u5907\u4e0a. return data . to ( device ) batchify\u7684\u6837\u5f0f\u8f6c\u5316\u56fe: \u5927\u5199\u5b57\u6bcdA\uff0cB\uff0cC ... \u4ee3\u8868\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd. torch.narrow\u6f14\u793a: >>> x = torch . tensor ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) >>> x . narrow ( 0 , 0 , 2 ) tensor ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> x . narrow ( 1 , 1 , 2 ) tensor ([[ 2 , 3 ], [ 5 , 6 ], [ 8 , 9 ]]) \u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u4f7f\u7528batchify\u6765\u5904\u7406\u8bad\u7ec3\u6570\u636e\uff0c\u9a8c\u8bc1\u6570\u636e\u4ee5\u53ca\u6d4b\u8bd5\u6570\u636e # \u8bad\u7ec3\u6570\u636e\u7684batch size batch_size = 20 # \u9a8c\u8bc1\u548c\u6d4b\u8bd5\u6570\u636e\uff08\u7edf\u79f0\u4e3a\u8bc4\u4f30\u6570\u636e\uff09\u7684batch size eval_batch_size = 10 # \u83b7\u5f97train_data, val_data, test_data train_data = batchify ( train_txt , batch_size ) val_data = batchify ( val_txt , eval_batch_size ) test_data = batchify ( test_txt , eval_batch_size ) \u4e0a\u9762\u7684\u5206\u5272\u6279\u6b21\u5e76\u6ca1\u6709\u8fdb\u884c\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e\u7684\u5904\u7406, \u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u6839\u636e\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u8bed\u6599\u89c4\u5b9a\u6765\u6784\u5efa\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e. \u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u8bed\u6599\u89c4\u5b9a: \u5982\u679c\u6e90\u6570\u636e\u4e3a\u53e5\u5b50ABCD, ABCD\u4ee3\u8868\u53e5\u5b50\u4e2d\u7684\u8bcd\u6c47\u6216\u7b26\u53f7, \u5219\u5b83\u7684\u76ee\u6807\u6570\u636e\u4e3aBCDE, BCDE\u5206\u522b\u4ee3\u8868ABCD\u7684\u4e0b\u4e00\u4e2a\u8bcd\u6c47. \u5982\u56fe\u6240\u793a\uff0c\u6211\u4eec\u8fd9\u91cc\u7684\u53e5\u5b50\u5e8f\u5217\u662f\u7ad6\u7740\u7684, \u800c\u4e14\u6211\u4eec\u53d1\u73b0\u5982\u679c\u7528\u4e00\u4e2a\u6279\u6b21\u5904\u7406\u5b8c\u6240\u6709\u6570\u636e, \u4ee5\u8bad\u7ec3\u6570\u636e\u4e3a\u4f8b, \u6bcf\u4e2a\u53e5\u5b50\u957f\u5ea6\u9ad8\u8fbe104335, \u8fd9\u660e\u663e\u662f\u4e0d\u79d1\u5b66\u7684, \u56e0\u6b64\u6211\u4eec\u5728\u8fd9\u91cc\u8981\u9650\u5b9a\u6bcf\u4e2a\u6279\u6b21\u4e2d\u7684\u53e5\u5b50\u957f\u5ea6\u5141\u8bb8\u7684\u6700\u5927\u503cbptt. \u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e8c\u4e2a\u51fd\u6570get_batch\u4ee3\u7801\u5206\u6790: # \u4ee4\u5b50\u957f\u5ea6\u5141\u8bb8\u7684\u6700\u5927\u503cbptt\u4e3a35 bptt = 35 def get_batch ( source , i ): \"\"\"\u7528\u4e8e\u83b7\u5f97\u6bcf\u4e2a\u6279\u6b21\u5408\u7406\u5927\u5c0f\u7684\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e. \u53c2\u6570source\u662f\u901a\u8fc7batchify\u5f97\u5230\u7684train_data/val_data/test_data. i\u662f\u5177\u4f53\u7684\u6279\u6b21\u6b21\u6570. \"\"\" # \u9996\u5148\u6211\u4eec\u786e\u5b9a\u53e5\u5b50\u957f\u5ea6, \u5b83\u5c06\u662f\u5728bptt\u548clen(source) - 1 - i\u4e2d\u6700\u5c0f\u503c # \u5b9e\u8d28\u4e0a, \u524d\u9762\u7684\u6279\u6b21\u4e2d\u90fd\u4f1a\u662fbptt\u7684\u503c, \u53ea\u4e0d\u8fc7\u6700\u540e\u4e00\u4e2a\u6279\u6b21\u4e2d, \u53e5\u5b50\u957f\u5ea6 # \u53ef\u80fd\u4e0d\u591fbptt\u768435\u4e2a, \u56e0\u6b64\u4f1a\u53d8\u4e3alen(source) - 1 - i\u7684\u503c. seq_len = min ( bptt , len ( source ) - 1 - i ) # \u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u6e90\u6570\u636e\u7684\u7b2ci\u6279\u6570\u636e\u5c06\u662fbatchify\u7684\u7ed3\u679c\u7684\u5207\u7247[i:i+seq_len] data = source [ i : i + seq_len ] # \u6839\u636e\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u8bed\u6599\u89c4\u5b9a, \u5b83\u7684\u76ee\u6807\u6570\u636e\u662f\u6e90\u6570\u636e\u5411\u540e\u79fb\u52a8\u4e00\u4f4d # \u56e0\u4e3a\u6700\u540e\u76ee\u6807\u6570\u636e\u7684\u5207\u7247\u4f1a\u8d8a\u754c, \u56e0\u6b64\u4f7f\u7528view(-1)\u6765\u4fdd\u8bc1\u5f62\u72b6\u6b63\u5e38. target = source [ i + 1 : i + 1 + seq_len ] . view ( - 1 ) return data , target \u8f93\u5165\u5b9e\u4f8b: # \u4ee5\u6d4b\u8bd5\u96c6\u6570\u636e\u4e3a\u4f8b source = test_data i = 1 \u8f93\u51fa\u6548\u679c: data = tensor([[ 12, 1053, 355, 134, 37, 7, 4, 0, 835, 9834], [ 635, 8, 5, 5, 421, 4, 88, 8, 573, 2511], [ 0, 58, 8, 8, 6, 692, 544, 0, 212, 5], [ 12, 0, 105, 26, 3, 5, 6, 0, 4, 56], [ 3, 16074, 21254, 320, 3, 262, 16, 6, 1087, 89], [ 3, 751, 3866, 10, 12, 31, 246, 238, 79, 49], [ 635, 943, 78, 36, 12, 475, 66, 10, 4, 924], [ 0, 2358, 52, 4, 12, 4, 5, 0, 19831, 21], [ 26, 38, 54, 40, 1589, 3729, 1014, 5, 8, 4], [ 33, 17597, 33, 1661, 15, 7, 5, 0, 4, 170], [ 335, 268, 117, 0, 0, 4, 3144, 1557, 0, 160], [ 106, 4, 4706, 2245, 12, 1074, 13, 2105, 5, 29], [ 5, 16074, 10, 1087, 12, 137, 251, 13238, 8, 4], [ 394, 746, 4, 9, 12, 6032, 4, 2190, 303, 12651], [ 8, 616, 2107, 4, 3, 4, 425, 0, 10, 510], [ 1339, 112, 23, 335, 3, 22251, 1162, 9, 11, 9], [ 1212, 468, 6, 820, 9, 7, 1231, 4202, 2866, 382], [ 6, 24, 104, 6, 4, 4, 7, 10, 9, 588], [ 31, 190, 0, 0, 230, 267, 4, 273, 278, 6], [ 34, 25, 47, 26, 1864, 6, 694, 0, 2112, 3], [ 11, 6, 52, 798, 8, 69, 20, 31, 63, 9], [ 1800, 25, 2141, 2442, 117, 31, 196, 7290, 4, 298], [ 15, 171, 15, 17, 1712, 13, 217, 59, 736, 5], [ 4210, 191, 142, 14, 5251, 939, 59, 38, 10055, 25132], [ 302, 23, 11718, 11, 11, 599, 382, 317, 8, 13], [ 16, 1564, 9, 4808, 6, 0, 6, 6, 4, 4], [ 4, 7, 39, 7, 3934, 5, 9, 3, 8047, 557], [ 394, 0, 10715, 3580, 8682, 31, 242, 0, 10055, 170], [ 96, 6, 144, 3403, 4, 13, 1014, 14, 6, 2395], [ 4, 3, 13729, 14, 40, 0, 5, 18, 676, 3267], [ 1031, 3, 0, 628, 1589, 22, 10916, 10969, 5, 22548], [ 9, 12, 6, 84, 15, 49, 3144, 7, 102, 15], [ 916, 12, 4, 203, 0, 273, 303, 333, 4318, 0], [ 6, 12, 0, 4842, 5, 17, 4, 47, 4138, 2072], [ 38, 237, 5, 50, 35, 27, 18530, 244, 20, 6]]) target = tensor([ 635, 8, 5, 5, 421, 4, 88, 8, 573, 2511, 0, 58, 8, 8, 6, 692, 544, 0, 212, 5, 12, 0, 105, 26, 3, 5, 6, 0, 4, 56, 3, 16074, 21254, 320, 3, 262, 16, 6, 1087, 89, 3, 751, 3866, 10, 12, 31, 246, 238, 79, 49, 635, 943, 78, 36, 12, 475, 66, 10, 4, 924, 0, 2358, 52, 4, 12, 4, 5, 0, 19831, 21, 26, 38, 54, 40, 1589, 3729, 1014, 5, 8, 4, 33, 17597, 33, 1661, 15, 7, 5, 0, 4, 170, 335, 268, 117, 0, 0, 4, 3144, 1557, 0, 160, 106, 4, 4706, 2245, 12, 1074, 13, 2105, 5, 29, 5, 16074, 10, 1087, 12, 137, 251, 13238, 8, 4, 394, 746, 4, 9, 12, 6032, 4, 2190, 303, 12651, 8, 616, 2107, 4, 3, 4, 425, 0, 10, 510, 1339, 112, 23, 335, 3, 22251, 1162, 9, 11, 9, 1212, 468, 6, 820, 9, 7, 1231, 4202, 2866, 382, 6, 24, 104, 6, 4, 4, 7, 10, 9, 588, 31, 190, 0, 0, 230, 267, 4, 273, 278, 6, 34, 25, 47, 26, 1864, 6, 694, 0, 2112, 3, 11, 6, 52, 798, 8, 69, 20, 31, 63, 9, 1800, 25, 2141, 2442, 117, 31, 196, 7290, 4, 298, 15, 171, 15, 17, 1712, 13, 217, 59, 736, 5, 4210, 191, 142, 14, 5251, 939, 59, 38, 10055, 25132, 302, 23, 11718, 11, 11, 599, 382, 317, 8, 13, 16, 1564, 9, 4808, 6, 0, 6, 6, 4, 4, 4, 7, 39, 7, 3934, 5, 9, 3, 8047, 557, 394, 0, 10715, 3580, 8682, 31, 242, 0, 10055, 170, 96, 6, 144, 3403, 4, 13, 1014, 14, 6, 2395, 4, 3, 13729, 14, 40, 0, 5, 18, 676, 3267, 1031, 3, 0, 628, 1589, 22, 10916, 10969, 5, 22548, 9, 12, 6, 84, 15, 49, 3144, 7, 102, 15, 916, 12, 4, 203, 0, 273, 303, 333, 4318, 0, 6, 12, 0, 4842, 5, 17, 4, 47, 4138, 2072, 38, 237, 5, 50, 35, 27, 18530, 244, 20, 6, 13, 1083, 35, 1990, 653, 13, 10, 11, 1538, 56]) \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570 \u00b6 \u8bbe\u7f6e\u6a21\u578b\u8d85\u53c2\u6570\u548c\u521d\u59cb\u5316\u6a21\u578b # \u901a\u8fc7TEXT.vocab.stoi\u65b9\u6cd5\u83b7\u5f97\u4e0d\u91cd\u590d\u8bcd\u6c47\u603b\u6570 ntokens = len ( TEXT . vocab . stoi ) # \u8bcd\u5d4c\u5165\u5927\u5c0f\u4e3a200 emsize = 200 # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u8282\u70b9\u6570 nhid = 200 # \u7f16\u7801\u5668\u5c42\u7684\u6570\u91cf nlayers = 2 # \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u5934\u6570 nhead = 2 # \u7f6e0\u6bd4\u7387 dropout = 0.2 # \u5c06\u53c2\u6570\u8f93\u5165\u5230TransformerModel\u4e2d model = TransformerModel ( ntokens , emsize , nhead , nhid , nlayers , dropout ) . to ( device ) # \u6a21\u578b\u521d\u59cb\u5316\u540e, \u63a5\u4e0b\u6765\u8fdb\u884c\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u9009\u62e9. # \u5173\u4e8e\u635f\u5931\u51fd\u6570, \u6211\u4eec\u4f7f\u7528nn\u81ea\u5e26\u7684\u4ea4\u53c9\u71b5\u635f\u5931 criterion = nn . CrossEntropyLoss () # \u5b66\u4e60\u7387\u521d\u59cb\u503c\u5b9a\u4e3a5.0 lr = 5.0 # \u4f18\u5316\u5668\u9009\u62e9torch\u81ea\u5e26\u7684SGD\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5, \u5e76\u628alr\u4f20\u5165\u5176\u4e2d optimizer = torch . optim . SGD ( model . parameters (), lr = lr ) # \u5b9a\u4e49\u5b66\u4e60\u7387\u8c03\u6574\u65b9\u6cd5, \u4f7f\u7528torch\u81ea\u5e26\u7684lr_scheduler, \u5c06\u4f18\u5316\u5668\u4f20\u5165\u5176\u4e2d. scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , 1.0 , gamma = 0.95 ) \u6a21\u578b\u8bad\u7ec3\u4ee3\u7801\u5206\u6790: # \u5bfc\u5165\u65f6\u95f4\u5de5\u5177\u5305 import time def train (): \"\"\"\u8bad\u7ec3\u51fd\u6570\"\"\" # \u6a21\u578b\u5f00\u542f\u8bad\u7ec3\u6a21\u5f0f model . train () # \u5b9a\u4e49\u521d\u59cb\u635f\u5931\u4e3a0 total_loss = 0. # \u83b7\u5f97\u5f53\u524d\u65f6\u95f4 start_time = time . time () # \u5f00\u59cb\u904d\u5386\u6279\u6b21\u6570\u636e for batch , i in enumerate ( range ( 0 , train_data . size ( 0 ) - 1 , bptt )): # \u901a\u8fc7get_batch\u83b7\u5f97\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e data , targets = get_batch ( train_data , i ) # \u8bbe\u7f6e\u4f18\u5316\u5668\u521d\u59cb\u68af\u5ea6\u4e3a0\u68af\u5ea6 optimizer . zero_grad () # \u5c06\u6570\u636e\u88c5\u5165model\u5f97\u5230\u8f93\u51fa output = model ( data ) # \u5c06\u8f93\u51fa\u548c\u76ee\u6807\u6570\u636e\u4f20\u5165\u635f\u5931\u51fd\u6570\u5bf9\u8c61 loss = criterion ( output . view ( - 1 , ntokens ), targets ) # \u635f\u5931\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u4ee5\u83b7\u5f97\u603b\u7684\u635f\u5931 loss . backward () # \u4f7f\u7528nn\u81ea\u5e26\u7684clip_grad_norm_\u65b9\u6cd5\u8fdb\u884c\u68af\u5ea6\u89c4\u8303\u5316, \u9632\u6b62\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8 torch . nn . utils . clip_grad_norm_ ( model . parameters (), 0.5 ) # \u6a21\u578b\u53c2\u6570\u8fdb\u884c\u66f4\u65b0 optimizer . step () # \u5c06\u6bcf\u5c42\u7684\u635f\u5931\u76f8\u52a0\u83b7\u5f97\u603b\u7684\u635f\u5931 total_loss += loss . item () # \u65e5\u5fd7\u6253\u5370\u95f4\u9694\u5b9a\u4e3a200 log_interval = 200 # \u5982\u679cbatch\u662f200\u7684\u500d\u6570\u4e14\u5927\u4e8e0\uff0c\u5219\u6253\u5370\u76f8\u5173\u65e5\u5fd7 if batch % log_interval == 0 and batch > 0 : # \u5e73\u5747\u635f\u5931\u4e3a\u603b\u635f\u5931\u9664\u4ee5log_interval cur_loss = total_loss / log_interval # \u9700\u8981\u7684\u65f6\u95f4\u4e3a\u5f53\u524d\u65f6\u95f4\u51cf\u53bb\u5f00\u59cb\u65f6\u95f4 elapsed = time . time () - start_time # \u6253\u5370\u8f6e\u6570, \u5f53\u524d\u6279\u6b21\u548c\u603b\u6279\u6b21, \u5f53\u524d\u5b66\u4e60\u7387, \u8bad\u7ec3\u901f\u5ea6(\u6bcf\u8c6a\u79d2\u5904\u7406\u591a\u5c11\u6279\u6b21), # \u5e73\u5747\u635f\u5931, \u4ee5\u53ca\u56f0\u60d1\u5ea6, \u56f0\u60d1\u5ea6\u662f\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6307\u6807, \u5b83\u7684\u8ba1\u7b97\u65b9\u6cd5\u5c31\u662f # \u5bf9\u4ea4\u53c9\u71b5\u5e73\u5747\u635f\u5931\u53d6\u81ea\u7136\u5bf9\u6570\u7684\u5e95\u6570. print ( '| epoch {:3d} | {:5d} / {:5d} batches | ' 'lr {:02.2f} | ms/batch {:5.2f} | ' 'loss {:5.2f} | ppl {:8.2f} ' . format ( epoch , batch , len ( train_data ) // bptt , scheduler . get_lr ()[ 0 ], elapsed * 1000 / log_interval , cur_loss , math . exp ( cur_loss ))) # \u6bcf\u4e2a\u6279\u6b21\u7ed3\u675f\u540e, \u603b\u635f\u5931\u5f520 total_loss = 0 # \u5f00\u59cb\u65f6\u95f4\u53d6\u5f53\u524d\u65f6\u95f4 start_time = time . time () \u6a21\u578b\u8bc4\u4f30\u4ee3\u7801\u5206\u6790: def evaluate ( eval_model , data_source ): \"\"\"\u8bc4\u4f30\u51fd\u6570, \u8bc4\u4f30\u9636\u6bb5\u5305\u62ec\u9a8c\u8bc1\u548c\u6d4b\u8bd5, \u5b83\u7684\u4e24\u4e2a\u53c2\u6570eval_model\u4e3a\u6bcf\u8f6e\u8bad\u7ec3\u4ea7\u751f\u7684\u6a21\u578b data_source\u4ee3\u8868\u9a8c\u8bc1\u6216\u6d4b\u8bd5\u6570\u636e\u96c6\"\"\" # \u6a21\u578b\u5f00\u542f\u8bc4\u4f30\u6a21\u5f0f eval_model . eval () # \u603b\u635f\u5931\u5f520 total_loss = 0 # \u56e0\u4e3a\u8bc4\u4f30\u6a21\u5f0f\u6a21\u578b\u53c2\u6570\u4e0d\u53d8, \u56e0\u6b64\u53cd\u5411\u4f20\u64ad\u4e0d\u9700\u8981\u6c42\u5bfc, \u4ee5\u52a0\u5feb\u8ba1\u7b97 with torch . no_grad (): # \u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u76f8\u540c, \u4f46\u662f\u56e0\u4e3a\u8fc7\u7a0b\u4e0d\u9700\u8981\u6253\u5370\u4fe1\u606f, \u56e0\u6b64\u4e0d\u9700\u8981batch\u6570 for i in range ( 0 , data_source . size ( 0 ) - 1 , bptt ): # \u9996\u5148\u8fd8\u662f\u901a\u8fc7\u901a\u8fc7get_batch\u83b7\u5f97\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e data , targets = get_batch ( data_source , i ) # \u901a\u8fc7eval_model\u83b7\u5f97\u8f93\u51fa output = eval_model ( data ) # \u5bf9\u8f93\u51fa\u5f62\u72b6\u6241\u5e73\u5316, \u53d8\u4e3a\u5168\u90e8\u8bcd\u6c47\u7684\u6982\u7387\u5206\u5e03 output_flat = output . view ( - 1 , ntokens ) # \u83b7\u5f97\u8bc4\u4f30\u8fc7\u7a0b\u7684\u603b\u635f\u5931 total_loss += criterion ( output_flat , targets ) . item () # \u8ba1\u7b97\u5e73\u5747\u635f\u5931 cur_loss = total_loss / (( data_source . size ( 0 ) - 1 ) / bptt ) # \u8fd4\u56de\u5e73\u5747\u635f\u5931 return cur_loss \u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5) \u00b6 \u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u4ee3\u7801\u5206\u6790: # \u9996\u5148\u521d\u59cb\u5316\u6700\u4f73\u9a8c\u8bc1\u635f\u5931\uff0c\u521d\u59cb\u503c\u4e3a\u65e0\u7a77\u5927 import copy best_val_loss = float ( \"inf\" ) # \u5b9a\u4e49\u8bad\u7ec3\u8f6e\u6570 epochs = 3 # \u5b9a\u4e49\u6700\u4f73\u6a21\u578b\u53d8\u91cf, \u521d\u59cb\u503c\u4e3aNone best_model = None # \u4f7f\u7528for\u5faa\u73af\u904d\u5386\u8f6e\u6570 for epoch in range ( 1 , epochs + 1 ): # \u9996\u5148\u83b7\u5f97\u8f6e\u6570\u5f00\u59cb\u65f6\u95f4 epoch_start_time = time . time () # \u8c03\u7528\u8bad\u7ec3\u51fd\u6570 train () # \u8be5\u8f6e\u8bad\u7ec3\u540e\u6211\u4eec\u7684\u6a21\u578b\u53c2\u6570\u5df2\u7ecf\u53d1\u751f\u4e86\u53d8\u5316 # \u5c06\u6a21\u578b\u548c\u8bc4\u4f30\u6570\u636e\u4f20\u5165\u5230\u8bc4\u4f30\u51fd\u6570\u4e2d val_loss = evaluate ( model , val_data ) # \u4e4b\u540e\u6253\u5370\u6bcf\u8f6e\u7684\u8bc4\u4f30\u65e5\u5fd7\uff0c\u5206\u522b\u6709\u8f6e\u6570\uff0c\u8017\u65f6\uff0c\u9a8c\u8bc1\u635f\u5931\u4ee5\u53ca\u9a8c\u8bc1\u56f0\u60d1\u5ea6 print ( '-' * 89 ) print ( '| end of epoch {:3d} | time: {:5.2f} s | valid loss {:5.2f} | ' 'valid ppl {:8.2f} ' . format ( epoch , ( time . time () - epoch_start_time ), val_loss , math . exp ( val_loss ))) print ( '-' * 89 ) # \u6211\u4eec\u5c06\u6bd4\u8f83\u54ea\u4e00\u8f6e\u635f\u5931\u6700\u5c0f\uff0c\u8d4b\u503c\u7ed9best_val_loss\uff0c # \u5e76\u53d6\u8be5\u635f\u5931\u4e0b\u7684\u6a21\u578b\u4e3abest_model if val_loss < best_val_loss : best_val_loss = val_loss # \u4f7f\u7528\u6df1\u62f7\u8d1d\uff0c\u62f7\u8d1d\u6700\u4f18\u6a21\u578b best_model = copy . deepcopy ( model ) # \u6bcf\u8f6e\u90fd\u4f1a\u5bf9\u4f18\u5316\u65b9\u6cd5\u7684\u5b66\u4e60\u7387\u505a\u8c03\u6574 scheduler . step () \u8f93\u51fa\u6548\u679c: | epoch 1 | 200/ 2981 batches | lr 5.00 | ms/batch 30.03 | loss 7.68 | ppl 2158.52 | epoch 1 | 400/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 5.26 | ppl 193.39 | epoch 1 | 600/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 4.07 | ppl 58.44 | epoch 1 | 800/ 2981 batches | lr 5.00 | ms/batch 28.88 | loss 3.41 | ppl 30.26 | epoch 1 | 1000/ 2981 batches | lr 5.00 | ms/batch 28.89 | loss 2.98 | ppl 19.72 | epoch 1 | 1200/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 2.79 | ppl 16.30 | epoch 1 | 1400/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.67 | ppl 14.38 | epoch 1 | 1600/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss 2.58 | ppl 13.19 | epoch 1 | 1800/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.43 | ppl 11.32 | epoch 1 | 2000/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss 2.39 | ppl 10.93 | epoch 1 | 2200/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.33 | ppl 10.24 | epoch 1 | 2400/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.36 | ppl 10.59 | epoch 1 | 2600/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 2.33 | ppl 10.31 | epoch 1 | 2800/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss 2.26 | ppl 9.54 ----------------------------------------------------------------------------------------- | end of epoch 1 | time: 90.01s | valid loss 1.32 | valid ppl 3.73 ----------------------------------------------------------------------------------------- | epoch 2 | 200/ 2981 batches | lr 4.75 | ms/batch 29.08 | loss 2.18 | ppl 8.83 | epoch 2 | 400/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 2.11 | ppl 8.24 | epoch 2 | 600/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.98 | ppl 7.23 | epoch 2 | 800/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 2.00 | ppl 7.39 | epoch 2 | 1000/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss 1.94 | ppl 6.96 | epoch 2 | 1200/ 2981 batches | lr 4.75 | ms/batch 28.92 | loss 1.97 | ppl 7.15 | epoch 2 | 1400/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss 1.98 | ppl 7.28 | epoch 2 | 1600/ 2981 batches | lr 4.75 | ms/batch 28.92 | loss 1.97 | ppl 7.16 | epoch 2 | 1800/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.92 | ppl 6.84 | epoch 2 | 2000/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.96 | ppl 7.11 | epoch 2 | 2200/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.92 | ppl 6.80 | epoch 2 | 2400/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss 1.94 | ppl 6.93 | epoch 2 | 2600/ 2981 batches | lr 4.75 | ms/batch 28.76 | loss 1.91 | ppl 6.76 | epoch 2 | 2800/ 2981 batches | lr 4.75 | ms/batch 28.75 | loss 1.89 | ppl 6.64 ----------------------------------------------------------------------------------------- | end of epoch 2 | time: 89.71s | valid loss 1.01 | valid ppl 2.74 ----------------------------------------------------------------------------------------- | epoch 3 | 200/ 2981 batches | lr 4.51 | ms/batch 28.88 | loss 1.78 | ppl 5.96 | epoch 3 | 400/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.89 | ppl 6.59 | epoch 3 | 600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.72 | ppl 5.58 | epoch 3 | 800/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.73 | ppl 5.63 | epoch 3 | 1000/ 2981 batches | lr 4.51 | ms/batch 28.73 | loss 1.65 | ppl 5.22 | epoch 3 | 1200/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.69 | ppl 5.40 | epoch 3 | 1400/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.73 | ppl 5.66 | epoch 3 | 1600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.75 | ppl 5.73 | epoch 3 | 1800/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.67 | ppl 5.33 | epoch 3 | 2000/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.69 | ppl 5.41 | epoch 3 | 2200/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.66 | ppl 5.26 | epoch 3 | 2400/ 2981 batches | lr 4.51 | ms/batch 28.76 | loss 1.69 | ppl 5.43 | epoch 3 | 2600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.71 | ppl 5.55 | epoch 3 | 2800/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.72 | ppl 5.58 ----------------------------------------------------------------------------------------- | end of epoch 3 | time: 89.26s | valid loss 0.85 | valid ppl 2.33 ----------------------------------------------------------------------------------------- \u6a21\u578b\u6d4b\u8bd5\u4ee3\u7801\u5206\u6790: # \u6211\u4eec\u4ecd\u7136\u4f7f\u7528evaluate\u51fd\u6570\uff0c\u8fd9\u6b21\u5b83\u7684\u53c2\u6570\u662fbest_model\u4ee5\u53ca\u6d4b\u8bd5\u6570\u636e test_loss = evaluate ( best_model , test_data ) # \u6253\u5370\u6d4b\u8bd5\u65e5\u5fd7\uff0c\u5305\u62ec\u6d4b\u8bd5\u635f\u5931\u548c\u6d4b\u8bd5\u56f0\u60d1\u5ea6 print ( '=' * 89 ) print ( '| End of training | test loss {:5.2f} | test ppl {:8.2f} ' . format ( test_loss , math . exp ( test_loss ))) print ( '=' * 89 ) \u8f93\u51fa\u6548\u679c: ========================================================================================= | End of training | test loss 0.83 | test ppl 2.30 ========================================================================================= \u5c0f\u8282\u603b\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u8bed\u8a00\u6a21\u578b: \u4ee5\u4e00\u4e2a\u7b26\u5408\u8bed\u8a00\u89c4\u5f8b\u7684\u5e8f\u5217\u4e3a\u8f93\u5165\uff0c\u6a21\u578b\u5c06\u5229\u7528\u5e8f\u5217\u95f4\u5173\u7cfb\u7b49\u7279\u5f81\uff0c\u8f93\u51fa\u4e00\u4e2a\u5728\u6240\u6709\u8bcd\u6c47\u4e0a\u7684\u6982\u7387\u5206\u5e03.\u8fd9\u6837\u7684\u6a21\u578b\u79f0\u4e3a\u8bed\u8a00\u6a21\u578b. \u5b66\u4e60\u4e86\u8bed\u8a00\u6a21\u578b\u80fd\u89e3\u51b3\u54ea\u4e9b\u95ee\u9898: 1, \u6839\u636e\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u5728\u5b83\u7684\u57fa\u7840\u4e0a\u5b8c\u6210\u673a\u5668\u7ffb\u8bd1\uff0c\u6587\u672c\u751f\u6210\u7b49\u4efb\u52a1\uff0c\u56e0\u4e3a\u6211\u4eec\u901a\u8fc7\u6700\u540e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u6c47\u662f\u4ec0\u4e48. 2, \u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5224\u65ad\u8f93\u5165\u7684\u5e8f\u5217\u662f\u5426\u4e3a\u4e00\u53e5\u5b8c\u6574\u7684\u8bdd\uff0c\u56e0\u4e3a\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u67e5\u770b\u6700\u5927\u6982\u7387\u662f\u5426\u843d\u5728\u53e5\u5b50\u7ed3\u675f\u7b26\u4e0a\uff0c\u6765\u5224\u65ad\u5b8c\u6574\u6027. 3, \u8bed\u8a00\u6a21\u578b\u672c\u8eab\u7684\u8bad\u7ec3\u76ee\u6807\u662f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\uff0c\u56e0\u4e3a\u5b83\u7684\u7279\u5f81\u63d0\u53d6\u90e8\u5206\u4f1a\u62bd\u8c61\u5f88\u591a\u8bed\u8a00\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5173\u7cfb\u53ef\u80fd\u540c\u6837\u5bf9\u5176\u4ed6\u8bed\u8a00\u7c7b\u4efb\u52a1\u6709\u6548\u679c.\u56e0\u6b64\u53ef\u4ee5\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6574\u4e2a\u6848\u4f8b\u7684\u4e94\u4e2a\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406 \u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570 \u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5) \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 torchtext\u4ecb\u7ecd: \u5b83\u662ftorch\u5de5\u5177\u4e2d\u5904\u7406NLP\u95ee\u9898\u7684\u5e38\u7528\u6570\u636e\u5904\u7406\u5305. \u5bf9\u6587\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406, \u6bd4\u5982\u6587\u672c\u8bed\u6599\u52a0\u8f7d, \u6587\u672c\u8fed\u4ee3\u5668\u6784\u5efa\u7b49. \u5305\u542b\u5f88\u591a\u7ecf\u5178\u6587\u672c\u8bed\u6599\u7684\u9884\u52a0\u8f7d\u65b9\u6cd5. \u5176\u4e2d\u5305\u62ec\u7684\u8bed\u6599\u6709\uff1a\u7528\u4e8e\u60c5\u611f\u5206\u6790\u7684SST\u548cIMDB, \u7528\u4e8e\u95ee\u9898\u5206\u7c7b\u7684TREC, \u7528\u4e8e\u53ca\u5176\u7ffb\u8bd1\u7684 WMT14\uff0c IWSLT\uff0c\u4ee5\u53ca\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u4efb\u52a1wikiText-2, WikiText103, PennTreebank. wikiText-2\u6570\u636e\u96c6\u7684\u4f53\u91cf\u4e2d\u7b49, \u8bad\u7ec3\u96c6\u5171\u6709600\u7bc7\u77ed\u6587, \u5171208\u4e07\u5de6\u53f3\u7684\u8bcd\u6c47, 33278\u4e2a\u4e0d\u91cd\u590d\u8bcd\u6c47, OvV\uff08\u6709\u591a\u5c11\u6b63\u5e38\u82f1\u6587\u8bcd\u6c47\u4e0d\u5728\u8be5\u6570\u636e\u96c6\u4e2d\u7684\u5360\u6bd4\uff09\u4e3a2.6%\uff0c\u6570\u636e\u96c6\u4e2d\u7684\u77ed\u6587\u90fd\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5bf9\u4e00\u4e9b\u6982\u5ff5\u7684\u4ecb\u7ecd\u548c\u63cf\u8ff0. \u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406 \u901a\u8fc7torchtext\u4e2d\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86train_txt, val_txt, test_txt. \u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e \u5b9e\u73b0\u4e86\u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e00\u4e2a\u51fd\u6570batchify, \u7528\u4e8e\u5c06\u6587\u672c\u6570\u636e\u6620\u5c04\u6210\u8fde\u7eed\u6570\u5b57, \u5e76\u8f6c\u6362\u6210\u6307\u5b9a\u7684\u6837\u5f0f. \u5b9e\u73b0\u4e86\u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e8c\u4e2a\u51fd\u6570get_batch, \u7528\u4e8e\u83b7\u5f97\u6bcf\u4e2a\u6279\u6b21\u5408\u7406\u5927\u5c0f\u7684\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e. \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570 \u6784\u5efa\u4e86\u7528\u4e8e\u8bad\u7ec3\u7684\u51fd\u6570train() \u6784\u5efa\u4e86\u7528\u4e8e\u8bc4\u4f30\u7684\u51fd\u6570evaluate() \u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5) \u9996\u5148\u5b9e\u73b0\u4e86\u6a21\u578b\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u8fc7\u7a0b, \u5e76\u6253\u5370\u4e86\u7ed3\u679c. \u6700\u540e\u5b9e\u73b0\u4e86\u6a21\u578b\u7684\u6d4b\u8bd5\u8fc7\u7a0b, \u5f97\u5230\u4e86\u4e0d\u9519\u7684\u56f0\u60d1\u5ea6\u6307\u6807.","title":"Transformer\u6784\u5efa\u8bed\u8a00\u6a21\u578b"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#31-transformer","text":"","title":"3.1 \u4f7f\u7528Transformer\u6784\u5efa\u8bed\u8a00\u6a21\u578b"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#_1","text":"\u4e86\u89e3\u6709\u5173\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6. \u638c\u63e1\u4f7f\u7528Transformer\u6784\u5efa\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u73b0\u8fc7\u7a0b. \u4ec0\u4e48\u662f\u8bed\u8a00\u6a21\u578b: \u4ee5\u4e00\u4e2a\u7b26\u5408\u8bed\u8a00\u89c4\u5f8b\u7684\u5e8f\u5217\u4e3a\u8f93\u5165\uff0c\u6a21\u578b\u5c06\u5229\u7528\u5e8f\u5217\u95f4\u5173\u7cfb\u7b49\u7279\u5f81\uff0c\u8f93\u51fa\u4e00\u4e2a\u5728\u6240\u6709\u8bcd\u6c47\u4e0a\u7684\u6982\u7387\u5206\u5e03.\u8fd9\u6837\u7684\u6a21\u578b\u79f0\u4e3a\u8bed\u8a00\u6a21\u578b. # \u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u8bed\u6599\u4e00\u822c\u6765\u81ea\u4e8e\u6587\u7ae0\uff0c\u5bf9\u5e94\u7684\u6e90\u6587\u672c\u548c\u76ee\u6807\u6587\u672c\u5f62\u5982: src1 = \"I can do\" tgt1 = \"can do it\" src2 = \"can do it\", tgt2 = \"do it <eos>\" \u8bed\u8a00\u6a21\u578b\u80fd\u89e3\u51b3\u54ea\u4e9b\u95ee\u9898: 1, \u6839\u636e\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u5728\u5b83\u7684\u57fa\u7840\u4e0a\u5b8c\u6210\u673a\u5668\u7ffb\u8bd1\uff0c\u6587\u672c\u751f\u6210\u7b49\u4efb\u52a1\uff0c\u56e0\u4e3a\u6211\u4eec\u901a\u8fc7\u6700\u540e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u6c47\u662f\u4ec0\u4e48. 2, \u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5224\u65ad\u8f93\u5165\u7684\u5e8f\u5217\u662f\u5426\u4e3a\u4e00\u53e5\u5b8c\u6574\u7684\u8bdd\uff0c\u56e0\u4e3a\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u67e5\u770b\u6700\u5927\u6982\u7387\u662f\u5426\u843d\u5728\u53e5\u5b50\u7ed3\u675f\u7b26\u4e0a\uff0c\u6765\u5224\u65ad\u5b8c\u6574\u6027. 3, \u8bed\u8a00\u6a21\u578b\u672c\u8eab\u7684\u8bad\u7ec3\u76ee\u6807\u662f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\uff0c\u56e0\u4e3a\u5b83\u7684\u7279\u5f81\u63d0\u53d6\u90e8\u5206\u4f1a\u62bd\u8c61\u5f88\u591a\u8bed\u8a00\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5173\u7cfb\u53ef\u80fd\u540c\u6837\u5bf9\u5176\u4ed6\u8bed\u8a00\u7c7b\u4efb\u52a1\u6709\u6548\u679c.\u56e0\u6b64\u53ef\u4ee5\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#_2","text":"\u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406 \u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570 \u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5)","title":"\u6574\u4e2a\u6848\u4f8b\u7684\u5b9e\u73b0\u53ef\u5206\u4e3a\u4ee5\u4e0b\u4e94\u4e2a\u6b65\u9aa4"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#_3","text":"pytorch\u7248\u672c\u5fc5\u987b\u4f7f\u75281.3.1, python\u7248\u672c\u4f7f\u75283.6.x pip install torch==1.3.1 # \u6570\u5b66\u8ba1\u7b97\u5de5\u5177\u5305math import math # torch\u4ee5\u53catorch.nn, torch.nn.functional import torch import torch.nn as nn import torch.nn.functional as F # torch\u4e2d\u7ecf\u5178\u6587\u672c\u6570\u636e\u96c6\u6709\u5173\u7684\u5de5\u5177\u5305 # \u5177\u4f53\u8be6\u60c5\u53c2\u8003\u4e0b\u65b9torchtext\u4ecb\u7ecd import torchtext # torchtext\u4e2d\u7684\u6570\u636e\u5904\u7406\u5de5\u5177, get_tokenizer\u7528\u4e8e\u82f1\u6587\u5206\u8bcd from torchtext.data.utils import get_tokenizer # \u5df2\u7ecf\u6784\u5efa\u5b8c\u6210\u7684TransformerModel from pyitcast.transformer import TransformerModel torchtext\u4ecb\u7ecd: \u5b83\u662ftorch\u5de5\u5177\u4e2d\u5904\u7406NLP\u95ee\u9898\u7684\u5e38\u7528\u6570\u636e\u5904\u7406\u5305. torchtext\u7684\u91cd\u8981\u529f\u80fd: \u5bf9\u6587\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406, \u6bd4\u5982\u6587\u672c\u8bed\u6599\u52a0\u8f7d, \u6587\u672c\u8fed\u4ee3\u5668\u6784\u5efa\u7b49. \u5305\u542b\u5f88\u591a\u7ecf\u5178\u6587\u672c\u8bed\u6599\u7684\u9884\u52a0\u8f7d\u65b9\u6cd5. \u5176\u4e2d\u5305\u62ec\u7684\u8bed\u6599\u6709\uff1a\u7528\u4e8e\u60c5\u611f\u5206\u6790\u7684SST\u548cIMDB, \u7528\u4e8e\u95ee\u9898\u5206\u7c7b\u7684TREC, \u7528\u4e8e\u53ca\u5176\u7ffb\u8bd1\u7684 WMT14\uff0c IWSLT\uff0c\u4ee5\u53ca\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u4efb\u52a1wikiText-2, WikiText103, PennTreebank. \u6211\u4eec\u8fd9\u91cc\u4f7f\u7528wikiText-2\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b, \u4e0b\u9762\u6709\u5173\u8be5\u6570\u636e\u96c6\u7684\u76f8\u5173\u8be6\u60c5: wikiText-2\u6570\u636e\u96c6\u7684\u4f53\u91cf\u4e2d\u7b49, \u8bad\u7ec3\u96c6\u5171\u6709600\u7bc7\u77ed\u6587, \u5171208\u4e07\u5de6\u53f3\u7684\u8bcd\u6c47, 33278\u4e2a\u4e0d\u91cd\u590d\u8bcd\u6c47, OoV\uff08\u6709\u591a\u5c11\u6b63\u5e38\u82f1\u6587\u8bcd\u6c47\u4e0d\u5728\u8be5\u6570\u636e\u96c6\u4e2d\u7684\u5360\u6bd4\uff09\u4e3a2.6%\uff0c\u6570\u636e\u96c6\u4e2d\u7684\u77ed\u6587\u90fd\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5bf9\u4e00\u4e9b\u6982\u5ff5\u7684\u4ecb\u7ecd\u548c\u63cf\u8ff0.","title":"\u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#wikitext-2","text":"# \u521b\u5efa\u8bed\u6599\u57df, \u8bed\u6599\u57df\u662f\u5b58\u653e\u8bed\u6599\u7684\u6570\u636e\u7ed3\u6784, # \u5b83\u7684\u56db\u4e2a\u53c2\u6570\u4ee3\u8868\u7ed9\u5b58\u653e\u8bed\u6599\uff08\u6216\u79f0\u4f5c\u6587\u672c\uff09\u65bd\u52a0\u7684\u4f5c\u7528. # \u5206\u522b\u4e3a tokenize,\u4f7f\u7528get_tokenizer(\"basic_english\")\u83b7\u5f97\u4e00\u4e2a\u5206\u5272\u5668\u5bf9\u8c61, # \u5206\u5272\u65b9\u5f0f\u6309\u7167\u6587\u672c\u4e3a\u57fa\u7840\u82f1\u6587\u8fdb\u884c\u5206\u5272. # init_token\u4e3a\u7ed9\u6587\u672c\u65bd\u52a0\u7684\u8d77\u59cb\u7b26 <sos>\u7ed9\u6587\u672c\u65bd\u52a0\u7684\u7ec8\u6b62\u7b26<eos>, # \u6700\u540e\u4e00\u4e2alower\u4e3aTrue, \u5b58\u653e\u7684\u6587\u672c\u5b57\u6bcd\u5168\u90e8\u5c0f\u5199. TEXT = torchtext . data . Field ( tokenize = get_tokenizer ( \"basic_english\" ), init_token = '<sos>' , eos_token = '<eos>' , lower = True ) # \u6700\u7ec8\u83b7\u5f97\u4e00\u4e2aField\u5bf9\u8c61. # <torchtext.data.field.Field object at 0x7fc42a02e7f0> # \u7136\u540e\u4f7f\u7528torchtext\u7684\u6570\u636e\u96c6\u65b9\u6cd5\u5bfc\u5165WikiText2\u6570\u636e, # \u5e76\u5207\u5206\u4e3a\u5bf9\u5e94\u8bad\u7ec3\u6587\u672c, \u9a8c\u8bc1\u6587\u672c\uff0c\u6d4b\u8bd5\u6587\u672c, \u5e76\u5bf9\u8fd9\u4e9b\u6587\u672c\u65bd\u52a0\u521a\u521a\u521b\u5efa\u7684\u8bed\u6599\u57df. train_txt , val_txt , test_txt = torchtext . datasets . WikiText2 . splits ( TEXT ) # \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7examples[0].text\u53d6\u51fa\u6587\u672c\u5bf9\u8c61\u8fdb\u884c\u67e5\u770b. # >>> test_txt.examples[0].text[:10] # ['<eos>', '=', 'robert', '<unk>', '=', '<eos>', '<eos>', 'robert', '<unk>', 'is'] # \u5c06\u8bad\u7ec3\u96c6\u6587\u672c\u6570\u636e\u6784\u5efa\u4e00\u4e2avocab\u5bf9\u8c61, # \u8fd9\u6837\u53ef\u4ee5\u4f7f\u7528vocab\u5bf9\u8c61\u7684stoi\u65b9\u6cd5\u7edf\u8ba1\u6587\u672c\u5171\u5305\u542b\u7684\u4e0d\u91cd\u590d\u8bcd\u6c47\u603b\u6570. TEXT . build_vocab ( train_txt ) # \u7136\u540e\u9009\u62e9\u8bbe\u5907cuda\u6216\u8005cpu device = torch . device ( \"cuda\" if torch . cuda . is_available () else \"cpu\" ) \u8be5\u6848\u4f8b\u7684\u6240\u6709\u4ee3\u7801\u90fd\u5c06\u5b9e\u73b0\u5728\u4e00\u4e2atransformer_lm.py\u6587\u4ef6\u4e2d.","title":"\u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#_4","text":"\u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e00\u4e2a\u51fd\u6570batchify\u4ee3\u7801\u5206\u6790: def batchify ( data , bsz ): \"\"\"batchify\u51fd\u6570\u7528\u4e8e\u5c06\u6587\u672c\u6570\u636e\u6620\u5c04\u6210\u8fde\u7eed\u6570\u5b57, \u5e76\u8f6c\u6362\u6210\u6307\u5b9a\u7684\u6837\u5f0f, \u6307\u5b9a\u7684\u6837\u5f0f\u53ef\u53c2\u8003\u4e0b\u56fe. \u5b83\u6709\u4e24\u4e2a\u8f93\u5165\u53c2\u6570, data\u5c31\u662f\u6211\u4eec\u4e4b\u524d\u5f97\u5230\u7684\u6587\u672c\u6570\u636e(train_txt, val_txt, test_txt), bsz\u662f\u5c31\u662fbatch_size, \u6bcf\u6b21\u6a21\u578b\u66f4\u65b0\u53c2\u6570\u7684\u6570\u636e\u91cf\"\"\" # \u4f7f\u7528TEXT\u7684numericalize\u65b9\u6cd5\u5c06\u5355\u8bcd\u6620\u5c04\u6210\u5bf9\u5e94\u7684\u8fde\u7eed\u6570\u5b57. data = TEXT . numericalize ([ data . examples [ 0 ] . text ]) # >>> data # tensor([[ 3], # [ 12], # [3852], # ..., # [ 6], # [ 3], # [ 3]]) # \u63a5\u7740\u7528\u6570\u636e\u8bcd\u6c47\u603b\u6570\u9664\u4ee5bsz, # \u53d6\u6574\u6570\u5f97\u5230\u4e00\u4e2anbatch\u4ee3\u8868\u9700\u8981\u591a\u5c11\u6b21batch\u540e\u80fd\u591f\u904d\u5386\u5b8c\u6240\u6709\u6570\u636e nbatch = data . size ( 0 ) // bsz # \u4e4b\u540e\u4f7f\u7528narrow\u65b9\u6cd5\u5bf9\u4e0d\u89c4\u6574\u7684\u5269\u4f59\u6570\u636e\u8fdb\u884c\u5220\u9664, # \u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u4ee3\u8868\u6a2a\u8f74\u5220\u9664\u8fd8\u662f\u7eb5\u8f74\u5220\u9664, 0\u4e3a\u6a2a\u8f74\uff0c1\u4e3a\u7eb5\u8f74 # \u7b2c\u4e8c\u4e2a\u548c\u7b2c\u4e09\u4e2a\u53c2\u6570\u4ee3\u8868\u4fdd\u7559\u5f00\u59cb\u8f74\u5230\u7ed3\u675f\u8f74\u7684\u6570\u503c.\u7c7b\u4f3c\u4e8e\u5207\u7247 # \u53ef\u53c2\u8003\u4e0b\u65b9\u6f14\u793a\u793a\u4f8b\u8fdb\u884c\u66f4\u6df1\u7406\u89e3. data = data . narrow ( 0 , 0 , nbatch * bsz ) # >>> data # tensor([[ 3], # [ 12], # [3852], # ..., # [ 78], # [ 299], # [ 36]]) # \u540e\u9762\u4e0d\u80fd\u5f62\u6210bsz\u4e2a\u7684\u4e00\u7ec4\u6570\u636e\u88ab\u5220\u9664 # \u63a5\u7740\u6211\u4eec\u4f7f\u7528view\u65b9\u6cd5\u5bf9data\u8fdb\u884c\u77e9\u9635\u53d8\u6362, \u4f7f\u5176\u6210\u4e3a\u5982\u4e0b\u6837\u5f0f: # tensor([[ 3, 25, 1849, ..., 5, 65, 30], # [ 12, 66, 13, ..., 35, 2438, 4064], # [ 3852, 13667, 2962, ..., 902, 33, 20], # ..., # [ 154, 7, 10, ..., 5, 1076, 78], # [ 25, 4, 4135, ..., 4, 56, 299], # [ 6, 57, 385, ..., 3168, 737, 36]]) # \u56e0\u4e3a\u4f1a\u505a\u8f6c\u7f6e\u64cd\u4f5c, \u56e0\u6b64\u8fd9\u4e2a\u77e9\u9635\u7684\u5f62\u72b6\u662f[None, bsz], # \u5982\u679c\u8f93\u5165\u662f\u8bad\u7ec3\u6570\u636e\u7684\u8bdd\uff0c\u5f62\u72b6\u4e3a[104335, 20], \u53ef\u4ee5\u901a\u8fc7\u6253\u5370data.shape\u83b7\u5f97. # \u4e5f\u5c31\u662fdata\u7684\u5217\u6570\u662f\u7b49\u4e8ebsz\u7684\u503c\u7684. data = data . view ( bsz , - 1 ) . t () . contiguous () # \u6700\u540e\u5c06\u6570\u636e\u5206\u914d\u5728\u6307\u5b9a\u7684\u8bbe\u5907\u4e0a. return data . to ( device ) batchify\u7684\u6837\u5f0f\u8f6c\u5316\u56fe: \u5927\u5199\u5b57\u6bcdA\uff0cB\uff0cC ... \u4ee3\u8868\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd. torch.narrow\u6f14\u793a: >>> x = torch . tensor ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) >>> x . narrow ( 0 , 0 , 2 ) tensor ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> x . narrow ( 1 , 1 , 2 ) tensor ([[ 2 , 3 ], [ 5 , 6 ], [ 8 , 9 ]]) \u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u4f7f\u7528batchify\u6765\u5904\u7406\u8bad\u7ec3\u6570\u636e\uff0c\u9a8c\u8bc1\u6570\u636e\u4ee5\u53ca\u6d4b\u8bd5\u6570\u636e # \u8bad\u7ec3\u6570\u636e\u7684batch size batch_size = 20 # \u9a8c\u8bc1\u548c\u6d4b\u8bd5\u6570\u636e\uff08\u7edf\u79f0\u4e3a\u8bc4\u4f30\u6570\u636e\uff09\u7684batch size eval_batch_size = 10 # \u83b7\u5f97train_data, val_data, test_data train_data = batchify ( train_txt , batch_size ) val_data = batchify ( val_txt , eval_batch_size ) test_data = batchify ( test_txt , eval_batch_size ) \u4e0a\u9762\u7684\u5206\u5272\u6279\u6b21\u5e76\u6ca1\u6709\u8fdb\u884c\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e\u7684\u5904\u7406, \u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u6839\u636e\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u8bed\u6599\u89c4\u5b9a\u6765\u6784\u5efa\u6e90\u6570\u636e\u4e0e\u76ee\u6807\u6570\u636e. \u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u8bed\u6599\u89c4\u5b9a: \u5982\u679c\u6e90\u6570\u636e\u4e3a\u53e5\u5b50ABCD, ABCD\u4ee3\u8868\u53e5\u5b50\u4e2d\u7684\u8bcd\u6c47\u6216\u7b26\u53f7, \u5219\u5b83\u7684\u76ee\u6807\u6570\u636e\u4e3aBCDE, BCDE\u5206\u522b\u4ee3\u8868ABCD\u7684\u4e0b\u4e00\u4e2a\u8bcd\u6c47. \u5982\u56fe\u6240\u793a\uff0c\u6211\u4eec\u8fd9\u91cc\u7684\u53e5\u5b50\u5e8f\u5217\u662f\u7ad6\u7740\u7684, \u800c\u4e14\u6211\u4eec\u53d1\u73b0\u5982\u679c\u7528\u4e00\u4e2a\u6279\u6b21\u5904\u7406\u5b8c\u6240\u6709\u6570\u636e, \u4ee5\u8bad\u7ec3\u6570\u636e\u4e3a\u4f8b, \u6bcf\u4e2a\u53e5\u5b50\u957f\u5ea6\u9ad8\u8fbe104335, \u8fd9\u660e\u663e\u662f\u4e0d\u79d1\u5b66\u7684, \u56e0\u6b64\u6211\u4eec\u5728\u8fd9\u91cc\u8981\u9650\u5b9a\u6bcf\u4e2a\u6279\u6b21\u4e2d\u7684\u53e5\u5b50\u957f\u5ea6\u5141\u8bb8\u7684\u6700\u5927\u503cbptt. \u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e8c\u4e2a\u51fd\u6570get_batch\u4ee3\u7801\u5206\u6790: # \u4ee4\u5b50\u957f\u5ea6\u5141\u8bb8\u7684\u6700\u5927\u503cbptt\u4e3a35 bptt = 35 def get_batch ( source , i ): \"\"\"\u7528\u4e8e\u83b7\u5f97\u6bcf\u4e2a\u6279\u6b21\u5408\u7406\u5927\u5c0f\u7684\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e. \u53c2\u6570source\u662f\u901a\u8fc7batchify\u5f97\u5230\u7684train_data/val_data/test_data. i\u662f\u5177\u4f53\u7684\u6279\u6b21\u6b21\u6570. \"\"\" # \u9996\u5148\u6211\u4eec\u786e\u5b9a\u53e5\u5b50\u957f\u5ea6, \u5b83\u5c06\u662f\u5728bptt\u548clen(source) - 1 - i\u4e2d\u6700\u5c0f\u503c # \u5b9e\u8d28\u4e0a, \u524d\u9762\u7684\u6279\u6b21\u4e2d\u90fd\u4f1a\u662fbptt\u7684\u503c, \u53ea\u4e0d\u8fc7\u6700\u540e\u4e00\u4e2a\u6279\u6b21\u4e2d, \u53e5\u5b50\u957f\u5ea6 # \u53ef\u80fd\u4e0d\u591fbptt\u768435\u4e2a, \u56e0\u6b64\u4f1a\u53d8\u4e3alen(source) - 1 - i\u7684\u503c. seq_len = min ( bptt , len ( source ) - 1 - i ) # \u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u6e90\u6570\u636e\u7684\u7b2ci\u6279\u6570\u636e\u5c06\u662fbatchify\u7684\u7ed3\u679c\u7684\u5207\u7247[i:i+seq_len] data = source [ i : i + seq_len ] # \u6839\u636e\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u8bed\u6599\u89c4\u5b9a, \u5b83\u7684\u76ee\u6807\u6570\u636e\u662f\u6e90\u6570\u636e\u5411\u540e\u79fb\u52a8\u4e00\u4f4d # \u56e0\u4e3a\u6700\u540e\u76ee\u6807\u6570\u636e\u7684\u5207\u7247\u4f1a\u8d8a\u754c, \u56e0\u6b64\u4f7f\u7528view(-1)\u6765\u4fdd\u8bc1\u5f62\u72b6\u6b63\u5e38. target = source [ i + 1 : i + 1 + seq_len ] . view ( - 1 ) return data , target \u8f93\u5165\u5b9e\u4f8b: # \u4ee5\u6d4b\u8bd5\u96c6\u6570\u636e\u4e3a\u4f8b source = test_data i = 1 \u8f93\u51fa\u6548\u679c: data = tensor([[ 12, 1053, 355, 134, 37, 7, 4, 0, 835, 9834], [ 635, 8, 5, 5, 421, 4, 88, 8, 573, 2511], [ 0, 58, 8, 8, 6, 692, 544, 0, 212, 5], [ 12, 0, 105, 26, 3, 5, 6, 0, 4, 56], [ 3, 16074, 21254, 320, 3, 262, 16, 6, 1087, 89], [ 3, 751, 3866, 10, 12, 31, 246, 238, 79, 49], [ 635, 943, 78, 36, 12, 475, 66, 10, 4, 924], [ 0, 2358, 52, 4, 12, 4, 5, 0, 19831, 21], [ 26, 38, 54, 40, 1589, 3729, 1014, 5, 8, 4], [ 33, 17597, 33, 1661, 15, 7, 5, 0, 4, 170], [ 335, 268, 117, 0, 0, 4, 3144, 1557, 0, 160], [ 106, 4, 4706, 2245, 12, 1074, 13, 2105, 5, 29], [ 5, 16074, 10, 1087, 12, 137, 251, 13238, 8, 4], [ 394, 746, 4, 9, 12, 6032, 4, 2190, 303, 12651], [ 8, 616, 2107, 4, 3, 4, 425, 0, 10, 510], [ 1339, 112, 23, 335, 3, 22251, 1162, 9, 11, 9], [ 1212, 468, 6, 820, 9, 7, 1231, 4202, 2866, 382], [ 6, 24, 104, 6, 4, 4, 7, 10, 9, 588], [ 31, 190, 0, 0, 230, 267, 4, 273, 278, 6], [ 34, 25, 47, 26, 1864, 6, 694, 0, 2112, 3], [ 11, 6, 52, 798, 8, 69, 20, 31, 63, 9], [ 1800, 25, 2141, 2442, 117, 31, 196, 7290, 4, 298], [ 15, 171, 15, 17, 1712, 13, 217, 59, 736, 5], [ 4210, 191, 142, 14, 5251, 939, 59, 38, 10055, 25132], [ 302, 23, 11718, 11, 11, 599, 382, 317, 8, 13], [ 16, 1564, 9, 4808, 6, 0, 6, 6, 4, 4], [ 4, 7, 39, 7, 3934, 5, 9, 3, 8047, 557], [ 394, 0, 10715, 3580, 8682, 31, 242, 0, 10055, 170], [ 96, 6, 144, 3403, 4, 13, 1014, 14, 6, 2395], [ 4, 3, 13729, 14, 40, 0, 5, 18, 676, 3267], [ 1031, 3, 0, 628, 1589, 22, 10916, 10969, 5, 22548], [ 9, 12, 6, 84, 15, 49, 3144, 7, 102, 15], [ 916, 12, 4, 203, 0, 273, 303, 333, 4318, 0], [ 6, 12, 0, 4842, 5, 17, 4, 47, 4138, 2072], [ 38, 237, 5, 50, 35, 27, 18530, 244, 20, 6]]) target = tensor([ 635, 8, 5, 5, 421, 4, 88, 8, 573, 2511, 0, 58, 8, 8, 6, 692, 544, 0, 212, 5, 12, 0, 105, 26, 3, 5, 6, 0, 4, 56, 3, 16074, 21254, 320, 3, 262, 16, 6, 1087, 89, 3, 751, 3866, 10, 12, 31, 246, 238, 79, 49, 635, 943, 78, 36, 12, 475, 66, 10, 4, 924, 0, 2358, 52, 4, 12, 4, 5, 0, 19831, 21, 26, 38, 54, 40, 1589, 3729, 1014, 5, 8, 4, 33, 17597, 33, 1661, 15, 7, 5, 0, 4, 170, 335, 268, 117, 0, 0, 4, 3144, 1557, 0, 160, 106, 4, 4706, 2245, 12, 1074, 13, 2105, 5, 29, 5, 16074, 10, 1087, 12, 137, 251, 13238, 8, 4, 394, 746, 4, 9, 12, 6032, 4, 2190, 303, 12651, 8, 616, 2107, 4, 3, 4, 425, 0, 10, 510, 1339, 112, 23, 335, 3, 22251, 1162, 9, 11, 9, 1212, 468, 6, 820, 9, 7, 1231, 4202, 2866, 382, 6, 24, 104, 6, 4, 4, 7, 10, 9, 588, 31, 190, 0, 0, 230, 267, 4, 273, 278, 6, 34, 25, 47, 26, 1864, 6, 694, 0, 2112, 3, 11, 6, 52, 798, 8, 69, 20, 31, 63, 9, 1800, 25, 2141, 2442, 117, 31, 196, 7290, 4, 298, 15, 171, 15, 17, 1712, 13, 217, 59, 736, 5, 4210, 191, 142, 14, 5251, 939, 59, 38, 10055, 25132, 302, 23, 11718, 11, 11, 599, 382, 317, 8, 13, 16, 1564, 9, 4808, 6, 0, 6, 6, 4, 4, 4, 7, 39, 7, 3934, 5, 9, 3, 8047, 557, 394, 0, 10715, 3580, 8682, 31, 242, 0, 10055, 170, 96, 6, 144, 3403, 4, 13, 1014, 14, 6, 2395, 4, 3, 13729, 14, 40, 0, 5, 18, 676, 3267, 1031, 3, 0, 628, 1589, 22, 10916, 10969, 5, 22548, 9, 12, 6, 84, 15, 49, 3144, 7, 102, 15, 916, 12, 4, 203, 0, 273, 303, 333, 4318, 0, 6, 12, 0, 4842, 5, 17, 4, 47, 4138, 2072, 38, 237, 5, 50, 35, 27, 18530, 244, 20, 6, 13, 1083, 35, 1990, 653, 13, 10, 11, 1538, 56])","title":"\u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#_5","text":"\u8bbe\u7f6e\u6a21\u578b\u8d85\u53c2\u6570\u548c\u521d\u59cb\u5316\u6a21\u578b # \u901a\u8fc7TEXT.vocab.stoi\u65b9\u6cd5\u83b7\u5f97\u4e0d\u91cd\u590d\u8bcd\u6c47\u603b\u6570 ntokens = len ( TEXT . vocab . stoi ) # \u8bcd\u5d4c\u5165\u5927\u5c0f\u4e3a200 emsize = 200 # \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u8282\u70b9\u6570 nhid = 200 # \u7f16\u7801\u5668\u5c42\u7684\u6570\u91cf nlayers = 2 # \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u5934\u6570 nhead = 2 # \u7f6e0\u6bd4\u7387 dropout = 0.2 # \u5c06\u53c2\u6570\u8f93\u5165\u5230TransformerModel\u4e2d model = TransformerModel ( ntokens , emsize , nhead , nhid , nlayers , dropout ) . to ( device ) # \u6a21\u578b\u521d\u59cb\u5316\u540e, \u63a5\u4e0b\u6765\u8fdb\u884c\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u9009\u62e9. # \u5173\u4e8e\u635f\u5931\u51fd\u6570, \u6211\u4eec\u4f7f\u7528nn\u81ea\u5e26\u7684\u4ea4\u53c9\u71b5\u635f\u5931 criterion = nn . CrossEntropyLoss () # \u5b66\u4e60\u7387\u521d\u59cb\u503c\u5b9a\u4e3a5.0 lr = 5.0 # \u4f18\u5316\u5668\u9009\u62e9torch\u81ea\u5e26\u7684SGD\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5, \u5e76\u628alr\u4f20\u5165\u5176\u4e2d optimizer = torch . optim . SGD ( model . parameters (), lr = lr ) # \u5b9a\u4e49\u5b66\u4e60\u7387\u8c03\u6574\u65b9\u6cd5, \u4f7f\u7528torch\u81ea\u5e26\u7684lr_scheduler, \u5c06\u4f18\u5316\u5668\u4f20\u5165\u5176\u4e2d. scheduler = torch . optim . lr_scheduler . StepLR ( optimizer , 1.0 , gamma = 0.95 ) \u6a21\u578b\u8bad\u7ec3\u4ee3\u7801\u5206\u6790: # \u5bfc\u5165\u65f6\u95f4\u5de5\u5177\u5305 import time def train (): \"\"\"\u8bad\u7ec3\u51fd\u6570\"\"\" # \u6a21\u578b\u5f00\u542f\u8bad\u7ec3\u6a21\u5f0f model . train () # \u5b9a\u4e49\u521d\u59cb\u635f\u5931\u4e3a0 total_loss = 0. # \u83b7\u5f97\u5f53\u524d\u65f6\u95f4 start_time = time . time () # \u5f00\u59cb\u904d\u5386\u6279\u6b21\u6570\u636e for batch , i in enumerate ( range ( 0 , train_data . size ( 0 ) - 1 , bptt )): # \u901a\u8fc7get_batch\u83b7\u5f97\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e data , targets = get_batch ( train_data , i ) # \u8bbe\u7f6e\u4f18\u5316\u5668\u521d\u59cb\u68af\u5ea6\u4e3a0\u68af\u5ea6 optimizer . zero_grad () # \u5c06\u6570\u636e\u88c5\u5165model\u5f97\u5230\u8f93\u51fa output = model ( data ) # \u5c06\u8f93\u51fa\u548c\u76ee\u6807\u6570\u636e\u4f20\u5165\u635f\u5931\u51fd\u6570\u5bf9\u8c61 loss = criterion ( output . view ( - 1 , ntokens ), targets ) # \u635f\u5931\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u4ee5\u83b7\u5f97\u603b\u7684\u635f\u5931 loss . backward () # \u4f7f\u7528nn\u81ea\u5e26\u7684clip_grad_norm_\u65b9\u6cd5\u8fdb\u884c\u68af\u5ea6\u89c4\u8303\u5316, \u9632\u6b62\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8 torch . nn . utils . clip_grad_norm_ ( model . parameters (), 0.5 ) # \u6a21\u578b\u53c2\u6570\u8fdb\u884c\u66f4\u65b0 optimizer . step () # \u5c06\u6bcf\u5c42\u7684\u635f\u5931\u76f8\u52a0\u83b7\u5f97\u603b\u7684\u635f\u5931 total_loss += loss . item () # \u65e5\u5fd7\u6253\u5370\u95f4\u9694\u5b9a\u4e3a200 log_interval = 200 # \u5982\u679cbatch\u662f200\u7684\u500d\u6570\u4e14\u5927\u4e8e0\uff0c\u5219\u6253\u5370\u76f8\u5173\u65e5\u5fd7 if batch % log_interval == 0 and batch > 0 : # \u5e73\u5747\u635f\u5931\u4e3a\u603b\u635f\u5931\u9664\u4ee5log_interval cur_loss = total_loss / log_interval # \u9700\u8981\u7684\u65f6\u95f4\u4e3a\u5f53\u524d\u65f6\u95f4\u51cf\u53bb\u5f00\u59cb\u65f6\u95f4 elapsed = time . time () - start_time # \u6253\u5370\u8f6e\u6570, \u5f53\u524d\u6279\u6b21\u548c\u603b\u6279\u6b21, \u5f53\u524d\u5b66\u4e60\u7387, \u8bad\u7ec3\u901f\u5ea6(\u6bcf\u8c6a\u79d2\u5904\u7406\u591a\u5c11\u6279\u6b21), # \u5e73\u5747\u635f\u5931, \u4ee5\u53ca\u56f0\u60d1\u5ea6, \u56f0\u60d1\u5ea6\u662f\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6307\u6807, \u5b83\u7684\u8ba1\u7b97\u65b9\u6cd5\u5c31\u662f # \u5bf9\u4ea4\u53c9\u71b5\u5e73\u5747\u635f\u5931\u53d6\u81ea\u7136\u5bf9\u6570\u7684\u5e95\u6570. print ( '| epoch {:3d} | {:5d} / {:5d} batches | ' 'lr {:02.2f} | ms/batch {:5.2f} | ' 'loss {:5.2f} | ppl {:8.2f} ' . format ( epoch , batch , len ( train_data ) // bptt , scheduler . get_lr ()[ 0 ], elapsed * 1000 / log_interval , cur_loss , math . exp ( cur_loss ))) # \u6bcf\u4e2a\u6279\u6b21\u7ed3\u675f\u540e, \u603b\u635f\u5931\u5f520 total_loss = 0 # \u5f00\u59cb\u65f6\u95f4\u53d6\u5f53\u524d\u65f6\u95f4 start_time = time . time () \u6a21\u578b\u8bc4\u4f30\u4ee3\u7801\u5206\u6790: def evaluate ( eval_model , data_source ): \"\"\"\u8bc4\u4f30\u51fd\u6570, \u8bc4\u4f30\u9636\u6bb5\u5305\u62ec\u9a8c\u8bc1\u548c\u6d4b\u8bd5, \u5b83\u7684\u4e24\u4e2a\u53c2\u6570eval_model\u4e3a\u6bcf\u8f6e\u8bad\u7ec3\u4ea7\u751f\u7684\u6a21\u578b data_source\u4ee3\u8868\u9a8c\u8bc1\u6216\u6d4b\u8bd5\u6570\u636e\u96c6\"\"\" # \u6a21\u578b\u5f00\u542f\u8bc4\u4f30\u6a21\u5f0f eval_model . eval () # \u603b\u635f\u5931\u5f520 total_loss = 0 # \u56e0\u4e3a\u8bc4\u4f30\u6a21\u5f0f\u6a21\u578b\u53c2\u6570\u4e0d\u53d8, \u56e0\u6b64\u53cd\u5411\u4f20\u64ad\u4e0d\u9700\u8981\u6c42\u5bfc, \u4ee5\u52a0\u5feb\u8ba1\u7b97 with torch . no_grad (): # \u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u76f8\u540c, \u4f46\u662f\u56e0\u4e3a\u8fc7\u7a0b\u4e0d\u9700\u8981\u6253\u5370\u4fe1\u606f, \u56e0\u6b64\u4e0d\u9700\u8981batch\u6570 for i in range ( 0 , data_source . size ( 0 ) - 1 , bptt ): # \u9996\u5148\u8fd8\u662f\u901a\u8fc7\u901a\u8fc7get_batch\u83b7\u5f97\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e data , targets = get_batch ( data_source , i ) # \u901a\u8fc7eval_model\u83b7\u5f97\u8f93\u51fa output = eval_model ( data ) # \u5bf9\u8f93\u51fa\u5f62\u72b6\u6241\u5e73\u5316, \u53d8\u4e3a\u5168\u90e8\u8bcd\u6c47\u7684\u6982\u7387\u5206\u5e03 output_flat = output . view ( - 1 , ntokens ) # \u83b7\u5f97\u8bc4\u4f30\u8fc7\u7a0b\u7684\u603b\u635f\u5931 total_loss += criterion ( output_flat , targets ) . item () # \u8ba1\u7b97\u5e73\u5747\u635f\u5931 cur_loss = total_loss / (( data_source . size ( 0 ) - 1 ) / bptt ) # \u8fd4\u56de\u5e73\u5747\u635f\u5931 return cur_loss","title":"\u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#_6","text":"\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u4ee3\u7801\u5206\u6790: # \u9996\u5148\u521d\u59cb\u5316\u6700\u4f73\u9a8c\u8bc1\u635f\u5931\uff0c\u521d\u59cb\u503c\u4e3a\u65e0\u7a77\u5927 import copy best_val_loss = float ( \"inf\" ) # \u5b9a\u4e49\u8bad\u7ec3\u8f6e\u6570 epochs = 3 # \u5b9a\u4e49\u6700\u4f73\u6a21\u578b\u53d8\u91cf, \u521d\u59cb\u503c\u4e3aNone best_model = None # \u4f7f\u7528for\u5faa\u73af\u904d\u5386\u8f6e\u6570 for epoch in range ( 1 , epochs + 1 ): # \u9996\u5148\u83b7\u5f97\u8f6e\u6570\u5f00\u59cb\u65f6\u95f4 epoch_start_time = time . time () # \u8c03\u7528\u8bad\u7ec3\u51fd\u6570 train () # \u8be5\u8f6e\u8bad\u7ec3\u540e\u6211\u4eec\u7684\u6a21\u578b\u53c2\u6570\u5df2\u7ecf\u53d1\u751f\u4e86\u53d8\u5316 # \u5c06\u6a21\u578b\u548c\u8bc4\u4f30\u6570\u636e\u4f20\u5165\u5230\u8bc4\u4f30\u51fd\u6570\u4e2d val_loss = evaluate ( model , val_data ) # \u4e4b\u540e\u6253\u5370\u6bcf\u8f6e\u7684\u8bc4\u4f30\u65e5\u5fd7\uff0c\u5206\u522b\u6709\u8f6e\u6570\uff0c\u8017\u65f6\uff0c\u9a8c\u8bc1\u635f\u5931\u4ee5\u53ca\u9a8c\u8bc1\u56f0\u60d1\u5ea6 print ( '-' * 89 ) print ( '| end of epoch {:3d} | time: {:5.2f} s | valid loss {:5.2f} | ' 'valid ppl {:8.2f} ' . format ( epoch , ( time . time () - epoch_start_time ), val_loss , math . exp ( val_loss ))) print ( '-' * 89 ) # \u6211\u4eec\u5c06\u6bd4\u8f83\u54ea\u4e00\u8f6e\u635f\u5931\u6700\u5c0f\uff0c\u8d4b\u503c\u7ed9best_val_loss\uff0c # \u5e76\u53d6\u8be5\u635f\u5931\u4e0b\u7684\u6a21\u578b\u4e3abest_model if val_loss < best_val_loss : best_val_loss = val_loss # \u4f7f\u7528\u6df1\u62f7\u8d1d\uff0c\u62f7\u8d1d\u6700\u4f18\u6a21\u578b best_model = copy . deepcopy ( model ) # \u6bcf\u8f6e\u90fd\u4f1a\u5bf9\u4f18\u5316\u65b9\u6cd5\u7684\u5b66\u4e60\u7387\u505a\u8c03\u6574 scheduler . step () \u8f93\u51fa\u6548\u679c: | epoch 1 | 200/ 2981 batches | lr 5.00 | ms/batch 30.03 | loss 7.68 | ppl 2158.52 | epoch 1 | 400/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 5.26 | ppl 193.39 | epoch 1 | 600/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 4.07 | ppl 58.44 | epoch 1 | 800/ 2981 batches | lr 5.00 | ms/batch 28.88 | loss 3.41 | ppl 30.26 | epoch 1 | 1000/ 2981 batches | lr 5.00 | ms/batch 28.89 | loss 2.98 | ppl 19.72 | epoch 1 | 1200/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 2.79 | ppl 16.30 | epoch 1 | 1400/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.67 | ppl 14.38 | epoch 1 | 1600/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss 2.58 | ppl 13.19 | epoch 1 | 1800/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.43 | ppl 11.32 | epoch 1 | 2000/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss 2.39 | ppl 10.93 | epoch 1 | 2200/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.33 | ppl 10.24 | epoch 1 | 2400/ 2981 batches | lr 5.00 | ms/batch 28.91 | loss 2.36 | ppl 10.59 | epoch 1 | 2600/ 2981 batches | lr 5.00 | ms/batch 28.90 | loss 2.33 | ppl 10.31 | epoch 1 | 2800/ 2981 batches | lr 5.00 | ms/batch 28.92 | loss 2.26 | ppl 9.54 ----------------------------------------------------------------------------------------- | end of epoch 1 | time: 90.01s | valid loss 1.32 | valid ppl 3.73 ----------------------------------------------------------------------------------------- | epoch 2 | 200/ 2981 batches | lr 4.75 | ms/batch 29.08 | loss 2.18 | ppl 8.83 | epoch 2 | 400/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 2.11 | ppl 8.24 | epoch 2 | 600/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.98 | ppl 7.23 | epoch 2 | 800/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 2.00 | ppl 7.39 | epoch 2 | 1000/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss 1.94 | ppl 6.96 | epoch 2 | 1200/ 2981 batches | lr 4.75 | ms/batch 28.92 | loss 1.97 | ppl 7.15 | epoch 2 | 1400/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss 1.98 | ppl 7.28 | epoch 2 | 1600/ 2981 batches | lr 4.75 | ms/batch 28.92 | loss 1.97 | ppl 7.16 | epoch 2 | 1800/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.92 | ppl 6.84 | epoch 2 | 2000/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.96 | ppl 7.11 | epoch 2 | 2200/ 2981 batches | lr 4.75 | ms/batch 28.93 | loss 1.92 | ppl 6.80 | epoch 2 | 2400/ 2981 batches | lr 4.75 | ms/batch 28.94 | loss 1.94 | ppl 6.93 | epoch 2 | 2600/ 2981 batches | lr 4.75 | ms/batch 28.76 | loss 1.91 | ppl 6.76 | epoch 2 | 2800/ 2981 batches | lr 4.75 | ms/batch 28.75 | loss 1.89 | ppl 6.64 ----------------------------------------------------------------------------------------- | end of epoch 2 | time: 89.71s | valid loss 1.01 | valid ppl 2.74 ----------------------------------------------------------------------------------------- | epoch 3 | 200/ 2981 batches | lr 4.51 | ms/batch 28.88 | loss 1.78 | ppl 5.96 | epoch 3 | 400/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.89 | ppl 6.59 | epoch 3 | 600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.72 | ppl 5.58 | epoch 3 | 800/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.73 | ppl 5.63 | epoch 3 | 1000/ 2981 batches | lr 4.51 | ms/batch 28.73 | loss 1.65 | ppl 5.22 | epoch 3 | 1200/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.69 | ppl 5.40 | epoch 3 | 1400/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.73 | ppl 5.66 | epoch 3 | 1600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.75 | ppl 5.73 | epoch 3 | 1800/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.67 | ppl 5.33 | epoch 3 | 2000/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.69 | ppl 5.41 | epoch 3 | 2200/ 2981 batches | lr 4.51 | ms/batch 28.74 | loss 1.66 | ppl 5.26 | epoch 3 | 2400/ 2981 batches | lr 4.51 | ms/batch 28.76 | loss 1.69 | ppl 5.43 | epoch 3 | 2600/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.71 | ppl 5.55 | epoch 3 | 2800/ 2981 batches | lr 4.51 | ms/batch 28.75 | loss 1.72 | ppl 5.58 ----------------------------------------------------------------------------------------- | end of epoch 3 | time: 89.26s | valid loss 0.85 | valid ppl 2.33 ----------------------------------------------------------------------------------------- \u6a21\u578b\u6d4b\u8bd5\u4ee3\u7801\u5206\u6790: # \u6211\u4eec\u4ecd\u7136\u4f7f\u7528evaluate\u51fd\u6570\uff0c\u8fd9\u6b21\u5b83\u7684\u53c2\u6570\u662fbest_model\u4ee5\u53ca\u6d4b\u8bd5\u6570\u636e test_loss = evaluate ( best_model , test_data ) # \u6253\u5370\u6d4b\u8bd5\u65e5\u5fd7\uff0c\u5305\u62ec\u6d4b\u8bd5\u635f\u5931\u548c\u6d4b\u8bd5\u56f0\u60d1\u5ea6 print ( '=' * 89 ) print ( '| End of training | test loss {:5.2f} | test ppl {:8.2f} ' . format ( test_loss , math . exp ( test_loss ))) print ( '=' * 89 ) \u8f93\u51fa\u6548\u679c: ========================================================================================= | End of training | test loss 0.83 | test ppl 2.30 =========================================================================================","title":"\u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5)"},{"location":"04_mkdocs_transformer/Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html#_7","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u8bed\u8a00\u6a21\u578b: \u4ee5\u4e00\u4e2a\u7b26\u5408\u8bed\u8a00\u89c4\u5f8b\u7684\u5e8f\u5217\u4e3a\u8f93\u5165\uff0c\u6a21\u578b\u5c06\u5229\u7528\u5e8f\u5217\u95f4\u5173\u7cfb\u7b49\u7279\u5f81\uff0c\u8f93\u51fa\u4e00\u4e2a\u5728\u6240\u6709\u8bcd\u6c47\u4e0a\u7684\u6982\u7387\u5206\u5e03.\u8fd9\u6837\u7684\u6a21\u578b\u79f0\u4e3a\u8bed\u8a00\u6a21\u578b. \u5b66\u4e60\u4e86\u8bed\u8a00\u6a21\u578b\u80fd\u89e3\u51b3\u54ea\u4e9b\u95ee\u9898: 1, \u6839\u636e\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u5728\u5b83\u7684\u57fa\u7840\u4e0a\u5b8c\u6210\u673a\u5668\u7ffb\u8bd1\uff0c\u6587\u672c\u751f\u6210\u7b49\u4efb\u52a1\uff0c\u56e0\u4e3a\u6211\u4eec\u901a\u8fc7\u6700\u540e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u6c47\u662f\u4ec0\u4e48. 2, \u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5224\u65ad\u8f93\u5165\u7684\u5e8f\u5217\u662f\u5426\u4e3a\u4e00\u53e5\u5b8c\u6574\u7684\u8bdd\uff0c\u56e0\u4e3a\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\u67e5\u770b\u6700\u5927\u6982\u7387\u662f\u5426\u843d\u5728\u53e5\u5b50\u7ed3\u675f\u7b26\u4e0a\uff0c\u6765\u5224\u65ad\u5b8c\u6574\u6027. 3, \u8bed\u8a00\u6a21\u578b\u672c\u8eab\u7684\u8bad\u7ec3\u76ee\u6807\u662f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\uff0c\u56e0\u4e3a\u5b83\u7684\u7279\u5f81\u63d0\u53d6\u90e8\u5206\u4f1a\u62bd\u8c61\u5f88\u591a\u8bed\u8a00\u5e8f\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5173\u7cfb\u53ef\u80fd\u540c\u6837\u5bf9\u5176\u4ed6\u8bed\u8a00\u7c7b\u4efb\u52a1\u6709\u6548\u679c.\u56e0\u6b64\u53ef\u4ee5\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60. \u5b66\u4e60\u5e76\u5b9e\u73b0\u4e86\u6574\u4e2a\u6848\u4f8b\u7684\u4e94\u4e2a\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 \u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406 \u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570 \u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5) \u7b2c\u4e00\u6b65: \u5bfc\u5165\u5fc5\u5907\u7684\u5de5\u5177\u5305 torchtext\u4ecb\u7ecd: \u5b83\u662ftorch\u5de5\u5177\u4e2d\u5904\u7406NLP\u95ee\u9898\u7684\u5e38\u7528\u6570\u636e\u5904\u7406\u5305. \u5bf9\u6587\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406, \u6bd4\u5982\u6587\u672c\u8bed\u6599\u52a0\u8f7d, \u6587\u672c\u8fed\u4ee3\u5668\u6784\u5efa\u7b49. \u5305\u542b\u5f88\u591a\u7ecf\u5178\u6587\u672c\u8bed\u6599\u7684\u9884\u52a0\u8f7d\u65b9\u6cd5. \u5176\u4e2d\u5305\u62ec\u7684\u8bed\u6599\u6709\uff1a\u7528\u4e8e\u60c5\u611f\u5206\u6790\u7684SST\u548cIMDB, \u7528\u4e8e\u95ee\u9898\u5206\u7c7b\u7684TREC, \u7528\u4e8e\u53ca\u5176\u7ffb\u8bd1\u7684 WMT14\uff0c IWSLT\uff0c\u4ee5\u53ca\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u4efb\u52a1wikiText-2, WikiText103, PennTreebank. wikiText-2\u6570\u636e\u96c6\u7684\u4f53\u91cf\u4e2d\u7b49, \u8bad\u7ec3\u96c6\u5171\u6709600\u7bc7\u77ed\u6587, \u5171208\u4e07\u5de6\u53f3\u7684\u8bcd\u6c47, 33278\u4e2a\u4e0d\u91cd\u590d\u8bcd\u6c47, OvV\uff08\u6709\u591a\u5c11\u6b63\u5e38\u82f1\u6587\u8bcd\u6c47\u4e0d\u5728\u8be5\u6570\u636e\u96c6\u4e2d\u7684\u5360\u6bd4\uff09\u4e3a2.6%\uff0c\u6570\u636e\u96c6\u4e2d\u7684\u77ed\u6587\u90fd\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u5bf9\u4e00\u4e9b\u6982\u5ff5\u7684\u4ecb\u7ecd\u548c\u63cf\u8ff0. \u7b2c\u4e8c\u6b65: \u5bfc\u5165wikiText-2\u6570\u636e\u96c6\u5e76\u4f5c\u57fa\u672c\u5904\u7406 \u901a\u8fc7torchtext\u4e2d\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86train_txt, val_txt, test_txt. \u7b2c\u4e09\u6b65: \u6784\u5efa\u7528\u4e8e\u6a21\u578b\u8f93\u5165\u7684\u6279\u6b21\u5316\u6570\u636e \u5b9e\u73b0\u4e86\u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e00\u4e2a\u51fd\u6570batchify, \u7528\u4e8e\u5c06\u6587\u672c\u6570\u636e\u6620\u5c04\u6210\u8fde\u7eed\u6570\u5b57, \u5e76\u8f6c\u6362\u6210\u6307\u5b9a\u7684\u6837\u5f0f. \u5b9e\u73b0\u4e86\u6279\u6b21\u5316\u8fc7\u7a0b\u7684\u7b2c\u4e8c\u4e2a\u51fd\u6570get_batch, \u7528\u4e8e\u83b7\u5f97\u6bcf\u4e2a\u6279\u6b21\u5408\u7406\u5927\u5c0f\u7684\u6e90\u6570\u636e\u548c\u76ee\u6807\u6570\u636e. \u7b2c\u56db\u6b65: \u6784\u5efa\u8bad\u7ec3\u548c\u8bc4\u4f30\u51fd\u6570 \u6784\u5efa\u4e86\u7528\u4e8e\u8bad\u7ec3\u7684\u51fd\u6570train() \u6784\u5efa\u4e86\u7528\u4e8e\u8bc4\u4f30\u7684\u51fd\u6570evaluate() \u7b2c\u4e94\u6b65: \u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30(\u5305\u62ec\u9a8c\u8bc1\u4ee5\u53ca\u6d4b\u8bd5) \u9996\u5148\u5b9e\u73b0\u4e86\u6a21\u578b\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u8fc7\u7a0b, \u5e76\u6253\u5370\u4e86\u7ed3\u679c. \u6700\u540e\u5b9e\u73b0\u4e86\u6a21\u578b\u7684\u6d4b\u8bd5\u8fc7\u7a0b, \u5f97\u5230\u4e86\u4e0d\u9519\u7684\u56f0\u60d1\u5ea6\u6307\u6807.","title":"\u5c0f\u8282\u603b\u7ed3"},{"location":"05_mkdocs_translearning/index.html","text":"","title":"Index"},{"location":"05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3fasttext\u5de5\u5177\u7684\u4f5c\u7528. \u4e86\u89e3fasttext\u5de5\u5177\u7684\u4f18\u52bf\u53ca\u5176\u539f\u56e0. \u638c\u63e1fasttext\u7684\u5b89\u88c5\u65b9\u6cd5. 1 fasttext\u4ecb\u7ecd \u00b6 1.1 fasttext\u4f5c\u7528 \u00b6 \u4f5c\u4e3aNLP\u5de5\u7a0b\u9886\u57df\u5e38\u7528\u7684\u5de5\u5177\u5305, fasttext\u6709\u4e24\u5927\u4f5c\u7528: \u8fdb\u884c\u6587\u672c\u5206\u7c7b \u8bad\u7ec3\u8bcd\u5411\u91cf 1.2 fasttext\u5de5\u5177\u5305\u7684\u4f18\u52bf \u00b6 \u6b63\u5982\u5b83\u7684\u540d\u5b57, \u5728\u4fdd\u6301\u8f83\u9ad8\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b, \u5feb\u901f\u7684\u8fdb\u884c\u8bad\u7ec3\u548c\u9884\u6d4b\u662ffasttext\u7684\u6700\u5927\u4f18\u52bf. fasttext\u4f18\u52bf\u7684\u539f\u56e0: fasttext\u5de5\u5177\u5305\u4e2d\u5185\u542b\u7684fasttext\u6a21\u578b\u5177\u6709\u5341\u5206\u7b80\u5355\u7684\u7f51\u7edc\u7ed3\u6784. \u4f7f\u7528fasttext\u6a21\u578b\u8bad\u7ec3\u8bcd\u5411\u91cf\u65f6\u4f7f\u7528\u5c42\u6b21softmax\u7ed3\u6784, \u6765\u63d0\u5347\u8d85\u591a\u7c7b\u522b\u4e0b\u7684\u6a21\u578b\u6027\u80fd. \u7531\u4e8efasttext\u6a21\u578b\u8fc7\u4e8e\u7b80\u5355\u65e0\u6cd5\u6355\u6349\u8bcd\u5e8f\u7279\u5f81, \u56e0\u6b64\u4f1a\u8fdb\u884cn-gram\u7279\u5f81\u63d0\u53d6\u4ee5\u5f25\u8865\u6a21\u578b\u7f3a\u9677\u63d0\u5347\u7cbe\u5ea6. 1.3 fasttext\u7684\u5b89\u88c5 \u00b6 pip install fasttext 1.4 \u9a8c\u8bc1\u5b89\u88c5 \u00b6 Python 3.8.12 ( default ) [ GCC 7.5.0 ] :: Anaconda , Inc . on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import fasttext >>>","title":"1 fasttext\u5de5\u5177\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3fasttext\u5de5\u5177\u7684\u4f5c\u7528. \u4e86\u89e3fasttext\u5de5\u5177\u7684\u4f18\u52bf\u53ca\u5176\u539f\u56e0. \u638c\u63e1fasttext\u7684\u5b89\u88c5\u65b9\u6cd5.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html#1-fasttext","text":"","title":"1 fasttext\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html#11-fasttext","text":"\u4f5c\u4e3aNLP\u5de5\u7a0b\u9886\u57df\u5e38\u7528\u7684\u5de5\u5177\u5305, fasttext\u6709\u4e24\u5927\u4f5c\u7528: \u8fdb\u884c\u6587\u672c\u5206\u7c7b \u8bad\u7ec3\u8bcd\u5411\u91cf","title":"1.1 fasttext\u4f5c\u7528"},{"location":"05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html#12-fasttext","text":"\u6b63\u5982\u5b83\u7684\u540d\u5b57, \u5728\u4fdd\u6301\u8f83\u9ad8\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b, \u5feb\u901f\u7684\u8fdb\u884c\u8bad\u7ec3\u548c\u9884\u6d4b\u662ffasttext\u7684\u6700\u5927\u4f18\u52bf. fasttext\u4f18\u52bf\u7684\u539f\u56e0: fasttext\u5de5\u5177\u5305\u4e2d\u5185\u542b\u7684fasttext\u6a21\u578b\u5177\u6709\u5341\u5206\u7b80\u5355\u7684\u7f51\u7edc\u7ed3\u6784. \u4f7f\u7528fasttext\u6a21\u578b\u8bad\u7ec3\u8bcd\u5411\u91cf\u65f6\u4f7f\u7528\u5c42\u6b21softmax\u7ed3\u6784, \u6765\u63d0\u5347\u8d85\u591a\u7c7b\u522b\u4e0b\u7684\u6a21\u578b\u6027\u80fd. \u7531\u4e8efasttext\u6a21\u578b\u8fc7\u4e8e\u7b80\u5355\u65e0\u6cd5\u6355\u6349\u8bcd\u5e8f\u7279\u5f81, \u56e0\u6b64\u4f1a\u8fdb\u884cn-gram\u7279\u5f81\u63d0\u53d6\u4ee5\u5f25\u8865\u6a21\u578b\u7f3a\u9677\u63d0\u5347\u7cbe\u5ea6.","title":"1.2 fasttext\u5de5\u5177\u5305\u7684\u4f18\u52bf"},{"location":"05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html#13-fasttext","text":"pip install fasttext","title":"1.3 fasttext\u7684\u5b89\u88c5"},{"location":"05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html#14","text":"Python 3.8.12 ( default ) [ GCC 7.5.0 ] :: Anaconda , Inc . on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import fasttext >>>","title":"1.4 \u9a8c\u8bc1\u5b89\u88c5"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3NLP\u4e2dGLUE\u6807\u51c6\u6570\u636e\u96c6\u5408\u7684\u76f8\u5173\u77e5\u8bc6. \u638c\u63e1GLUE\u6807\u51c6\u6570\u636e\u96c6\u5408\u7684\u4e0b\u8f7d\u65b9\u5f0f, \u6570\u636e\u6837\u5f0f\u53ca\u5176\u5bf9\u5e94\u7684\u4efb\u52a1\u7c7b\u578b. 1 GLUE\u6570\u636e\u96c6\u5408\u4ecb\u7ecd \u00b6 1.1 \u6570\u636e\u96c6\u5408\u4ecb\u7ecd \u00b6 GLUE\u7531\u7ebd\u7ea6\u5927\u5b66, \u534e\u76db\u987f\u5927\u5b66, Google\u8054\u5408\u63a8\u51fa, \u6db5\u76d6\u4e0d\u540cNLP\u4efb\u52a1\u7c7b\u578b, \u622a\u6b62\u81f32020\u5e741\u6708\u5176\u4e2d\u5305\u62ec11\u4e2a\u5b50\u4efb\u52a1\u6570\u636e\u96c6, \u6210\u4e3a\u8861\u91cfNLP\u7814\u7a76\u53d1\u5c55\u7684\u8861\u91cf\u6807\u51c6. CoLA \u6570\u636e\u96c6 SST-2 \u6570\u636e\u96c6 MRPC \u6570\u636e\u96c6 STS-B \u6570\u636e\u96c6 QQP \u6570\u636e\u96c6 MNLI \u6570\u636e\u96c6 SNLI \u6570\u636e\u96c6 QNLI \u6570\u636e\u96c6 RTE \u6570\u636e\u96c6 WNLI \u6570\u636e\u96c6 diagnostics\u6570\u636e\u96c6(\u5b98\u65b9\u672a\u5b8c\u5584) 1.2 \u6570\u636e\u96c6\u5408\u8def\u5f84 \u00b6 \u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/glue_data\u4e0b \u53e6\u5916\u8fd9GLUE\u768411\u4e2a\u6570\u636e\u96c6\u90fd\u653e\u5230\u7684\u767e\u5ea6\u4e91 , \u9700\u8981\u7684\u53ef\u4ee5\u81ea\u53d6: GLUE\u6570\u636e\u96c6 \u63d0\u53d6\u7801: b6se 2 GLUE\u5b50\u6570\u636e\u96c6\u7684\u6837\u5f0f\u53ca\u5176\u4efb\u52a1\u7c7b\u578b \u00b6 2.1 CoLA\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f \u00b6 \u6570\u636e\u96c6\u91ca\u4e49:CoLA(The Corpus of Linguistic Acceptability\uff0c\u8bed\u8a00\u53ef\u63a5\u53d7\u6027\u8bed\u6599\u5e93)\u7ebd\u7ea6\u5927\u5b66\u53d1\u5e03\u7684\u6709\u5173\u8bed\u6cd5\u7684\u6570\u636e\u96c6 \u672c\u8d28: \u662f\u5bf9\u4e00\u4e2a\u7ed9\u5b9a\u53e5\u5b50\uff0c\u5224\u5b9a\u5176\u662f\u5426\u8bed\u6cd5\u6b63\u786e\u7684\u5355\u4e2a\u53e5\u5b50\u7684**\u6587\u672c\u4e8c\u5206\u7c7b\u4efb\u52a1**. - CoLA/ - dev.tsv - original/ - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: ... gj04 1 She coughed herself awake as the leaf landed on her nose. gj04 1 The worm wriggled onto the carpet. gj04 1 The chocolate melted onto the carpet. gj04 0 * The ball wriggled itself loose. gj04 1 Bill wriggled himself loose. bc01 1 The sinking of the ship to collect the insurance was very devious. bc01 1 The ship's sinking was very devious. bc01 0 * The ship's sinking to collect the insurance was very devious. bc01 1 The testing of such drugs on oneself is too risky. bc01 0 * This drug's testing on oneself is too risky. ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a4\u5217, \u7b2c\u4e00\u5217\u6570\u636e, \u5982gj04, bc01\u7b49\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u6765\u6e90\u5373\u51fa\u7248\u7269\u4ee3\u53f7; \u7b2c\u4e8c\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u8bed\u6cd5\u662f\u5426\u6b63\u786e, 0\u4ee3\u8868\u4e0d\u6b63\u786e, 1\u4ee3\u8868\u6b63\u786e; \u7b2c\u4e09\u5217\u6570\u636e, ' ', \u662f\u4f5c\u8005\u6700\u521d\u7684\u6b63\u8d1f\u6837\u672c\u6807\u8bb0, \u4e0e\u7b2c\u4e8c\u5217\u610f\u4e49\u76f8\u540c, ' '\u8868\u793a\u4e0d\u6b63\u786e; \u7b2c\u56db\u5217\u5373\u662f\u88ab\u6807\u6ce8\u7684\u8bed\u6cd5\u4f7f\u7528\u662f\u5426\u6b63\u786e\u7684\u6587\u672c\u53e5\u5b50. test.tsv\u6570\u636e\u6837\u5f0f: index sentence 0 Bill whistled past the house. 1 The car honked its way down the road. 2 Bill pushed Harry off the sofa. 3 the kittens yawned awake and played. 4 I demand that the more John eats, the more he pay. 5 If John eats more, keep your mouth shut tighter, OK? 6 His expectations are always lower than mine are. 7 The sooner you call, the more carefully I will word the letter. 8 The more timid he feels, the more people he interviews without asking questions of. 9 Once Janet left, Fred became a lot crazier. ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u6570\u636e\u4ee3\u8868\u7528\u4e8e\u6d4b\u8bd5\u7684\u53e5\u5b50. CoLA\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: MCC(\u9a6c\u4fee\u65af\u76f8\u5173\u7cfb\u6570, \u5728\u6b63\u8d1f\u6837\u672c\u5206\u5e03\u5341\u5206\u4e0d\u5747\u8861\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u7684\u4e8c\u5206\u7c7b\u8bc4\u4f30\u6307\u6807) 2.2 SST-2\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f \u00b6 \u6570\u636e\u96c6\u91ca\u4e49:SST-2(The Stanford Sentiment Treebank\uff0c\u65af\u5766\u798f\u60c5\u611f\u6811\u5e93),\u5355\u53e5\u5b50\u5206\u7c7b\u4efb\u52a1\uff0c\u5305\u542b\u7535\u5f71\u8bc4\u8bba\u4e2d\u7684\u53e5\u5b50\u548c\u5b83\u4eec\u60c5\u611f\u7684\u4eba\u7c7b\u6ce8\u91ca. \u672c\u8d28:\u53e5\u5b50\u7ea7\u522b\u7684**\u4e8c\u5206\u7c7b\u4efb\u52a1** - SST-2/ - dev.tsv - original/ - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: sentence label hide new secretions from the parental units 0 contains no wit , only labored gags 0 that loves its characters and communicates something rather beautiful about human nature 1 remains utterly satisfied to remain the same throughout 0 on the worst revenge-of-the-nerds clich\u00e9s the filmmakers could dredge up 0 that 's far too tragic to merit such superficial treatment 0 demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . 1 of saucy 1 a depressed fifteen-year-old 's suicidal poetry 0 ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u5177\u6709\u611f\u60c5\u8272\u5f69\u7684\u8bc4\u8bba\u6587\u672c; \u7b2c\u4e8c\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u662f\u79ef\u6781\u6216\u8005\u6d88\u6781\u7684\u8bc4\u8bba, 0\u4ee3\u8868\u6d88\u6781, 1\u4ee3\u8868\u79ef\u6781. test.tsv\u6570\u636e\u6837\u5f0f: index sentence 0 uneasy mishmash of styles and genres . 1 this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 2 by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 3 director rob marshall went out gunning to make a great one . 4 lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 5 a well-made and often lovely depiction of the mysteries of friendship . 6 none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 7 although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 8 it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 9 this is junk food cinema at its greasiest . ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u6570\u636e\u4ee3\u8868\u7528\u4e8e\u6d4b\u8bd5\u7684\u53e5\u5b50. SST-2\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC 2.3 MRPC\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f \u00b6 \u6570\u636e\u96c6\u91ca\u4e49:MRPC(The Microsoft Research Paraphrase Corpus\uff0c\u5fae\u8f6f\u7814\u7a76\u9662\u91ca\u4e49\u8bed\u6599\u5e93),\u76f8\u4f3c\u6027\u548c\u91ca\u4e49\u4efb\u52a1\uff0c\u662f\u4ece\u5728\u7ebf\u65b0\u95fb\u6e90\u4e2d\u81ea\u52a8\u62bd\u53d6\u53e5\u5b50\u5bf9\u8bed\u6599\u5e93\uff0c\u5e76\u4eba\u5de5\u6ce8\u91ca\u53e5\u5b50\u5bf9\u4e2d\u7684\u53e5\u5b50\u662f\u5426\u5728\u8bed\u4e49\u4e0a\u7b49\u6548\u3002 \u672c\u8d28:\u53e5\u5b50\u7ea7\u522b\u7684**\u4e8c\u5206\u7c7b\u4efb\u52a1** - MRPC/ - dev.tsv - test.tsv - train.tsv - dev_ids.tsv - msr_paraphrase_test.txt - msr_paraphrase_train.txt \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: Quality #1 ID #2 ID #1 String #2 String 1 702876 702977 Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence . Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence . 0 2108705 2108831 Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion . Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 . 1 1330381 1330521 They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale . 0 3344667 3344648 Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 . Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 . 1 1236820 1236712 The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange . PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday . 1 738533 737951 Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier . With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier . 0 264589 264502 The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday . The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 . 1 579975 579810 The DVD-CCA then appealed to the state Supreme Court . The DVD CCA appealed that decision to the U.S. Supreme Court . ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a5\u5217, \u7b2c\u4e00\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u5bf9\u53e5\u5b50\u662f\u5426\u5177\u6709\u76f8\u540c\u7684\u542b\u4e49, 0\u4ee3\u8868\u542b\u4e49\u4e0d\u76f8\u540c, 1\u4ee3\u8868\u542b\u4e49\u76f8\u540c. \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u5206\u522b\u4ee3\u8868\u6bcf\u5bf9\u53e5\u5b50\u7684id, \u7b2c\u56db\u5217\u548c\u7b2c\u4e94\u5217\u5206\u522b\u5177\u6709\u76f8\u540c/\u4e0d\u540c\u542b\u4e49\u7684\u53e5\u5b50\u5bf9. test.tsv\u6570\u636e\u6837\u5f0f: index #1 ID #2 ID #1 String #2 String 0 1089874 1089925 PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So . 1 3019446 3019327 The world 's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected . Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash . 2 1945605 1945824 According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 . The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 . 3 1430402 1430329 A tropical storm rapidly developed in the Gulf of Mexico Sunday and was expected to hit somewhere along the Texas or Louisiana coasts by Monday night . A tropical storm rapidly developed in the Gulf of Mexico on Sunday and could have hurricane-force winds when it hits land somewhere along the Louisiana coast Monday night . 4 3354381 3354396 The company didn 't detail the costs of the replacement and repairs . But company officials expect the costs of the replacement work to run into the millions of dollars . 5 1390995 1391183 The settling companies would also assign their possible claims against the underwriters to the investor plaintiffs , he added . Under the agreement , the settling companies will also assign their potential claims against the underwriters to the investors , he added . 6 2201401 2201285 Air Commodore Quaife said the Hornets remained on three-minute alert throughout the operation . Air Commodore John Quaife said the security operation was unprecedented . 7 2453843 2453998 A Washington County man may have the countys first human case of West Nile virus , the health department said Friday . The countys first and only human case of West Nile this year was confirmed by health officials on Sept . 8 . ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a5\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u5176\u4f59\u5217\u7684\u542b\u4e49\u4e0etrain.tsv\u4e2d\u76f8\u540c. MRPC\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC\u548cF1 2.4 STS-B\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f \u00b6 \u6570\u636e\u96c6\u91ca\u4e49: STSB(The Semantic Textual Similarity Benchmark\uff0c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u57fa\u51c6\u6d4b\u8bd5) \u672c\u8d28: \u56de\u5f52\u4efb\u52a1/\u53e5\u5b50\u5bf9\u7684\u6587\u672c\u4e94\u5206\u7c7b\u4efb\u52a1 - STS-B/ - dev.tsv - test.tsv - train.tsv - LICENSE.txt - readme.txt - original/ \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: index genre filename year old_index source1 source2 sentence1 sentence2 score 0 main-captions MSRvid 2012test 0001 none none A plane is taking off. An air plane is taking off. 5.000 1 main-captions MSRvid 2012test 0004 none none A man is playing a large flute. A man is playing a flute. 3.800 2 main-captions MSRvid 2012test 0005 none none A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza. 3.800 3 main-captions MSRvid 2012test 0006 none none Three men are playing chess.Two men are playing chess. 2.600 4 main-captions MSRvid 2012test 0009 none none A man is playing the cello.A man seated is playing the cello. 4.250 5 main-captions MSRvid 2012test 0011 none none Some men are fighting. Two men are fighting. 4.250 6 main-captions MSRvid 2012test 0012 none none A man is smoking. A man is skating. 0.500 7 main-captions MSRvid 2012test 0013 none none The man is playing the piano. The man is playing the guitar. 1.600 8 main-captions MSRvid 2012test 0014 none none A man is playing on a guitar and singing. A woman is playing an acoustic guitar and singing. 2.200 9 main-captions MSRvid 2012test 0016 none none A person is throwing a cat on to the ceiling. A person throws a cat on the ceiling. 5.000 ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a10\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u662f\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u4ee3\u8868\u6bcf\u5bf9\u53e5\u5b50\u7684\u6765\u6e90, \u5982main-captions\u8868\u793a\u6765\u81ea\u5b57\u5e55; \u7b2c\u4e09\u5217\u4ee3\u8868\u6765\u6e90\u7684\u5177\u4f53\u4fdd\u5b58\u6587\u4ef6\u540d, \u7b2c\u56db\u5217\u4ee3\u8868\u51fa\u73b0\u65f6\u95f4(\u5e74); \u7b2c\u4e94\u5217\u4ee3\u8868\u539f\u59cb\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u516d\u5217\u548c\u7b2c\u4e03\u5217\u5206\u522b\u4ee3\u8868\u53e5\u5b50\u5bf9\u539f\u59cb\u6765\u6e90; \u7b2c\u516b\u5217\u548c\u7b2c\u4e5d\u5217\u4ee3\u8868\u76f8\u4f3c\u7a0b\u5ea6\u4e0d\u540c\u7684\u53e5\u5b50\u5bf9; \u7b2c\u5341\u5217\u4ee3\u8868\u53e5\u5b50\u5bf9\u7684\u76f8\u4f3c\u7a0b\u5ea6\u7531\u4f4e\u5230\u9ad8, \u503c\u57df\u8303\u56f4\u662f[0, 5]. test.tsv\u6570\u636e\u6837\u5f0f: index genre filename year old_index source1 source2 sentence1 sentence2 0 main-captions MSRvid 2012test 0024 none none A girl is styling her hair. A girl is brushing her hair. 1 main-captions MSRvid 2012test 0033 none none A group of men play soccer on the beach. A group of boys are playing soccer on the beach. 2 main-captions MSRvid 2012test 0045 none none One woman is measuring another woman's ankle. A woman measures another woman's ankle. 3 main-captions MSRvid 2012test 0063 none none A man is cutting up a cucumber. A man is slicing a cucumber. 4 main-captions MSRvid 2012test 0066 none none A man is playing a harp. A man is playing a keyboard. 5 main-captions MSRvid 2012test 0074 none none A woman is cutting onions. A woman is cutting tofu. 6 main-captions MSRvid 2012test 0076 none none A man is riding an electric bicycle. A man is riding a bicycle. 7 main-captions MSRvid 2012test 0082 none none A man is playing the drums. A man is playing the guitar. 8 main-captions MSRvid 2012test 0092 none none A man is playing guitar. A lady is playing the guitar. 9 main-captions MSRvid 2012test 0095 none none A man is playing a guitar. A man is playing a trumpet. 10 main-captions MSRvid 2012test 0096 none none A man is playing a guitar. A man is playing a trumpet. ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a9\u5217, \u542b\u4e49\u4e0etrain.tsv\u524d9\u5217\u76f8\u540c. STS-B\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u591a\u5206\u7c7b\u4efb\u52a1/\u53e5\u5b50\u5bf9\u56de\u5f52\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: Pearson-Spearman Corr 2.5 QQP\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f \u00b6 \u6570\u636e\u96c6\u91ca\u4e49: QQP(The Quora Question Pairs, Quora\u95ee\u9898\u5bf9\u6570\u96c6),\u76f8\u4f3c\u6027\u548c\u91ca\u4e49\u4efb\u52a1\uff0c\u662f\u793e\u533a\u95ee\u7b54\u7f51\u7ad9Quora\u4e2d\u95ee\u9898\u5bf9\u7684\u96c6\u5408\u3002 \u672c\u8d28: \u53e5\u5b50\u5bf9\u7684**\u4e8c\u5206\u7c7b\u4efb\u52a1** - QQP/ - dev.tsv - original/ - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: id qid1 qid2 question1 question2 is_duplicate 133273 213221 213222 How is the life of a math student? Could you describe your own experiences?Which level of prepration is enough for the exam jlpt5? 0 402555 536040 536041 How do I control my horny emotions? How do you control your horniness? 1 360472 364011 490273 What causes stool color to change to yellow? What can cause stool to come out as little balls? 0 150662 155721 7256 What can one do after MBBS? What do i do after my MBBS ? 1 183004 279958 279959 Where can I find a power outlet for my laptop at Melbourne Airport? Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney? 0 119056 193387 193388 How not to feel guilty since I am Muslim and I'm conscious we won't have sex together? I don't beleive I am bulimic, but I force throw up atleast once a day after I eat something and feel guilty. Should I tell somebody, and if so who? 0 356863 422862 96457 How is air traffic controlled? How do you become an air traffic controller?0 106969 147570 787 What is the best self help book you have read? Why? How did it change your life? What are the top self help books I should read? 1 ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a6\u5217, \u7b2c\u4e00\u5217\u4ee3\u8868\u6587\u672c\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u5206\u522b\u4ee3\u8868\u95ee\u98981\u548c\u95ee\u98982\u7684id; \u7b2c\u56db\u5217\u548c\u7b2c\u4e94\u5217\u4ee3\u8868\u9700\u8981\u8fdb\u884c'\u662f\u5426\u91cd\u590d'\u5224\u5b9a\u7684\u53e5\u5b50\u5bf9; \u7b2c\u516d\u5217\u4ee3\u8868\u4e0a\u8ff0\u95ee\u9898\u662f/\u4e0d\u662f\u91cd\u590d\u6027\u95ee\u9898\u7684\u6807\u7b7e, 0\u4ee3\u8868\u4e0d\u91cd\u590d, 1\u4ee3\u8868\u91cd\u590d. test.tsv\u6570\u636e\u6837\u5f0f: id question1 question2 0 Would the idea of Trump and Putin in bed together scare you, given the geopolitical implications? Do you think that if Donald Trump were elected President, he would be able to restore relations with Putin and Russia as he said he could, based on the rocky relationship Putin had with Obama and Bush? 1 What are the top ten Consumer-to-Consumer E-commerce online? What are the top ten Consumer-to-Business E-commerce online? 2 Why don't people simply 'Google' instead of asking questions on Quora? Why do people ask Quora questions instead of just searching google? 3 Is it safe to invest in social trade biz? Is social trade geniune? 4 If the universe is expanding then does matter also expand? If universe and space is expanding? Does that mean anything that occupies space is also expanding? 5 What is the plural of hypothesis? What is the plural of thesis? 6 What is the application form you need for launching a company? What is the application form you need for launching a company in Austria? 7 What is Big Theta? When should I use Big Theta as opposed to big O? Is O(Log n) close to O(n) or O(1)? 8 What are the health implications of accidentally eating a small quantity of aluminium foil?What are the implications of not eating vegetables? ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a3\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u4ee3\u8868\u7528\u4e8e\u6d4b\u8bd5\u7684\u95ee\u9898\u53e5\u5b50\u5bf9. QQP\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC/F1 2.6 (MNLI/SNLI)\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f \u00b6 \u6570\u636e\u96c6\u91ca\u4e49: MNLI(The Multi-Genre Natural Language Inference Corpus, \u591a\u7c7b\u578b\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6570\u636e\u5e93) \u672c\u8d28: \u53e5\u5b50\u5bf9\u7684**\u4e09\u5206\u7c7b\u4efb\u52a1** - (MNLI/SNLI)/ - dev_matched.tsv - dev_mismatched.tsv - original/ - test_matched.tsv - test_mismatched.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev_matched.tsv, dev_mismatched.tsv, test_matched.tsv, test_mismatched.tsv\u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e00\u540c\u91c7\u96c6\u7684\u9a8c\u8bc1\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e0d\u662f\u4e00\u540c\u91c7\u96c6\u9a8c\u8bc1\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e00\u540c\u91c7\u96c6\u7684\u6d4b\u8bd5\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e0d\u662f\u4e00\u540c\u91c7\u96c6\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev_matched.tsv\u548cdev_mismatched.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest_matched.tsv\u4e0etest_mismatched.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: index promptID pairID genre sentence1_binary_parse sentence2_binary_parse sentence1_parse sentence2_parse sentence1 sentence2 label1 gold_label 0 31193 31193n government ( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) ) ( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) ) (ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .))) (ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .))) Conceptually cream skimming has two basic dimensions - product and geography. Product and geography are what make cream skimming work. neutral neutral 1 101457 101457e telephone ( you ( ( know ( during ( ( ( the season ) and ) ( i guess ) ) ) ) ( at ( at ( ( your level ) ( uh ( you ( ( ( lose them ) ( to ( the ( next level ) ) ) ) ( if ( ( if ( they ( decide ( to ( recall ( the ( the ( parent team ) ) ) ) ) ) ) ) ( ( the Braves ) ( decide ( to ( call ( to ( ( recall ( a guy ) ) ( from ( ( triple A ) ( ( ( then ( ( a ( double ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) and ) ( ( a ( single ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( You ( ( ( ( lose ( the things ) ) ( to ( the ( following level ) ) ) ) ( if ( ( the people ) recall ) ) ) . ) ) (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN during) (NP (NP (DT the) (NN season)) (CC and) (NP (FW i) (FW guess)))) (PP (IN at) (IN at) (NP (NP (PRP$ your) (NN level)) (SBAR (S (INTJ (UH uh)) (NP (PRP you)) (VP (VBP lose) (NP (PRP them)) (PP (TO to) (NP (DT the) (JJ next) (NN level))) (SBAR (IN if) (S (SBAR (IN if) (S (NP (PRP they)) (VP (VBP decide) (S (VP (TO to) (VP (VB recall) (NP (DT the) (DT the) (NN parent) (NN team)))))))) (NP (DT the) (NNPS Braves)) (VP (VBP decide) (S (VP (TO to) (VP (VB call) (S (VP (TO to) (VP (VB recall) (NP (DT a) (NN guy)) (PP (IN from) (NP (NP (RB triple) (DT A)) (SBAR (S (S (ADVP (RB then)) (NP (DT a) (JJ double) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))) (CC and) (S (NP (DT a) (JJ single) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him)))))))))))))))))))))))))))) (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT the) (NNS things)) (PP (TO to) (NP (DT the) (JJ following) (NN level))) (SBAR (IN if) (S (NP (DT the) (NNS people)) (VP (VBP recall))))) (. .))) you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him You lose the things to the following level if the people recall. entailment entailment 2 134793 134793e fiction ( ( One ( of ( our number ) ) ) ( ( will ( ( ( carry out ) ( your instructions ) ) minutely ) ) . ) ) ( ( ( A member ) ( of ( my team ) ) ) ( ( will ( ( execute ( your orders ) ) ( with ( immense precision ) ) ) ) . ) ) (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PRP$ our) (NN number)))) (VP (MD will) (VP (VB carry) (PRT (RP out)) (NP (PRP$ your) (NNS instructions)) (ADVP (RB minutely)))) (. .))) (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN of) (NP (PRP$ my) (NN team)))) (VP (MD will) (VP (VB execute) (NP (PRP$ your) (NNS orders)) (PP (IN with) (NP (JJ immense) (NN precision))))) (. .))) One of our number will carry out your instructions minutely. A member of my team will execute your orders with immense precision. entailment entailment 3 37397 37397e fiction ( ( How ( ( ( do you ) know ) ? ) ) ( ( All this ) ( ( ( is ( their information ) ) again ) . ) ) ) ( ( This information ) ( ( belongs ( to them ) ) . ) ) (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do) (NP (PRP you)) (VP (VB know))) (. ?)) (NP (PDT All) (DT this)) (VP (VBZ is) (NP (PRP$ their) (NN information)) (ADVP (RB again))) (. .))) (ROOT (S (NP (DT This) (NN information)) (VP (VBZ belongs) (PP (TO to) (NP (PRP them)))) (. .))) How do you know? All this is their information again. This information belongs to them. entailment entailment ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a12\u5217, \u7b2c\u4e00\u5217\u4ee3\u8868\u6587\u672c\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u5206\u522b\u4ee3\u8868\u53e5\u5b50\u5bf9\u7684\u4e0d\u540c\u7c7b\u578bid; \u7b2c\u56db\u5217\u4ee3\u8868\u53e5\u5b50\u5bf9\u7684\u6765\u6e90; \u7b2c\u4e94\u5217\u548c\u7b2c\u516d\u5217\u4ee3\u8868\u5177\u6709\u53e5\u6cd5\u7ed3\u6784\u5206\u6790\u7684\u53e5\u5b50\u5bf9\u8868\u793a; \u7b2c\u4e03\u5217\u548c\u7b2c\u516b\u5217\u4ee3\u8868\u5177\u6709\u53e5\u6cd5\u7ed3\u6784\u548c\u8bcd\u6027\u6807\u6ce8\u7684\u53e5\u5b50\u5bf9\u8868\u793a, \u7b2c\u4e5d\u5217\u548c\u7b2c\u5341\u5217\u4ee3\u8868\u539f\u59cb\u7684\u53e5\u5b50\u5bf9, \u7b2c\u5341\u4e00\u548c\u7b2c\u5341\u4e8c\u5217\u4ee3\u8868\u4e0d\u540c\u6807\u51c6\u7684\u6807\u6ce8\u65b9\u6cd5\u4ea7\u751f\u7684\u6807\u7b7e, \u5728\u8fd9\u91cc\uff0c\u4ed6\u4eec\u59cb\u7ec8\u76f8\u540c, \u4e00\u5171\u6709\u4e09\u79cd\u7c7b\u578b\u7684\u6807\u7b7e, neutral\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u65e2\u4e0d\u77db\u76fe\u4e5f\u4e0d\u8574\u542b, entailment\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u5177\u6709\u8574\u542b\u5173\u7cfb, contradiction\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u89c2\u70b9\u77db\u76fe. test_matched.tsv\u6570\u636e\u6837\u5f0f: index promptID pairID genre sentence1_binary_parse sentence2_binary_parse sentence1_parse sentence2_parse sentence1 sentence2 0 31493 31493 travel ( ( ( ( ( ( ( ( Hierbas , ) ( ans seco ) ) , ) ( ans dulce ) ) , ) and ) frigola ) ( ( ( are just ) ( ( a ( few names ) ) ( worth ( ( keeping ( a look-out ) ) for ) ) ) ) . ) ) ( Hierbas ( ( is ( ( a name ) ( worth ( ( looking out ) for ) ) ) ) . ) ) (ROOT (S (NP (NP (NNS Hierbas)) (, ,) (NP (NN ans) (NN seco)) (, ,) (NP (NN ans) (NN dulce)) (, ,) (CC and) (NP (NN frigola))) (VP (VBP are) (ADVP (RB just)) (NP (NP (DT a) (JJ few) (NNS names)) (PP (JJ worth) (S (VP (VBG keeping) (NP (DT a) (NN look-out)) (PP (IN for))))))) (. .))) (ROOT (S (NP (NNS Hierbas)) (VP (VBZ is) (NP (NP (DT a) (NN name)) (PP (JJ worth) (S (VP (VBG looking) (PRT (RP out)) (PP (IN for))))))) (. .))) Hierbas, ans seco, ans dulce, and frigola are just a few names worth keeping a look-out for. Hierbas is a name worth looking out for. 1 92164 92164 government ( ( ( The extent ) ( of ( the ( behavioral effects ) ) ) ) ( ( would ( ( depend ( in ( part ( on ( ( the structure ) ( of ( ( ( the ( individual ( account program ) ) ) and ) ( any limits ) ) ) ) ) ) ) ) ( on ( accessing ( the funds ) ) ) ) ) . ) ) ( ( Many people ) ( ( would ( be ( very ( unhappy ( to ( ( loose control ) ( over ( their ( own money ) ) ) ) ) ) ) ) ) . ) ) (ROOT (S (NP (NP (DT The) (NN extent)) (PP (IN of) (NP (DT the) (JJ behavioral) (NNS effects)))) (VP (MD would) (VP (VB depend) (PP (IN in) (NP (NP (NN part)) (PP (IN on) (NP (NP (DT the) (NN structure)) (PP (IN of) (NP (NP (DT the) (JJ individual) (NN account) (NN program)) (CC and) (NP (DT any) (NNS limits)))))))) (PP (IN on) (S (VP (VBG accessing) (NP (DT the) (NNS funds))))))) (. .))) (ROOT (S (NP (JJ Many) (NNS people)) (VP (MD would) (VP (VB be) (ADJP (RB very) (JJ unhappy) (PP (TO to) (NP (NP (JJ loose) (NN control)) (PP (IN over) (NP (PRP$ their) (JJ own) (NN money)))))))) (. .))) The extent of the behavioral effects would depend in part on the structure of the individual account program and any limits on accessing the funds. Many people would be very unhappy to loose control over their own money. 2 9662 9662 government ( ( ( Timely access ) ( to information ) ) ( ( is ( in ( ( the ( best interests ) ) ( of ( ( ( both GAO ) and ) ( the agencies ) ) ) ) ) ) . ) ) ( It ( ( ( is ( in ( ( everyone 's ) ( best interest ) ) ) ) ( to ( ( have access ) ( to ( information ( in ( a ( timely manner ) ) ) ) ) ) ) ) . ) ) (ROOT (S (NP (NP (JJ Timely) (NN access)) (PP (TO to) (NP (NN information)))) (VP (VBZ is) (PP (IN in) (NP (NP (DT the) (JJS best) (NNS interests)) (PP (IN of) (NP (NP (DT both) (NNP GAO)) (CC and) (NP (DT the) (NNS agencies))))))) (. .))) (ROOT (S (NP (PRP It)) (VP (VBZ is) (PP (IN in) (NP (NP (NN everyone) (POS 's)) (JJS best) (NN interest))) (S (VP (TO to) (VP (VB have) (NP (NN access)) (PP (TO to) (NP (NP (NN information)) (PP (IN in) (NP (DT a) (JJ timely) (NN manner))))))))) (. .))) Timely access to information is in the best interests of both GAO and the agencies. It is in everyone's best interest to have access to information in a timely manner. 3 5991 5991 travel ( ( Based ( in ( ( the ( Auvergnat ( spa town ) ) ) ( of Vichy ) ) ) ) ( , ( ( the ( French government ) ) ( often ( ( ( ( proved ( more zealous ) ) ( than ( its masters ) ) ) ( in ( ( ( suppressing ( civil liberties ) ) and ) ( ( drawing up ) ( anti-Jewish legislation ) ) ) ) ) . ) ) ) ) ) ( ( The ( French government ) ) ( ( passed ( ( anti-Jewish laws ) ( aimed ( at ( helping ( the Nazi ) ) ) ) ) ) . ) ) (ROOT (S (PP (VBN Based) (PP (IN in) (NP (NP (DT the) (NNP Auvergnat) (NN spa) (NN town)) (PP (IN of) (NP (NNP Vichy)))))) (, ,) (NP (DT the) (JJ French) (NN government)) (ADVP (RB often)) (VP (VBD proved) (NP (JJR more) (NNS zealous)) (PP (IN than) (NP (PRP$ its) (NNS masters))) (PP (IN in) (S (VP (VP (VBG suppressing) (NP (JJ civil) (NNS liberties))) (CC and) (VP (VBG drawing) (PRT (RP up)) (NP (JJ anti-Jewish) (NN legislation))))))) (. .))) (ROOT (S (NP (DT The) (JJ French) (NN government)) (VP (VBD passed) (NP (NP (JJ anti-Jewish) (NNS laws)) (VP (VBN aimed) (PP (IN at) (S (VP (VBG helping) (NP (DT the) (JJ Nazi)))))))) (. .))) Based in the Auvergnat spa town of Vichy, the French government often proved more zealous than its masters in suppressing civil liberties and drawing up anti-Jewish legislation. The French government passed anti-Jewish laws aimed at helping the Nazi. ... test_matched.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test_matched.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a10\u5217, \u4e0etrain.tsv\u7684\u524d10\u5217\u542b\u4e49\u76f8\u540c. (MNLI/SNLI)\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u591a\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC 2.7 (QNLI/RTE/WNLI)\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f \u00b6 \u6570\u636e\u96c6\u91ca\u4e49: QNLI(Qusetion-answering NLI\uff0c\u95ee\u7b54\u81ea\u7136\u8bed\u8a00\u63a8\u65ad)\uff0c\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u4efb\u52a1\u3002QNLI\u662f\u4ece\u53e6\u4e00\u4e2a\u6570\u636e\u96c6The Stanford Question Answering Dataset(\u65af\u5766\u798f\u95ee\u7b54\u6570\u636e\u96c6, SQuAD 1.0)[ 3] \u8f6c\u6362\u800c\u6765\u7684. RTE(The Recognizing Textual Entailment datasets\uff0c\u8bc6\u522b\u6587\u672c\u8574\u542b\u6570\u636e\u96c6)\uff0c\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u4efb\u52a1\uff0c\u5b83\u662f\u5c06\u4e00\u7cfb\u5217\u7684\u5e74\u5ea6\u6587\u672c\u8574\u542b\u6311\u6218\u8d5b\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6574\u5408\u5408\u5e76\u800c\u6765\u7684. WNLI(Winograd NLI\uff0cWinograd\u81ea\u7136\u8bed\u8a00\u63a8\u65ad)\uff0c\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u4efb\u52a1\uff0c\u6570\u636e\u96c6\u6765\u81ea\u4e8e\u7ade\u8d5b\u6570\u636e\u7684\u8f6c\u6362\u3002 \u672c\u8d28: QNLI\u662f\u4e8c\u5206\u7c7b\u4efb\u52a1. RTE\u662f\u4e8c\u5206\u7c7b\u4efb\u52a1. WNLI\u662f\u4e8c\u5206\u7c7b\u4efb\u52a1. QNLI, RTE, WNLI\u4e09\u4e2a\u6570\u636e\u96c6\u7684\u6837\u5f0f\u57fa\u672c\u76f8\u540c. - (QNLI/RTE/WNLI)/ - dev.tsv - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. QNLI\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f: index question sentence label 0 When did the third Digimon series begin? Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese. not_entailment 1 Which missile batteries often have individual launchers several kilometres from one another? When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs. not_entailment 2 What two things does Popper argue Tarski's theory involves in an evaluation of truth? He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. entailment 3 What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians? On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854. entailment 4 What famous palace is located in London? London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0\u00b0 longitude, and GMT). not_entailment 5 When is the term 'German dialects' used in regard to the German language? When talking about the German language, the term German dialects is only used for the traditional regional varieties. entailment 6 What was the name of the island the English traded to the Dutch in return for New Amsterdam? At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) in North America in exchange for Dutch control of Run, an Indonesian island. entailment 7 How were the Portuguese expelled from Myanmar? From the 1720s onward, the kingdom was beset with repeated Meithei raids into Upper Myanmar and a nagging rebellion in Lan Na. not_entailment 8 What does the word 'customer' properly apply to? The bill also required rotation of principal maintenance inspectors and stipulated that the word \"customer\" properly applies to the flying public, not those entities regulated by the FAA. entailment ... RTE\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f: index sentence1 sentence2 label 0 No Weapons of Mass Destruction Found in Iraq Yet. Weapons of Mass Destruction Found in Iraq. not_entailment 1 A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.Pope Benedict XVI is the new leader of the Roman Catholic Church. entailment 2 Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients. Herceptin can be used to treat breast cancer. entailment 3 Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment. The previous name of Ho Chi Minh City was Saigon.entailment 4 A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later. Paul Stewart Hutchinson is accused of having stabbed a girl. not_entailment 5 Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, where he was released by police after being detained for 24 hours. Bakri was briefly detained, but was released. entailment 6 Nearly 4 million children who have at least one parent who entered the U.S. illegally were born in the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That's about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found. Three quarters of U.S. illegal immigrants have children. not_entailment 7 Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together. Aristide had Prime Minister Robert Malval murdered in Port-au-Prince. not_entailment 8 WASHINGTON -- A newly declassified narrative of the Bush administration's advice to the CIA on harsh interrogations shows that the small group of Justice Department lawyers who wrote memos authorizing controversial interrogation techniques were operating not on their own but with direction from top administration officials, including then-Vice President Dick Cheney and national security adviser Condoleezza Rice. At the same time, the narrative suggests that then-Defense Secretary Donald H. Rumsfeld and then-Secretary of State Colin Powell were largely left out of the decision-making process. Dick Cheney was the Vice President of Bush. entailment WNLI\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f: index sentence1 sentence2 label 0 I stuck a pin through a carrot. When I pulled the pin out, it had a hole. The carrot had a hole. 1 1 John couldn't see the stage with Billy in front of him because he is so short. John is so short. 1 2 The police arrested all of the gang members. They were trying to stop the drug trade in the neighborhood. The police were trying to stop the drug trade in the neighborhood. 1 3 Steve follows Fred's example in everything. He influences him hugely. Steve influences him hugely. 0 4 When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth. mother was careful not to disturb her, undressing and climbing back into her berth. 0 5 George got free tickets to the play, but he gave them to Eric, because he was particularly eager to see it. George was particularly eager to see it. 0 6 John was jogging through the park when he saw a man juggling watermelons. He was very impressive. John was very impressive. 0 7 I couldn't put the pot on the shelf because it was too tall. The pot was too tall. 1 8 We had hoped to place copies of our newsletter on all the chairs in the auditorium, but there were simply not enough of them. There were simply not enough copies of the newsletter. 1 (QNLI/RTE/WNLI)\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a4\u5217, \u7b2c\u4e00\u5217\u4ee3\u8868\u6587\u672c\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u4ee3\u8868\u9700\u8981\u8fdb\u884c'\u662f\u5426\u8574\u542b'\u5224\u5b9a\u7684\u53e5\u5b50\u5bf9; \u7b2c\u56db\u5217\u6570\u636e\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u5177\u6709\u8574\u542b\u5173\u7cfb, 0/not_entailment\u4ee3\u8868\u4e0d\u662f\u8574\u542b\u5173\u7cfb, 1/entailment\u4ee3\u8868\u8574\u542b\u5173\u7cfb. QNLI\u4e2d\u7684test.tsv\u6570\u636e\u6837\u5f0f: index question sentence 0 What organization is devoted to Jihad against Israel? For some decades prior to the First Palestine Intifada in 1987, the Muslim Brotherhood in Palestine took a \"quiescent\" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's \"indulgence\" to build up a network of mosques and charitable organizations. 1 In what century was the Yarrow-Schlick-Tweedy balancing system used? In the late 19th century, the Yarrow-Schlick-Tweedy balancing 'system' was used on some marine triple expansion engines. 2 The largest brand of what store in the UK is located in Kingston Park? Close to Newcastle, the largest indoor shopping centre in Europe, the MetroCentre, is located in Gateshead. 3 What does the IPCC rely on for research? In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included. 4 What is the principle about relating spin and space variables? Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive. 5 Which network broadcasted Super Bowl 50 in the U.S.? CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game. 6 What did the museum acquire from the Royal College of Science? To link this to the rest of the museum, a new entrance building was constructed on the site of the former boiler house, the intended site of the Spiral, between 1978 and 1982. 7 What is the name of the old north branch of the Rhine? From Wijk bij Duurstede, the old north branch of the Rhine is called Kromme Rijn (\"Bent Rhine\") past Utrecht, first Leidse Rijn (\"Rhine of Leiden\") and then, Oude Rijn (\"Old Rhine\"). 8 What was one of Luther's most personal writings? It remains in use today, along with Luther's hymns and his translation of the Bible. ... (RTE/WNLI)\u4e2d\u7684test.tsv\u6570\u636e\u6837\u5f0f: index sentence1 sentence2 0 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when Maude and Dora came in sight. 1 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the trains came in sight. 2 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the puffs came in sight. 3 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the roars came in sight. 4 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the whistles came in sight. 5 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the horses came in sight. 6 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. Maude and Dora saw a train coming. 7 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. The trains saw a train coming. 8 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. The puffs saw a train coming. ... (QNLI/RTE/WNLI)\u4e2d\u7684test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a3\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u4ee3\u8868\u9700\u8981\u8fdb\u884c'\u662f\u5426\u8574\u542b'\u5224\u5b9a\u7684\u53e5\u5b50\u5bf9. (QNLI/RTE/WNLI)\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86GLUE\u6570\u636e\u96c6\u5408\u7684\u4ecb\u7ecd: GLUE\u7531\u7ebd\u7ea6\u5927\u5b66, \u534e\u76db\u987f\u5927\u5b66, Google\u8054\u5408\u63a8\u51fa, \u6db5\u76d6\u4e0d\u540cNLP\u4efb\u52a1\u7c7b\u578b, \u622a\u6b62\u81f32020\u5e741\u6708\u5176\u4e2d\u5305\u62ec11\u4e2a\u5b50\u4efb\u52a1\u6570\u636e\u96c6, \u6210\u4e3a\u8861\u91cfNLP\u7814\u7a76\u53d1\u5c55\u7684\u8861\u91cf\u6807\u51c6. GLUE\u6570\u636e\u96c6\u5408\u5305\u542b\u4ee5\u4e0b\u6570\u636e\u96c6: CoLA \u6570\u636e\u96c6 SST-2 \u6570\u636e\u96c6 MRPC \u6570\u636e\u96c6 STS-B \u6570\u636e\u96c6 QQP \u6570\u636e\u96c6 MNLI \u6570\u636e\u96c6 SNLI \u6570\u636e\u96c6 QNLI \u6570\u636e\u96c6 RTE \u6570\u636e\u96c6 WNLI \u6570\u636e\u96c6","title":"10 NLP\u4e2d\u7684\u6807\u51c6\u6570\u636e\u96c6(\u62d3\u5c55\u8d44\u6599)"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#_1","text":"\u4e86\u89e3NLP\u4e2dGLUE\u6807\u51c6\u6570\u636e\u96c6\u5408\u7684\u76f8\u5173\u77e5\u8bc6. \u638c\u63e1GLUE\u6807\u51c6\u6570\u636e\u96c6\u5408\u7684\u4e0b\u8f7d\u65b9\u5f0f, \u6570\u636e\u6837\u5f0f\u53ca\u5176\u5bf9\u5e94\u7684\u4efb\u52a1\u7c7b\u578b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#1-glue","text":"","title":"1 GLUE\u6570\u636e\u96c6\u5408\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#11","text":"GLUE\u7531\u7ebd\u7ea6\u5927\u5b66, \u534e\u76db\u987f\u5927\u5b66, Google\u8054\u5408\u63a8\u51fa, \u6db5\u76d6\u4e0d\u540cNLP\u4efb\u52a1\u7c7b\u578b, \u622a\u6b62\u81f32020\u5e741\u6708\u5176\u4e2d\u5305\u62ec11\u4e2a\u5b50\u4efb\u52a1\u6570\u636e\u96c6, \u6210\u4e3a\u8861\u91cfNLP\u7814\u7a76\u53d1\u5c55\u7684\u8861\u91cf\u6807\u51c6. CoLA \u6570\u636e\u96c6 SST-2 \u6570\u636e\u96c6 MRPC \u6570\u636e\u96c6 STS-B \u6570\u636e\u96c6 QQP \u6570\u636e\u96c6 MNLI \u6570\u636e\u96c6 SNLI \u6570\u636e\u96c6 QNLI \u6570\u636e\u96c6 RTE \u6570\u636e\u96c6 WNLI \u6570\u636e\u96c6 diagnostics\u6570\u636e\u96c6(\u5b98\u65b9\u672a\u5b8c\u5584)","title":"1.1 \u6570\u636e\u96c6\u5408\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#12","text":"\u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/glue_data\u4e0b \u53e6\u5916\u8fd9GLUE\u768411\u4e2a\u6570\u636e\u96c6\u90fd\u653e\u5230\u7684\u767e\u5ea6\u4e91 , \u9700\u8981\u7684\u53ef\u4ee5\u81ea\u53d6: GLUE\u6570\u636e\u96c6 \u63d0\u53d6\u7801: b6se","title":"1.2 \u6570\u636e\u96c6\u5408\u8def\u5f84"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#2-glue","text":"","title":"2 GLUE\u5b50\u6570\u636e\u96c6\u7684\u6837\u5f0f\u53ca\u5176\u4efb\u52a1\u7c7b\u578b"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#21-cola","text":"\u6570\u636e\u96c6\u91ca\u4e49:CoLA(The Corpus of Linguistic Acceptability\uff0c\u8bed\u8a00\u53ef\u63a5\u53d7\u6027\u8bed\u6599\u5e93)\u7ebd\u7ea6\u5927\u5b66\u53d1\u5e03\u7684\u6709\u5173\u8bed\u6cd5\u7684\u6570\u636e\u96c6 \u672c\u8d28: \u662f\u5bf9\u4e00\u4e2a\u7ed9\u5b9a\u53e5\u5b50\uff0c\u5224\u5b9a\u5176\u662f\u5426\u8bed\u6cd5\u6b63\u786e\u7684\u5355\u4e2a\u53e5\u5b50\u7684**\u6587\u672c\u4e8c\u5206\u7c7b\u4efb\u52a1**. - CoLA/ - dev.tsv - original/ - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: ... gj04 1 She coughed herself awake as the leaf landed on her nose. gj04 1 The worm wriggled onto the carpet. gj04 1 The chocolate melted onto the carpet. gj04 0 * The ball wriggled itself loose. gj04 1 Bill wriggled himself loose. bc01 1 The sinking of the ship to collect the insurance was very devious. bc01 1 The ship's sinking was very devious. bc01 0 * The ship's sinking to collect the insurance was very devious. bc01 1 The testing of such drugs on oneself is too risky. bc01 0 * This drug's testing on oneself is too risky. ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a4\u5217, \u7b2c\u4e00\u5217\u6570\u636e, \u5982gj04, bc01\u7b49\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u6765\u6e90\u5373\u51fa\u7248\u7269\u4ee3\u53f7; \u7b2c\u4e8c\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u8bed\u6cd5\u662f\u5426\u6b63\u786e, 0\u4ee3\u8868\u4e0d\u6b63\u786e, 1\u4ee3\u8868\u6b63\u786e; \u7b2c\u4e09\u5217\u6570\u636e, ' ', \u662f\u4f5c\u8005\u6700\u521d\u7684\u6b63\u8d1f\u6837\u672c\u6807\u8bb0, \u4e0e\u7b2c\u4e8c\u5217\u610f\u4e49\u76f8\u540c, ' '\u8868\u793a\u4e0d\u6b63\u786e; \u7b2c\u56db\u5217\u5373\u662f\u88ab\u6807\u6ce8\u7684\u8bed\u6cd5\u4f7f\u7528\u662f\u5426\u6b63\u786e\u7684\u6587\u672c\u53e5\u5b50. test.tsv\u6570\u636e\u6837\u5f0f: index sentence 0 Bill whistled past the house. 1 The car honked its way down the road. 2 Bill pushed Harry off the sofa. 3 the kittens yawned awake and played. 4 I demand that the more John eats, the more he pay. 5 If John eats more, keep your mouth shut tighter, OK? 6 His expectations are always lower than mine are. 7 The sooner you call, the more carefully I will word the letter. 8 The more timid he feels, the more people he interviews without asking questions of. 9 Once Janet left, Fred became a lot crazier. ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u6570\u636e\u4ee3\u8868\u7528\u4e8e\u6d4b\u8bd5\u7684\u53e5\u5b50. CoLA\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: MCC(\u9a6c\u4fee\u65af\u76f8\u5173\u7cfb\u6570, \u5728\u6b63\u8d1f\u6837\u672c\u5206\u5e03\u5341\u5206\u4e0d\u5747\u8861\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u7684\u4e8c\u5206\u7c7b\u8bc4\u4f30\u6307\u6807)","title":"2.1 CoLA\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#22-sst-2","text":"\u6570\u636e\u96c6\u91ca\u4e49:SST-2(The Stanford Sentiment Treebank\uff0c\u65af\u5766\u798f\u60c5\u611f\u6811\u5e93),\u5355\u53e5\u5b50\u5206\u7c7b\u4efb\u52a1\uff0c\u5305\u542b\u7535\u5f71\u8bc4\u8bba\u4e2d\u7684\u53e5\u5b50\u548c\u5b83\u4eec\u60c5\u611f\u7684\u4eba\u7c7b\u6ce8\u91ca. \u672c\u8d28:\u53e5\u5b50\u7ea7\u522b\u7684**\u4e8c\u5206\u7c7b\u4efb\u52a1** - SST-2/ - dev.tsv - original/ - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: sentence label hide new secretions from the parental units 0 contains no wit , only labored gags 0 that loves its characters and communicates something rather beautiful about human nature 1 remains utterly satisfied to remain the same throughout 0 on the worst revenge-of-the-nerds clich\u00e9s the filmmakers could dredge up 0 that 's far too tragic to merit such superficial treatment 0 demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . 1 of saucy 1 a depressed fifteen-year-old 's suicidal poetry 0 ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u5177\u6709\u611f\u60c5\u8272\u5f69\u7684\u8bc4\u8bba\u6587\u672c; \u7b2c\u4e8c\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u662f\u79ef\u6781\u6216\u8005\u6d88\u6781\u7684\u8bc4\u8bba, 0\u4ee3\u8868\u6d88\u6781, 1\u4ee3\u8868\u79ef\u6781. test.tsv\u6570\u636e\u6837\u5f0f: index sentence 0 uneasy mishmash of styles and genres . 1 this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation . 2 by the end of no such thing the audience , like beatrice , has a watchful affection for the monster . 3 director rob marshall went out gunning to make a great one . 4 lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new . 5 a well-made and often lovely depiction of the mysteries of friendship . 6 none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor . 7 although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious . 8 it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another . 9 this is junk food cinema at its greasiest . ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a2\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u6570\u636e\u4ee3\u8868\u7528\u4e8e\u6d4b\u8bd5\u7684\u53e5\u5b50. SST-2\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC","title":"2.2 SST-2\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#23-mrpc","text":"\u6570\u636e\u96c6\u91ca\u4e49:MRPC(The Microsoft Research Paraphrase Corpus\uff0c\u5fae\u8f6f\u7814\u7a76\u9662\u91ca\u4e49\u8bed\u6599\u5e93),\u76f8\u4f3c\u6027\u548c\u91ca\u4e49\u4efb\u52a1\uff0c\u662f\u4ece\u5728\u7ebf\u65b0\u95fb\u6e90\u4e2d\u81ea\u52a8\u62bd\u53d6\u53e5\u5b50\u5bf9\u8bed\u6599\u5e93\uff0c\u5e76\u4eba\u5de5\u6ce8\u91ca\u53e5\u5b50\u5bf9\u4e2d\u7684\u53e5\u5b50\u662f\u5426\u5728\u8bed\u4e49\u4e0a\u7b49\u6548\u3002 \u672c\u8d28:\u53e5\u5b50\u7ea7\u522b\u7684**\u4e8c\u5206\u7c7b\u4efb\u52a1** - MRPC/ - dev.tsv - test.tsv - train.tsv - dev_ids.tsv - msr_paraphrase_test.txt - msr_paraphrase_train.txt \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: Quality #1 ID #2 ID #1 String #2 String 1 702876 702977 Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence . Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence . 0 2108705 2108831 Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion . Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 . 1 1330381 1330521 They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale . 0 3344667 3344648 Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 . Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 . 1 1236820 1236712 The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange . PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday . 1 738533 737951 Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier . With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier . 0 264589 264502 The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday . The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 . 1 579975 579810 The DVD-CCA then appealed to the state Supreme Court . The DVD CCA appealed that decision to the U.S. Supreme Court . ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a5\u5217, \u7b2c\u4e00\u5217\u6570\u636e, 0\u62161, \u4ee3\u8868\u6bcf\u5bf9\u53e5\u5b50\u662f\u5426\u5177\u6709\u76f8\u540c\u7684\u542b\u4e49, 0\u4ee3\u8868\u542b\u4e49\u4e0d\u76f8\u540c, 1\u4ee3\u8868\u542b\u4e49\u76f8\u540c. \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u5206\u522b\u4ee3\u8868\u6bcf\u5bf9\u53e5\u5b50\u7684id, \u7b2c\u56db\u5217\u548c\u7b2c\u4e94\u5217\u5206\u522b\u5177\u6709\u76f8\u540c/\u4e0d\u540c\u542b\u4e49\u7684\u53e5\u5b50\u5bf9. test.tsv\u6570\u636e\u6837\u5f0f: index #1 ID #2 ID #1 String #2 String 0 1089874 1089925 PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So . 1 3019446 3019327 The world 's two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected . Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash . 2 1945605 1945824 According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 . The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 . 3 1430402 1430329 A tropical storm rapidly developed in the Gulf of Mexico Sunday and was expected to hit somewhere along the Texas or Louisiana coasts by Monday night . A tropical storm rapidly developed in the Gulf of Mexico on Sunday and could have hurricane-force winds when it hits land somewhere along the Louisiana coast Monday night . 4 3354381 3354396 The company didn 't detail the costs of the replacement and repairs . But company officials expect the costs of the replacement work to run into the millions of dollars . 5 1390995 1391183 The settling companies would also assign their possible claims against the underwriters to the investor plaintiffs , he added . Under the agreement , the settling companies will also assign their potential claims against the underwriters to the investors , he added . 6 2201401 2201285 Air Commodore Quaife said the Hornets remained on three-minute alert throughout the operation . Air Commodore John Quaife said the security operation was unprecedented . 7 2453843 2453998 A Washington County man may have the countys first human case of West Nile virus , the health department said Friday . The countys first and only human case of West Nile this year was confirmed by health officials on Sept . 8 . ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a5\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u5176\u4f59\u5217\u7684\u542b\u4e49\u4e0etrain.tsv\u4e2d\u76f8\u540c. MRPC\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC\u548cF1","title":"2.3 MRPC\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#24-sts-b","text":"\u6570\u636e\u96c6\u91ca\u4e49: STSB(The Semantic Textual Similarity Benchmark\uff0c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u57fa\u51c6\u6d4b\u8bd5) \u672c\u8d28: \u56de\u5f52\u4efb\u52a1/\u53e5\u5b50\u5bf9\u7684\u6587\u672c\u4e94\u5206\u7c7b\u4efb\u52a1 - STS-B/ - dev.tsv - test.tsv - train.tsv - LICENSE.txt - readme.txt - original/ \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: index genre filename year old_index source1 source2 sentence1 sentence2 score 0 main-captions MSRvid 2012test 0001 none none A plane is taking off. An air plane is taking off. 5.000 1 main-captions MSRvid 2012test 0004 none none A man is playing a large flute. A man is playing a flute. 3.800 2 main-captions MSRvid 2012test 0005 none none A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza. 3.800 3 main-captions MSRvid 2012test 0006 none none Three men are playing chess.Two men are playing chess. 2.600 4 main-captions MSRvid 2012test 0009 none none A man is playing the cello.A man seated is playing the cello. 4.250 5 main-captions MSRvid 2012test 0011 none none Some men are fighting. Two men are fighting. 4.250 6 main-captions MSRvid 2012test 0012 none none A man is smoking. A man is skating. 0.500 7 main-captions MSRvid 2012test 0013 none none The man is playing the piano. The man is playing the guitar. 1.600 8 main-captions MSRvid 2012test 0014 none none A man is playing on a guitar and singing. A woman is playing an acoustic guitar and singing. 2.200 9 main-captions MSRvid 2012test 0016 none none A person is throwing a cat on to the ceiling. A person throws a cat on the ceiling. 5.000 ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a10\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u662f\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u4ee3\u8868\u6bcf\u5bf9\u53e5\u5b50\u7684\u6765\u6e90, \u5982main-captions\u8868\u793a\u6765\u81ea\u5b57\u5e55; \u7b2c\u4e09\u5217\u4ee3\u8868\u6765\u6e90\u7684\u5177\u4f53\u4fdd\u5b58\u6587\u4ef6\u540d, \u7b2c\u56db\u5217\u4ee3\u8868\u51fa\u73b0\u65f6\u95f4(\u5e74); \u7b2c\u4e94\u5217\u4ee3\u8868\u539f\u59cb\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u516d\u5217\u548c\u7b2c\u4e03\u5217\u5206\u522b\u4ee3\u8868\u53e5\u5b50\u5bf9\u539f\u59cb\u6765\u6e90; \u7b2c\u516b\u5217\u548c\u7b2c\u4e5d\u5217\u4ee3\u8868\u76f8\u4f3c\u7a0b\u5ea6\u4e0d\u540c\u7684\u53e5\u5b50\u5bf9; \u7b2c\u5341\u5217\u4ee3\u8868\u53e5\u5b50\u5bf9\u7684\u76f8\u4f3c\u7a0b\u5ea6\u7531\u4f4e\u5230\u9ad8, \u503c\u57df\u8303\u56f4\u662f[0, 5]. test.tsv\u6570\u636e\u6837\u5f0f: index genre filename year old_index source1 source2 sentence1 sentence2 0 main-captions MSRvid 2012test 0024 none none A girl is styling her hair. A girl is brushing her hair. 1 main-captions MSRvid 2012test 0033 none none A group of men play soccer on the beach. A group of boys are playing soccer on the beach. 2 main-captions MSRvid 2012test 0045 none none One woman is measuring another woman's ankle. A woman measures another woman's ankle. 3 main-captions MSRvid 2012test 0063 none none A man is cutting up a cucumber. A man is slicing a cucumber. 4 main-captions MSRvid 2012test 0066 none none A man is playing a harp. A man is playing a keyboard. 5 main-captions MSRvid 2012test 0074 none none A woman is cutting onions. A woman is cutting tofu. 6 main-captions MSRvid 2012test 0076 none none A man is riding an electric bicycle. A man is riding a bicycle. 7 main-captions MSRvid 2012test 0082 none none A man is playing the drums. A man is playing the guitar. 8 main-captions MSRvid 2012test 0092 none none A man is playing guitar. A lady is playing the guitar. 9 main-captions MSRvid 2012test 0095 none none A man is playing a guitar. A man is playing a trumpet. 10 main-captions MSRvid 2012test 0096 none none A man is playing a guitar. A man is playing a trumpet. ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a9\u5217, \u542b\u4e49\u4e0etrain.tsv\u524d9\u5217\u76f8\u540c. STS-B\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u591a\u5206\u7c7b\u4efb\u52a1/\u53e5\u5b50\u5bf9\u56de\u5f52\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: Pearson-Spearman Corr","title":"2.4 STS-B\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#25-qqp","text":"\u6570\u636e\u96c6\u91ca\u4e49: QQP(The Quora Question Pairs, Quora\u95ee\u9898\u5bf9\u6570\u96c6),\u76f8\u4f3c\u6027\u548c\u91ca\u4e49\u4efb\u52a1\uff0c\u662f\u793e\u533a\u95ee\u7b54\u7f51\u7ad9Quora\u4e2d\u95ee\u9898\u5bf9\u7684\u96c6\u5408\u3002 \u672c\u8d28: \u53e5\u5b50\u5bf9\u7684**\u4e8c\u5206\u7c7b\u4efb\u52a1** - QQP/ - dev.tsv - original/ - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: id qid1 qid2 question1 question2 is_duplicate 133273 213221 213222 How is the life of a math student? Could you describe your own experiences?Which level of prepration is enough for the exam jlpt5? 0 402555 536040 536041 How do I control my horny emotions? How do you control your horniness? 1 360472 364011 490273 What causes stool color to change to yellow? What can cause stool to come out as little balls? 0 150662 155721 7256 What can one do after MBBS? What do i do after my MBBS ? 1 183004 279958 279959 Where can I find a power outlet for my laptop at Melbourne Airport? Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney? 0 119056 193387 193388 How not to feel guilty since I am Muslim and I'm conscious we won't have sex together? I don't beleive I am bulimic, but I force throw up atleast once a day after I eat something and feel guilty. Should I tell somebody, and if so who? 0 356863 422862 96457 How is air traffic controlled? How do you become an air traffic controller?0 106969 147570 787 What is the best self help book you have read? Why? How did it change your life? What are the top self help books I should read? 1 ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a6\u5217, \u7b2c\u4e00\u5217\u4ee3\u8868\u6587\u672c\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u5206\u522b\u4ee3\u8868\u95ee\u98981\u548c\u95ee\u98982\u7684id; \u7b2c\u56db\u5217\u548c\u7b2c\u4e94\u5217\u4ee3\u8868\u9700\u8981\u8fdb\u884c'\u662f\u5426\u91cd\u590d'\u5224\u5b9a\u7684\u53e5\u5b50\u5bf9; \u7b2c\u516d\u5217\u4ee3\u8868\u4e0a\u8ff0\u95ee\u9898\u662f/\u4e0d\u662f\u91cd\u590d\u6027\u95ee\u9898\u7684\u6807\u7b7e, 0\u4ee3\u8868\u4e0d\u91cd\u590d, 1\u4ee3\u8868\u91cd\u590d. test.tsv\u6570\u636e\u6837\u5f0f: id question1 question2 0 Would the idea of Trump and Putin in bed together scare you, given the geopolitical implications? Do you think that if Donald Trump were elected President, he would be able to restore relations with Putin and Russia as he said he could, based on the rocky relationship Putin had with Obama and Bush? 1 What are the top ten Consumer-to-Consumer E-commerce online? What are the top ten Consumer-to-Business E-commerce online? 2 Why don't people simply 'Google' instead of asking questions on Quora? Why do people ask Quora questions instead of just searching google? 3 Is it safe to invest in social trade biz? Is social trade geniune? 4 If the universe is expanding then does matter also expand? If universe and space is expanding? Does that mean anything that occupies space is also expanding? 5 What is the plural of hypothesis? What is the plural of thesis? 6 What is the application form you need for launching a company? What is the application form you need for launching a company in Austria? 7 What is Big Theta? When should I use Big Theta as opposed to big O? Is O(Log n) close to O(n) or O(1)? 8 What are the health implications of accidentally eating a small quantity of aluminium foil?What are the implications of not eating vegetables? ... test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a3\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u4ee3\u8868\u7528\u4e8e\u6d4b\u8bd5\u7684\u95ee\u9898\u53e5\u5b50\u5bf9. QQP\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC/F1","title":"2.5 QQP\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#26-mnlisnli","text":"\u6570\u636e\u96c6\u91ca\u4e49: MNLI(The Multi-Genre Natural Language Inference Corpus, \u591a\u7c7b\u578b\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6570\u636e\u5e93) \u672c\u8d28: \u53e5\u5b50\u5bf9\u7684**\u4e09\u5206\u7c7b\u4efb\u52a1** - (MNLI/SNLI)/ - dev_matched.tsv - dev_mismatched.tsv - original/ - test_matched.tsv - test_mismatched.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev_matched.tsv, dev_mismatched.tsv, test_matched.tsv, test_mismatched.tsv\u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e00\u540c\u91c7\u96c6\u7684\u9a8c\u8bc1\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e0d\u662f\u4e00\u540c\u91c7\u96c6\u9a8c\u8bc1\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e00\u540c\u91c7\u96c6\u7684\u6d4b\u8bd5\u96c6, \u4e0e\u8bad\u7ec3\u96c6\u4e0d\u662f\u4e00\u540c\u91c7\u96c6\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev_matched.tsv\u548cdev_mismatched.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest_matched.tsv\u4e0etest_mismatched.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. train.tsv\u6570\u636e\u6837\u5f0f: index promptID pairID genre sentence1_binary_parse sentence2_binary_parse sentence1_parse sentence2_parse sentence1 sentence2 label1 gold_label 0 31193 31193n government ( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) ) ( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) ) (ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .))) (ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .))) Conceptually cream skimming has two basic dimensions - product and geography. Product and geography are what make cream skimming work. neutral neutral 1 101457 101457e telephone ( you ( ( know ( during ( ( ( the season ) and ) ( i guess ) ) ) ) ( at ( at ( ( your level ) ( uh ( you ( ( ( lose them ) ( to ( the ( next level ) ) ) ) ( if ( ( if ( they ( decide ( to ( recall ( the ( the ( parent team ) ) ) ) ) ) ) ) ( ( the Braves ) ( decide ( to ( call ( to ( ( recall ( a guy ) ) ( from ( ( triple A ) ( ( ( then ( ( a ( double ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) and ) ( ( a ( single ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( You ( ( ( ( lose ( the things ) ) ( to ( the ( following level ) ) ) ) ( if ( ( the people ) recall ) ) ) . ) ) (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN during) (NP (NP (DT the) (NN season)) (CC and) (NP (FW i) (FW guess)))) (PP (IN at) (IN at) (NP (NP (PRP$ your) (NN level)) (SBAR (S (INTJ (UH uh)) (NP (PRP you)) (VP (VBP lose) (NP (PRP them)) (PP (TO to) (NP (DT the) (JJ next) (NN level))) (SBAR (IN if) (S (SBAR (IN if) (S (NP (PRP they)) (VP (VBP decide) (S (VP (TO to) (VP (VB recall) (NP (DT the) (DT the) (NN parent) (NN team)))))))) (NP (DT the) (NNPS Braves)) (VP (VBP decide) (S (VP (TO to) (VP (VB call) (S (VP (TO to) (VP (VB recall) (NP (DT a) (NN guy)) (PP (IN from) (NP (NP (RB triple) (DT A)) (SBAR (S (S (ADVP (RB then)) (NP (DT a) (JJ double) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))) (CC and) (S (NP (DT a) (JJ single) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him)))))))))))))))))))))))))))) (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT the) (NNS things)) (PP (TO to) (NP (DT the) (JJ following) (NN level))) (SBAR (IN if) (S (NP (DT the) (NNS people)) (VP (VBP recall))))) (. .))) you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him You lose the things to the following level if the people recall. entailment entailment 2 134793 134793e fiction ( ( One ( of ( our number ) ) ) ( ( will ( ( ( carry out ) ( your instructions ) ) minutely ) ) . ) ) ( ( ( A member ) ( of ( my team ) ) ) ( ( will ( ( execute ( your orders ) ) ( with ( immense precision ) ) ) ) . ) ) (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PRP$ our) (NN number)))) (VP (MD will) (VP (VB carry) (PRT (RP out)) (NP (PRP$ your) (NNS instructions)) (ADVP (RB minutely)))) (. .))) (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN of) (NP (PRP$ my) (NN team)))) (VP (MD will) (VP (VB execute) (NP (PRP$ your) (NNS orders)) (PP (IN with) (NP (JJ immense) (NN precision))))) (. .))) One of our number will carry out your instructions minutely. A member of my team will execute your orders with immense precision. entailment entailment 3 37397 37397e fiction ( ( How ( ( ( do you ) know ) ? ) ) ( ( All this ) ( ( ( is ( their information ) ) again ) . ) ) ) ( ( This information ) ( ( belongs ( to them ) ) . ) ) (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do) (NP (PRP you)) (VP (VB know))) (. ?)) (NP (PDT All) (DT this)) (VP (VBZ is) (NP (PRP$ their) (NN information)) (ADVP (RB again))) (. .))) (ROOT (S (NP (DT This) (NN information)) (VP (VBZ belongs) (PP (TO to) (NP (PRP them)))) (. .))) How do you know? All this is their information again. This information belongs to them. entailment entailment ... train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a12\u5217, \u7b2c\u4e00\u5217\u4ee3\u8868\u6587\u672c\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u5206\u522b\u4ee3\u8868\u53e5\u5b50\u5bf9\u7684\u4e0d\u540c\u7c7b\u578bid; \u7b2c\u56db\u5217\u4ee3\u8868\u53e5\u5b50\u5bf9\u7684\u6765\u6e90; \u7b2c\u4e94\u5217\u548c\u7b2c\u516d\u5217\u4ee3\u8868\u5177\u6709\u53e5\u6cd5\u7ed3\u6784\u5206\u6790\u7684\u53e5\u5b50\u5bf9\u8868\u793a; \u7b2c\u4e03\u5217\u548c\u7b2c\u516b\u5217\u4ee3\u8868\u5177\u6709\u53e5\u6cd5\u7ed3\u6784\u548c\u8bcd\u6027\u6807\u6ce8\u7684\u53e5\u5b50\u5bf9\u8868\u793a, \u7b2c\u4e5d\u5217\u548c\u7b2c\u5341\u5217\u4ee3\u8868\u539f\u59cb\u7684\u53e5\u5b50\u5bf9, \u7b2c\u5341\u4e00\u548c\u7b2c\u5341\u4e8c\u5217\u4ee3\u8868\u4e0d\u540c\u6807\u51c6\u7684\u6807\u6ce8\u65b9\u6cd5\u4ea7\u751f\u7684\u6807\u7b7e, \u5728\u8fd9\u91cc\uff0c\u4ed6\u4eec\u59cb\u7ec8\u76f8\u540c, \u4e00\u5171\u6709\u4e09\u79cd\u7c7b\u578b\u7684\u6807\u7b7e, neutral\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u65e2\u4e0d\u77db\u76fe\u4e5f\u4e0d\u8574\u542b, entailment\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u5177\u6709\u8574\u542b\u5173\u7cfb, contradiction\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u89c2\u70b9\u77db\u76fe. test_matched.tsv\u6570\u636e\u6837\u5f0f: index promptID pairID genre sentence1_binary_parse sentence2_binary_parse sentence1_parse sentence2_parse sentence1 sentence2 0 31493 31493 travel ( ( ( ( ( ( ( ( Hierbas , ) ( ans seco ) ) , ) ( ans dulce ) ) , ) and ) frigola ) ( ( ( are just ) ( ( a ( few names ) ) ( worth ( ( keeping ( a look-out ) ) for ) ) ) ) . ) ) ( Hierbas ( ( is ( ( a name ) ( worth ( ( looking out ) for ) ) ) ) . ) ) (ROOT (S (NP (NP (NNS Hierbas)) (, ,) (NP (NN ans) (NN seco)) (, ,) (NP (NN ans) (NN dulce)) (, ,) (CC and) (NP (NN frigola))) (VP (VBP are) (ADVP (RB just)) (NP (NP (DT a) (JJ few) (NNS names)) (PP (JJ worth) (S (VP (VBG keeping) (NP (DT a) (NN look-out)) (PP (IN for))))))) (. .))) (ROOT (S (NP (NNS Hierbas)) (VP (VBZ is) (NP (NP (DT a) (NN name)) (PP (JJ worth) (S (VP (VBG looking) (PRT (RP out)) (PP (IN for))))))) (. .))) Hierbas, ans seco, ans dulce, and frigola are just a few names worth keeping a look-out for. Hierbas is a name worth looking out for. 1 92164 92164 government ( ( ( The extent ) ( of ( the ( behavioral effects ) ) ) ) ( ( would ( ( depend ( in ( part ( on ( ( the structure ) ( of ( ( ( the ( individual ( account program ) ) ) and ) ( any limits ) ) ) ) ) ) ) ) ( on ( accessing ( the funds ) ) ) ) ) . ) ) ( ( Many people ) ( ( would ( be ( very ( unhappy ( to ( ( loose control ) ( over ( their ( own money ) ) ) ) ) ) ) ) ) . ) ) (ROOT (S (NP (NP (DT The) (NN extent)) (PP (IN of) (NP (DT the) (JJ behavioral) (NNS effects)))) (VP (MD would) (VP (VB depend) (PP (IN in) (NP (NP (NN part)) (PP (IN on) (NP (NP (DT the) (NN structure)) (PP (IN of) (NP (NP (DT the) (JJ individual) (NN account) (NN program)) (CC and) (NP (DT any) (NNS limits)))))))) (PP (IN on) (S (VP (VBG accessing) (NP (DT the) (NNS funds))))))) (. .))) (ROOT (S (NP (JJ Many) (NNS people)) (VP (MD would) (VP (VB be) (ADJP (RB very) (JJ unhappy) (PP (TO to) (NP (NP (JJ loose) (NN control)) (PP (IN over) (NP (PRP$ their) (JJ own) (NN money)))))))) (. .))) The extent of the behavioral effects would depend in part on the structure of the individual account program and any limits on accessing the funds. Many people would be very unhappy to loose control over their own money. 2 9662 9662 government ( ( ( Timely access ) ( to information ) ) ( ( is ( in ( ( the ( best interests ) ) ( of ( ( ( both GAO ) and ) ( the agencies ) ) ) ) ) ) . ) ) ( It ( ( ( is ( in ( ( everyone 's ) ( best interest ) ) ) ) ( to ( ( have access ) ( to ( information ( in ( a ( timely manner ) ) ) ) ) ) ) ) . ) ) (ROOT (S (NP (NP (JJ Timely) (NN access)) (PP (TO to) (NP (NN information)))) (VP (VBZ is) (PP (IN in) (NP (NP (DT the) (JJS best) (NNS interests)) (PP (IN of) (NP (NP (DT both) (NNP GAO)) (CC and) (NP (DT the) (NNS agencies))))))) (. .))) (ROOT (S (NP (PRP It)) (VP (VBZ is) (PP (IN in) (NP (NP (NN everyone) (POS 's)) (JJS best) (NN interest))) (S (VP (TO to) (VP (VB have) (NP (NN access)) (PP (TO to) (NP (NP (NN information)) (PP (IN in) (NP (DT a) (JJ timely) (NN manner))))))))) (. .))) Timely access to information is in the best interests of both GAO and the agencies. It is in everyone's best interest to have access to information in a timely manner. 3 5991 5991 travel ( ( Based ( in ( ( the ( Auvergnat ( spa town ) ) ) ( of Vichy ) ) ) ) ( , ( ( the ( French government ) ) ( often ( ( ( ( proved ( more zealous ) ) ( than ( its masters ) ) ) ( in ( ( ( suppressing ( civil liberties ) ) and ) ( ( drawing up ) ( anti-Jewish legislation ) ) ) ) ) . ) ) ) ) ) ( ( The ( French government ) ) ( ( passed ( ( anti-Jewish laws ) ( aimed ( at ( helping ( the Nazi ) ) ) ) ) ) . ) ) (ROOT (S (PP (VBN Based) (PP (IN in) (NP (NP (DT the) (NNP Auvergnat) (NN spa) (NN town)) (PP (IN of) (NP (NNP Vichy)))))) (, ,) (NP (DT the) (JJ French) (NN government)) (ADVP (RB often)) (VP (VBD proved) (NP (JJR more) (NNS zealous)) (PP (IN than) (NP (PRP$ its) (NNS masters))) (PP (IN in) (S (VP (VP (VBG suppressing) (NP (JJ civil) (NNS liberties))) (CC and) (VP (VBG drawing) (PRT (RP up)) (NP (JJ anti-Jewish) (NN legislation))))))) (. .))) (ROOT (S (NP (DT The) (JJ French) (NN government)) (VP (VBD passed) (NP (NP (JJ anti-Jewish) (NNS laws)) (VP (VBN aimed) (PP (IN at) (S (VP (VBG helping) (NP (DT the) (JJ Nazi)))))))) (. .))) Based in the Auvergnat spa town of Vichy, the French government often proved more zealous than its masters in suppressing civil liberties and drawing up anti-Jewish legislation. The French government passed anti-Jewish laws aimed at helping the Nazi. ... test_matched.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test_matched.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a10\u5217, \u4e0etrain.tsv\u7684\u524d10\u5217\u542b\u4e49\u76f8\u540c. (MNLI/SNLI)\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u591a\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC","title":"2.6 (MNLI/SNLI)\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#27-qnlirtewnli","text":"\u6570\u636e\u96c6\u91ca\u4e49: QNLI(Qusetion-answering NLI\uff0c\u95ee\u7b54\u81ea\u7136\u8bed\u8a00\u63a8\u65ad)\uff0c\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u4efb\u52a1\u3002QNLI\u662f\u4ece\u53e6\u4e00\u4e2a\u6570\u636e\u96c6The Stanford Question Answering Dataset(\u65af\u5766\u798f\u95ee\u7b54\u6570\u636e\u96c6, SQuAD 1.0)[ 3] \u8f6c\u6362\u800c\u6765\u7684. RTE(The Recognizing Textual Entailment datasets\uff0c\u8bc6\u522b\u6587\u672c\u8574\u542b\u6570\u636e\u96c6)\uff0c\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u4efb\u52a1\uff0c\u5b83\u662f\u5c06\u4e00\u7cfb\u5217\u7684\u5e74\u5ea6\u6587\u672c\u8574\u542b\u6311\u6218\u8d5b\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6574\u5408\u5408\u5e76\u800c\u6765\u7684. WNLI(Winograd NLI\uff0cWinograd\u81ea\u7136\u8bed\u8a00\u63a8\u65ad)\uff0c\u81ea\u7136\u8bed\u8a00\u63a8\u65ad\u4efb\u52a1\uff0c\u6570\u636e\u96c6\u6765\u81ea\u4e8e\u7ade\u8d5b\u6570\u636e\u7684\u8f6c\u6362\u3002 \u672c\u8d28: QNLI\u662f\u4e8c\u5206\u7c7b\u4efb\u52a1. RTE\u662f\u4e8c\u5206\u7c7b\u4efb\u52a1. WNLI\u662f\u4e8c\u5206\u7c7b\u4efb\u52a1. QNLI, RTE, WNLI\u4e09\u4e2a\u6570\u636e\u96c6\u7684\u6837\u5f0f\u57fa\u672c\u76f8\u540c. - (QNLI/RTE/WNLI)/ - dev.tsv - test.tsv - train.tsv \u6587\u4ef6\u6837\u5f0f\u8bf4\u660e: \u5728\u4f7f\u7528\u4e2d\u5e38\u7528\u5230\u7684\u6587\u4ef6\u662ftrain.tsv, dev.tsv, test.tsv, \u5206\u522b\u4ee3\u8868\u8bad\u7ec3\u96c6, \u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6. \u5176\u4e2dtrain.tsv\u4e0edev.tsv\u6570\u636e\u6837\u5f0f\u76f8\u540c, \u90fd\u662f\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e, \u5176\u4e2dtest.tsv\u662f\u4e0d\u5e26\u6709\u6807\u7b7e\u7684\u6570\u636e. QNLI\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f: index question sentence label 0 When did the third Digimon series begin? Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese. not_entailment 1 Which missile batteries often have individual launchers several kilometres from one another? When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs. not_entailment 2 What two things does Popper argue Tarski's theory involves in an evaluation of truth? He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. entailment 3 What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians? On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854. entailment 4 What famous palace is located in London? London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0\u00b0 longitude, and GMT). not_entailment 5 When is the term 'German dialects' used in regard to the German language? When talking about the German language, the term German dialects is only used for the traditional regional varieties. entailment 6 What was the name of the island the English traded to the Dutch in return for New Amsterdam? At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) in North America in exchange for Dutch control of Run, an Indonesian island. entailment 7 How were the Portuguese expelled from Myanmar? From the 1720s onward, the kingdom was beset with repeated Meithei raids into Upper Myanmar and a nagging rebellion in Lan Na. not_entailment 8 What does the word 'customer' properly apply to? The bill also required rotation of principal maintenance inspectors and stipulated that the word \"customer\" properly applies to the flying public, not those entities regulated by the FAA. entailment ... RTE\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f: index sentence1 sentence2 label 0 No Weapons of Mass Destruction Found in Iraq Yet. Weapons of Mass Destruction Found in Iraq. not_entailment 1 A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.Pope Benedict XVI is the new leader of the Roman Catholic Church. entailment 2 Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients. Herceptin can be used to treat breast cancer. entailment 3 Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment. The previous name of Ho Chi Minh City was Saigon.entailment 4 A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later. Paul Stewart Hutchinson is accused of having stabbed a girl. not_entailment 5 Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, where he was released by police after being detained for 24 hours. Bakri was briefly detained, but was released. entailment 6 Nearly 4 million children who have at least one parent who entered the U.S. illegally were born in the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That's about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found. Three quarters of U.S. illegal immigrants have children. not_entailment 7 Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together. Aristide had Prime Minister Robert Malval murdered in Port-au-Prince. not_entailment 8 WASHINGTON -- A newly declassified narrative of the Bush administration's advice to the CIA on harsh interrogations shows that the small group of Justice Department lawyers who wrote memos authorizing controversial interrogation techniques were operating not on their own but with direction from top administration officials, including then-Vice President Dick Cheney and national security adviser Condoleezza Rice. At the same time, the narrative suggests that then-Defense Secretary Donald H. Rumsfeld and then-Secretary of State Colin Powell were largely left out of the decision-making process. Dick Cheney was the Vice President of Bush. entailment WNLI\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f: index sentence1 sentence2 label 0 I stuck a pin through a carrot. When I pulled the pin out, it had a hole. The carrot had a hole. 1 1 John couldn't see the stage with Billy in front of him because he is so short. John is so short. 1 2 The police arrested all of the gang members. They were trying to stop the drug trade in the neighborhood. The police were trying to stop the drug trade in the neighborhood. 1 3 Steve follows Fred's example in everything. He influences him hugely. Steve influences him hugely. 0 4 When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth. mother was careful not to disturb her, undressing and climbing back into her berth. 0 5 George got free tickets to the play, but he gave them to Eric, because he was particularly eager to see it. George was particularly eager to see it. 0 6 John was jogging through the park when he saw a man juggling watermelons. He was very impressive. John was very impressive. 0 7 I couldn't put the pot on the shelf because it was too tall. The pot was too tall. 1 8 We had hoped to place copies of our newsletter on all the chairs in the auditorium, but there were simply not enough of them. There were simply not enough copies of the newsletter. 1 (QNLI/RTE/WNLI)\u4e2d\u7684train.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: train.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a4\u5217, \u7b2c\u4e00\u5217\u4ee3\u8868\u6587\u672c\u6570\u636e\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u4ee3\u8868\u9700\u8981\u8fdb\u884c'\u662f\u5426\u8574\u542b'\u5224\u5b9a\u7684\u53e5\u5b50\u5bf9; \u7b2c\u56db\u5217\u6570\u636e\u4ee3\u8868\u4e24\u4e2a\u53e5\u5b50\u662f\u5426\u5177\u6709\u8574\u542b\u5173\u7cfb, 0/not_entailment\u4ee3\u8868\u4e0d\u662f\u8574\u542b\u5173\u7cfb, 1/entailment\u4ee3\u8868\u8574\u542b\u5173\u7cfb. QNLI\u4e2d\u7684test.tsv\u6570\u636e\u6837\u5f0f: index question sentence 0 What organization is devoted to Jihad against Israel? For some decades prior to the First Palestine Intifada in 1987, the Muslim Brotherhood in Palestine took a \"quiescent\" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's \"indulgence\" to build up a network of mosques and charitable organizations. 1 In what century was the Yarrow-Schlick-Tweedy balancing system used? In the late 19th century, the Yarrow-Schlick-Tweedy balancing 'system' was used on some marine triple expansion engines. 2 The largest brand of what store in the UK is located in Kingston Park? Close to Newcastle, the largest indoor shopping centre in Europe, the MetroCentre, is located in Gateshead. 3 What does the IPCC rely on for research? In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included. 4 What is the principle about relating spin and space variables? Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive. 5 Which network broadcasted Super Bowl 50 in the U.S.? CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game. 6 What did the museum acquire from the Royal College of Science? To link this to the rest of the museum, a new entrance building was constructed on the site of the former boiler house, the intended site of the Spiral, between 1978 and 1982. 7 What is the name of the old north branch of the Rhine? From Wijk bij Duurstede, the old north branch of the Rhine is called Kromme Rijn (\"Bent Rhine\") past Utrecht, first Leidse Rijn (\"Rhine of Leiden\") and then, Oude Rijn (\"Old Rhine\"). 8 What was one of Luther's most personal writings? It remains in use today, along with Luther's hymns and his translation of the Bible. ... (RTE/WNLI)\u4e2d\u7684test.tsv\u6570\u636e\u6837\u5f0f: index sentence1 sentence2 0 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when Maude and Dora came in sight. 1 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the trains came in sight. 2 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the puffs came in sight. 3 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the roars came in sight. 4 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the whistles came in sight. 5 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the horses came in sight. 6 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. Maude and Dora saw a train coming. 7 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. The trains saw a train coming. 8 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. The puffs saw a train coming. ... (QNLI/RTE/WNLI)\u4e2d\u7684test.tsv\u6570\u636e\u6837\u5f0f\u8bf4\u660e: test.tsv\u4e2d\u7684\u6570\u636e\u5185\u5bb9\u5171\u5206\u4e3a3\u5217, \u7b2c\u4e00\u5217\u6570\u636e\u4ee3\u8868\u6bcf\u6761\u6587\u672c\u6570\u636e\u7684\u7d22\u5f15; \u7b2c\u4e8c\u5217\u548c\u7b2c\u4e09\u5217\u6570\u636e\u4ee3\u8868\u9700\u8981\u8fdb\u884c'\u662f\u5426\u8574\u542b'\u5224\u5b9a\u7684\u53e5\u5b50\u5bf9. (QNLI/RTE/WNLI)\u6570\u636e\u96c6\u7684\u4efb\u52a1\u7c7b\u578b: \u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 \u8bc4\u4f30\u6307\u6807\u4e3a: ACC","title":"2.7 (QNLI/RTE/WNLI)\u6570\u636e\u96c6\u6587\u4ef6\u6837\u5f0f"},{"location":"05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html#3","text":"\u5b66\u4e60\u4e86GLUE\u6570\u636e\u96c6\u5408\u7684\u4ecb\u7ecd: GLUE\u7531\u7ebd\u7ea6\u5927\u5b66, \u534e\u76db\u987f\u5927\u5b66, Google\u8054\u5408\u63a8\u51fa, \u6db5\u76d6\u4e0d\u540cNLP\u4efb\u52a1\u7c7b\u578b, \u622a\u6b62\u81f32020\u5e741\u6708\u5176\u4e2d\u5305\u62ec11\u4e2a\u5b50\u4efb\u52a1\u6570\u636e\u96c6, \u6210\u4e3a\u8861\u91cfNLP\u7814\u7a76\u53d1\u5c55\u7684\u8861\u91cf\u6807\u51c6. GLUE\u6570\u636e\u96c6\u5408\u5305\u542b\u4ee5\u4e0b\u6570\u636e\u96c6: CoLA \u6570\u636e\u96c6 SST-2 \u6570\u636e\u96c6 MRPC \u6570\u636e\u96c6 STS-B \u6570\u636e\u96c6 QQP \u6570\u636e\u96c6 MNLI \u6570\u636e\u96c6 SNLI \u6570\u636e\u96c6 QNLI \u6570\u636e\u96c6 RTE \u6570\u636e\u96c6 WNLI \u6570\u636e\u96c6","title":"3 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1huggingface\u5e73\u53f0\u4f7f\u7528 1 huggingface\u4ecb\u7ecd \u00b6 Huggingface\u603b\u90e8\u4f4d\u4e8e\u7ebd\u7ea6\uff0c\u662f\u4e00\u5bb6\u4e13\u6ce8\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u4eba\u5de5\u667a\u80fd\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u521b\u4e1a\u516c\u53f8\u3002\u4ed6\u4eec\u6240\u63d0\u4f9b\u7684\u804a\u5929\u673a\u5668\u4eba\u6280\u672f\u4e00\u76f4\u9887\u53d7\u6b22\u8fce\uff0c\u4f46\u66f4\u51fa\u540d\u7684\u662f\u4ed6\u4eec\u5728NLP\u5f00\u6e90\u793e\u533a\u4e0a\u7684\u8d21\u732e\u3002Huggingface\u4e00\u76f4\u81f4\u529b\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u6280\u672f\u7684\u5e73\u6c11\u5316(democratize)\uff0c\u5e0c\u671b\u6bcf\u4e2a\u4eba\u90fd\u80fd\u7528\u4e0a\u6700\u5148\u8fdb(SOTA, state-of-the-art)\u7684NLP\u6280\u672f\uff0c\u800c\u975e\u56f0\u7a98\u4e8e\u8bad\u7ec3\u8d44\u6e90\u7684\u532e\u4e4f\u3002\u540c\u65f6Hugging Face\u4e13\u6ce8\u4e8eNLP\u6280\u672f\uff0c\u62e5\u6709\u5927\u578b\u7684\u5f00\u6e90\u793e\u533a\u3002\u5c24\u5176\u662f\u5728github\u4e0a\u5f00\u6e90\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5e93 Transformers\uff0c\u5df2\u88ab\u4e0b\u8f7d\u8d85\u8fc7\u4e00\u767e\u4e07\u6b21\uff0cgithub\u4e0a\u8d85\u8fc724000\u4e2astar\u3002Transformers \u63d0\u4f9b\u4e86NLP\u9886\u57df\u5927\u91cfstate-of-art\u7684 \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u7684\u6a21\u578b\u548c\u8c03\u7528\u6846\u67b6\u3002 2 \u4f7f\u7528\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65: \u5728https://huggingface.co/join\u4e0a\u521b\u5efa\u4e00\u4e2a\u5e10\u6237 \u7b2c\u4e8c\u6b65: \u5728\u53ef\u89c6\u5316\u754c\u9762\u767b\u9646\u7528\u6237 \u7b2c\u4e09\u6b65: \u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93 \u7b2c\u56db\u6b65: \u901a\u8fc7git\u628a\u672c\u5730\u6a21\u578b\uff0c\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u7684\u6a21\u578b\u4ed3\u5e93\u4e2d \u7b2c\u4e94\u6b65: \u901a\u8fc7git clone\u8fdb\u884c\u6a21\u578b\u4e0b\u8f7d \u7b2c\u516d\u6b65: \u52a0\u8f7d\u4e0b\u8f7d\u7684\u6a21\u578b 2.1 \u521b\u5efa\u4e00\u4e2a\u5e10\u6237 \u00b6 \u5728 https://huggingface.co/join \u4e0a\u521b\u5efa\u4e00\u4e2a\u5e10\u6237 2.2 \u767b\u5f55 \u00b6 2.3 \u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93 \u00b6 \u5728huggingFace\u5e73\u53f0\u4e0a\u6ce8\u518c\u5b8c\u6bd5\u540e\uff0c\u4f1a\u5f39\u51fa\u6b22\u8fce\u9875\u9762\uff1a https://huggingface.co/welcome \u8be5\u9875\u9762\u663e\u793a\u4e86\u8be6\u7ec6\u7684\u4e0a\u4f20\u6a21\u578b\uff0c\u4e0b\u8f7d\u6a21\u578b\u7684\u65b9\u6cd5\u3002 \u8be6\u7ec6\u5982\u4e0b\uff1a \u901a\u8fc7\u754c\u9762\u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93 \u70b9\u51fb\u4e2a\u4eba\u5934\u50cf\uff0c\u70b9\u51fb\u521b\u5efa\u6a21\u578b\u547d\u4ee4\u3010new Mode\u3011 \u8f93\u5165\u3010\u81ea\u5df1\u540d\u79f0\u3011\u3001\u3010\u6a21\u578b\u540d\u79f0\u3011 \u663e\u793a\u81ea\u5df1\u521b\u5efa\u7684\u6a21\u578b 2.4 \u4e0a\u4f20\u672c\u5730\u6a21\u578b\u5230\u5e73\u53f0 \u00b6 \u901a\u8fc7git\u628a\u672c\u5730\u6a21\u578b\uff0c\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u7684\u6a21\u578b\u4ed3\u5e93\u4e2d 1 \u9875\u9762\u53d1\u5e03\u6b65\u9aa4\u4ecb\u7ecd \u00b6 2 git clone\u64cd\u4f5c \u00b6 \u5148\u901a\u8fc7git clone\u64cd\u4f5c\u628ahuggingface\u670d\u52a1\u5668\u4e0a\u7684\u6587\u4ef6\u76ee\u5f55\u7ed9\u201c\u62c9\u201d\u4e0b\u6765\u5728\u672c\u5730\u8def\u5f84\u4e0b\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a # xxx/mymodel04 --> \u8fd9\u4e2a\u662f\u4f60\u5728huggingface\u4e0a\u521b\u5efa\u7684\u4ee3\u7801\u4ed3\u5e93, \u6839\u636e\u81ea\u5df1\u7684\u60c5\u51b5\u9002\u5f53\u66f4\u6362\u4e00\u4e0b. git clone https : // huggingface . co / xxx / mymodel04 \u6ce8\u610f\u70b9: \u5728\u672c\u5730\u4f1a\u51fa\u73b0\u4e00\u4e2amymodel04\u6587\u4ef6\u5939 \u5728\u6267\u884cgit clone\u4e4b\u524d\u786e\u4fdd\u672c\u5730\u6587\u4ef6\u5939\u662f\u5426\u5df2\u7ecf\u5b58\u5728mymodel04\uff0c\u907f\u514d\u672c\u5730\u6587\u4ef6\u88ab\u8986\u76d6\u3002\u6216\u8005\u628a\u5df2\u7ecf\u5b58\u5728\u7684mymodel04\u76ee\u5f55\u4fee\u6539\u540d\u5b57. 3 \u628a\u6211\u4eec\u8981\u4e0a\u4f20\u7684\u6a21\u578b\u6587\u4ef6copy\u5230\u672c\u5730mymodel04\u6587\u4ef6\u5939\u4e2d \u00b6 \u5148\u5c06\u76ee\u5f55\u5148\u5207\u6362\u81f3mymodel04\u6587\u4ef6\u5939\u4e2d cd mymodel04 \u6839\u636e\u76ee\u5f55\u7ed3\u6784\uff0c\u9009\u4e2d\u628abert_finetuning_test\u76ee\u5f55\u4e0b\u7684\u6a21\u578b\u6587\u4ef6\u4e0a\u4f20\u5230huggingFace\u5e73\u53f0\uff0c\u9700\u8981\u628abert_finetuning_test\u76ee\u5f55\u4e0b\u7684\u6a21\u578b\u6587\u4ef6\uff0ccopy\u5230mymodel04\u76ee\u5f55\u4e0b\u3002 cp - r / root / transformers / examples / pytorch / text - classification / bert - base - uncased - finetuning . 4 \u4e0a\u4f20\u672c\u5730mymodel04\u6587\u4ef6\u5939\u4e2d\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u5230\u670d\u52a1\u5668mymodel04\u4e2d \u00b6 git add . # \u628a\u672c\u5730\u5f85\u4e0a\u4f20\u7684\u6a21\u578b\u6587\u4ef6\u4e0ehugging\u5e73\u53f0\u5efa\u7acb\u5173\u8054 git commit - m \"commit from $USER\" # \u6dfb\u52a0\u8bc4\u6ce8 git push # \u5411huggingface\u5e73\u53f0\u4e0a\u4f20\u6a21\u578b\u6587\u4ef6 \u6ce8\u610f\u70b9: git push \u5411\u670d\u52a1\u5668\u4e0a\u4f20\u6a21\u578b\u6587\u4ef6\uff0c\u9700\u8981\u4e24\u6b21\u8f93\u5165\u5bc6\u7801 5 \u786e\u8ba4\u6a21\u578b\u662f\u5426\u5df2\u7ecf\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u4e0a \u00b6 2.5 \u901a\u8fc7git clone\u8fdb\u884c\u6a21\u578b\u4e0b\u8f7d \u00b6 git clone https : // huggingface . co / xxx / mymodel4 2.6 \u52a0\u8f7d\u4e0b\u8f7d\u7684\u6a21\u578b \u00b6 import torch from transformers import AutoModel , AutoTokenizer # \u7f51\u7edc\u52a0\u8f7d tokenizer = AutoTokenizer . from_pretrained ( 'xxx/mymodel4' ) model = AutoModel . from_pretrained ( 'xxx/mymodel4' ) index = tokenizer . encode ( \"Talk is cheap\" , \"Please show me your code!\" ) # 102\u662fbert\u6a21\u578b\u4e2d\u7684\u95f4\u9694(\u7ed3\u675f)\u7b26\u53f7\u7684\u6570\u503c\u6620\u5c04 mark = 102 # \u627e\u5230\u7b2c\u4e00\u4e2a102\u7684\u7d22\u5f15, \u5373\u53e5\u5b50\u5bf9\u7684\u95f4\u9694\u7b26\u53f7 k = index . index ( mark ) # \u53e5\u5b50\u5bf9\u5206\u5272id\u5217\u8868, \u75310\uff0c1\u7ec4\u6210, 0\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50, 1\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u53e5\u5b50 segments_ids = [ 0 ] * ( k + 1 ) + [ 1 ] * ( len ( index ) - k - 1 ) # \u8f6c\u5316\u4e3atensor tokens_tensor = torch . tensor ([ index ]) segments_tensors = torch . tensor ([ segments_ids ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor , token_type_ids = segments_tensors ) # \u6253\u5370\u9884\u6d4b\u7ed3\u679c\u4ee5\u53ca\u5f20\u91cf\u5c3a\u5bf8 print ( result ) print ( result [ 0 ] . shape ) \u8f93\u51fa\u6548\u679c: (tensor([[[-0.1591, 0.0816, 0.4366, ..., 0.0307, -0.0419, 0.3326], [-0.3387, -0.0445, 0.9261, ..., -0.0232, -0.0023, 0.2407], [-0.0427, -0.1688, 0.5533, ..., -0.1092, 0.1071, 0.4287], ..., [-0.1800, -0.3889, -0.1001, ..., -0.1369, 0.0469, 0.9429], [-0.2970, -0.0023, 0.1976, ..., 0.3776, -0.0069, 0.2029], [ 0.7061, 0.0102, -0.4738, ..., 0.2246, -0.7604, -0.2503]]]), tensor([[-3.5925e-01, 2.0294e-02, -2.3487e-01, 4.5763e-01, -6.1821e-02, 2.4697e-02, 3.8172e-01, -1.8212e-01, 3.4533e-01, -9.7177e-01, 1.1063e-01, 7.8944e-02, 8.2582e-01, 1.9020e-01, 6.5513e-01, -1.8114e-01, 3.9617e-02, -5.6230e-02, 1.5207e-01, -3.2552e-01, ... 1.4417e-01, 3.0337e-01, -6.6146e-01, -9.6959e-02, 8.9790e-02, 1.2345e-01, -5.9831e-02, 2.2399e-01, 8.2549e-02, 6.7749e-01, 1.4473e-01, 5.4490e-01, 5.9272e-01, 3.4453e-01, -8.9982e-02, -1.2631e-01, -1.9465e-01, 6.5992e-01]])) torch.Size([1, 12, 768]) 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u901a\u8fc7\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u540e\u6a21\u578b\u7684\u4f7f\u7528\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u5728https://huggingface.co/join\u4e0a\u521b\u5efa\u4e00\u4e2a\u5e10\u6237 \u7b2c\u4e8c\u6b65: \u5728\u53ef\u89c6\u5316\u754c\u9762\u767b\u9646\u7528\u6237 \u7b2c\u4e09\u6b65: \u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93 \u7b2c\u56db\u6b65: \u901a\u8fc7git\u628a\u672c\u5730\u6a21\u578b\uff0c\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u7684\u6a21\u578b\u4ed3\u5e93\u4e2d \u7b2c\u4e94\u6b65: \u901a\u8fc7git clone\u8fdb\u884c\u6a21\u578b\u4e0b\u8f7d \u7b2c\u516d\u6b65: \u52a0\u8f7d\u4e0b\u8f7d\u7684\u6a21\u578b","title":"11 huggingface\u5e73\u53f0\u4f7f\u7528\u6307\u5357 old"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#_1","text":"\u638c\u63e1huggingface\u5e73\u53f0\u4f7f\u7528","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#1-huggingface","text":"Huggingface\u603b\u90e8\u4f4d\u4e8e\u7ebd\u7ea6\uff0c\u662f\u4e00\u5bb6\u4e13\u6ce8\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u4eba\u5de5\u667a\u80fd\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u521b\u4e1a\u516c\u53f8\u3002\u4ed6\u4eec\u6240\u63d0\u4f9b\u7684\u804a\u5929\u673a\u5668\u4eba\u6280\u672f\u4e00\u76f4\u9887\u53d7\u6b22\u8fce\uff0c\u4f46\u66f4\u51fa\u540d\u7684\u662f\u4ed6\u4eec\u5728NLP\u5f00\u6e90\u793e\u533a\u4e0a\u7684\u8d21\u732e\u3002Huggingface\u4e00\u76f4\u81f4\u529b\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u6280\u672f\u7684\u5e73\u6c11\u5316(democratize)\uff0c\u5e0c\u671b\u6bcf\u4e2a\u4eba\u90fd\u80fd\u7528\u4e0a\u6700\u5148\u8fdb(SOTA, state-of-the-art)\u7684NLP\u6280\u672f\uff0c\u800c\u975e\u56f0\u7a98\u4e8e\u8bad\u7ec3\u8d44\u6e90\u7684\u532e\u4e4f\u3002\u540c\u65f6Hugging Face\u4e13\u6ce8\u4e8eNLP\u6280\u672f\uff0c\u62e5\u6709\u5927\u578b\u7684\u5f00\u6e90\u793e\u533a\u3002\u5c24\u5176\u662f\u5728github\u4e0a\u5f00\u6e90\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5e93 Transformers\uff0c\u5df2\u88ab\u4e0b\u8f7d\u8d85\u8fc7\u4e00\u767e\u4e07\u6b21\uff0cgithub\u4e0a\u8d85\u8fc724000\u4e2astar\u3002Transformers \u63d0\u4f9b\u4e86NLP\u9886\u57df\u5927\u91cfstate-of-art\u7684 \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u7684\u6a21\u578b\u548c\u8c03\u7528\u6846\u67b6\u3002","title":"1 huggingface\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#2","text":"\u7b2c\u4e00\u6b65: \u5728https://huggingface.co/join\u4e0a\u521b\u5efa\u4e00\u4e2a\u5e10\u6237 \u7b2c\u4e8c\u6b65: \u5728\u53ef\u89c6\u5316\u754c\u9762\u767b\u9646\u7528\u6237 \u7b2c\u4e09\u6b65: \u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93 \u7b2c\u56db\u6b65: \u901a\u8fc7git\u628a\u672c\u5730\u6a21\u578b\uff0c\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u7684\u6a21\u578b\u4ed3\u5e93\u4e2d \u7b2c\u4e94\u6b65: \u901a\u8fc7git clone\u8fdb\u884c\u6a21\u578b\u4e0b\u8f7d \u7b2c\u516d\u6b65: \u52a0\u8f7d\u4e0b\u8f7d\u7684\u6a21\u578b","title":"2 \u4f7f\u7528\u6b65\u9aa4"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#21","text":"\u5728 https://huggingface.co/join \u4e0a\u521b\u5efa\u4e00\u4e2a\u5e10\u6237","title":"2.1 \u521b\u5efa\u4e00\u4e2a\u5e10\u6237"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#22","text":"","title":"2.2 \u767b\u5f55"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#23-huggingface","text":"\u5728huggingFace\u5e73\u53f0\u4e0a\u6ce8\u518c\u5b8c\u6bd5\u540e\uff0c\u4f1a\u5f39\u51fa\u6b22\u8fce\u9875\u9762\uff1a https://huggingface.co/welcome \u8be5\u9875\u9762\u663e\u793a\u4e86\u8be6\u7ec6\u7684\u4e0a\u4f20\u6a21\u578b\uff0c\u4e0b\u8f7d\u6a21\u578b\u7684\u65b9\u6cd5\u3002 \u8be6\u7ec6\u5982\u4e0b\uff1a \u901a\u8fc7\u754c\u9762\u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93 \u70b9\u51fb\u4e2a\u4eba\u5934\u50cf\uff0c\u70b9\u51fb\u521b\u5efa\u6a21\u578b\u547d\u4ee4\u3010new Mode\u3011 \u8f93\u5165\u3010\u81ea\u5df1\u540d\u79f0\u3011\u3001\u3010\u6a21\u578b\u540d\u79f0\u3011 \u663e\u793a\u81ea\u5df1\u521b\u5efa\u7684\u6a21\u578b","title":"2.3 \u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#24","text":"\u901a\u8fc7git\u628a\u672c\u5730\u6a21\u578b\uff0c\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u7684\u6a21\u578b\u4ed3\u5e93\u4e2d","title":"2.4 \u4e0a\u4f20\u672c\u5730\u6a21\u578b\u5230\u5e73\u53f0"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#1","text":"","title":"1 \u9875\u9762\u53d1\u5e03\u6b65\u9aa4\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#2-git-clone","text":"\u5148\u901a\u8fc7git clone\u64cd\u4f5c\u628ahuggingface\u670d\u52a1\u5668\u4e0a\u7684\u6587\u4ef6\u76ee\u5f55\u7ed9\u201c\u62c9\u201d\u4e0b\u6765\u5728\u672c\u5730\u8def\u5f84\u4e0b\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a # xxx/mymodel04 --> \u8fd9\u4e2a\u662f\u4f60\u5728huggingface\u4e0a\u521b\u5efa\u7684\u4ee3\u7801\u4ed3\u5e93, \u6839\u636e\u81ea\u5df1\u7684\u60c5\u51b5\u9002\u5f53\u66f4\u6362\u4e00\u4e0b. git clone https : // huggingface . co / xxx / mymodel04 \u6ce8\u610f\u70b9: \u5728\u672c\u5730\u4f1a\u51fa\u73b0\u4e00\u4e2amymodel04\u6587\u4ef6\u5939 \u5728\u6267\u884cgit clone\u4e4b\u524d\u786e\u4fdd\u672c\u5730\u6587\u4ef6\u5939\u662f\u5426\u5df2\u7ecf\u5b58\u5728mymodel04\uff0c\u907f\u514d\u672c\u5730\u6587\u4ef6\u88ab\u8986\u76d6\u3002\u6216\u8005\u628a\u5df2\u7ecf\u5b58\u5728\u7684mymodel04\u76ee\u5f55\u4fee\u6539\u540d\u5b57.","title":"2 git clone\u64cd\u4f5c"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#3-copymymodel04","text":"\u5148\u5c06\u76ee\u5f55\u5148\u5207\u6362\u81f3mymodel04\u6587\u4ef6\u5939\u4e2d cd mymodel04 \u6839\u636e\u76ee\u5f55\u7ed3\u6784\uff0c\u9009\u4e2d\u628abert_finetuning_test\u76ee\u5f55\u4e0b\u7684\u6a21\u578b\u6587\u4ef6\u4e0a\u4f20\u5230huggingFace\u5e73\u53f0\uff0c\u9700\u8981\u628abert_finetuning_test\u76ee\u5f55\u4e0b\u7684\u6a21\u578b\u6587\u4ef6\uff0ccopy\u5230mymodel04\u76ee\u5f55\u4e0b\u3002 cp - r / root / transformers / examples / pytorch / text - classification / bert - base - uncased - finetuning .","title":"3 \u628a\u6211\u4eec\u8981\u4e0a\u4f20\u7684\u6a21\u578b\u6587\u4ef6copy\u5230\u672c\u5730mymodel04\u6587\u4ef6\u5939\u4e2d"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#4-mymodel04mymodel04","text":"git add . # \u628a\u672c\u5730\u5f85\u4e0a\u4f20\u7684\u6a21\u578b\u6587\u4ef6\u4e0ehugging\u5e73\u53f0\u5efa\u7acb\u5173\u8054 git commit - m \"commit from $USER\" # \u6dfb\u52a0\u8bc4\u6ce8 git push # \u5411huggingface\u5e73\u53f0\u4e0a\u4f20\u6a21\u578b\u6587\u4ef6 \u6ce8\u610f\u70b9: git push \u5411\u670d\u52a1\u5668\u4e0a\u4f20\u6a21\u578b\u6587\u4ef6\uff0c\u9700\u8981\u4e24\u6b21\u8f93\u5165\u5bc6\u7801","title":"4 \u4e0a\u4f20\u672c\u5730mymodel04\u6587\u4ef6\u5939\u4e2d\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u5230\u670d\u52a1\u5668mymodel04\u4e2d"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#5-huggingface","text":"","title":"5 \u786e\u8ba4\u6a21\u578b\u662f\u5426\u5df2\u7ecf\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u4e0a"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#25-git-clone","text":"git clone https : // huggingface . co / xxx / mymodel4","title":"2.5  \u901a\u8fc7git clone\u8fdb\u884c\u6a21\u578b\u4e0b\u8f7d"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#26","text":"import torch from transformers import AutoModel , AutoTokenizer # \u7f51\u7edc\u52a0\u8f7d tokenizer = AutoTokenizer . from_pretrained ( 'xxx/mymodel4' ) model = AutoModel . from_pretrained ( 'xxx/mymodel4' ) index = tokenizer . encode ( \"Talk is cheap\" , \"Please show me your code!\" ) # 102\u662fbert\u6a21\u578b\u4e2d\u7684\u95f4\u9694(\u7ed3\u675f)\u7b26\u53f7\u7684\u6570\u503c\u6620\u5c04 mark = 102 # \u627e\u5230\u7b2c\u4e00\u4e2a102\u7684\u7d22\u5f15, \u5373\u53e5\u5b50\u5bf9\u7684\u95f4\u9694\u7b26\u53f7 k = index . index ( mark ) # \u53e5\u5b50\u5bf9\u5206\u5272id\u5217\u8868, \u75310\uff0c1\u7ec4\u6210, 0\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50, 1\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u53e5\u5b50 segments_ids = [ 0 ] * ( k + 1 ) + [ 1 ] * ( len ( index ) - k - 1 ) # \u8f6c\u5316\u4e3atensor tokens_tensor = torch . tensor ([ index ]) segments_tensors = torch . tensor ([ segments_ids ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor , token_type_ids = segments_tensors ) # \u6253\u5370\u9884\u6d4b\u7ed3\u679c\u4ee5\u53ca\u5f20\u91cf\u5c3a\u5bf8 print ( result ) print ( result [ 0 ] . shape ) \u8f93\u51fa\u6548\u679c: (tensor([[[-0.1591, 0.0816, 0.4366, ..., 0.0307, -0.0419, 0.3326], [-0.3387, -0.0445, 0.9261, ..., -0.0232, -0.0023, 0.2407], [-0.0427, -0.1688, 0.5533, ..., -0.1092, 0.1071, 0.4287], ..., [-0.1800, -0.3889, -0.1001, ..., -0.1369, 0.0469, 0.9429], [-0.2970, -0.0023, 0.1976, ..., 0.3776, -0.0069, 0.2029], [ 0.7061, 0.0102, -0.4738, ..., 0.2246, -0.7604, -0.2503]]]), tensor([[-3.5925e-01, 2.0294e-02, -2.3487e-01, 4.5763e-01, -6.1821e-02, 2.4697e-02, 3.8172e-01, -1.8212e-01, 3.4533e-01, -9.7177e-01, 1.1063e-01, 7.8944e-02, 8.2582e-01, 1.9020e-01, 6.5513e-01, -1.8114e-01, 3.9617e-02, -5.6230e-02, 1.5207e-01, -3.2552e-01, ... 1.4417e-01, 3.0337e-01, -6.6146e-01, -9.6959e-02, 8.9790e-02, 1.2345e-01, -5.9831e-02, 2.2399e-01, 8.2549e-02, 6.7749e-01, 1.4473e-01, 5.4490e-01, 5.9272e-01, 3.4453e-01, -8.9982e-02, -1.2631e-01, -1.9465e-01, 6.5992e-01]])) torch.Size([1, 12, 768])","title":"2.6 \u52a0\u8f7d\u4e0b\u8f7d\u7684\u6a21\u578b"},{"location":"05_mkdocs_translearning/11%20huggingface%E5%B9%B3%E5%8F%B0%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97-old.html#3","text":"\u5b66\u4e60\u4e86\u901a\u8fc7\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u540e\u6a21\u578b\u7684\u4f7f\u7528\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u5728https://huggingface.co/join\u4e0a\u521b\u5efa\u4e00\u4e2a\u5e10\u6237 \u7b2c\u4e8c\u6b65: \u5728\u53ef\u89c6\u5316\u754c\u9762\u767b\u9646\u7528\u6237 \u7b2c\u4e09\u6b65: \u5728huggingface\u4e0a\u521b\u5efa\u6a21\u578b\u4ed3\u5e93 \u7b2c\u56db\u6b65: \u901a\u8fc7git\u628a\u672c\u5730\u6a21\u578b\uff0c\u4e0a\u4f20\u5230HuggingFace\u5e73\u53f0\u7684\u6a21\u578b\u4ed3\u5e93\u4e2d \u7b2c\u4e94\u6b65: \u901a\u8fc7git clone\u8fdb\u884c\u6a21\u578b\u4e0b\u8f7d \u7b2c\u516d\u6b65: \u52a0\u8f7d\u4e0b\u8f7d\u7684\u6a21\u578b","title":"3 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3fasttext\u7684\u6a21\u578b\u67b6\u6784. \u4e86\u89e3fasttext\u6a21\u578b\u4e2d\u5c42\u6b21\u5316\u7684softmax. \u4e86\u89e3\u8d1f\u91c7\u6837. 1 Fasttext\u6a21\u578b\u67b6\u6784 \u00b6 FastText \u6a21\u578b\u67b6\u6784\u548c Word2Vec \u4e2d\u7684 CBOW \u6a21\u578b\u5f88\u7c7b\u4f3c, \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e, FastText \u9884\u6d4b\u6807\u7b7e, \u800c CBOW \u6a21\u578b\u9884\u6d4b\u4e2d\u95f4\u8bcd. FastText\u7684\u6a21\u578b\u5206\u4e3a\u4e09\u5c42\u67b6\u6784: \u8f93\u5165\u5c42: \u662f\u5bf9\u6587\u6863embedding\u4e4b\u540e\u7684\u5411\u91cf, \u5305\u542bN-gram\u7279\u5f81 \u9690\u85cf\u5c42: \u662f\u5bf9\u8f93\u5165\u6570\u636e\u7684\u6c42\u548c\u5e73\u5747 \u8f93\u51fa\u5c42: \u662f\u6587\u6863\u5bf9\u5e94\u7684label 2 \u5c42\u6b21softmax(hierarchical softmax) \u00b6 \u4e3a\u4e86\u63d0\u9ad8\u6548\u7387, \u5728fastText\u4e2d\u8ba1\u7b97\u5206\u7c7b\u6807\u7b7e\u6982\u7387\u7684\u65f6\u5019, \u4e0d\u518d\u4f7f\u7528\u4f20\u7edf\u7684softmax\u6765\u8fdb\u884c\u591a\u5206\u7c7b\u7684\u8ba1\u7b97, \u800c\u662f\u4f7f\u7528\u54c8\u592b\u66fc\u6811, \u4f7f\u7528\u5c42\u6b21\u5316\u7684softmax\u6765\u8fdb\u884c\u6982\u7387\u7684\u8ba1\u7b97. 2.1 \u54c8\u592b\u66fc\u6811 \u00b6 \u6982\u5ff5: \u5f53\u5229\u7528n \u4e2a\u7ed3\u70b9\u8bd5\u56fe\u6784\u5efa\u4e00\u68f5\u6811\u65f6, \u5982\u679c\u6784\u5efa\u7684\u8fd9\u68f5\u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u6700\u5c0f, \u79f0\u8fd9\u68f5\u6811\u4e3a\u201c\u6700\u4f18\u4e8c\u53c9\u6811\u201d, \u6709\u65f6\u4e5f\u53eb\u201c\u8d6b\u592b\u66fc\u6811\u201d\u6216\u8005\u201c\u54c8\u592b\u66fc\u6811\u201d. \u7279\u70b9: \u6743\u503c\u8d8a\u5927\u7684\u8282\u70b9\u8ddd\u79bb\u6839\u8282\u70b9\u4e5f\u8f83\u8fd1. 2.2 \u54c8\u592b\u66fc\u6811\u76f8\u5173\u6982\u5ff5 \u00b6 \u4e8c\u53c9\u6811: \u6bcf\u4e2a\u8282\u70b9\u6700\u591a\u67092\u4e2a\u5b50\u6811\u7684\u6709\u5e8f\u6811, \u4e24\u4e2a\u5b50\u6811\u5206\u522b\u79f0\u4e3a\u5de6\u5b50\u6811\u3001\u53f3\u5b50\u6811. \u6709\u5e8f\u7684\u610f\u601d\u662f: \u6811\u6709\u5de6\u53f3\u4e4b\u5206, \u4e0d\u80fd\u98a0\u5012. \u53f6\u5b50\u8282\u70b9: \u4e00\u68f5\u6811\u5f53\u4e2d\u6ca1\u6709\u5b50\u8282\u70b9\u7684\u8282\u70b9\u79f0\u4e3a\u53f6\u5b50\u8282\u70b9. \u8def\u5f84\u548c\u8def\u5f84\u957f\u5ea6: \u5728\u4e00\u68f5\u6811\u4e2d, \u4ece\u4e00\u4e2a\u8282\u70b9\u5f80\u4e0b\u53ef\u4ee5\u5230\u8fbe\u5b69\u5b50\u6216\u5b59\u5b50\u8282\u70b9\u4e4b\u95f4\u7684\u901a\u8def, \u79f0\u4e3a\u8def\u5f84. \u901a\u8def\u4e2d\u5206\u652f\u7684\u6570\u76ee\u79f0\u4e3a\u8def\u5f84\u957f\u5ea6. \u8282\u70b9\u7684\u6743\u53ca\u5e26\u6743\u8def\u5f84\u957f\u5ea6: \u82e5\u5c06\u6811\u4e2d\u8282\u70b9\u8d4b\u4e88\u4e00\u4e2a\u6709\u67d0\u79cd\u542b\u4e49\u7684\u6570\u503c, \u5219\u8fd9\u4e2a\u6570\u503c\u79f0\u4e3a\u8be5\u8282\u70b9\u7684\u6743, \u8282\u70b9\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u4e3a: \u4ece\u6839\u8282\u70b9\u5230\u8be5\u8282\u70b9\u4e4b\u95f4\u7684\u8def\u5f84\u957f\u5ea6\u4e0e\u8be5\u8282\u70b9\u7684\u6743\u7684\u4e58\u79ef. \u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6: \u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u89c4\u5b9a\u4e3a\u6240\u6709\u53f6\u5b50\u8282\u70b9\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u4e4b\u548c, \u8bb0\u4e3aWPL(weighted path length). WPL\u6700\u5c0f\u7684\u4e8c\u53c9\u6811\u5c31\u662f\u8d6b\u592b\u66fc\u6811 2.3 \u6784\u5efa\u54c8\u592b\u66fc\u6811 \u00b6 \u5047\u8bbe\u6709n\u4e2a\u6743\u503c, \u5219\u6784\u9020\u51fa\u7684\u54c8\u592b\u66fc\u6811\u6709n\u4e2a\u53f6\u5b50\u8282\u70b9. n\u4e2a\u6743\u503c\u5206\u522b\u8bbe\u4e3a w1\u3001w2\u3001\u2026\u3001wn, \u5219\u54c8\u592b\u66fc\u6811\u7684\u6784\u9020\u89c4\u5219\u4e3a: \u6b65\u9aa41: \u5c06w1\u3001w2\u3001\u2026, wn\u770b\u6210\u662f\u6709n \u68f5\u6811\u7684\u68ee\u6797(\u6bcf\u68f5\u6811\u4ec5\u6709\u4e00\u4e2a\u8282\u70b9); \u6b65\u9aa42: \u5728\u68ee\u6797\u4e2d\u9009\u51fa\u4e24\u4e2a\u6839\u8282\u70b9\u7684\u6743\u503c\u6700\u5c0f\u7684\u6811\u5408\u5e76, \u4f5c\u4e3a\u4e00\u9897\u65b0\u6811\u7684\u5de6\u3001\u53f3\u5b50\u6811, \u4e14\u65b0\u6811\u7684\u6839\u8282\u70b9\u6743\u503c\u4e3a\u5176\u5de6\u3001\u53f3\u5b50\u6811\u6839\u8282\u70b9\u6743\u503c\u4e4b\u548c; \u6b65\u9aa43: \u4ece\u68ee\u6797\u4e2d\u5220\u9664\u9009\u53d6\u7684\u4e24\u68f5\u6811, \u5e76\u5c06\u65b0\u6811\u52a0\u5165\u68ee\u6797; \u6b65\u9aa44: \u91cd\u590d2-3\u6b65\u9aa4, \u76f4\u5230\u68ee\u6797\u53ea\u6709\u4e00\u9897\u6811\u4e3a\u6b62, \u8be5\u6811\u5c31\u662f\u6240\u6c42\u7684\u54c8\u592b\u66fc\u6811. \u4e3e\u4f8b\u8bf4\u660e, \u6784\u5efahuffman\u6811: \u5047\u8bbe\u6709\u56db\u4e2aLabel\u5206\u522b\u4e3a: A~D, \u7edf\u8ba1\u5176\u5728\u8bed\u6599\u5e93\u51fa\u73b0\u7684\u9891\u6570: \u7b2c\u4e00\u6b21\u5408\u5e76\u5efa\u6811: \u7b2c\u4e8c\u6b21\u5408\u5e76\u5efa\u6811: \u7b2c\u4e09\u6b21\u5408\u5e76\u5efa\u6811: \u7531\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\u6743\u91cd\u8d8a\u5927, \u8ddd\u79bb\u6839\u8282\u70b9\u8d8a\u8fd1. \u53f6\u5b50\u7684\u4e2a\u6570\u4e3an, \u6784\u9020\u54c8\u592b\u66fc\u6811\u4e2d\u65b0\u589e\u7684\u8282\u70b9\u7684\u4e2a\u6570\u4e3an-1. 2.4 \u54c8\u592b\u66fc\u6811\u7f16\u7801 \u00b6 \u54c8\u592b\u66fc\u7f16\u7801\u4e00\u822c\u89c4\u5b9a\u54c8\u592b\u66fc\u6811\u4e2d\u7684\u5de6\u5206\u652f\u4e3a 0, \u53f3\u5206\u652f\u4e3a 1, \u4ece\u6839\u8282\u70b9\u5230\u6bcf\u4e2a\u53f6\u8282\u70b9\u6240\u7ecf\u8fc7\u7684\u5206\u652f\u5bf9\u5e94\u7684 0 \u548c 1 \u7ec4\u6210\u7684\u5e8f\u5217\u4fbf\u4e3a\u8be5\u8282\u70b9\u5bf9\u5e94\u5b57\u7b26\u7684\u7f16\u7801. \u8fd9\u6837\u7684\u7f16\u7801\u79f0\u4e3a\u54c8\u592b\u66fc\u7f16\u7801. \u4e0a\u56fe\u4f8b\u5b50\u4e2d\u5bf9\u5e94\u7684\u7f16\u7801\u5982\u4e0b: 2.5 \u8f6c\u5316\u4e3a\u68af\u5ea6\u8ba1\u7b97 \u00b6 \u4e0a\u56fe\u4e2d, \u7ea2\u8272\u4e3a\u54c8\u592b\u66fc\u7f16\u7801, \u5373D\u7684\u54c8\u592b\u66fc\u7f16\u7801\u4e3a110, \u90a3\u4e48\u6b64\u65f6\u5982\u4f55\u5b9a\u4e49\u6761\u4ef6\u6982\u7387 P(D|context)\u200b P(D|context)\u200b ? \u4ee5D\u4e3a\u4f8b, \u4ece\u6839\u8282\u70b9\u5230D\u4e2d\u95f4\u7ecf\u5386\u4e863\u6b21\u5206\u652f, \u6bcf\u6b21\u5206\u652f\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662f\u8fdb\u884c\u4e86\u4e00\u6b212\u5206\u7c7b, \u6839\u636e\u54c8\u592b\u66fc\u7f16\u7801, \u53ef\u4ee5\u628a\u6570\u5b570\u5bf9\u5e94\u5206\u652f\u8ba4\u4e3a\u662f\u8d1f\u7c7b, \u6570\u5b571\u5bf9\u5e94\u7684\u5206\u652f\u8ba4\u4e3a\u662f\u6b63\u7c7b. \u5728\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u4e2d\u903b\u8f91\u56de\u5f52\u4e2d\u4f7f\u7528sigmoid\u51fd\u6570\u8fdb\u884c2\u5206\u7c7b\u7684\u8fc7\u7a0b\u4e2d: \u4e00\u4e2a\u8282\u70b9\u88ab\u5206\u4e3a\u6b63\u7c7b\u7684\u6982\u7387\u662f: \\sigma(X^T\\theta) = 1/(1+e^{-x^T\\theta}) \\sigma(X^T\\theta) = 1/(1+e^{-x^T\\theta}) , \u4e00\u4e2a\u8282\u70b9\u88ab\u5206\u4e3a\u8d1f\u7c7b\u7684\u6982\u7387\u662f: 1-\\sigma(X^T\\theta) 1-\\sigma(X^T\\theta) , \u5176\u4e2d \\theta \\theta \u5c31\u662f\u56fe\u4e2d\u975e\u53f6\u5b50\u8282\u70b9\u5bf9\u5e94\u7684\u53c2\u6570. \u5bf9\u4e8e\u4ece\u6839\u8282\u70b9\u51fa\u53d1, \u5230\u8fbeD\u4e00\u5171\u7ecf\u5386\u4e09\u6b21\u5206\u652f, \u5c06\u6bcf\u6b21\u5206\u7c7b\u7ed3\u679c\u7684\u6982\u7387\u7f57\u5217\u51fa\u6765: \u7b2c\u4e00\u6b21: P(1|X, \\theta1) = \\sigma(X^T\\theta1)\u200b P(1|X, \\theta1) = \\sigma(X^T\\theta1)\u200b , \u5373\u4ece\u6839\u8282\u70b9\u523024\u8282\u70b9\u7684\u6982\u7387\u662f\u5728\u77e5\u9053 x\u200b x\u200b \u548c \\theta1\u200b \\theta1\u200b \u7684\u60c5\u51b5\u4e0b\u53d6\u503c\u4e3a1\u7684\u6982\u7387 \u7b2c\u4e8c\u6b21: P(1|X, \\theta2) = \\sigma(X^T\\theta2)\u200b P(1|X, \\theta2) = \\sigma(X^T\\theta2)\u200b \u7b2c\u4e09\u6b21: P(0|X, \\theta3) = 1-\\sigma(X^T\\theta3)\u200b P(0|X, \\theta3) = 1-\\sigma(X^T\\theta3)\u200b \u4f46\u662f\u6211\u4eec\u9700\u8981\u6c42\u7684\u662f P(D|context) P(D|context) , \u5b83\u7b49\u4e8e\u524d3\u8bcd\u7684\u6982\u7387\u4e58\u79ef, \u516c\u5f0f\u5982\u4e0b\uff08 d_j^w d_j^w \u662f\u7b2c j\u200b j\u200b \u4e2a\u8282\u70b9\u7684\u54c8\u592b\u66fc\u7f16\u7801\uff09 \u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u4e2d, \u6211\u4eec\u4f1a\u7ecf\u5e38\u628a\u4e8c\u5206\u7c7b\u7684\u635f\u5931\u51fd\u6570\u5b9a\u4e49\u4e3a\u5bf9\u6570\u4f3c\u7136\u635f\u5931, \u5373 \u5f0f\u5b50\u4e2d, \u6c42\u548c\u7b26\u53f7\u8868\u793a\u7684\u662f\u4f7f\u7528\u6837\u672c\u7684\u8fc7\u7a0b\u4e2d, \u6bcf\u4e2alabel\u5bf9\u5e94\u7684\u6982\u7387\u53d6\u5bf9\u6570\u540e\u7684\u548c, \u4e4b\u540e\u6c42\u53d6\u5747\u503c. \u5e26\u5165\u524d\u9762 P(Label|Context) P(Label|Context) \u7684\u5b9a\u4e49\u5f97\u5230\u635f\u5931\u51fd\u6570: \u6709\u4e86\u635f\u5931\u51fd\u6570\u4e4b\u540e, \u63a5\u4e0b\u6765\u5c31\u662f\u5bf9\u5176\u4e2d\u7684 X,\\theta\u200b X,\\theta\u200b \u8fdb\u884c\u6c42\u5bfc, \u5e76\u66f4\u65b0. 2.6 \u5c42\u6b21softmax\u7684\u4f18\u52bf \u00b6 \u4f20\u7edf\u7684softmax\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aL(labels\u7684\u6570\u91cf), \u4f46\u662f\u4f7f\u7528\u5c42\u6b21\u5316softmax\u4e4b\u540e\u65f6\u95f4\u590d\u6742\u5ea6\u7684log(L) (\u4e8c\u53c9\u6811\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u8fd1\u4f3c), \u4ece\u800c\u5728\u591a\u5206\u7c7b\u7684\u573a\u666f\u63d0\u9ad8\u4e86\u6548\u7387. 3 \u8d1f\u91c7\u6837(negative sampling) \u00b6 3.1 \u8d1f\u91c7\u6837\u539f\u7406 \u00b6 \u5f53\u6211\u4eec\u8bad\u7ec3\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u610f\u5473\u7740\u8981\u8f93\u5165\u8bad\u7ec3\u6837\u672c\u5e76\u4e14\u4e0d\u65ad\u8c03\u6574\u795e\u7ecf\u5143\u7684\u6743\u91cd, \u4ece\u800c\u4e0d\u65ad\u63d0\u9ad8\u5bf9\u76ee\u6807\u7684\u51c6\u786e\u9884\u6d4b. \u6bcf\u5f53\u795e\u7ecf\u7f51\u7edc\u7ecf\u8fc7\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u8bad\u7ec3, \u5b83\u7684\u6743\u91cd\u5c31\u4f1a\u8fdb\u884c\u4e00\u6b21\u8c03\u6574. \u6bd4\u5982\u6211\u4eec\u5229\u7528Skip-Gram\u8fdb\u884c\u8bcd\u5411\u91cf\u7684\u8bad\u7ec3, \u5982\u679c\u8bcd\u6c47\u91cf\u7684\u6570\u91cf\u4e3a\u4e0a\u4e07\u4e2a, \u90a3\u4e48\u6211\u4eec\u5229\u7528softmax\u8ba1\u7b97\u6982\u7387\u65f6, \u9700\u8981\u5bf9\u8ba1\u7b97\u4e0a\u4e07\u4e2a\u6982\u7387\u503c, \u4e14\u6bcf\u4e2a\u503c\u90fd\u9700\u8981\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u6a21\u578b\u53c2\u6570, \u8fd9\u662f\u975e\u5e38\u6d88\u8017\u8ba1\u7b97\u8d44\u6e90\u7684, \u5e76\u4e14\u5b9e\u9645\u4e2d\u8bad\u7ec3\u8d77\u6765\u4f1a\u975e\u5e38\u6162. \u4e0d\u540c\u4e8e\u539f\u672c\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u66f4\u65b0\u6240\u6709\u7684\u6743\u91cd, \u8d1f\u91c7\u6837\u6bcf\u6b21\u8ba9\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4ec5\u4ec5\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u7684\u6743\u91cd, \u8fd9\u6837\u5c31\u4f1a\u964d\u4f4e\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u91cf. \u4e3e\u4f8b\u8bf4\u660e\uff08\u8d1f\u91c7\u6837\u539f\u7406\uff09: \u5f53\u6211\u4eec\u7528\u8bad\u7ec3\u6837\u672c ( input word: \"hello\", output word: \"man\") \u6765\u8bad\u7ec3\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u65f6, \u201c hello\u201d\u548c\u201cman\u201d\u90fd\u662f\u7ecf\u8fc7one-hot\u7f16\u7801\u7684. \u5982\u679c\u6211\u4eec\u7684vocabulary\u5927\u5c0f\u4e3a10000\u65f6, \u5728\u8f93\u51fa\u5c42, \u6211\u4eec\u671f\u671b\u5bf9\u5e94\u201cman\u201d\u5355\u8bcd\u7684\u90a3\u4e2a\u795e\u7ecf\u5143\u7ed3\u70b9\u8f93\u51fa1, \u5176\u4f599999\u4e2a\u90fd\u5e94\u8be5\u8f93\u51fa0. \u5728\u8fd9\u91cc, \u8fd99999\u4e2a\u6211\u4eec\u671f\u671b\u8f93\u51fa\u4e3a0\u7684\u795e\u7ecf\u5143\u7ed3\u70b9\u6240\u5bf9\u5e94\u7684\u5355\u8bcd\u6211\u4eec\u79f0\u4e3a\u201cnegative\u201d word. \u5f53\u4f7f\u7528\u8d1f\u91c7\u6837\u65f6, \u6211\u4eec\u5c06\u968f\u673a\u9009\u62e9\u4e00\u5c0f\u90e8\u5206\u7684negative words\uff08\u6bd4\u5982\u90095\u4e2anegative words\uff09\u6765\u66f4\u65b0\u5bf9\u5e94\u7684\u6743\u91cd. \u6211\u4eec\u4e5f\u4f1a\u5bf9\u6211\u4eec\u7684\u201cpositive\u201d word\u8fdb\u884c\u6743\u91cd\u66f4\u65b0\uff08\u5728\u6211\u4eec\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u8fd9\u4e2a\u5355\u8bcd\u6307\u7684\u662f\u201dman\u201c\uff09. \u6ce8\u610f, \u5bf9\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6, \u9009\u62e95-20\u4e2anegative words\u4f1a\u6bd4\u8f83\u597d, \u5bf9\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u53ef\u4ee5\u4ec5\u9009\u62e92-5\u4e2anegative words. \u5047\u5982\u6211\u4eec\u7684\u9690\u5c42-\u8f93\u51fa\u5c42\u62e5\u6709300 x 10000\u7684\u6743\u91cd\u77e9\u9635. \u5982\u679c\u4f7f\u7528\u4e86\u8d1f\u91c7\u6837\u7684\u65b9\u6cd5\u6211\u4eec\u4ec5\u4ec5\u53bb\u66f4\u65b0\u6211\u4eec\u7684positive word-\u201cman\u201d\u7684\u548c\u6211\u4eec\u9009\u62e9\u7684\u5176\u4ed65\u4e2anegative words\u7684\u7ed3\u70b9\u5bf9\u5e94\u7684\u6743\u91cd, \u5171\u8ba16\u4e2a\u8f93\u51fa\u795e\u7ecf\u5143, \u76f8\u5f53\u4e8e\u6bcf\u6b21\u53ea\u66f4\u65b0300\u00d76=1800\u4e2a\u6743\u91cd. \u5bf9\u4e8e3\u767e\u4e07\u7684\u6743\u91cd\u6765\u8bf4, \u76f8\u5f53\u4e8e\u53ea\u8ba1\u7b97\u4e860.06%\u7684\u6743\u91cd, \u8fd9\u6837\u8ba1\u7b97\u6548\u7387\u5c31\u5927\u5e45\u5ea6\u63d0\u9ad8. 3.2 \u8d1f\u91c7\u6837\u7684\u4f18\u52bf \u00b6 \u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6, \u9009\u62e9\u4e86\u90e8\u5206\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u635f\u5931, \u635f\u5931\u8ba1\u7b97\u66f4\u52a0\u7b80\u5355. \u6539\u8fdb\u6548\u679c, \u589e\u52a0\u90e8\u5206\u8d1f\u6837\u672c, \u80fd\u591f\u6a21\u62df\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u566a\u58f0\u60c5\u51b5, \u80fd\u591f\u8ba9\u6a21\u578b\u7684\u7a33\u5065\u6027\u66f4\u5f3a. 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u89e3\u4ec0\u4e48\u662ffasttext\u6a21\u578b\u67b6\u6784: fastText \u6a21\u578b\u67b6\u6784\u548c Word2Vec \u4e2d\u7684 CBOW \u6a21\u578b\u5f88\u7c7b\u4f3c, \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e, fastText \u9884\u6d4b\u6807\u7b7e, \u800c CBOW \u6a21\u578b\u9884\u6d4b\u4e2d\u95f4\u8bcd. \u5b66\u4e60\u4e86\u5c42\u6b21softmax: \u4e3a\u4e86\u63d0\u9ad8\u6548\u7387, \u5728fastText\u4e2d\u8ba1\u7b97\u5206\u7c7b\u6807\u7b7e\u6982\u7387\u7684\u65f6\u5019, \u4e0d\u518d\u4f7f\u7528\u4f20\u7edf\u7684softmax\u6765\u8fdb\u884c\u591a\u5206\u7c7b\u7684\u8ba1\u7b97, \u800c\u662f\u4f7f\u7528\u54c8\u592b\u66fc\u6811, \u4f7f\u7528\u5c42\u6b21\u5316\u7684softmax\u6765\u8fdb\u884c\u6982\u7387\u7684\u8ba1\u7b97. \u54c8\u592b\u66fc\u6811\u5b9a\u4e49: \u5f53\u5229\u7528n \u4e2a\u7ed3\u70b9\u8bd5\u56fe\u6784\u5efa\u4e00\u68f5\u6811\u65f6, \u5982\u679c\u6784\u5efa\u7684\u8fd9\u68f5\u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u6700\u5c0f, \u79f0\u8fd9\u68f5\u6811\u4e3a\u201c\u6700\u4f18\u4e8c\u53c9\u6811\u201d, \u6709\u65f6\u4e5f\u53eb\u201c\u8d6b\u592b\u66fc\u6811\u201d\u6216\u8005\u201c\u54c8\u592b\u66fc\u6811\u201d. \u4f18\u70b9: \u4f20\u7edf\u7684softmax\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aL(labels\u7684\u6570\u91cf), \u4f46\u662f\u4f7f\u7528\u5c42\u6b21\u5316softmax\u4e4b\u540e\u65f6\u95f4\u590d\u6742\u5ea6\u7684log(L) (\u4e8c\u53c9\u6811\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u8fd1\u4f3c), \u4ece\u800c\u5728\u591a\u5206\u7c7b\u7684\u573a\u666f\u63d0\u9ad8\u4e86\u6548\u7387. \u5b66\u4e60\u4e86\u8d1f\u91c7\u6837: \u8d1f\u91c7\u6837\u539f\u7406: \u8d1f\u91c7\u6837\u6bcf\u6b21\u8ba9\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4ec5\u4ec5\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u7684\u6743\u91cd, \u8fd9\u6837\u5c31\u4f1a\u964d\u4f4e\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u91cf. \u4f18\u70b9: \u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6, \u9009\u62e9\u4e86\u90e8\u5206\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u635f\u5931, \u635f\u5931\u8ba1\u7b97\u66f4\u52a0\u7b80\u5355. \u6539\u8fdb\u6548\u679c, \u589e\u52a0\u90e8\u5206\u8d1f\u6837\u672c, \u80fd\u591f\u6a21\u62df\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u566a\u58f0\u60c5\u51b5, \u80fd\u591f\u8ba9\u6a21\u578b\u7684\u7a33\u5065\u6027\u66f4\u5f3a.","title":"2 fasttext\u6a21\u578b\u67b6\u6784"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#_1","text":"\u4e86\u89e3fasttext\u7684\u6a21\u578b\u67b6\u6784. \u4e86\u89e3fasttext\u6a21\u578b\u4e2d\u5c42\u6b21\u5316\u7684softmax. \u4e86\u89e3\u8d1f\u91c7\u6837.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#1-fasttext","text":"FastText \u6a21\u578b\u67b6\u6784\u548c Word2Vec \u4e2d\u7684 CBOW \u6a21\u578b\u5f88\u7c7b\u4f3c, \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e, FastText \u9884\u6d4b\u6807\u7b7e, \u800c CBOW \u6a21\u578b\u9884\u6d4b\u4e2d\u95f4\u8bcd. FastText\u7684\u6a21\u578b\u5206\u4e3a\u4e09\u5c42\u67b6\u6784: \u8f93\u5165\u5c42: \u662f\u5bf9\u6587\u6863embedding\u4e4b\u540e\u7684\u5411\u91cf, \u5305\u542bN-gram\u7279\u5f81 \u9690\u85cf\u5c42: \u662f\u5bf9\u8f93\u5165\u6570\u636e\u7684\u6c42\u548c\u5e73\u5747 \u8f93\u51fa\u5c42: \u662f\u6587\u6863\u5bf9\u5e94\u7684label","title":"1 Fasttext\u6a21\u578b\u67b6\u6784"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#2-softmaxhierarchical-softmax","text":"\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387, \u5728fastText\u4e2d\u8ba1\u7b97\u5206\u7c7b\u6807\u7b7e\u6982\u7387\u7684\u65f6\u5019, \u4e0d\u518d\u4f7f\u7528\u4f20\u7edf\u7684softmax\u6765\u8fdb\u884c\u591a\u5206\u7c7b\u7684\u8ba1\u7b97, \u800c\u662f\u4f7f\u7528\u54c8\u592b\u66fc\u6811, \u4f7f\u7528\u5c42\u6b21\u5316\u7684softmax\u6765\u8fdb\u884c\u6982\u7387\u7684\u8ba1\u7b97.","title":"2 \u5c42\u6b21softmax(hierarchical softmax)"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#21","text":"\u6982\u5ff5: \u5f53\u5229\u7528n \u4e2a\u7ed3\u70b9\u8bd5\u56fe\u6784\u5efa\u4e00\u68f5\u6811\u65f6, \u5982\u679c\u6784\u5efa\u7684\u8fd9\u68f5\u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u6700\u5c0f, \u79f0\u8fd9\u68f5\u6811\u4e3a\u201c\u6700\u4f18\u4e8c\u53c9\u6811\u201d, \u6709\u65f6\u4e5f\u53eb\u201c\u8d6b\u592b\u66fc\u6811\u201d\u6216\u8005\u201c\u54c8\u592b\u66fc\u6811\u201d. \u7279\u70b9: \u6743\u503c\u8d8a\u5927\u7684\u8282\u70b9\u8ddd\u79bb\u6839\u8282\u70b9\u4e5f\u8f83\u8fd1.","title":"2.1 \u54c8\u592b\u66fc\u6811"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#22","text":"\u4e8c\u53c9\u6811: \u6bcf\u4e2a\u8282\u70b9\u6700\u591a\u67092\u4e2a\u5b50\u6811\u7684\u6709\u5e8f\u6811, \u4e24\u4e2a\u5b50\u6811\u5206\u522b\u79f0\u4e3a\u5de6\u5b50\u6811\u3001\u53f3\u5b50\u6811. \u6709\u5e8f\u7684\u610f\u601d\u662f: \u6811\u6709\u5de6\u53f3\u4e4b\u5206, \u4e0d\u80fd\u98a0\u5012. \u53f6\u5b50\u8282\u70b9: \u4e00\u68f5\u6811\u5f53\u4e2d\u6ca1\u6709\u5b50\u8282\u70b9\u7684\u8282\u70b9\u79f0\u4e3a\u53f6\u5b50\u8282\u70b9. \u8def\u5f84\u548c\u8def\u5f84\u957f\u5ea6: \u5728\u4e00\u68f5\u6811\u4e2d, \u4ece\u4e00\u4e2a\u8282\u70b9\u5f80\u4e0b\u53ef\u4ee5\u5230\u8fbe\u5b69\u5b50\u6216\u5b59\u5b50\u8282\u70b9\u4e4b\u95f4\u7684\u901a\u8def, \u79f0\u4e3a\u8def\u5f84. \u901a\u8def\u4e2d\u5206\u652f\u7684\u6570\u76ee\u79f0\u4e3a\u8def\u5f84\u957f\u5ea6. \u8282\u70b9\u7684\u6743\u53ca\u5e26\u6743\u8def\u5f84\u957f\u5ea6: \u82e5\u5c06\u6811\u4e2d\u8282\u70b9\u8d4b\u4e88\u4e00\u4e2a\u6709\u67d0\u79cd\u542b\u4e49\u7684\u6570\u503c, \u5219\u8fd9\u4e2a\u6570\u503c\u79f0\u4e3a\u8be5\u8282\u70b9\u7684\u6743, \u8282\u70b9\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u4e3a: \u4ece\u6839\u8282\u70b9\u5230\u8be5\u8282\u70b9\u4e4b\u95f4\u7684\u8def\u5f84\u957f\u5ea6\u4e0e\u8be5\u8282\u70b9\u7684\u6743\u7684\u4e58\u79ef. \u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6: \u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u89c4\u5b9a\u4e3a\u6240\u6709\u53f6\u5b50\u8282\u70b9\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u4e4b\u548c, \u8bb0\u4e3aWPL(weighted path length). WPL\u6700\u5c0f\u7684\u4e8c\u53c9\u6811\u5c31\u662f\u8d6b\u592b\u66fc\u6811","title":"2.2 \u54c8\u592b\u66fc\u6811\u76f8\u5173\u6982\u5ff5"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#23","text":"\u5047\u8bbe\u6709n\u4e2a\u6743\u503c, \u5219\u6784\u9020\u51fa\u7684\u54c8\u592b\u66fc\u6811\u6709n\u4e2a\u53f6\u5b50\u8282\u70b9. n\u4e2a\u6743\u503c\u5206\u522b\u8bbe\u4e3a w1\u3001w2\u3001\u2026\u3001wn, \u5219\u54c8\u592b\u66fc\u6811\u7684\u6784\u9020\u89c4\u5219\u4e3a: \u6b65\u9aa41: \u5c06w1\u3001w2\u3001\u2026, wn\u770b\u6210\u662f\u6709n \u68f5\u6811\u7684\u68ee\u6797(\u6bcf\u68f5\u6811\u4ec5\u6709\u4e00\u4e2a\u8282\u70b9); \u6b65\u9aa42: \u5728\u68ee\u6797\u4e2d\u9009\u51fa\u4e24\u4e2a\u6839\u8282\u70b9\u7684\u6743\u503c\u6700\u5c0f\u7684\u6811\u5408\u5e76, \u4f5c\u4e3a\u4e00\u9897\u65b0\u6811\u7684\u5de6\u3001\u53f3\u5b50\u6811, \u4e14\u65b0\u6811\u7684\u6839\u8282\u70b9\u6743\u503c\u4e3a\u5176\u5de6\u3001\u53f3\u5b50\u6811\u6839\u8282\u70b9\u6743\u503c\u4e4b\u548c; \u6b65\u9aa43: \u4ece\u68ee\u6797\u4e2d\u5220\u9664\u9009\u53d6\u7684\u4e24\u68f5\u6811, \u5e76\u5c06\u65b0\u6811\u52a0\u5165\u68ee\u6797; \u6b65\u9aa44: \u91cd\u590d2-3\u6b65\u9aa4, \u76f4\u5230\u68ee\u6797\u53ea\u6709\u4e00\u9897\u6811\u4e3a\u6b62, \u8be5\u6811\u5c31\u662f\u6240\u6c42\u7684\u54c8\u592b\u66fc\u6811. \u4e3e\u4f8b\u8bf4\u660e, \u6784\u5efahuffman\u6811: \u5047\u8bbe\u6709\u56db\u4e2aLabel\u5206\u522b\u4e3a: A~D, \u7edf\u8ba1\u5176\u5728\u8bed\u6599\u5e93\u51fa\u73b0\u7684\u9891\u6570: \u7b2c\u4e00\u6b21\u5408\u5e76\u5efa\u6811: \u7b2c\u4e8c\u6b21\u5408\u5e76\u5efa\u6811: \u7b2c\u4e09\u6b21\u5408\u5e76\u5efa\u6811: \u7531\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\u6743\u91cd\u8d8a\u5927, \u8ddd\u79bb\u6839\u8282\u70b9\u8d8a\u8fd1. \u53f6\u5b50\u7684\u4e2a\u6570\u4e3an, \u6784\u9020\u54c8\u592b\u66fc\u6811\u4e2d\u65b0\u589e\u7684\u8282\u70b9\u7684\u4e2a\u6570\u4e3an-1.","title":"2.3 \u6784\u5efa\u54c8\u592b\u66fc\u6811"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#24","text":"\u54c8\u592b\u66fc\u7f16\u7801\u4e00\u822c\u89c4\u5b9a\u54c8\u592b\u66fc\u6811\u4e2d\u7684\u5de6\u5206\u652f\u4e3a 0, \u53f3\u5206\u652f\u4e3a 1, \u4ece\u6839\u8282\u70b9\u5230\u6bcf\u4e2a\u53f6\u8282\u70b9\u6240\u7ecf\u8fc7\u7684\u5206\u652f\u5bf9\u5e94\u7684 0 \u548c 1 \u7ec4\u6210\u7684\u5e8f\u5217\u4fbf\u4e3a\u8be5\u8282\u70b9\u5bf9\u5e94\u5b57\u7b26\u7684\u7f16\u7801. \u8fd9\u6837\u7684\u7f16\u7801\u79f0\u4e3a\u54c8\u592b\u66fc\u7f16\u7801. \u4e0a\u56fe\u4f8b\u5b50\u4e2d\u5bf9\u5e94\u7684\u7f16\u7801\u5982\u4e0b:","title":"2.4 \u54c8\u592b\u66fc\u6811\u7f16\u7801"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#25","text":"\u4e0a\u56fe\u4e2d, \u7ea2\u8272\u4e3a\u54c8\u592b\u66fc\u7f16\u7801, \u5373D\u7684\u54c8\u592b\u66fc\u7f16\u7801\u4e3a110, \u90a3\u4e48\u6b64\u65f6\u5982\u4f55\u5b9a\u4e49\u6761\u4ef6\u6982\u7387 P(D|context)\u200b P(D|context)\u200b ? \u4ee5D\u4e3a\u4f8b, \u4ece\u6839\u8282\u70b9\u5230D\u4e2d\u95f4\u7ecf\u5386\u4e863\u6b21\u5206\u652f, \u6bcf\u6b21\u5206\u652f\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662f\u8fdb\u884c\u4e86\u4e00\u6b212\u5206\u7c7b, \u6839\u636e\u54c8\u592b\u66fc\u7f16\u7801, \u53ef\u4ee5\u628a\u6570\u5b570\u5bf9\u5e94\u5206\u652f\u8ba4\u4e3a\u662f\u8d1f\u7c7b, \u6570\u5b571\u5bf9\u5e94\u7684\u5206\u652f\u8ba4\u4e3a\u662f\u6b63\u7c7b. \u5728\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u4e2d\u903b\u8f91\u56de\u5f52\u4e2d\u4f7f\u7528sigmoid\u51fd\u6570\u8fdb\u884c2\u5206\u7c7b\u7684\u8fc7\u7a0b\u4e2d: \u4e00\u4e2a\u8282\u70b9\u88ab\u5206\u4e3a\u6b63\u7c7b\u7684\u6982\u7387\u662f: \\sigma(X^T\\theta) = 1/(1+e^{-x^T\\theta}) \\sigma(X^T\\theta) = 1/(1+e^{-x^T\\theta}) , \u4e00\u4e2a\u8282\u70b9\u88ab\u5206\u4e3a\u8d1f\u7c7b\u7684\u6982\u7387\u662f: 1-\\sigma(X^T\\theta) 1-\\sigma(X^T\\theta) , \u5176\u4e2d \\theta \\theta \u5c31\u662f\u56fe\u4e2d\u975e\u53f6\u5b50\u8282\u70b9\u5bf9\u5e94\u7684\u53c2\u6570. \u5bf9\u4e8e\u4ece\u6839\u8282\u70b9\u51fa\u53d1, \u5230\u8fbeD\u4e00\u5171\u7ecf\u5386\u4e09\u6b21\u5206\u652f, \u5c06\u6bcf\u6b21\u5206\u7c7b\u7ed3\u679c\u7684\u6982\u7387\u7f57\u5217\u51fa\u6765: \u7b2c\u4e00\u6b21: P(1|X, \\theta1) = \\sigma(X^T\\theta1)\u200b P(1|X, \\theta1) = \\sigma(X^T\\theta1)\u200b , \u5373\u4ece\u6839\u8282\u70b9\u523024\u8282\u70b9\u7684\u6982\u7387\u662f\u5728\u77e5\u9053 x\u200b x\u200b \u548c \\theta1\u200b \\theta1\u200b \u7684\u60c5\u51b5\u4e0b\u53d6\u503c\u4e3a1\u7684\u6982\u7387 \u7b2c\u4e8c\u6b21: P(1|X, \\theta2) = \\sigma(X^T\\theta2)\u200b P(1|X, \\theta2) = \\sigma(X^T\\theta2)\u200b \u7b2c\u4e09\u6b21: P(0|X, \\theta3) = 1-\\sigma(X^T\\theta3)\u200b P(0|X, \\theta3) = 1-\\sigma(X^T\\theta3)\u200b \u4f46\u662f\u6211\u4eec\u9700\u8981\u6c42\u7684\u662f P(D|context) P(D|context) , \u5b83\u7b49\u4e8e\u524d3\u8bcd\u7684\u6982\u7387\u4e58\u79ef, \u516c\u5f0f\u5982\u4e0b\uff08 d_j^w d_j^w \u662f\u7b2c j\u200b j\u200b \u4e2a\u8282\u70b9\u7684\u54c8\u592b\u66fc\u7f16\u7801\uff09 \u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u903b\u8f91\u56de\u5f52\u4e2d, \u6211\u4eec\u4f1a\u7ecf\u5e38\u628a\u4e8c\u5206\u7c7b\u7684\u635f\u5931\u51fd\u6570\u5b9a\u4e49\u4e3a\u5bf9\u6570\u4f3c\u7136\u635f\u5931, \u5373 \u5f0f\u5b50\u4e2d, \u6c42\u548c\u7b26\u53f7\u8868\u793a\u7684\u662f\u4f7f\u7528\u6837\u672c\u7684\u8fc7\u7a0b\u4e2d, \u6bcf\u4e2alabel\u5bf9\u5e94\u7684\u6982\u7387\u53d6\u5bf9\u6570\u540e\u7684\u548c, \u4e4b\u540e\u6c42\u53d6\u5747\u503c. \u5e26\u5165\u524d\u9762 P(Label|Context) P(Label|Context) \u7684\u5b9a\u4e49\u5f97\u5230\u635f\u5931\u51fd\u6570: \u6709\u4e86\u635f\u5931\u51fd\u6570\u4e4b\u540e, \u63a5\u4e0b\u6765\u5c31\u662f\u5bf9\u5176\u4e2d\u7684 X,\\theta\u200b X,\\theta\u200b \u8fdb\u884c\u6c42\u5bfc, \u5e76\u66f4\u65b0.","title":"2.5 \u8f6c\u5316\u4e3a\u68af\u5ea6\u8ba1\u7b97"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#26-softmax","text":"\u4f20\u7edf\u7684softmax\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aL(labels\u7684\u6570\u91cf), \u4f46\u662f\u4f7f\u7528\u5c42\u6b21\u5316softmax\u4e4b\u540e\u65f6\u95f4\u590d\u6742\u5ea6\u7684log(L) (\u4e8c\u53c9\u6811\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u8fd1\u4f3c), \u4ece\u800c\u5728\u591a\u5206\u7c7b\u7684\u573a\u666f\u63d0\u9ad8\u4e86\u6548\u7387.","title":"2.6 \u5c42\u6b21softmax\u7684\u4f18\u52bf"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#3-negative-sampling","text":"","title":"3 \u8d1f\u91c7\u6837(negative sampling)"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#31","text":"\u5f53\u6211\u4eec\u8bad\u7ec3\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u610f\u5473\u7740\u8981\u8f93\u5165\u8bad\u7ec3\u6837\u672c\u5e76\u4e14\u4e0d\u65ad\u8c03\u6574\u795e\u7ecf\u5143\u7684\u6743\u91cd, \u4ece\u800c\u4e0d\u65ad\u63d0\u9ad8\u5bf9\u76ee\u6807\u7684\u51c6\u786e\u9884\u6d4b. \u6bcf\u5f53\u795e\u7ecf\u7f51\u7edc\u7ecf\u8fc7\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u8bad\u7ec3, \u5b83\u7684\u6743\u91cd\u5c31\u4f1a\u8fdb\u884c\u4e00\u6b21\u8c03\u6574. \u6bd4\u5982\u6211\u4eec\u5229\u7528Skip-Gram\u8fdb\u884c\u8bcd\u5411\u91cf\u7684\u8bad\u7ec3, \u5982\u679c\u8bcd\u6c47\u91cf\u7684\u6570\u91cf\u4e3a\u4e0a\u4e07\u4e2a, \u90a3\u4e48\u6211\u4eec\u5229\u7528softmax\u8ba1\u7b97\u6982\u7387\u65f6, \u9700\u8981\u5bf9\u8ba1\u7b97\u4e0a\u4e07\u4e2a\u6982\u7387\u503c, \u4e14\u6bcf\u4e2a\u503c\u90fd\u9700\u8981\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u6a21\u578b\u53c2\u6570, \u8fd9\u662f\u975e\u5e38\u6d88\u8017\u8ba1\u7b97\u8d44\u6e90\u7684, \u5e76\u4e14\u5b9e\u9645\u4e2d\u8bad\u7ec3\u8d77\u6765\u4f1a\u975e\u5e38\u6162. \u4e0d\u540c\u4e8e\u539f\u672c\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u66f4\u65b0\u6240\u6709\u7684\u6743\u91cd, \u8d1f\u91c7\u6837\u6bcf\u6b21\u8ba9\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4ec5\u4ec5\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u7684\u6743\u91cd, \u8fd9\u6837\u5c31\u4f1a\u964d\u4f4e\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u91cf. \u4e3e\u4f8b\u8bf4\u660e\uff08\u8d1f\u91c7\u6837\u539f\u7406\uff09: \u5f53\u6211\u4eec\u7528\u8bad\u7ec3\u6837\u672c ( input word: \"hello\", output word: \"man\") \u6765\u8bad\u7ec3\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u65f6, \u201c hello\u201d\u548c\u201cman\u201d\u90fd\u662f\u7ecf\u8fc7one-hot\u7f16\u7801\u7684. \u5982\u679c\u6211\u4eec\u7684vocabulary\u5927\u5c0f\u4e3a10000\u65f6, \u5728\u8f93\u51fa\u5c42, \u6211\u4eec\u671f\u671b\u5bf9\u5e94\u201cman\u201d\u5355\u8bcd\u7684\u90a3\u4e2a\u795e\u7ecf\u5143\u7ed3\u70b9\u8f93\u51fa1, \u5176\u4f599999\u4e2a\u90fd\u5e94\u8be5\u8f93\u51fa0. \u5728\u8fd9\u91cc, \u8fd99999\u4e2a\u6211\u4eec\u671f\u671b\u8f93\u51fa\u4e3a0\u7684\u795e\u7ecf\u5143\u7ed3\u70b9\u6240\u5bf9\u5e94\u7684\u5355\u8bcd\u6211\u4eec\u79f0\u4e3a\u201cnegative\u201d word. \u5f53\u4f7f\u7528\u8d1f\u91c7\u6837\u65f6, \u6211\u4eec\u5c06\u968f\u673a\u9009\u62e9\u4e00\u5c0f\u90e8\u5206\u7684negative words\uff08\u6bd4\u5982\u90095\u4e2anegative words\uff09\u6765\u66f4\u65b0\u5bf9\u5e94\u7684\u6743\u91cd. \u6211\u4eec\u4e5f\u4f1a\u5bf9\u6211\u4eec\u7684\u201cpositive\u201d word\u8fdb\u884c\u6743\u91cd\u66f4\u65b0\uff08\u5728\u6211\u4eec\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u8fd9\u4e2a\u5355\u8bcd\u6307\u7684\u662f\u201dman\u201c\uff09. \u6ce8\u610f, \u5bf9\u4e8e\u5c0f\u89c4\u6a21\u6570\u636e\u96c6, \u9009\u62e95-20\u4e2anegative words\u4f1a\u6bd4\u8f83\u597d, \u5bf9\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u53ef\u4ee5\u4ec5\u9009\u62e92-5\u4e2anegative words. \u5047\u5982\u6211\u4eec\u7684\u9690\u5c42-\u8f93\u51fa\u5c42\u62e5\u6709300 x 10000\u7684\u6743\u91cd\u77e9\u9635. \u5982\u679c\u4f7f\u7528\u4e86\u8d1f\u91c7\u6837\u7684\u65b9\u6cd5\u6211\u4eec\u4ec5\u4ec5\u53bb\u66f4\u65b0\u6211\u4eec\u7684positive word-\u201cman\u201d\u7684\u548c\u6211\u4eec\u9009\u62e9\u7684\u5176\u4ed65\u4e2anegative words\u7684\u7ed3\u70b9\u5bf9\u5e94\u7684\u6743\u91cd, \u5171\u8ba16\u4e2a\u8f93\u51fa\u795e\u7ecf\u5143, \u76f8\u5f53\u4e8e\u6bcf\u6b21\u53ea\u66f4\u65b0300\u00d76=1800\u4e2a\u6743\u91cd. \u5bf9\u4e8e3\u767e\u4e07\u7684\u6743\u91cd\u6765\u8bf4, \u76f8\u5f53\u4e8e\u53ea\u8ba1\u7b97\u4e860.06%\u7684\u6743\u91cd, \u8fd9\u6837\u8ba1\u7b97\u6548\u7387\u5c31\u5927\u5e45\u5ea6\u63d0\u9ad8.","title":"3.1 \u8d1f\u91c7\u6837\u539f\u7406"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#32","text":"\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6, \u9009\u62e9\u4e86\u90e8\u5206\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u635f\u5931, \u635f\u5931\u8ba1\u7b97\u66f4\u52a0\u7b80\u5355. \u6539\u8fdb\u6548\u679c, \u589e\u52a0\u90e8\u5206\u8d1f\u6837\u672c, \u80fd\u591f\u6a21\u62df\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u566a\u58f0\u60c5\u51b5, \u80fd\u591f\u8ba9\u6a21\u578b\u7684\u7a33\u5065\u6027\u66f4\u5f3a.","title":"3.2 \u8d1f\u91c7\u6837\u7684\u4f18\u52bf"},{"location":"05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html#4","text":"\u5b66\u4e60\u4e86\u89e3\u4ec0\u4e48\u662ffasttext\u6a21\u578b\u67b6\u6784: fastText \u6a21\u578b\u67b6\u6784\u548c Word2Vec \u4e2d\u7684 CBOW \u6a21\u578b\u5f88\u7c7b\u4f3c, \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e, fastText \u9884\u6d4b\u6807\u7b7e, \u800c CBOW \u6a21\u578b\u9884\u6d4b\u4e2d\u95f4\u8bcd. \u5b66\u4e60\u4e86\u5c42\u6b21softmax: \u4e3a\u4e86\u63d0\u9ad8\u6548\u7387, \u5728fastText\u4e2d\u8ba1\u7b97\u5206\u7c7b\u6807\u7b7e\u6982\u7387\u7684\u65f6\u5019, \u4e0d\u518d\u4f7f\u7528\u4f20\u7edf\u7684softmax\u6765\u8fdb\u884c\u591a\u5206\u7c7b\u7684\u8ba1\u7b97, \u800c\u662f\u4f7f\u7528\u54c8\u592b\u66fc\u6811, \u4f7f\u7528\u5c42\u6b21\u5316\u7684softmax\u6765\u8fdb\u884c\u6982\u7387\u7684\u8ba1\u7b97. \u54c8\u592b\u66fc\u6811\u5b9a\u4e49: \u5f53\u5229\u7528n \u4e2a\u7ed3\u70b9\u8bd5\u56fe\u6784\u5efa\u4e00\u68f5\u6811\u65f6, \u5982\u679c\u6784\u5efa\u7684\u8fd9\u68f5\u6811\u7684\u5e26\u6743\u8def\u5f84\u957f\u5ea6\u6700\u5c0f, \u79f0\u8fd9\u68f5\u6811\u4e3a\u201c\u6700\u4f18\u4e8c\u53c9\u6811\u201d, \u6709\u65f6\u4e5f\u53eb\u201c\u8d6b\u592b\u66fc\u6811\u201d\u6216\u8005\u201c\u54c8\u592b\u66fc\u6811\u201d. \u4f18\u70b9: \u4f20\u7edf\u7684softmax\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aL(labels\u7684\u6570\u91cf), \u4f46\u662f\u4f7f\u7528\u5c42\u6b21\u5316softmax\u4e4b\u540e\u65f6\u95f4\u590d\u6742\u5ea6\u7684log(L) (\u4e8c\u53c9\u6811\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u7684\u8fd1\u4f3c), \u4ece\u800c\u5728\u591a\u5206\u7c7b\u7684\u573a\u666f\u63d0\u9ad8\u4e86\u6548\u7387. \u5b66\u4e60\u4e86\u8d1f\u91c7\u6837: \u8d1f\u91c7\u6837\u539f\u7406: \u8d1f\u91c7\u6837\u6bcf\u6b21\u8ba9\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4ec5\u4ec5\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u7684\u6743\u91cd, \u8fd9\u6837\u5c31\u4f1a\u964d\u4f4e\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u91cf. \u4f18\u70b9: \u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6, \u9009\u62e9\u4e86\u90e8\u5206\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u635f\u5931, \u635f\u5931\u8ba1\u7b97\u66f4\u52a0\u7b80\u5355. \u6539\u8fdb\u6548\u679c, \u589e\u52a0\u90e8\u5206\u8d1f\u6837\u672c, \u80fd\u591f\u6a21\u62df\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u566a\u58f0\u60c5\u51b5, \u80fd\u591f\u8ba9\u6a21\u578b\u7684\u7a33\u5065\u6027\u66f4\u5f3a.","title":"4 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662f\u6587\u672c\u5206\u7c7b\u53ca\u5176\u79cd\u7c7b. \u638c\u63e1fasttext\u5de5\u5177\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u8fc7\u7a0b. 1 \u6587\u672c\u5206\u7c7b\u4ecb\u7ecd \u00b6 1.1 \u6587\u672c\u5206\u7c7b\u6982\u5ff5 \u00b6 \u6587\u672c\u5206\u7c7b\u7684\u662f\u5c06\u6587\u6863\uff08\u4f8b\u5982\u7535\u5b50\u90ae\u4ef6\uff0c\u5e16\u5b50\uff0c\u6587\u672c\u6d88\u606f\uff0c\u4ea7\u54c1\u8bc4\u8bba\u7b49\uff09\u5206\u914d\u7ed9\u4e00\u4e2a\u6216\u591a\u4e2a\u7c7b\u522b. \u5f53\u4eca\u6587\u672c\u5206\u7c7b\u7684\u5b9e\u73b0\u591a\u662f\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u5206\u7c7b\u89c4\u5219\u4ee5\u8fdb\u884c\u5206\u7c7b, \u56e0\u6b64\u6784\u5efa\u6587\u672c\u5206\u7c7b\u5668\u9700\u8981\u5e26\u6807\u7b7e\u7684\u6570\u636e. 1.2 \u6587\u672c\u5206\u7c7b\u79cd\u7c7b \u00b6 \u4e8c\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u7c7b\u4e24\u4e2a\u7c7b\u522b\u4e2d, \u5f80\u5f80\u8fd9\u4e24\u4e2a\u7c7b\u522b\u662f\u5bf9\u7acb\u9762, \u6bd4\u5982: \u5224\u65ad\u4e00\u53e5\u8bc4\u8bba\u662f\u597d\u8bc4\u8fd8\u662f\u5dee\u8bc4. \u5355\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u5165\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4e14\u6bcf\u6761\u6587\u672c\u53ea\u80fd\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u67d0\u4e00\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u4e2a\u4eba\u540d, \u5224\u65ad\u5b83\u662f\u6765\u81ea\u54ea\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d. \u591a\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u4eba\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4f46\u6bcf\u6761\u6587\u672c\u53ef\u4ee5\u5c5e\u4e8e\u591a\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u591a\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u6bb5\u63cf\u8ff0, \u5224\u65ad\u53ef\u80fd\u662f\u548c\u54ea\u4e9b\u5174\u8da3\u7231\u597d\u6709\u5173, \u4e00\u6bb5\u63cf\u8ff0\u4e2d\u53ef\u80fd\u5373\u8ba8\u8bba\u4e86\u7f8e\u98df, \u53c8\u592a\u8ba8\u8bba\u4e86\u6e38\u620f\u7231\u597d. 2 \u6587\u672c\u5206\u7c7b\u7684\u8fc7\u7a0b \u00b6 \u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u7684\u5212\u5206 \u7b2c\u4e09\u6b65: \u8bad\u7ec3\u6a21\u578b \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u5e76\u8bc4\u4f30 \u7b2c\u4e94\u6b65: \u6a21\u578b\u8c03\u4f18 \u7b2c\u516d\u6b65: \u6a21\u578b\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d 2.1 \u83b7\u53d6\u6570\u636e \u00b6 \u6570\u636e\u96c6\u4ecb\u7ecd\uff0c\u672c\u6848\u4f8b\u70f9\u996a\u76f8\u5173\u7684\u6570\u636e\u96c6, \u5b83\u662f\u7531facebook AI\u5b9e\u9a8c\u5ba4\u63d0\u4f9b\u7684\u6f14\u793a\u6570\u636e\u96c6 # \u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/cooking\u4e0b # \u67e5\u770b\u6570\u636e\u7684\u524d10\u6761 $ head cooking.stackexchange.txt __label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe? __label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments __label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove? __label__restaurant Michelin Three Star Restaurant; but if the chef is not there __label__knife-skills __label__dicing Without knife skills, how can I quickly and accurately dice vegetables? __label__storage-method __label__equipment __label__bread What's the purpose of a bread box? __label__baking __label__food-safety __label__substitutions __label__peanuts how to seperate peanut oil from roasted peanuts at home? __label__chocolate American equivalent for British chocolate terms __label__baking __label__oven __label__convection Fan bake vs bake __label__sauce __label__storage-lifetime __label__acidity __label__mayonnaise Regulation and balancing of readymade packed mayonnaise and other sauces \u6570\u636e\u8bf4\u660e: cooking.stackexchange.txt\u4e2d\u7684\u6bcf\u4e00\u884c\u90fd\u5305\u542b\u4e00\u4e2a\u6807\u7b7e\u5217\u8868\uff0c\u540e\u8ddf\u76f8\u5e94\u7684\u6587\u6863, \u6807\u7b7e\u5217\u8868\u4ee5\u7c7b\u4f3c\"__label__sauce __label__cheese\"\u7684\u5f62\u5f0f\u5c55\u73b0, \u4ee3\u8868\u6709\u4e24\u4e2a\u6807\u7b7esauce\u548ccheese, \u6240\u6709\u6807\u7b7e__label__\u5747\u4ee5\u524d\u7f00\u5f00\u5934\uff0c\u8fd9\u662ffastText\u8bc6\u522b\u6807\u7b7e\u6216\u5355\u8bcd\u7684\u65b9\u5f0f. \u6807\u7b7e\u4e4b\u540e\u7684\u4e00\u6bb5\u8bdd\u5c31\u662f\u6587\u672c\u4fe1\u606f.\u5982: How much does potato starch affect a cheese sauce recipe? 2.2 \u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u7684\u5212\u5206 \u00b6 # \u67e5\u770b\u6570\u636e\u603b\u6570 $ wc cooking . stackexchange . txt 15404 169582 1401900 cooking . stackexchange . txt # \u591a\u5c11\u884c \u591a\u5c11\u5355\u8bcd \u5360\u7528\u5b57\u8282\u6570(\u591a\u5927) # 12404\u6761\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e $ head -n 12404 cooking.stackexchange.txt > cooking.train # 3000\u6761\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u6570\u636e $ tail -n 3000 cooking.stackexchange.txt > cooking.valid 2.3 \u8bad\u7ec3\u6a21\u578b \u00b6 # \u5bfc\u5165fasttext import fasttext # \u4f7f\u7528fasttext\u7684train_supervised\u65b9\u6cd5\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u6a21\u578b\u7684\u8bad\u7ec3 model = fasttext . train_supervised ( input = \"data/cooking/cooking.train\" ) \u7ed3\u679c\u8f93\u51fa # \u83b7\u5f97\u7ed3\u679c Read 0 M words # \u4e0d\u91cd\u590d\u7684\u8bcd\u6c47\u603b\u6570 Number of words : 14543 # \u6807\u7b7e\u603b\u6570 Number of labels : 735 # Progress: \u8bad\u7ec3\u8fdb\u5ea6, \u56e0\u4e3a\u6211\u4eec\u8fd9\u91cc\u663e\u793a\u7684\u662f\u6700\u540e\u7684\u8bad\u7ec3\u5b8c\u6210\u4fe1\u606f, \u6240\u4ee5\u8fdb\u5ea6\u662f100% # words/sec/thread: \u6bcf\u4e2a\u7ebf\u7a0b\u6bcf\u79d2\u5904\u7406\u7684\u5e73\u5747\u8bcd\u6c47\u6570 # lr: \u5f53\u524d\u7684\u5b66\u4e60\u7387, \u56e0\u4e3a\u8bad\u7ec3\u5b8c\u6210\u6240\u4ee5\u5b66\u4e60\u7387\u662f0 # avg.loss: \u8bad\u7ec3\u8fc7\u7a0b\u7684\u5e73\u5747\u635f\u5931 # ETA: \u9884\u8ba1\u5269\u4f59\u8bad\u7ec3\u65f6\u95f4, \u56e0\u4e3a\u5df2\u8bad\u7ec3\u5b8c\u6210\u6240\u4ee5\u662f0 Progress : 100.0 % words / sec / thread : 60162 lr : 0.000000 avg . loss : 10.056812 ETA : 0 h 0 m 0 s 2.4 \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u5e76\u8bc4\u4f30 \u00b6 # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u4e00\u6bb5\u8f93\u5165\u6587\u672c, \u901a\u8fc7\u6211\u4eec\u5e38\u8bc6, \u53ef\u77e5\u9884\u6d4b\u662f\u6b63\u786e\u7684, \u4f46\u662f\u5bf9\u5e94\u9884\u6d4b\u6982\u7387\u5e76\u4e0d\u5927 >>> model . predict ( \"Which baking dish is best to bake a banana bread ?\" ) # \u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u9879\u4ee3\u8868\u6807\u7b7e, \u7b2c\u4e8c\u9879\u4ee3\u8868\u5bf9\u5e94\u7684\u6982\u7387 (( '__label__baking' ,), array ([ 0.06550845 ])) # \u901a\u8fc7\u6211\u4eec\u5e38\u8bc6\u53ef\u77e5\u9884\u6d4b\u662f\u9519\u8bef\u7684 >>> model.predict(\"Why not put knives in the dishwasher?\") (('__label__food-safety',), array([0.07541209])) # \u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b\u5230\u5e95\u8868\u73b0\u5982\u4f55, \u6211\u4eec\u57283000\u6761\u7684\u9a8c\u8bc1\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5 >>> model . test ( \"data/cooking/cooking.valid\" ) # \u5143\u7ec4\u4e2d\u7684\u6bcf\u9879\u5206\u522b\u4ee3\u8868, \u9a8c\u8bc1\u96c6\u6837\u672c\u6570\u91cf, \u7cbe\u5ea6\u4ee5\u53ca\u53ec\u56de\u7387 # \u6211\u4eec\u770b\u5230\u6a21\u578b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u8868\u73b0\u90fd\u5f88\u5dee, \u63a5\u4e0b\u6765\u6211\u4eec\u8bb2\u5b66\u4e60\u5982\u4f55\u8fdb\u884c\u4f18\u5316. ( 3000 , 0.124 , 0.0541 ) 2.5 \u6a21\u578b\u8c03\u4f18 \u00b6 1 \u539f\u59cb\u6570\u636e\u5904\u7406: \u00b6 # \u901a\u8fc7\u67e5\u770b\u6570\u636e, \u6211\u4eec\u53d1\u73b0\u6570\u636e\u4e2d\u5b58\u5728\u8bb8\u591a\u6807\u70b9\u7b26\u53f7\u4e0e\u5355\u8bcd\u76f8\u8fde\u4ee5\u53ca\u5927\u5c0f\u5199\u4e0d\u7edf\u4e00, # \u8fd9\u4e9b\u56e0\u7d20\u5bf9\u6211\u4eec\u6700\u7ec8\u7684\u5206\u7c7b\u76ee\u6807\u6ca1\u6709\u76ca\u5904, \u53cd\u662f\u589e\u52a0\u4e86\u6a21\u578b\u63d0\u53d6\u5206\u7c7b\u89c4\u5f8b\u7684\u96be\u5ea6, # \u56e0\u6b64\u6211\u4eec\u9009\u62e9\u5c06\u5b83\u4eec\u53bb\u9664\u6216\u8f6c\u5316 # \u5904\u7406\u524d\u7684\u90e8\u5206\u6570\u636e __label__fish Arctic char available in North-America __label__pasta __label__salt __label__boiling When cooking pasta in salted water how much of the salt is absorbed? __label__coffee Emergency Coffee via Chocolate Covered Coffee Beans? __label__cake Non-beet alternatives to standard red food dye __label__cheese __label__lentils Could cheese \"halt\" the tenderness of cooking lentils? __label__asian-cuisine __label__chili-peppers __label__kimchi __label__korean-cuisine What kind of peppers are used in Gochugaru ()? __label__consistency Pavlova Roll failure __label__eggs __label__bread What qualities should I be looking for when making the best French Toast? __label__meat __label__flour __label__stews __label__braising Coating meat in flour before browning, bad idea? __label__food-safety Raw roast beef on the edge of safe? __label__pork __label__food-identification How do I determine the cut of a pork steak prior to purchasing it? # \u901a\u8fc7\u670d\u52a1\u5668\u7ec8\u7aef\u8fdb\u884c\u7b80\u5355\u7684\u6570\u636e\u9884\u5904\u7406 # \u4f7f\u6807\u70b9\u7b26\u53f7\u4e0e\u5355\u8bcd\u5206\u79bb\u5e76\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd >> cat cooking.stackexchange.txt | sed -e \"s/\\([.\\!?,'/()]\\)/ \\1 /g\" | tr \"[:upper:]\" \"[:lower:]\" > cooking.preprocessed.txt >> head -n 12404 cooking.preprocessed.txt > cooking.pre.train >> tail -n 3000 cooking.preprocessed.txt > cooking.pre.valid # \u5904\u7406\u540e\u7684\u90e8\u5206\u6570\u636e __label__fish arctic char available in north-america __label__pasta __label__salt __label__boiling when cooking pasta in salted water how much of the salt is absorbed ? __label__coffee emergency coffee via chocolate covered coffee beans ? __label__cake non-beet alternatives to standard red food dye __label__cheese __label__lentils could cheese \"halt\" the tenderness of cooking lentils ? __label__asian-cuisine __label__chili-peppers __label__kimchi __label__korean-cuisine what kind of peppers are used in gochugaru ( ) ? __label__consistency pavlova roll failure __label__eggs __label__bread what qualities should i be looking for when making the best french toast ? __label__meat __label__flour __label__stews __label__braising coating meat in flour before browning , bad idea ? __label__food-safety raw roast beef on the edge of safe ? __label__pork __label__food-identification how do i determine the cut of a pork steak prior to purchasing it ? 2 \u6570\u636e\u5904\u7406\u540e\u8fdb\u884c\u8bad\u7ec3\u5e76\u6d4b\u8bd5: \u00b6 # \u91cd\u65b0\u8bad\u7ec3 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" ) Read 0 M words # \u4e0d\u91cd\u590d\u7684\u8bcd\u6c47\u603b\u6570\u51cf\u5c11\u5f88\u591a, \u56e0\u4e3a\u4e4b\u524d\u4f1a\u628a\u5e26\u5927\u5199\u5b57\u6bcd\u6216\u8005\u4e0e\u6807\u70b9\u7b26\u53f7\u76f8\u8fde\u63a5\u7684\u5355\u8bcd\u90fd\u8ba4\u4e3a\u662f\u65b0\u7684\u5355\u8bcd Number of words : 8952 Number of labels : 735 # \u6211\u4eec\u770b\u5230\u5e73\u5747\u635f\u5931\u6709\u6240\u4e0b\u964d Progress : 100.0 % words / sec / thread : 65737 lr : 0.000000 avg . loss : 9.966091 ETA : 0 h 0 m 0 s # \u91cd\u65b0\u6d4b\u8bd5 >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u90fd\u6709\u6240\u63d0\u5347 ( 3000 , 0.161 , 0.06962663975782038 ) 3 \u589e\u52a0\u8bad\u7ec3\u8f6e\u6570: \u00b6 # \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570epoch\u6765\u589e\u52a0\u8bad\u7ec3\u8f6e\u6570, \u9ed8\u8ba4\u7684\u8f6e\u6570\u662f5\u6b21 # \u589e\u52a0\u8f6e\u6570\u610f\u5473\u7740\u6a21\u578b\u80fd\u591f\u6709\u66f4\u591a\u673a\u4f1a\u5728\u6709\u9650\u6570\u636e\u4e2d\u8c03\u6574\u5206\u7c7b\u89c4\u5f8b, \u5f53\u7136\u8fd9\u4e5f\u4f1a\u589e\u52a0\u8bad\u7ec3\u65f6\u95f4 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , epoch = 25 ) Read 0 M words Number of words : 8952 Number of labels : 735 # \u6211\u4eec\u770b\u5230\u5e73\u5747\u635f\u5931\u7ee7\u7eed\u4e0b\u964d Progress : 100.0 % words / sec / thread : 66283 lr : 0.000000 avg . loss : 7.203885 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u5df2\u7ecf\u63d0\u5347\u5230\u4e8642%, \u53ec\u56de\u7387\u63d0\u5347\u81f318%. ( 3000 , 0.4206666666666667 , 0.1819230214790255 ) 4 \u8c03\u6574\u5b66\u4e60\u7387: \u00b6 # \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570lr\u6765\u8c03\u6574\u5b66\u4e60\u7387, \u9ed8\u8ba4\u7684\u5b66\u4e60\u7387\u5927\u5c0f\u662f0.1 # \u589e\u5927\u5b66\u4e60\u7387\u610f\u5473\u7740\u589e\u5927\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u6b65\u957f\u4f7f\u5176\u5728\u6709\u9650\u7684\u8fed\u4ee3\u6b65\u9aa4\u4e0b\u66f4\u63a5\u8fd1\u6700\u4f18\u70b9 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 1.0 , epoch = 25 ) Read 0 M words Number of words : 8952 Number of labels : 735 # \u5e73\u5747\u635f\u5931\u7ee7\u7eed\u4e0b\u964d Progress : 100.0 % words / sec / thread : 66027 lr : 0.000000 avg . loss : 4.278283 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u5df2\u7ecf\u63d0\u5347\u5230\u4e8647%, \u53ec\u56de\u7387\u63d0\u5347\u81f320%. ( 3000 , 0.47633333333333333 , 0.20599682860025947 ) 5 \u589e\u52a0n-gram\u7279\u5f81: \u00b6 # \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570wordNgrams\u6765\u6dfb\u52a0n-gram\u7279\u5f81, \u9ed8\u8ba4\u662f1, \u4e5f\u5c31\u662f\u6ca1\u6709n-gram\u7279\u5f81 # \u6211\u4eec\u8fd9\u91cc\u5c06\u5176\u8bbe\u7f6e\u4e3a2\u610f\u5473\u7740\u6dfb\u52a02-gram\u7279\u5f81, \u8fd9\u4e9b\u7279\u5f81\u5e2e\u52a9\u6a21\u578b\u6355\u6349\u524d\u540e\u8bcd\u6c47\u4e4b\u95f4\u7684\u5173\u8054, \u66f4\u597d\u7684\u63d0\u53d6\u5206\u7c7b\u89c4\u5219\u7528\u4e8e\u6a21\u578b\u5206\u7c7b, \u5f53\u7136\u8fd9\u4e5f\u4f1a\u589e\u52a0\u6a21\u578b\u8bad\u65f6\u7ec3\u5360\u7528\u7684\u8d44\u6e90\u548c\u65f6\u95f4. >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 1.0 , epoch = 25 , wordNgrams = 2 ) Read 0 M words Number of words : 8952 Number of labels : 735 # \u5e73\u5747\u635f\u5931\u7ee7\u7eed\u4e0b\u964d Progress : 100.0 % words / sec / thread : 65084 lr : 0.000000 avg . loss : 3.189422 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u5df2\u7ecf\u63d0\u5347\u5230\u4e8649%, \u53ec\u56de\u7387\u63d0\u5347\u81f321%. ( 3000 , 0.49233333333333335 , 0.2129162462159435 ) 6 \u4fee\u6539\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f: \u00b6 # \u968f\u7740\u6211\u4eec\u4e0d\u65ad\u7684\u6dfb\u52a0\u4f18\u5316\u7b56\u7565, \u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u4e5f\u8d8a\u6765\u8d8a\u6162 # \u4e3a\u4e86\u80fd\u591f\u63d0\u5347fasttext\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387, \u51cf\u5c0f\u8bad\u7ec3\u65f6\u95f4 # \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570loss\u6765\u4fee\u6539\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f(\u7b49\u6548\u4e8e\u8f93\u51fa\u5c42\u7684\u7ed3\u6784), \u9ed8\u8ba4\u662fsoftmax\u5c42\u7ed3\u6784 # \u6211\u4eec\u8fd9\u91cc\u5c06\u5176\u8bbe\u7f6e\u4e3a'hs', \u4ee3\u8868\u5c42\u6b21softmax\u7ed3\u6784, \u610f\u5473\u7740\u8f93\u51fa\u5c42\u7684\u7ed3\u6784(\u8ba1\u7b97\u65b9\u5f0f)\u53d1\u751f\u4e86\u53d8\u5316, \u5c06\u4ee5\u4e00\u79cd\u66f4\u4f4e\u590d\u6742\u5ea6\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u635f\u5931. >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 1.0 , epoch = 25 , wordNgrams = 2 , loss = 'hs' ) Read 0 M words Number of words : 8952 Number of labels : 735 Progress : 100.0 % words / sec / thread : 1341740 lr : 0.000000 avg . loss : 2.225962 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7a0d\u6709\u6ce2\u52a8, \u4f46\u8bad\u7ec3\u65f6\u95f4\u5374\u7f29\u77ed\u5230\u4ec5\u4ec5\u51e0\u79d2 ( 3000 , 0.483 , 0.20887991927346114 ) 7 \u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u4f18: \u00b6 # \u624b\u52a8\u8c03\u8282\u548c\u5bfb\u627e\u8d85\u53c2\u6570\u662f\u975e\u5e38\u56f0\u96be\u7684, \u56e0\u4e3a\u53c2\u6570\u4e4b\u95f4\u53ef\u80fd\u76f8\u5173, \u5e76\u4e14\u4e0d\u540c\u6570\u636e\u96c6\u9700\u8981\u7684\u8d85\u53c2\u6570\u4e5f\u4e0d\u540c, # \u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528fasttext\u7684autotuneValidationFile\u53c2\u6570\u8fdb\u884c\u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u4f18. # autotuneValidationFile\u53c2\u6570\u9700\u8981\u6307\u5b9a\u9a8c\u8bc1\u6570\u636e\u96c6\u6240\u5728\u8def\u5f84, \u5b83\u5c06\u5728\u9a8c\u8bc1\u96c6\u4e0a\u4f7f\u7528\u968f\u673a\u641c\u7d22\u65b9\u6cd5\u5bfb\u627e\u53ef\u80fd\u6700\u4f18\u7684\u8d85\u53c2\u6570. # \u4f7f\u7528autotuneDuration\u53c2\u6570\u53ef\u4ee5\u63a7\u5236\u968f\u673a\u641c\u7d22\u7684\u65f6\u95f4, \u9ed8\u8ba4\u662f300s, \u6839\u636e\u4e0d\u540c\u7684\u9700\u6c42, \u6211\u4eec\u53ef\u4ee5\u5ef6\u957f\u6216\u7f29\u77ed\u65f6\u95f4. # \u9a8c\u8bc1\u96c6\u8def\u5f84'cooking.valid', \u968f\u673a\u641c\u7d22600\u79d2 >>> model = fasttext . train_supervised ( input = 'data/cooking/cooking.pre.train' , autotuneValidationFile = 'data/cooking/cooking.pre.valid' , autotuneDuration = 600 ) Progress : 100.0 % Trials : 38 Best score : 0.376170 ETA : 0 h 0 m 0 s Training again with best arguments Read 0 M words Number of words : 8952 Number of labels : 735 Progress : 100.0 % words / sec / thread : 63791 lr : 0.000000 avg . loss : 1.888165 ETA : 0 h 0 m 0 s 8 \u5b9e\u9645\u751f\u4ea7\u4e2d\u591a\u6807\u7b7e\u591a\u5206\u7c7b\u95ee\u9898\u7684\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f \u00b6 # \u9488\u5bf9\u591a\u6807\u7b7e\u591a\u5206\u7c7b\u95ee\u9898, \u4f7f\u7528'softmax'\u6216\u8005'hs'\u6709\u65f6\u5e76\u4e0d\u662f\u6700\u4f73\u9009\u62e9, \u56e0\u4e3a\u6211\u4eec\u6700\u7ec8\u5f97\u5230\u7684\u5e94\u8be5\u662f\u591a\u4e2a\u6807\u7b7e, \u800csoftmax\u5374\u53ea\u80fd\u6700\u5927\u5316\u4e00\u4e2a\u6807\u7b7e. # \u6240\u4ee5\u6211\u4eec\u5f80\u5f80\u4f1a\u9009\u62e9\u4e3a\u6bcf\u4e2a\u6807\u7b7e\u4f7f\u7528\u72ec\u7acb\u7684\u4e8c\u5206\u7c7b\u5668\u4f5c\u4e3a\u8f93\u51fa\u5c42\u7ed3\u6784, # \u5bf9\u5e94\u7684\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f\u4e3a'ova'\u8868\u793aone vs all. # \u8fd9\u79cd\u8f93\u51fa\u5c42\u7684\u6539\u53d8\u610f\u5473\u7740\u6211\u4eec\u5728\u7edf\u4e00\u8bed\u6599\u4e0b\u540c\u65f6\u8bad\u7ec3\u591a\u4e2a\u4e8c\u5206\u7c7b\u6a21\u578b, # \u5bf9\u4e8e\u4e8c\u5206\u7c7b\u6a21\u578b\u6765\u8bb2, lr\u4e0d\u5b9c\u8fc7\u5927, \u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u4e3a0.2 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 0.2 , epoch = 25 , wordNgrams = 2 , loss = 'ova' ) Read 0 M words Number of words : 8952 Number of labels : 735 Progress : 100.0 % words / sec / thread : 65044 lr : 0.000000 avg . loss : 7.713312 ETA : 0 h 0 m 0 s # \u6211\u4eec\u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u5355\u6761\u6837\u672c\u7684\u9884\u6d4b, \u6765\u770b\u4e00\u4e0b\u5b83\u7684\u8f93\u51fa\u7ed3\u679c. # \u53c2\u6570k\u4ee3\u8868\u6307\u5b9a\u6a21\u578b\u8f93\u51fa\u591a\u5c11\u4e2a\u6807\u7b7e, \u9ed8\u8ba4\u4e3a1, \u8fd9\u91cc\u8bbe\u7f6e\u4e3a-1, \u610f\u5473\u7740\u5c3d\u53ef\u80fd\u591a\u7684\u8f93\u51fa. # \u53c2\u6570threshold\u4ee3\u8868\u663e\u793a\u7684\u6807\u7b7e\u6982\u7387\u9608\u503c, \u8bbe\u7f6e\u4e3a0.5, \u610f\u5473\u7740\u663e\u793a\u6982\u7387\u5927\u4e8e0.5\u7684\u6807\u7b7e >>> model . predict ( \"Which baking dish is best to bake a banana bread ?\" , k =- 1 , threshold = 0.5 ) # \u6211\u770b\u5230\u6839\u636e\u8f93\u5165\u6587\u672c, \u8f93\u51fa\u4e86\u5b83\u7684\u4e09\u4e2a\u6700\u6709\u53ef\u80fd\u7684\u6807\u7b7e (( u '__label__baking' , u '__label__bananas' , u '__label__bread' ), array ([ 1.00000 , 0.939923 , 0.592677 ])) 2.6 \u6a21\u578b\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d \u00b6 # \u4f7f\u7528model\u7684save_model\u65b9\u6cd5\u4fdd\u5b58\u6a21\u578b\u5230\u6307\u5b9a\u76ee\u5f55 # \u4f60\u53ef\u4ee5\u5728\u6307\u5b9a\u76ee\u5f55\u4e0b\u627e\u5230model_cooking.bin\u6587\u4ef6 >>> model . save_model ( \"data/model/model_cooking.bin\" ) # \u4f7f\u7528fasttext\u7684load_model\u8fdb\u884c\u6a21\u578b\u7684\u91cd\u52a0\u8f7d >>> model = fasttext . load_model ( \"data/model/model_cooking.bin\" ) # \u91cd\u52a0\u8f7d\u540e\u7684\u6a21\u578b\u4f7f\u7528\u65b9\u6cd5\u548c\u4e4b\u524d\u5b8c\u5168\u76f8\u540c >>> model . predict ( \"Which baking dish is best to bake a banana bread ?\" , k =- 1 , threshold = 0.5 ) (( u '__label__baking' , u '__label__bananas' , u '__label__bread' ), array ([ 1.00000 , 0.939923 , 0.592677 ])) 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6587\u672c\u5206\u7c7b: \u6587\u672c\u5206\u7c7b\u7684\u662f\u5c06\u6587\u6863\uff08\u4f8b\u5982\u7535\u5b50\u90ae\u4ef6\uff0c\u5e16\u5b50\uff0c\u6587\u672c\u6d88\u606f\uff0c\u4ea7\u54c1\u8bc4\u8bba\u7b49\uff09\u5206\u914d\u7ed9\u4e00\u4e2a\u6216\u591a\u4e2a\u7c7b\u522b. \u5f53\u4eca\u6587\u672c\u5206\u7c7b\u7684\u5b9e\u73b0\u591a\u662f\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u5206\u7c7b\u89c4\u5219\u4ee5\u8fdb\u884c\u5206\u7c7b, \u56e0\u6b64\u6784\u5efa\u6587\u672c\u5206\u7c7b\u5668\u9700\u8981\u5e26\u6807\u7b7e\u7684\u6570\u636e. \u6587\u672c\u5206\u7c7b\u7684\u79cd\u7c7b: \u4e8c\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u7c7b\u4e24\u4e2a\u7c7b\u522b\u4e2d, \u5f80\u5f80\u8fd9\u4e24\u4e2a\u7c7b\u522b\u662f\u5bf9\u7acb\u9762, \u6bd4\u5982: \u5224\u65ad\u4e00\u53e5\u8bc4\u8bba\u662f\u597d\u8bc4\u8fd8\u662f\u5dee\u8bc4. \u5355\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u5165\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4e14\u6bcf\u6761\u6587\u672c\u53ea\u80fd\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u67d0\u4e00\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u4e2a\u4eba\u540d, \u5224\u65ad\u5b83\u662f\u6765\u81ea\u54ea\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d. \u591a\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u4eba\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4f46\u6bcf\u6761\u6587\u672c\u53ef\u4ee5\u5c5e\u4e8e\u591a\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u591a\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u6bb5\u63cf\u8ff0, \u5224\u65ad\u53ef\u80fd\u662f\u548c\u54ea\u4e9b\u5174\u8da3\u7231\u597d\u6709\u5173, \u4e00\u6bb5\u63cf\u8ff0\u4e2d\u53ef\u80fd\u5373\u8ba8\u8bba\u4e86\u7f8e\u98df, \u53c8\u592a\u8ba8\u8bba\u4e86\u6e38\u620f\u7231\u597d. \u4f7f\u7528fasttext\u5de5\u5177\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u8fc7\u7a0b: \u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u7684\u5212\u5206 \u7b2c\u4e09\u6b65: \u8bad\u7ec3\u6a21\u578b \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u5e76\u8bc4\u4f30 \u7b2c\u4e94\u6b65: \u6a21\u578b\u8c03\u4f18 \u7b2c\u516d\u6b65: \u6a21\u578b\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d","title":"3 fasttext\u6587\u672c\u5206\u7c7b"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662f\u6587\u672c\u5206\u7c7b\u53ca\u5176\u79cd\u7c7b. \u638c\u63e1fasttext\u5de5\u5177\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#1","text":"","title":"1 \u6587\u672c\u5206\u7c7b\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#11","text":"\u6587\u672c\u5206\u7c7b\u7684\u662f\u5c06\u6587\u6863\uff08\u4f8b\u5982\u7535\u5b50\u90ae\u4ef6\uff0c\u5e16\u5b50\uff0c\u6587\u672c\u6d88\u606f\uff0c\u4ea7\u54c1\u8bc4\u8bba\u7b49\uff09\u5206\u914d\u7ed9\u4e00\u4e2a\u6216\u591a\u4e2a\u7c7b\u522b. \u5f53\u4eca\u6587\u672c\u5206\u7c7b\u7684\u5b9e\u73b0\u591a\u662f\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u5206\u7c7b\u89c4\u5219\u4ee5\u8fdb\u884c\u5206\u7c7b, \u56e0\u6b64\u6784\u5efa\u6587\u672c\u5206\u7c7b\u5668\u9700\u8981\u5e26\u6807\u7b7e\u7684\u6570\u636e.","title":"1.1 \u6587\u672c\u5206\u7c7b\u6982\u5ff5"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#12","text":"\u4e8c\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u7c7b\u4e24\u4e2a\u7c7b\u522b\u4e2d, \u5f80\u5f80\u8fd9\u4e24\u4e2a\u7c7b\u522b\u662f\u5bf9\u7acb\u9762, \u6bd4\u5982: \u5224\u65ad\u4e00\u53e5\u8bc4\u8bba\u662f\u597d\u8bc4\u8fd8\u662f\u5dee\u8bc4. \u5355\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u5165\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4e14\u6bcf\u6761\u6587\u672c\u53ea\u80fd\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u67d0\u4e00\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u4e2a\u4eba\u540d, \u5224\u65ad\u5b83\u662f\u6765\u81ea\u54ea\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d. \u591a\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u4eba\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4f46\u6bcf\u6761\u6587\u672c\u53ef\u4ee5\u5c5e\u4e8e\u591a\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u591a\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u6bb5\u63cf\u8ff0, \u5224\u65ad\u53ef\u80fd\u662f\u548c\u54ea\u4e9b\u5174\u8da3\u7231\u597d\u6709\u5173, \u4e00\u6bb5\u63cf\u8ff0\u4e2d\u53ef\u80fd\u5373\u8ba8\u8bba\u4e86\u7f8e\u98df, \u53c8\u592a\u8ba8\u8bba\u4e86\u6e38\u620f\u7231\u597d.","title":"1.2 \u6587\u672c\u5206\u7c7b\u79cd\u7c7b"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#2","text":"\u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u7684\u5212\u5206 \u7b2c\u4e09\u6b65: \u8bad\u7ec3\u6a21\u578b \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u5e76\u8bc4\u4f30 \u7b2c\u4e94\u6b65: \u6a21\u578b\u8c03\u4f18 \u7b2c\u516d\u6b65: \u6a21\u578b\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d","title":"2 \u6587\u672c\u5206\u7c7b\u7684\u8fc7\u7a0b"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#21","text":"\u6570\u636e\u96c6\u4ecb\u7ecd\uff0c\u672c\u6848\u4f8b\u70f9\u996a\u76f8\u5173\u7684\u6570\u636e\u96c6, \u5b83\u662f\u7531facebook AI\u5b9e\u9a8c\u5ba4\u63d0\u4f9b\u7684\u6f14\u793a\u6570\u636e\u96c6 # \u6570\u636e\u96c6\u5728\u865a\u62df\u673a/root/data/cooking\u4e0b # \u67e5\u770b\u6570\u636e\u7684\u524d10\u6761 $ head cooking.stackexchange.txt __label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe? __label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments __label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove? __label__restaurant Michelin Three Star Restaurant; but if the chef is not there __label__knife-skills __label__dicing Without knife skills, how can I quickly and accurately dice vegetables? __label__storage-method __label__equipment __label__bread What's the purpose of a bread box? __label__baking __label__food-safety __label__substitutions __label__peanuts how to seperate peanut oil from roasted peanuts at home? __label__chocolate American equivalent for British chocolate terms __label__baking __label__oven __label__convection Fan bake vs bake __label__sauce __label__storage-lifetime __label__acidity __label__mayonnaise Regulation and balancing of readymade packed mayonnaise and other sauces \u6570\u636e\u8bf4\u660e: cooking.stackexchange.txt\u4e2d\u7684\u6bcf\u4e00\u884c\u90fd\u5305\u542b\u4e00\u4e2a\u6807\u7b7e\u5217\u8868\uff0c\u540e\u8ddf\u76f8\u5e94\u7684\u6587\u6863, \u6807\u7b7e\u5217\u8868\u4ee5\u7c7b\u4f3c\"__label__sauce __label__cheese\"\u7684\u5f62\u5f0f\u5c55\u73b0, \u4ee3\u8868\u6709\u4e24\u4e2a\u6807\u7b7esauce\u548ccheese, \u6240\u6709\u6807\u7b7e__label__\u5747\u4ee5\u524d\u7f00\u5f00\u5934\uff0c\u8fd9\u662ffastText\u8bc6\u522b\u6807\u7b7e\u6216\u5355\u8bcd\u7684\u65b9\u5f0f. \u6807\u7b7e\u4e4b\u540e\u7684\u4e00\u6bb5\u8bdd\u5c31\u662f\u6587\u672c\u4fe1\u606f.\u5982: How much does potato starch affect a cheese sauce recipe?","title":"2.1 \u83b7\u53d6\u6570\u636e"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#22","text":"# \u67e5\u770b\u6570\u636e\u603b\u6570 $ wc cooking . stackexchange . txt 15404 169582 1401900 cooking . stackexchange . txt # \u591a\u5c11\u884c \u591a\u5c11\u5355\u8bcd \u5360\u7528\u5b57\u8282\u6570(\u591a\u5927) # 12404\u6761\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e $ head -n 12404 cooking.stackexchange.txt > cooking.train # 3000\u6761\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u6570\u636e $ tail -n 3000 cooking.stackexchange.txt > cooking.valid","title":"2.2 \u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u7684\u5212\u5206"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#23","text":"# \u5bfc\u5165fasttext import fasttext # \u4f7f\u7528fasttext\u7684train_supervised\u65b9\u6cd5\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u6a21\u578b\u7684\u8bad\u7ec3 model = fasttext . train_supervised ( input = \"data/cooking/cooking.train\" ) \u7ed3\u679c\u8f93\u51fa # \u83b7\u5f97\u7ed3\u679c Read 0 M words # \u4e0d\u91cd\u590d\u7684\u8bcd\u6c47\u603b\u6570 Number of words : 14543 # \u6807\u7b7e\u603b\u6570 Number of labels : 735 # Progress: \u8bad\u7ec3\u8fdb\u5ea6, \u56e0\u4e3a\u6211\u4eec\u8fd9\u91cc\u663e\u793a\u7684\u662f\u6700\u540e\u7684\u8bad\u7ec3\u5b8c\u6210\u4fe1\u606f, \u6240\u4ee5\u8fdb\u5ea6\u662f100% # words/sec/thread: \u6bcf\u4e2a\u7ebf\u7a0b\u6bcf\u79d2\u5904\u7406\u7684\u5e73\u5747\u8bcd\u6c47\u6570 # lr: \u5f53\u524d\u7684\u5b66\u4e60\u7387, \u56e0\u4e3a\u8bad\u7ec3\u5b8c\u6210\u6240\u4ee5\u5b66\u4e60\u7387\u662f0 # avg.loss: \u8bad\u7ec3\u8fc7\u7a0b\u7684\u5e73\u5747\u635f\u5931 # ETA: \u9884\u8ba1\u5269\u4f59\u8bad\u7ec3\u65f6\u95f4, \u56e0\u4e3a\u5df2\u8bad\u7ec3\u5b8c\u6210\u6240\u4ee5\u662f0 Progress : 100.0 % words / sec / thread : 60162 lr : 0.000000 avg . loss : 10.056812 ETA : 0 h 0 m 0 s","title":"2.3 \u8bad\u7ec3\u6a21\u578b"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#24","text":"# \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u4e00\u6bb5\u8f93\u5165\u6587\u672c, \u901a\u8fc7\u6211\u4eec\u5e38\u8bc6, \u53ef\u77e5\u9884\u6d4b\u662f\u6b63\u786e\u7684, \u4f46\u662f\u5bf9\u5e94\u9884\u6d4b\u6982\u7387\u5e76\u4e0d\u5927 >>> model . predict ( \"Which baking dish is best to bake a banana bread ?\" ) # \u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u9879\u4ee3\u8868\u6807\u7b7e, \u7b2c\u4e8c\u9879\u4ee3\u8868\u5bf9\u5e94\u7684\u6982\u7387 (( '__label__baking' ,), array ([ 0.06550845 ])) # \u901a\u8fc7\u6211\u4eec\u5e38\u8bc6\u53ef\u77e5\u9884\u6d4b\u662f\u9519\u8bef\u7684 >>> model.predict(\"Why not put knives in the dishwasher?\") (('__label__food-safety',), array([0.07541209])) # \u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b\u5230\u5e95\u8868\u73b0\u5982\u4f55, \u6211\u4eec\u57283000\u6761\u7684\u9a8c\u8bc1\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5 >>> model . test ( \"data/cooking/cooking.valid\" ) # \u5143\u7ec4\u4e2d\u7684\u6bcf\u9879\u5206\u522b\u4ee3\u8868, \u9a8c\u8bc1\u96c6\u6837\u672c\u6570\u91cf, \u7cbe\u5ea6\u4ee5\u53ca\u53ec\u56de\u7387 # \u6211\u4eec\u770b\u5230\u6a21\u578b\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u8868\u73b0\u90fd\u5f88\u5dee, \u63a5\u4e0b\u6765\u6211\u4eec\u8bb2\u5b66\u4e60\u5982\u4f55\u8fdb\u884c\u4f18\u5316. ( 3000 , 0.124 , 0.0541 )","title":"2.4 \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u5e76\u8bc4\u4f30"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#25","text":"","title":"2.5 \u6a21\u578b\u8c03\u4f18"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#1_1","text":"# \u901a\u8fc7\u67e5\u770b\u6570\u636e, \u6211\u4eec\u53d1\u73b0\u6570\u636e\u4e2d\u5b58\u5728\u8bb8\u591a\u6807\u70b9\u7b26\u53f7\u4e0e\u5355\u8bcd\u76f8\u8fde\u4ee5\u53ca\u5927\u5c0f\u5199\u4e0d\u7edf\u4e00, # \u8fd9\u4e9b\u56e0\u7d20\u5bf9\u6211\u4eec\u6700\u7ec8\u7684\u5206\u7c7b\u76ee\u6807\u6ca1\u6709\u76ca\u5904, \u53cd\u662f\u589e\u52a0\u4e86\u6a21\u578b\u63d0\u53d6\u5206\u7c7b\u89c4\u5f8b\u7684\u96be\u5ea6, # \u56e0\u6b64\u6211\u4eec\u9009\u62e9\u5c06\u5b83\u4eec\u53bb\u9664\u6216\u8f6c\u5316 # \u5904\u7406\u524d\u7684\u90e8\u5206\u6570\u636e __label__fish Arctic char available in North-America __label__pasta __label__salt __label__boiling When cooking pasta in salted water how much of the salt is absorbed? __label__coffee Emergency Coffee via Chocolate Covered Coffee Beans? __label__cake Non-beet alternatives to standard red food dye __label__cheese __label__lentils Could cheese \"halt\" the tenderness of cooking lentils? __label__asian-cuisine __label__chili-peppers __label__kimchi __label__korean-cuisine What kind of peppers are used in Gochugaru ()? __label__consistency Pavlova Roll failure __label__eggs __label__bread What qualities should I be looking for when making the best French Toast? __label__meat __label__flour __label__stews __label__braising Coating meat in flour before browning, bad idea? __label__food-safety Raw roast beef on the edge of safe? __label__pork __label__food-identification How do I determine the cut of a pork steak prior to purchasing it? # \u901a\u8fc7\u670d\u52a1\u5668\u7ec8\u7aef\u8fdb\u884c\u7b80\u5355\u7684\u6570\u636e\u9884\u5904\u7406 # \u4f7f\u6807\u70b9\u7b26\u53f7\u4e0e\u5355\u8bcd\u5206\u79bb\u5e76\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd >> cat cooking.stackexchange.txt | sed -e \"s/\\([.\\!?,'/()]\\)/ \\1 /g\" | tr \"[:upper:]\" \"[:lower:]\" > cooking.preprocessed.txt >> head -n 12404 cooking.preprocessed.txt > cooking.pre.train >> tail -n 3000 cooking.preprocessed.txt > cooking.pre.valid # \u5904\u7406\u540e\u7684\u90e8\u5206\u6570\u636e __label__fish arctic char available in north-america __label__pasta __label__salt __label__boiling when cooking pasta in salted water how much of the salt is absorbed ? __label__coffee emergency coffee via chocolate covered coffee beans ? __label__cake non-beet alternatives to standard red food dye __label__cheese __label__lentils could cheese \"halt\" the tenderness of cooking lentils ? __label__asian-cuisine __label__chili-peppers __label__kimchi __label__korean-cuisine what kind of peppers are used in gochugaru ( ) ? __label__consistency pavlova roll failure __label__eggs __label__bread what qualities should i be looking for when making the best french toast ? __label__meat __label__flour __label__stews __label__braising coating meat in flour before browning , bad idea ? __label__food-safety raw roast beef on the edge of safe ? __label__pork __label__food-identification how do i determine the cut of a pork steak prior to purchasing it ?","title":"1 \u539f\u59cb\u6570\u636e\u5904\u7406:"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#2_1","text":"# \u91cd\u65b0\u8bad\u7ec3 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" ) Read 0 M words # \u4e0d\u91cd\u590d\u7684\u8bcd\u6c47\u603b\u6570\u51cf\u5c11\u5f88\u591a, \u56e0\u4e3a\u4e4b\u524d\u4f1a\u628a\u5e26\u5927\u5199\u5b57\u6bcd\u6216\u8005\u4e0e\u6807\u70b9\u7b26\u53f7\u76f8\u8fde\u63a5\u7684\u5355\u8bcd\u90fd\u8ba4\u4e3a\u662f\u65b0\u7684\u5355\u8bcd Number of words : 8952 Number of labels : 735 # \u6211\u4eec\u770b\u5230\u5e73\u5747\u635f\u5931\u6709\u6240\u4e0b\u964d Progress : 100.0 % words / sec / thread : 65737 lr : 0.000000 avg . loss : 9.966091 ETA : 0 h 0 m 0 s # \u91cd\u65b0\u6d4b\u8bd5 >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u90fd\u6709\u6240\u63d0\u5347 ( 3000 , 0.161 , 0.06962663975782038 )","title":"2 \u6570\u636e\u5904\u7406\u540e\u8fdb\u884c\u8bad\u7ec3\u5e76\u6d4b\u8bd5:"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#3","text":"# \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570epoch\u6765\u589e\u52a0\u8bad\u7ec3\u8f6e\u6570, \u9ed8\u8ba4\u7684\u8f6e\u6570\u662f5\u6b21 # \u589e\u52a0\u8f6e\u6570\u610f\u5473\u7740\u6a21\u578b\u80fd\u591f\u6709\u66f4\u591a\u673a\u4f1a\u5728\u6709\u9650\u6570\u636e\u4e2d\u8c03\u6574\u5206\u7c7b\u89c4\u5f8b, \u5f53\u7136\u8fd9\u4e5f\u4f1a\u589e\u52a0\u8bad\u7ec3\u65f6\u95f4 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , epoch = 25 ) Read 0 M words Number of words : 8952 Number of labels : 735 # \u6211\u4eec\u770b\u5230\u5e73\u5747\u635f\u5931\u7ee7\u7eed\u4e0b\u964d Progress : 100.0 % words / sec / thread : 66283 lr : 0.000000 avg . loss : 7.203885 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u5df2\u7ecf\u63d0\u5347\u5230\u4e8642%, \u53ec\u56de\u7387\u63d0\u5347\u81f318%. ( 3000 , 0.4206666666666667 , 0.1819230214790255 )","title":"3 \u589e\u52a0\u8bad\u7ec3\u8f6e\u6570:"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#4","text":"# \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570lr\u6765\u8c03\u6574\u5b66\u4e60\u7387, \u9ed8\u8ba4\u7684\u5b66\u4e60\u7387\u5927\u5c0f\u662f0.1 # \u589e\u5927\u5b66\u4e60\u7387\u610f\u5473\u7740\u589e\u5927\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u6b65\u957f\u4f7f\u5176\u5728\u6709\u9650\u7684\u8fed\u4ee3\u6b65\u9aa4\u4e0b\u66f4\u63a5\u8fd1\u6700\u4f18\u70b9 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 1.0 , epoch = 25 ) Read 0 M words Number of words : 8952 Number of labels : 735 # \u5e73\u5747\u635f\u5931\u7ee7\u7eed\u4e0b\u964d Progress : 100.0 % words / sec / thread : 66027 lr : 0.000000 avg . loss : 4.278283 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u5df2\u7ecf\u63d0\u5347\u5230\u4e8647%, \u53ec\u56de\u7387\u63d0\u5347\u81f320%. ( 3000 , 0.47633333333333333 , 0.20599682860025947 )","title":"4 \u8c03\u6574\u5b66\u4e60\u7387:"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#5-n-gram","text":"# \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570wordNgrams\u6765\u6dfb\u52a0n-gram\u7279\u5f81, \u9ed8\u8ba4\u662f1, \u4e5f\u5c31\u662f\u6ca1\u6709n-gram\u7279\u5f81 # \u6211\u4eec\u8fd9\u91cc\u5c06\u5176\u8bbe\u7f6e\u4e3a2\u610f\u5473\u7740\u6dfb\u52a02-gram\u7279\u5f81, \u8fd9\u4e9b\u7279\u5f81\u5e2e\u52a9\u6a21\u578b\u6355\u6349\u524d\u540e\u8bcd\u6c47\u4e4b\u95f4\u7684\u5173\u8054, \u66f4\u597d\u7684\u63d0\u53d6\u5206\u7c7b\u89c4\u5219\u7528\u4e8e\u6a21\u578b\u5206\u7c7b, \u5f53\u7136\u8fd9\u4e5f\u4f1a\u589e\u52a0\u6a21\u578b\u8bad\u65f6\u7ec3\u5360\u7528\u7684\u8d44\u6e90\u548c\u65f6\u95f4. >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 1.0 , epoch = 25 , wordNgrams = 2 ) Read 0 M words Number of words : 8952 Number of labels : 735 # \u5e73\u5747\u635f\u5931\u7ee7\u7eed\u4e0b\u964d Progress : 100.0 % words / sec / thread : 65084 lr : 0.000000 avg . loss : 3.189422 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u5df2\u7ecf\u63d0\u5347\u5230\u4e8649%, \u53ec\u56de\u7387\u63d0\u5347\u81f321%. ( 3000 , 0.49233333333333335 , 0.2129162462159435 )","title":"5 \u589e\u52a0n-gram\u7279\u5f81:"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#6","text":"# \u968f\u7740\u6211\u4eec\u4e0d\u65ad\u7684\u6dfb\u52a0\u4f18\u5316\u7b56\u7565, \u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u4e5f\u8d8a\u6765\u8d8a\u6162 # \u4e3a\u4e86\u80fd\u591f\u63d0\u5347fasttext\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387, \u51cf\u5c0f\u8bad\u7ec3\u65f6\u95f4 # \u8bbe\u7f6etrain_supervised\u65b9\u6cd5\u4e2d\u7684\u53c2\u6570loss\u6765\u4fee\u6539\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f(\u7b49\u6548\u4e8e\u8f93\u51fa\u5c42\u7684\u7ed3\u6784), \u9ed8\u8ba4\u662fsoftmax\u5c42\u7ed3\u6784 # \u6211\u4eec\u8fd9\u91cc\u5c06\u5176\u8bbe\u7f6e\u4e3a'hs', \u4ee3\u8868\u5c42\u6b21softmax\u7ed3\u6784, \u610f\u5473\u7740\u8f93\u51fa\u5c42\u7684\u7ed3\u6784(\u8ba1\u7b97\u65b9\u5f0f)\u53d1\u751f\u4e86\u53d8\u5316, \u5c06\u4ee5\u4e00\u79cd\u66f4\u4f4e\u590d\u6742\u5ea6\u7684\u65b9\u5f0f\u6765\u8ba1\u7b97\u635f\u5931. >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 1.0 , epoch = 25 , wordNgrams = 2 , loss = 'hs' ) Read 0 M words Number of words : 8952 Number of labels : 735 Progress : 100.0 % words / sec / thread : 1341740 lr : 0.000000 avg . loss : 2.225962 ETA : 0 h 0 m 0 s >>> model . test ( \"data/cooking/cooking.pre.valid\" ) # \u6211\u4eec\u770b\u5230\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7a0d\u6709\u6ce2\u52a8, \u4f46\u8bad\u7ec3\u65f6\u95f4\u5374\u7f29\u77ed\u5230\u4ec5\u4ec5\u51e0\u79d2 ( 3000 , 0.483 , 0.20887991927346114 )","title":"6 \u4fee\u6539\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f:"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#7","text":"# \u624b\u52a8\u8c03\u8282\u548c\u5bfb\u627e\u8d85\u53c2\u6570\u662f\u975e\u5e38\u56f0\u96be\u7684, \u56e0\u4e3a\u53c2\u6570\u4e4b\u95f4\u53ef\u80fd\u76f8\u5173, \u5e76\u4e14\u4e0d\u540c\u6570\u636e\u96c6\u9700\u8981\u7684\u8d85\u53c2\u6570\u4e5f\u4e0d\u540c, # \u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528fasttext\u7684autotuneValidationFile\u53c2\u6570\u8fdb\u884c\u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u4f18. # autotuneValidationFile\u53c2\u6570\u9700\u8981\u6307\u5b9a\u9a8c\u8bc1\u6570\u636e\u96c6\u6240\u5728\u8def\u5f84, \u5b83\u5c06\u5728\u9a8c\u8bc1\u96c6\u4e0a\u4f7f\u7528\u968f\u673a\u641c\u7d22\u65b9\u6cd5\u5bfb\u627e\u53ef\u80fd\u6700\u4f18\u7684\u8d85\u53c2\u6570. # \u4f7f\u7528autotuneDuration\u53c2\u6570\u53ef\u4ee5\u63a7\u5236\u968f\u673a\u641c\u7d22\u7684\u65f6\u95f4, \u9ed8\u8ba4\u662f300s, \u6839\u636e\u4e0d\u540c\u7684\u9700\u6c42, \u6211\u4eec\u53ef\u4ee5\u5ef6\u957f\u6216\u7f29\u77ed\u65f6\u95f4. # \u9a8c\u8bc1\u96c6\u8def\u5f84'cooking.valid', \u968f\u673a\u641c\u7d22600\u79d2 >>> model = fasttext . train_supervised ( input = 'data/cooking/cooking.pre.train' , autotuneValidationFile = 'data/cooking/cooking.pre.valid' , autotuneDuration = 600 ) Progress : 100.0 % Trials : 38 Best score : 0.376170 ETA : 0 h 0 m 0 s Training again with best arguments Read 0 M words Number of words : 8952 Number of labels : 735 Progress : 100.0 % words / sec / thread : 63791 lr : 0.000000 avg . loss : 1.888165 ETA : 0 h 0 m 0 s","title":"7 \u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u4f18:"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#8","text":"# \u9488\u5bf9\u591a\u6807\u7b7e\u591a\u5206\u7c7b\u95ee\u9898, \u4f7f\u7528'softmax'\u6216\u8005'hs'\u6709\u65f6\u5e76\u4e0d\u662f\u6700\u4f73\u9009\u62e9, \u56e0\u4e3a\u6211\u4eec\u6700\u7ec8\u5f97\u5230\u7684\u5e94\u8be5\u662f\u591a\u4e2a\u6807\u7b7e, \u800csoftmax\u5374\u53ea\u80fd\u6700\u5927\u5316\u4e00\u4e2a\u6807\u7b7e. # \u6240\u4ee5\u6211\u4eec\u5f80\u5f80\u4f1a\u9009\u62e9\u4e3a\u6bcf\u4e2a\u6807\u7b7e\u4f7f\u7528\u72ec\u7acb\u7684\u4e8c\u5206\u7c7b\u5668\u4f5c\u4e3a\u8f93\u51fa\u5c42\u7ed3\u6784, # \u5bf9\u5e94\u7684\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f\u4e3a'ova'\u8868\u793aone vs all. # \u8fd9\u79cd\u8f93\u51fa\u5c42\u7684\u6539\u53d8\u610f\u5473\u7740\u6211\u4eec\u5728\u7edf\u4e00\u8bed\u6599\u4e0b\u540c\u65f6\u8bad\u7ec3\u591a\u4e2a\u4e8c\u5206\u7c7b\u6a21\u578b, # \u5bf9\u4e8e\u4e8c\u5206\u7c7b\u6a21\u578b\u6765\u8bb2, lr\u4e0d\u5b9c\u8fc7\u5927, \u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u4e3a0.2 >>> model = fasttext . train_supervised ( input = \"data/cooking/cooking.pre.train\" , lr = 0.2 , epoch = 25 , wordNgrams = 2 , loss = 'ova' ) Read 0 M words Number of words : 8952 Number of labels : 735 Progress : 100.0 % words / sec / thread : 65044 lr : 0.000000 avg . loss : 7.713312 ETA : 0 h 0 m 0 s # \u6211\u4eec\u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u5355\u6761\u6837\u672c\u7684\u9884\u6d4b, \u6765\u770b\u4e00\u4e0b\u5b83\u7684\u8f93\u51fa\u7ed3\u679c. # \u53c2\u6570k\u4ee3\u8868\u6307\u5b9a\u6a21\u578b\u8f93\u51fa\u591a\u5c11\u4e2a\u6807\u7b7e, \u9ed8\u8ba4\u4e3a1, \u8fd9\u91cc\u8bbe\u7f6e\u4e3a-1, \u610f\u5473\u7740\u5c3d\u53ef\u80fd\u591a\u7684\u8f93\u51fa. # \u53c2\u6570threshold\u4ee3\u8868\u663e\u793a\u7684\u6807\u7b7e\u6982\u7387\u9608\u503c, \u8bbe\u7f6e\u4e3a0.5, \u610f\u5473\u7740\u663e\u793a\u6982\u7387\u5927\u4e8e0.5\u7684\u6807\u7b7e >>> model . predict ( \"Which baking dish is best to bake a banana bread ?\" , k =- 1 , threshold = 0.5 ) # \u6211\u770b\u5230\u6839\u636e\u8f93\u5165\u6587\u672c, \u8f93\u51fa\u4e86\u5b83\u7684\u4e09\u4e2a\u6700\u6709\u53ef\u80fd\u7684\u6807\u7b7e (( u '__label__baking' , u '__label__bananas' , u '__label__bread' ), array ([ 1.00000 , 0.939923 , 0.592677 ]))","title":"8 \u5b9e\u9645\u751f\u4ea7\u4e2d\u591a\u6807\u7b7e\u591a\u5206\u7c7b\u95ee\u9898\u7684\u635f\u5931\u8ba1\u7b97\u65b9\u5f0f"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#26","text":"# \u4f7f\u7528model\u7684save_model\u65b9\u6cd5\u4fdd\u5b58\u6a21\u578b\u5230\u6307\u5b9a\u76ee\u5f55 # \u4f60\u53ef\u4ee5\u5728\u6307\u5b9a\u76ee\u5f55\u4e0b\u627e\u5230model_cooking.bin\u6587\u4ef6 >>> model . save_model ( \"data/model/model_cooking.bin\" ) # \u4f7f\u7528fasttext\u7684load_model\u8fdb\u884c\u6a21\u578b\u7684\u91cd\u52a0\u8f7d >>> model = fasttext . load_model ( \"data/model/model_cooking.bin\" ) # \u91cd\u52a0\u8f7d\u540e\u7684\u6a21\u578b\u4f7f\u7528\u65b9\u6cd5\u548c\u4e4b\u524d\u5b8c\u5168\u76f8\u540c >>> model . predict ( \"Which baking dish is best to bake a banana bread ?\" , k =- 1 , threshold = 0.5 ) (( u '__label__baking' , u '__label__bananas' , u '__label__bread' ), array ([ 1.00000 , 0.939923 , 0.592677 ]))","title":"2.6 \u6a21\u578b\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d"},{"location":"05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html#3_1","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u6587\u672c\u5206\u7c7b: \u6587\u672c\u5206\u7c7b\u7684\u662f\u5c06\u6587\u6863\uff08\u4f8b\u5982\u7535\u5b50\u90ae\u4ef6\uff0c\u5e16\u5b50\uff0c\u6587\u672c\u6d88\u606f\uff0c\u4ea7\u54c1\u8bc4\u8bba\u7b49\uff09\u5206\u914d\u7ed9\u4e00\u4e2a\u6216\u591a\u4e2a\u7c7b\u522b. \u5f53\u4eca\u6587\u672c\u5206\u7c7b\u7684\u5b9e\u73b0\u591a\u662f\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u5206\u7c7b\u89c4\u5219\u4ee5\u8fdb\u884c\u5206\u7c7b, \u56e0\u6b64\u6784\u5efa\u6587\u672c\u5206\u7c7b\u5668\u9700\u8981\u5e26\u6807\u7b7e\u7684\u6570\u636e. \u6587\u672c\u5206\u7c7b\u7684\u79cd\u7c7b: \u4e8c\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u7c7b\u4e24\u4e2a\u7c7b\u522b\u4e2d, \u5f80\u5f80\u8fd9\u4e24\u4e2a\u7c7b\u522b\u662f\u5bf9\u7acb\u9762, \u6bd4\u5982: \u5224\u65ad\u4e00\u53e5\u8bc4\u8bba\u662f\u597d\u8bc4\u8fd8\u662f\u5dee\u8bc4. \u5355\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u5165\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4e14\u6bcf\u6761\u6587\u672c\u53ea\u80fd\u5c5e\u4e8e\u67d0\u4e00\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u67d0\u4e00\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u4e2a\u4eba\u540d, \u5224\u65ad\u5b83\u662f\u6765\u81ea\u54ea\u4e2a\u56fd\u5bb6\u7684\u4eba\u540d. \u591a\u6807\u7b7e\u591a\u5206\u7c7b: \u6587\u672c\u88ab\u5206\u4eba\u5230\u591a\u4e2a\u7c7b\u522b\u4e2d, \u4f46\u6bcf\u6761\u6587\u672c\u53ef\u4ee5\u5c5e\u4e8e\u591a\u4e2a\u7c7b\u522b(\u5373\u88ab\u6253\u4e0a\u591a\u4e2a\u6807\u7b7e), \u6bd4\u5982: \u8f93\u5165\u4e00\u6bb5\u63cf\u8ff0, \u5224\u65ad\u53ef\u80fd\u662f\u548c\u54ea\u4e9b\u5174\u8da3\u7231\u597d\u6709\u5173, \u4e00\u6bb5\u63cf\u8ff0\u4e2d\u53ef\u80fd\u5373\u8ba8\u8bba\u4e86\u7f8e\u98df, \u53c8\u592a\u8ba8\u8bba\u4e86\u6e38\u620f\u7231\u597d. \u4f7f\u7528fasttext\u5de5\u5177\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u8fc7\u7a0b: \u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\u7684\u5212\u5206 \u7b2c\u4e09\u6b65: \u8bad\u7ec3\u6a21\u578b \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u5e76\u8bc4\u4f30 \u7b2c\u4e94\u6b65: \u6a21\u578b\u8c03\u4f18 \u7b2c\u516d\u6b65: \u6a21\u578b\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d","title":"3 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u8bcd\u5411\u91cf\u7684\u76f8\u5173\u77e5\u8bc6. \u638c\u63e1fasttext\u5de5\u5177\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u8fc7\u7a0b. 1 \u8bad\u7ec3\u8bcd\u5411\u91cf\u4ecb\u7ecd \u00b6 1.1 \u8bcd\u5411\u91cf\u7684\u76f8\u5173\u77e5\u8bc6: \u00b6 \u7528\u5411\u91cf\u8868\u793a\u6587\u672c\u4e2d\u7684\u8bcd\u6c47(\u6216\u5b57\u7b26)\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u6700\u6d41\u884c\u7684\u505a\u6cd5, \u8fd9\u4e9b\u5411\u91cf\u80fd\u591f\u5f88\u597d\u7684\u6355\u6349\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u8bcd\u5411\u91cf\u7684\u5404\u79cdNLP\u4efb\u52a1\u7684\u6548\u679c. 1.2 \u7ec3\u8bcd\u5411\u91cf\u7684\u8fc7\u7a0b \u00b6 \u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d 2 \u5b9e\u73b0\u6b65\u9aa4 \u00b6 2.1 \u6570\u636e\u4ecb\u7ecd \u00b6 \u6570\u636e\u96c6\u4ecd\u7136\u4f7f\u7528\uff1a\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u90e8\u5206\u7f51\u9875\u4fe1\u606f \u6ce8\u610f\uff1a\u539f\u59cb\u6570\u636e\u96c6\u5df2\u7ecf\u653e\u5728/root/data/enwik9.zip\uff0c\u89e3\u538b\u540e\u6570\u636e\u4e3a/root/data/enwik9\uff0c\u9884\u5904\u7406\u540e\u7684\u6570\u636e\u4e3a/root/data/fil9 \u67e5\u770b\u9884\u5904\u7406\u540e\u7684\u6570\u636e: # \u67e5\u770b\u524d80\u4e2a\u5b57\u7b26 head -c 80 data/fil9 # \u8f93\u51fa\u7ed3\u679c\u4e3a\u7531\u7a7a\u683c\u5206\u5272\u7684\u5355\u8bcd anarchism originated as a term of abuse first used against early working class 2.2 \u8bad\u7ec3\u8bcd\u5411\u91cf \u00b6 # \u4ee3\u7801\u8fd0\u884c\u5728python\u89e3\u91ca\u5668\u4e2d # \u5bfc\u5165fasttext >>> import fasttext # \u4f7f\u7528fasttext\u7684train_unsupervised(\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5)\u8fdb\u884c\u8bcd\u5411\u91cf\u7684\u8bad\u7ec3 # \u5b83\u7684\u53c2\u6570\u662f\u6570\u636e\u96c6\u7684\u6301\u4e45\u5316\u6587\u4ef6\u8def\u5f84'data/fil9' # \u6ce8\u610f\uff0c\u8be5\u884c\u4ee3\u7801\u6267\u884c\u8017\u65f6\u5f88\u957f >>> model1 = fasttext . train_unsupervised ( 'data/fil9' ) # \u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b >>> model = fasttext . load_model ( \"data/fil9.bin\" ) # \u6709\u6548\u8bad\u7ec3\u8bcd\u6c47\u91cf\u4e3a124M, \u5171218316\u4e2a\u5355\u8bcd Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 53996 lr : 0.000000 loss : 0.734999 ETA : 0 h 0 m \u67e5\u770b\u5355\u8bcd\u5bf9\u5e94\u7684\u8bcd\u5411\u91cf: # \u901a\u8fc7get_word_vector\u65b9\u6cd5\u6765\u83b7\u5f97\u6307\u5b9a\u8bcd\u6c47\u7684\u8bcd\u5411\u91cf >>> model . get_word_vector ( \"the\" ) array ([ - 0.03087516 , 0.09221972 , 0.17660329 , 0.17308897 , 0.12863874 , 0.13912526 , - 0.09851588 , 0.00739991 , 0.37038437 , - 0.00845221 , ... - 0.21184735 , - 0.05048715 , - 0.34571868 , 0.23765688 , 0.23726143 ], dtype = float32 ) 2.3 \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u00b6 # \u5728\u8bad\u7ec3\u8bcd\u5411\u91cf\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u53ef\u4ee5\u8bbe\u5b9a\u5f88\u591a\u5e38\u7528\u8d85\u53c2\u6570\u6765\u8c03\u8282\u6211\u4eec\u7684\u6a21\u578b\u6548\u679c, \u5982: # \u65e0\u76d1\u7763\u8bad\u7ec3\u6a21\u5f0f: 'skipgram' \u6216\u8005 'cbow', \u9ed8\u8ba4\u4e3a'skipgram', \u5728\u5b9e\u8df5\u4e2d\uff0cskipgram\u6a21\u5f0f\u5728\u5229\u7528\u5b50\u8bcd\u65b9\u9762\u6bd4cbow\u66f4\u597d. # \u8bcd\u5d4c\u5165\u7ef4\u5ea6dim: \u9ed8\u8ba4\u4e3a100, \u4f46\u968f\u7740\u8bed\u6599\u5e93\u7684\u589e\u5927, \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5f80\u5f80\u4e5f\u8981\u66f4\u5927. # \u6570\u636e\u5faa\u73af\u6b21\u6570epoch: \u9ed8\u8ba4\u4e3a5, \u4f46\u5f53\u4f60\u7684\u6570\u636e\u96c6\u8db3\u591f\u5927, \u53ef\u80fd\u4e0d\u9700\u8981\u90a3\u4e48\u591a\u6b21. # \u5b66\u4e60\u7387lr: \u9ed8\u8ba4\u4e3a0.05, \u6839\u636e\u7ecf\u9a8c, \u5efa\u8bae\u9009\u62e9[0.01\uff0c1]\u8303\u56f4\u5185. # \u4f7f\u7528\u7684\u7ebf\u7a0b\u6570thread: \u9ed8\u8ba4\u4e3a12\u4e2a\u7ebf\u7a0b, \u4e00\u822c\u5efa\u8bae\u548c\u4f60\u7684cpu\u6838\u6570\u76f8\u540c. >>> model = fasttext . train_unsupervised ( 'data/fil9' , \"cbow\" , dim = 300 , epoch = 1 , lr = 0.1 , thread = 8 ) Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 49523 lr : 0.000000 avg . loss : 1.777205 ETA : 0 h 0 m 0 s 2.4 \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u00b6 # \u68c0\u67e5\u5355\u8bcd\u5411\u91cf\u8d28\u91cf\u7684\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\u5c31\u662f\u67e5\u770b\u5176\u90bb\u8fd1\u5355\u8bcd, \u901a\u8fc7\u6211\u4eec\u4e3b\u89c2\u6765\u5224\u65ad\u8fd9\u4e9b\u90bb\u8fd1\u5355\u8bcd\u662f\u5426\u4e0e\u76ee\u6807\u5355\u8bcd\u76f8\u5173\u6765\u7c97\u7565\u8bc4\u5b9a\u6a21\u578b\u6548\u679c\u597d\u574f. # \u67e5\u627e\"\u8fd0\u52a8\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\"\u4f53\u80b2\u7f51\", \"\u8fd0\u52a8\u6c7d\u8f66\", \"\u8fd0\u52a8\u670d\"\u7b49. >>> model . get_nearest_neighbors ( 'sports' ) [( 0.8414610624313354 , 'sportsnet' ), ( 0.8134572505950928 , 'sport' ), ( 0.8100415468215942 , 'sportscars' ), ( 0.8021156787872314 , 'sportsground' ), ( 0.7889881134033203 , 'sportswomen' ), ( 0.7863013744354248 , 'sportsplex' ), ( 0.7786710262298584 , 'sporty' ), ( 0.7696356177330017 , 'sportscar' ), ( 0.7619683146476746 , 'sportswear' ), ( 0.7600985765457153 , 'sportin' )] # \u67e5\u627e\"\u97f3\u4e50\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u97f3\u4e50\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'music' ) [( 0.8908010125160217 , 'emusic' ), ( 0.8464668393135071 , 'musicmoz' ), ( 0.8444250822067261 , 'musics' ), ( 0.8113634586334229 , 'allmusic' ), ( 0.8106718063354492 , 'musices' ), ( 0.8049437999725342 , 'musicam' ), ( 0.8004694581031799 , 'musicom' ), ( 0.7952923774719238 , 'muchmusic' ), ( 0.7852965593338013 , 'musicweb' ), ( 0.7767147421836853 , 'musico' )] # \u67e5\u627e\"\u5c0f\u72d7\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u5c0f\u72d7\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'dog' ) [( 0.8456876873970032 , 'catdog' ), ( 0.7480780482292175 , 'dogcow' ), ( 0.7289096117019653 , 'sleddog' ), ( 0.7269964218139648 , 'hotdog' ), ( 0.7114801406860352 , 'sheepdog' ), ( 0.6947550773620605 , 'dogo' ), ( 0.6897546648979187 , 'bodog' ), ( 0.6621081829071045 , 'maddog' ), ( 0.6605004072189331 , 'dogs' ), ( 0.6398137211799622 , 'dogpile' )] 2.5 \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d \u00b6 # \u4f7f\u7528save_model\u4fdd\u5b58\u6a21\u578b >>> model . save_model ( \"data/fil91.bin\" ) # \u4f7f\u7528fasttext.load_model\u52a0\u8f7d\u6a21\u578b >>> model = fasttext . load_model ( \"data/fil91.bin\" ) >>> model . get_word_vector ( \"the\" ) array ([ - 0.03087516 , 0.09221972 , 0.17660329 , 0.17308897 , 0.12863874 , 0.13912526 , - 0.09851588 , 0.00739991 , 0.37038437 , - 0.00845221 , ... - 0.21184735 , - 0.05048715 , - 0.34571868 , 0.23765688 , 0.23726143 ], dtype = float32 ) 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u8bcd\u5411\u91cf\u7684\u76f8\u5173\u77e5\u8bc6: \u7528\u5411\u91cf\u8868\u793a\u6587\u672c\u4e2d\u7684\u8bcd\u6c47(\u6216\u5b57\u7b26)\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u6700\u6d41\u884c\u7684\u505a\u6cd5, \u8fd9\u4e9b\u5411\u91cf\u80fd\u591f\u5f88\u597d\u7684\u6355\u6349\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u8bcd\u5411\u91cf\u7684\u5404\u79cdNLP\u4efb\u52a1\u7684\u6548\u679c. \u4f7f\u7528fasttext\u5de5\u5177\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u8fc7\u7a0b: \u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d","title":"4 \u8bad\u7ec3\u8bcd\u5411\u91cf"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#_1","text":"\u4e86\u89e3\u8bcd\u5411\u91cf\u7684\u76f8\u5173\u77e5\u8bc6. \u638c\u63e1fasttext\u5de5\u5177\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#1","text":"","title":"1 \u8bad\u7ec3\u8bcd\u5411\u91cf\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#11","text":"\u7528\u5411\u91cf\u8868\u793a\u6587\u672c\u4e2d\u7684\u8bcd\u6c47(\u6216\u5b57\u7b26)\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u6700\u6d41\u884c\u7684\u505a\u6cd5, \u8fd9\u4e9b\u5411\u91cf\u80fd\u591f\u5f88\u597d\u7684\u6355\u6349\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u8bcd\u5411\u91cf\u7684\u5404\u79cdNLP\u4efb\u52a1\u7684\u6548\u679c.","title":"1.1 \u8bcd\u5411\u91cf\u7684\u76f8\u5173\u77e5\u8bc6:"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#12","text":"\u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d","title":"1.2 \u7ec3\u8bcd\u5411\u91cf\u7684\u8fc7\u7a0b"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#2","text":"","title":"2 \u5b9e\u73b0\u6b65\u9aa4"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#21","text":"\u6570\u636e\u96c6\u4ecd\u7136\u4f7f\u7528\uff1a\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u90e8\u5206\u7f51\u9875\u4fe1\u606f \u6ce8\u610f\uff1a\u539f\u59cb\u6570\u636e\u96c6\u5df2\u7ecf\u653e\u5728/root/data/enwik9.zip\uff0c\u89e3\u538b\u540e\u6570\u636e\u4e3a/root/data/enwik9\uff0c\u9884\u5904\u7406\u540e\u7684\u6570\u636e\u4e3a/root/data/fil9 \u67e5\u770b\u9884\u5904\u7406\u540e\u7684\u6570\u636e: # \u67e5\u770b\u524d80\u4e2a\u5b57\u7b26 head -c 80 data/fil9 # \u8f93\u51fa\u7ed3\u679c\u4e3a\u7531\u7a7a\u683c\u5206\u5272\u7684\u5355\u8bcd anarchism originated as a term of abuse first used against early working class","title":"2.1 \u6570\u636e\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#22","text":"# \u4ee3\u7801\u8fd0\u884c\u5728python\u89e3\u91ca\u5668\u4e2d # \u5bfc\u5165fasttext >>> import fasttext # \u4f7f\u7528fasttext\u7684train_unsupervised(\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5)\u8fdb\u884c\u8bcd\u5411\u91cf\u7684\u8bad\u7ec3 # \u5b83\u7684\u53c2\u6570\u662f\u6570\u636e\u96c6\u7684\u6301\u4e45\u5316\u6587\u4ef6\u8def\u5f84'data/fil9' # \u6ce8\u610f\uff0c\u8be5\u884c\u4ee3\u7801\u6267\u884c\u8017\u65f6\u5f88\u957f >>> model1 = fasttext . train_unsupervised ( 'data/fil9' ) # \u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b >>> model = fasttext . load_model ( \"data/fil9.bin\" ) # \u6709\u6548\u8bad\u7ec3\u8bcd\u6c47\u91cf\u4e3a124M, \u5171218316\u4e2a\u5355\u8bcd Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 53996 lr : 0.000000 loss : 0.734999 ETA : 0 h 0 m \u67e5\u770b\u5355\u8bcd\u5bf9\u5e94\u7684\u8bcd\u5411\u91cf: # \u901a\u8fc7get_word_vector\u65b9\u6cd5\u6765\u83b7\u5f97\u6307\u5b9a\u8bcd\u6c47\u7684\u8bcd\u5411\u91cf >>> model . get_word_vector ( \"the\" ) array ([ - 0.03087516 , 0.09221972 , 0.17660329 , 0.17308897 , 0.12863874 , 0.13912526 , - 0.09851588 , 0.00739991 , 0.37038437 , - 0.00845221 , ... - 0.21184735 , - 0.05048715 , - 0.34571868 , 0.23765688 , 0.23726143 ], dtype = float32 )","title":"2.2 \u8bad\u7ec3\u8bcd\u5411\u91cf"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#23","text":"# \u5728\u8bad\u7ec3\u8bcd\u5411\u91cf\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u53ef\u4ee5\u8bbe\u5b9a\u5f88\u591a\u5e38\u7528\u8d85\u53c2\u6570\u6765\u8c03\u8282\u6211\u4eec\u7684\u6a21\u578b\u6548\u679c, \u5982: # \u65e0\u76d1\u7763\u8bad\u7ec3\u6a21\u5f0f: 'skipgram' \u6216\u8005 'cbow', \u9ed8\u8ba4\u4e3a'skipgram', \u5728\u5b9e\u8df5\u4e2d\uff0cskipgram\u6a21\u5f0f\u5728\u5229\u7528\u5b50\u8bcd\u65b9\u9762\u6bd4cbow\u66f4\u597d. # \u8bcd\u5d4c\u5165\u7ef4\u5ea6dim: \u9ed8\u8ba4\u4e3a100, \u4f46\u968f\u7740\u8bed\u6599\u5e93\u7684\u589e\u5927, \u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u5f80\u5f80\u4e5f\u8981\u66f4\u5927. # \u6570\u636e\u5faa\u73af\u6b21\u6570epoch: \u9ed8\u8ba4\u4e3a5, \u4f46\u5f53\u4f60\u7684\u6570\u636e\u96c6\u8db3\u591f\u5927, \u53ef\u80fd\u4e0d\u9700\u8981\u90a3\u4e48\u591a\u6b21. # \u5b66\u4e60\u7387lr: \u9ed8\u8ba4\u4e3a0.05, \u6839\u636e\u7ecf\u9a8c, \u5efa\u8bae\u9009\u62e9[0.01\uff0c1]\u8303\u56f4\u5185. # \u4f7f\u7528\u7684\u7ebf\u7a0b\u6570thread: \u9ed8\u8ba4\u4e3a12\u4e2a\u7ebf\u7a0b, \u4e00\u822c\u5efa\u8bae\u548c\u4f60\u7684cpu\u6838\u6570\u76f8\u540c. >>> model = fasttext . train_unsupervised ( 'data/fil9' , \"cbow\" , dim = 300 , epoch = 1 , lr = 0.1 , thread = 8 ) Read 124 M words Number of words : 218316 Number of labels : 0 Progress : 100.0 % words / sec / thread : 49523 lr : 0.000000 avg . loss : 1.777205 ETA : 0 h 0 m 0 s","title":"2.3 \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#24","text":"# \u68c0\u67e5\u5355\u8bcd\u5411\u91cf\u8d28\u91cf\u7684\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\u5c31\u662f\u67e5\u770b\u5176\u90bb\u8fd1\u5355\u8bcd, \u901a\u8fc7\u6211\u4eec\u4e3b\u89c2\u6765\u5224\u65ad\u8fd9\u4e9b\u90bb\u8fd1\u5355\u8bcd\u662f\u5426\u4e0e\u76ee\u6807\u5355\u8bcd\u76f8\u5173\u6765\u7c97\u7565\u8bc4\u5b9a\u6a21\u578b\u6548\u679c\u597d\u574f. # \u67e5\u627e\"\u8fd0\u52a8\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\"\u4f53\u80b2\u7f51\", \"\u8fd0\u52a8\u6c7d\u8f66\", \"\u8fd0\u52a8\u670d\"\u7b49. >>> model . get_nearest_neighbors ( 'sports' ) [( 0.8414610624313354 , 'sportsnet' ), ( 0.8134572505950928 , 'sport' ), ( 0.8100415468215942 , 'sportscars' ), ( 0.8021156787872314 , 'sportsground' ), ( 0.7889881134033203 , 'sportswomen' ), ( 0.7863013744354248 , 'sportsplex' ), ( 0.7786710262298584 , 'sporty' ), ( 0.7696356177330017 , 'sportscar' ), ( 0.7619683146476746 , 'sportswear' ), ( 0.7600985765457153 , 'sportin' )] # \u67e5\u627e\"\u97f3\u4e50\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u97f3\u4e50\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'music' ) [( 0.8908010125160217 , 'emusic' ), ( 0.8464668393135071 , 'musicmoz' ), ( 0.8444250822067261 , 'musics' ), ( 0.8113634586334229 , 'allmusic' ), ( 0.8106718063354492 , 'musices' ), ( 0.8049437999725342 , 'musicam' ), ( 0.8004694581031799 , 'musicom' ), ( 0.7952923774719238 , 'muchmusic' ), ( 0.7852965593338013 , 'musicweb' ), ( 0.7767147421836853 , 'musico' )] # \u67e5\u627e\"\u5c0f\u72d7\"\u7684\u90bb\u8fd1\u5355\u8bcd, \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u4e0e\u5c0f\u72d7\u6709\u5173\u7684\u8bcd\u6c47. >>> model . get_nearest_neighbors ( 'dog' ) [( 0.8456876873970032 , 'catdog' ), ( 0.7480780482292175 , 'dogcow' ), ( 0.7289096117019653 , 'sleddog' ), ( 0.7269964218139648 , 'hotdog' ), ( 0.7114801406860352 , 'sheepdog' ), ( 0.6947550773620605 , 'dogo' ), ( 0.6897546648979187 , 'bodog' ), ( 0.6621081829071045 , 'maddog' ), ( 0.6605004072189331 , 'dogs' ), ( 0.6398137211799622 , 'dogpile' )]","title":"2.4 \u6a21\u578b\u6548\u679c\u68c0\u9a8c"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#25","text":"# \u4f7f\u7528save_model\u4fdd\u5b58\u6a21\u578b >>> model . save_model ( \"data/fil91.bin\" ) # \u4f7f\u7528fasttext.load_model\u52a0\u8f7d\u6a21\u578b >>> model = fasttext . load_model ( \"data/fil91.bin\" ) >>> model . get_word_vector ( \"the\" ) array ([ - 0.03087516 , 0.09221972 , 0.17660329 , 0.17308897 , 0.12863874 , 0.13912526 , - 0.09851588 , 0.00739991 , 0.37038437 , - 0.00845221 , ... - 0.21184735 , - 0.05048715 , - 0.34571868 , 0.23765688 , 0.23726143 ], dtype = float32 )","title":"2.5 \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d"},{"location":"05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html#3","text":"\u5b66\u4e60\u4e86\u8bcd\u5411\u91cf\u7684\u76f8\u5173\u77e5\u8bc6: \u7528\u5411\u91cf\u8868\u793a\u6587\u672c\u4e2d\u7684\u8bcd\u6c47(\u6216\u5b57\u7b26)\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u6700\u6d41\u884c\u7684\u505a\u6cd5, \u8fd9\u4e9b\u5411\u91cf\u80fd\u591f\u5f88\u597d\u7684\u6355\u6349\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u63d0\u5347\u57fa\u4e8e\u8bcd\u5411\u91cf\u7684\u5404\u79cdNLP\u4efb\u52a1\u7684\u6548\u679c. \u4f7f\u7528fasttext\u5de5\u5177\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u8fc7\u7a0b: \u7b2c\u4e00\u6b65: \u83b7\u53d6\u6570\u636e \u7b2c\u4e8c\u6b65: \u8bad\u7ec3\u8bcd\u5411\u91cf \u7b2c\u4e09\u6b65: \u6a21\u578b\u8d85\u53c2\u6570\u8bbe\u5b9a \u7b2c\u56db\u6b65: \u6a21\u578b\u6548\u679c\u68c0\u9a8c \u7b2c\u4e94\u6b65: \u6a21\u578b\u7684\u4fdd\u5b58\u4e0e\u91cd\u52a0\u8f7d","title":"3 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662f\u8bcd\u5411\u91cf\u8fc1\u79fb. \u4e86\u89e3fasttext\u5de5\u5177\u4e2d\u6709\u54ea\u4e9b\u53ef\u8fc1\u79fb\u7684\u8bcd\u5411\u91cf\u6a21\u578b. \u638c\u63e1\u5982\u4f55\u4f7f\u7528fasttext\u8fdb\u884c\u8bcd\u5411\u91cf\u6a21\u578b\u8fc1\u79fb. 1 \u8bcd\u5411\u91cf\u8fc1\u79fb\u4ecb\u7ecd \u00b6 \u4f7f\u7528\u5728\u5927\u578b\u8bed\u6599\u5e93\u4e0a\u5df2\u7ecf\u8fdb\u884c\u8bad\u7ec3\u5b8c\u6210\u7684\u8bcd\u5411\u91cf\u6a21\u578b. fasttext\u5de5\u5177\u4e2d\u53ef\u4ee5\u63d0\u4f9b\u7684\u53ef\u8fc1\u79fb\u7684\u8bcd\u5411\u91cf: fasttext\u63d0\u4f9b\u4e86157\u79cd\u8bed\u8a00\u7684\u5728CommonCrawl\u548cWikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528CBOW\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/crawl-vectors.html fasttext\u63d0\u4f9b\u4e86294\u79cd\u8bed\u8a00\u7684\u5728Wikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528skipgram\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u540c\u6837\u662f300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/pretrained-vectors.html 2 \u4f7f\u7528fasttext\u8fdb\u884c\u8bcd\u5411\u91cf\u8fc1\u79fb \u00b6 \u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u8bcd\u5411\u91cf\u6a21\u578b\u538b\u7f29\u7684bin.gz\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u89e3\u538bbin.gz\u6587\u4ef6\u5230bin\u6587\u4ef6 \u7b2c\u4e09\u6b65: \u52a0\u8f7dbin\u6587\u4ef6\u83b7\u53d6\u8bcd\u5411\u91cf \u7b2c\u56db\u6b65: \u5229\u7528\u90bb\u8fd1\u8bcd\u8fdb\u884c\u6548\u679c\u68c0\u9a8c 2.1 \u4e0b\u8f7d\u8bcd\u5411\u91cf\u6a21\u578b\u538b\u7f29\u7684bin.gz\u6587\u4ef6 \u00b6 # \u8fd9\u91cc\u6211\u4eec\u4ee5\u8fc1\u79fb\u5728CommonCrawl\u548cWikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u4e2d\u6587\u8bcd\u5411\u91cf\u6a21\u578b\u4e3a\u4f8b: # \u4e0b\u8f7d\u4e2d\u6587\u8bcd\u5411\u91cf\u6a21\u578b(bin.gz\u6587\u4ef6) wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.bin.gz 2.2 \u89e3\u538bbin.gz\u6587\u4ef6\u5230bin\u6587\u4ef6 \u00b6 # \u4f7f\u7528gunzip\u8fdb\u884c\u89e3\u538b, \u83b7\u53d6cc.zh.300.bin\u6587\u4ef6 gunzip cc.zh.300.bin.gz 2.3 \u52a0\u8f7dbin\u6587\u4ef6\u83b7\u53d6\u8bcd\u5411\u91cf \u00b6 # \u52a0\u8f7d\u6a21\u578b >>> model = fasttext . load_model ( \"cc.zh.300.bin\" ) # \u67e5\u770b\u524d100\u4e2a\u8bcd\u6c47(\u8fd9\u91cc\u7684\u8bcd\u6c47\u662f\u5e7f\u4e49\u7684, \u53ef\u4ee5\u662f\u4e2d\u6587\u7b26\u53f7\u6216\u6c49\u5b57)) >>> model . words [: 100 ] [ '\uff0c' , '\u7684' , '\u3002' , '</s>' , '\u3001' , '\u662f' , '\u4e00' , '\u5728' , '\uff1a' , '\u4e86' , '\uff08' , '\uff09' , \"'\" , '\u548c' , '\u4e0d' , '\u6709' , '\u6211' , ',' , ')' , '(' , '\u201c' , '\u201d' , '\u4e5f' , '\u4eba' , '\u4e2a' , ':' , '\u4e2d' , '.' , '\u5c31' , '\u4ed6' , '\u300b' , '\u300a' , '-' , '\u4f60' , '\u90fd' , '\u4e0a' , '\u5927' , '\uff01' , '\u8fd9' , '\u4e3a' , '\u591a' , '\u4e0e' , '\u7ae0' , '\u300c' , '\u5230' , '\u300d' , '\u8981' , '\uff1f' , '\u88ab' , '\u800c' , '\u80fd' , '\u7b49' , '\u53ef\u4ee5' , '\u5e74' , '\uff1b' , '|' , '\u4ee5' , '\u53ca' , '\u4e4b' , '\u516c\u53f8' , '\u5bf9' , '\u4e2d\u56fd' , '\u5f88' , '\u4f1a' , '\u5c0f' , '\u4f46' , '\u6211\u4eec' , '\u6700' , '\u66f4' , '/' , '1' , '\u4e09' , '\u65b0' , '\u81ea\u5df1' , '\u53ef' , '2' , '\u6216' , '\u6b21' , '\u597d' , '\u5c06' , '\u7b2c' , '\u79cd' , '\u5979' , '\u2026' , '3' , '\u5730' , '\u5c0d' , '\u7528' , '\u5de5\u4f5c' , '\u4e0b' , '\u540e' , '\u7531' , '\u4e24' , '\u4f7f\u7528' , '\u8fd8' , '\u53c8' , '\u60a8' , '?' , '\u5176' , '\u5df2' ] # \u4f7f\u7528\u6a21\u578b\u83b7\u5f97'\u97f3\u4e50'\u8fd9\u4e2a\u540d\u8bcd\u7684\u8bcd\u5411\u91cf >>> model . get_word_vector ( \"\u97f3\u4e50\" ) array ([ - 6.81843981e-02 , 3.84048335e-02 , 4.63239700e-01 , 6.11658543e-02 , 9.38086119e-03 , - 9.63955745e-02 , 1.28141120e-01 , - 6.51574507e-02 , ... 3.13430429e-02 , - 6.43611327e-02 , 1.68979481e-01 , - 1.95011273e-01 ], dtype = float32 ) 2.4 \u5229\u7528\u90bb\u8fd1\u8bcd\u8fdb\u884c\u6548\u679c\u68c0\u9a8c \u00b6 # \u4ee5'\u97f3\u4e50'\u4e3a\u4f8b, \u8fd4\u56de\u7684\u90bb\u8fd1\u8bcd\u57fa\u672c\u4e0a\u4e0e\u97f3\u4e50\u90fd\u6709\u5173\u7cfb, \u5982\u4e50\u66f2, \u97f3\u4e50\u4f1a, \u58f0\u4e50\u7b49. >>> model . get_nearest_neighbors ( \"\u97f3\u4e50\" ) [( 0.6703276634216309 , '\u4e50\u66f2' ), ( 0.6569967269897461 , '\u97f3\u4e50\u4eba' ), ( 0.6565821170806885 , '\u58f0\u4e50' ), ( 0.6557438373565674 , '\u8f7b\u97f3\u4e50' ), ( 0.6536258459091187 , '\u97f3\u4e50\u5bb6' ), ( 0.6502416133880615 , '\u914d\u4e50' ), ( 0.6501686573028564 , '\u827a\u672f' ), ( 0.6437276005744934 , '\u97f3\u4e50\u4f1a' ), ( 0.639589250087738 , '\u539f\u58f0' ), ( 0.6368917226791382 , '\u97f3\u54cd' )] # \u4ee5'\u7f8e\u672f'\u4e3a\u4f8b, \u8fd4\u56de\u7684\u90bb\u8fd1\u8bcd\u57fa\u672c\u4e0a\u4e0e\u7f8e\u672f\u90fd\u6709\u5173\u7cfb, \u5982\u827a\u672f, \u7ed8\u753b, \u970d\u5ef7\u9704(\u6ee1\u57ce\u5c3d\u5e26\u9ec4\u91d1\u7532\u7684\u7f8e\u672f\u5e08)\u7b49. >>> model . get_nearest_neighbors ( \"\u7f8e\u672f\" ) [( 0.724744975566864 , '\u827a\u672f' ), ( 0.7165924310684204 , '\u7ed8\u753b' ), ( 0.6741853356361389 , '\u970d\u5ef7\u9704' ), ( 0.6470299363136292 , '\u7eaf\u827a' ), ( 0.6335071921348572 , '\u7f8e\u672f\u5bb6' ), ( 0.6304370164871216 , '\u7f8e\u9662' ), ( 0.624431312084198 , '\u827a\u672f\u7c7b' ), ( 0.6244068741798401 , '\u9648\u6d69\u5fe0' ), ( 0.62302166223526 , '\u7f8e\u672f\u53f2' ), ( 0.621710479259491 , '\u73af\u827a\u7cfb' )] # \u4ee5'\u5468\u6770\u4f26'\u4e3a\u4f8b, \u8fd4\u56de\u7684\u90bb\u8fd1\u8bcd\u57fa\u672c\u4e0a\u4e0e\u660e\u661f\u6709\u5173\u7cfb, \u5982\u6770\u4f26, \u5468\u8463, \u9648\u5955\u8fc5\u7b49. >>> model . get_nearest_neighbors ( \"\u5468\u6770\u4f26\" ) [( 0.6995140910148621 , '\u6770\u4f26' ), ( 0.6967097520828247 , '\u5468\u6770\u502b' ), ( 0.6859776377677917 , '\u5468\u8463' ), ( 0.6381043195724487 , '\u9648\u5955\u8fc5' ), ( 0.6367626190185547 , '\u5f20\u9753\u9896' ), ( 0.6313326358795166 , '\u5f20\u97f6\u6db5' ), ( 0.6271176338195801 , '\u8c22\u9706\u950b' ), ( 0.6188404560089111 , '\u5468\u534e\u5065' ), ( 0.6184280514717102 , '\u6797\u4fca\u6770' ), ( 0.6143589019775391 , '\u738b\u529b\u5b8f' )] 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u8bcd\u5411\u91cf\u8fc1\u79fb: \u4f7f\u7528\u5728\u5927\u578b\u8bed\u6599\u5e93\u4e0a\u5df2\u7ecf\u8fdb\u884c\u8bad\u7ec3\u5b8c\u6210\u7684\u8bcd\u5411\u91cf\u6a21\u578b. \u5b66\u4e60\u4e86fasttext\u5de5\u5177\u4e2d\u53ef\u4ee5\u63d0\u4f9b\u7684\u53ef\u8fc1\u79fb\u7684\u8bcd\u5411\u91cf: fasttext\u63d0\u4f9b\u4e86157\u79cd\u8bed\u8a00\u7684\u5728CommonCrawl\u548cWikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528CBOW\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/crawl-vectors.html fasttext\u63d0\u4f9b\u4e86294\u79cd\u8bed\u8a00\u7684\u5728Wikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528skipgram\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u540c\u6837\u662f300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/pretrained-vectors.html \u5982\u4f55\u4f7f\u7528fasttext\u8fdb\u884c\u8bcd\u5411\u91cf\u6a21\u578b\u8fc1\u79fb: \u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u8bcd\u5411\u91cf\u6a21\u578b\u538b\u7f29\u7684bin.gz\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u89e3\u538bbin.gz\u6587\u4ef6\u5230bin\u6587\u4ef6 \u7b2c\u4e09\u6b65: \u52a0\u8f7dbin\u6587\u4ef6\u83b7\u53d6\u8bcd\u5411\u91cf \u7b2c\u56db\u6b65: \u5229\u7528\u90bb\u8fd1\u8bcd\u8fdb\u884c\u6548\u679c\u68c0\u9a8c","title":"5 \u8bcd\u5411\u91cf\u8fc1\u79fb"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662f\u8bcd\u5411\u91cf\u8fc1\u79fb. \u4e86\u89e3fasttext\u5de5\u5177\u4e2d\u6709\u54ea\u4e9b\u53ef\u8fc1\u79fb\u7684\u8bcd\u5411\u91cf\u6a21\u578b. \u638c\u63e1\u5982\u4f55\u4f7f\u7528fasttext\u8fdb\u884c\u8bcd\u5411\u91cf\u6a21\u578b\u8fc1\u79fb.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#1","text":"\u4f7f\u7528\u5728\u5927\u578b\u8bed\u6599\u5e93\u4e0a\u5df2\u7ecf\u8fdb\u884c\u8bad\u7ec3\u5b8c\u6210\u7684\u8bcd\u5411\u91cf\u6a21\u578b. fasttext\u5de5\u5177\u4e2d\u53ef\u4ee5\u63d0\u4f9b\u7684\u53ef\u8fc1\u79fb\u7684\u8bcd\u5411\u91cf: fasttext\u63d0\u4f9b\u4e86157\u79cd\u8bed\u8a00\u7684\u5728CommonCrawl\u548cWikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528CBOW\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/crawl-vectors.html fasttext\u63d0\u4f9b\u4e86294\u79cd\u8bed\u8a00\u7684\u5728Wikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528skipgram\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u540c\u6837\u662f300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/pretrained-vectors.html","title":"1 \u8bcd\u5411\u91cf\u8fc1\u79fb\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#2-fasttext","text":"\u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u8bcd\u5411\u91cf\u6a21\u578b\u538b\u7f29\u7684bin.gz\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u89e3\u538bbin.gz\u6587\u4ef6\u5230bin\u6587\u4ef6 \u7b2c\u4e09\u6b65: \u52a0\u8f7dbin\u6587\u4ef6\u83b7\u53d6\u8bcd\u5411\u91cf \u7b2c\u56db\u6b65: \u5229\u7528\u90bb\u8fd1\u8bcd\u8fdb\u884c\u6548\u679c\u68c0\u9a8c","title":"2 \u4f7f\u7528fasttext\u8fdb\u884c\u8bcd\u5411\u91cf\u8fc1\u79fb"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#21-bingz","text":"# \u8fd9\u91cc\u6211\u4eec\u4ee5\u8fc1\u79fb\u5728CommonCrawl\u548cWikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u4e2d\u6587\u8bcd\u5411\u91cf\u6a21\u578b\u4e3a\u4f8b: # \u4e0b\u8f7d\u4e2d\u6587\u8bcd\u5411\u91cf\u6a21\u578b(bin.gz\u6587\u4ef6) wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.bin.gz","title":"2.1 \u4e0b\u8f7d\u8bcd\u5411\u91cf\u6a21\u578b\u538b\u7f29\u7684bin.gz\u6587\u4ef6"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#22-bingzbin","text":"# \u4f7f\u7528gunzip\u8fdb\u884c\u89e3\u538b, \u83b7\u53d6cc.zh.300.bin\u6587\u4ef6 gunzip cc.zh.300.bin.gz","title":"2.2 \u89e3\u538bbin.gz\u6587\u4ef6\u5230bin\u6587\u4ef6"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#23-bin","text":"# \u52a0\u8f7d\u6a21\u578b >>> model = fasttext . load_model ( \"cc.zh.300.bin\" ) # \u67e5\u770b\u524d100\u4e2a\u8bcd\u6c47(\u8fd9\u91cc\u7684\u8bcd\u6c47\u662f\u5e7f\u4e49\u7684, \u53ef\u4ee5\u662f\u4e2d\u6587\u7b26\u53f7\u6216\u6c49\u5b57)) >>> model . words [: 100 ] [ '\uff0c' , '\u7684' , '\u3002' , '</s>' , '\u3001' , '\u662f' , '\u4e00' , '\u5728' , '\uff1a' , '\u4e86' , '\uff08' , '\uff09' , \"'\" , '\u548c' , '\u4e0d' , '\u6709' , '\u6211' , ',' , ')' , '(' , '\u201c' , '\u201d' , '\u4e5f' , '\u4eba' , '\u4e2a' , ':' , '\u4e2d' , '.' , '\u5c31' , '\u4ed6' , '\u300b' , '\u300a' , '-' , '\u4f60' , '\u90fd' , '\u4e0a' , '\u5927' , '\uff01' , '\u8fd9' , '\u4e3a' , '\u591a' , '\u4e0e' , '\u7ae0' , '\u300c' , '\u5230' , '\u300d' , '\u8981' , '\uff1f' , '\u88ab' , '\u800c' , '\u80fd' , '\u7b49' , '\u53ef\u4ee5' , '\u5e74' , '\uff1b' , '|' , '\u4ee5' , '\u53ca' , '\u4e4b' , '\u516c\u53f8' , '\u5bf9' , '\u4e2d\u56fd' , '\u5f88' , '\u4f1a' , '\u5c0f' , '\u4f46' , '\u6211\u4eec' , '\u6700' , '\u66f4' , '/' , '1' , '\u4e09' , '\u65b0' , '\u81ea\u5df1' , '\u53ef' , '2' , '\u6216' , '\u6b21' , '\u597d' , '\u5c06' , '\u7b2c' , '\u79cd' , '\u5979' , '\u2026' , '3' , '\u5730' , '\u5c0d' , '\u7528' , '\u5de5\u4f5c' , '\u4e0b' , '\u540e' , '\u7531' , '\u4e24' , '\u4f7f\u7528' , '\u8fd8' , '\u53c8' , '\u60a8' , '?' , '\u5176' , '\u5df2' ] # \u4f7f\u7528\u6a21\u578b\u83b7\u5f97'\u97f3\u4e50'\u8fd9\u4e2a\u540d\u8bcd\u7684\u8bcd\u5411\u91cf >>> model . get_word_vector ( \"\u97f3\u4e50\" ) array ([ - 6.81843981e-02 , 3.84048335e-02 , 4.63239700e-01 , 6.11658543e-02 , 9.38086119e-03 , - 9.63955745e-02 , 1.28141120e-01 , - 6.51574507e-02 , ... 3.13430429e-02 , - 6.43611327e-02 , 1.68979481e-01 , - 1.95011273e-01 ], dtype = float32 )","title":"2.3 \u52a0\u8f7dbin\u6587\u4ef6\u83b7\u53d6\u8bcd\u5411\u91cf"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#24","text":"# \u4ee5'\u97f3\u4e50'\u4e3a\u4f8b, \u8fd4\u56de\u7684\u90bb\u8fd1\u8bcd\u57fa\u672c\u4e0a\u4e0e\u97f3\u4e50\u90fd\u6709\u5173\u7cfb, \u5982\u4e50\u66f2, \u97f3\u4e50\u4f1a, \u58f0\u4e50\u7b49. >>> model . get_nearest_neighbors ( \"\u97f3\u4e50\" ) [( 0.6703276634216309 , '\u4e50\u66f2' ), ( 0.6569967269897461 , '\u97f3\u4e50\u4eba' ), ( 0.6565821170806885 , '\u58f0\u4e50' ), ( 0.6557438373565674 , '\u8f7b\u97f3\u4e50' ), ( 0.6536258459091187 , '\u97f3\u4e50\u5bb6' ), ( 0.6502416133880615 , '\u914d\u4e50' ), ( 0.6501686573028564 , '\u827a\u672f' ), ( 0.6437276005744934 , '\u97f3\u4e50\u4f1a' ), ( 0.639589250087738 , '\u539f\u58f0' ), ( 0.6368917226791382 , '\u97f3\u54cd' )] # \u4ee5'\u7f8e\u672f'\u4e3a\u4f8b, \u8fd4\u56de\u7684\u90bb\u8fd1\u8bcd\u57fa\u672c\u4e0a\u4e0e\u7f8e\u672f\u90fd\u6709\u5173\u7cfb, \u5982\u827a\u672f, \u7ed8\u753b, \u970d\u5ef7\u9704(\u6ee1\u57ce\u5c3d\u5e26\u9ec4\u91d1\u7532\u7684\u7f8e\u672f\u5e08)\u7b49. >>> model . get_nearest_neighbors ( \"\u7f8e\u672f\" ) [( 0.724744975566864 , '\u827a\u672f' ), ( 0.7165924310684204 , '\u7ed8\u753b' ), ( 0.6741853356361389 , '\u970d\u5ef7\u9704' ), ( 0.6470299363136292 , '\u7eaf\u827a' ), ( 0.6335071921348572 , '\u7f8e\u672f\u5bb6' ), ( 0.6304370164871216 , '\u7f8e\u9662' ), ( 0.624431312084198 , '\u827a\u672f\u7c7b' ), ( 0.6244068741798401 , '\u9648\u6d69\u5fe0' ), ( 0.62302166223526 , '\u7f8e\u672f\u53f2' ), ( 0.621710479259491 , '\u73af\u827a\u7cfb' )] # \u4ee5'\u5468\u6770\u4f26'\u4e3a\u4f8b, \u8fd4\u56de\u7684\u90bb\u8fd1\u8bcd\u57fa\u672c\u4e0a\u4e0e\u660e\u661f\u6709\u5173\u7cfb, \u5982\u6770\u4f26, \u5468\u8463, \u9648\u5955\u8fc5\u7b49. >>> model . get_nearest_neighbors ( \"\u5468\u6770\u4f26\" ) [( 0.6995140910148621 , '\u6770\u4f26' ), ( 0.6967097520828247 , '\u5468\u6770\u502b' ), ( 0.6859776377677917 , '\u5468\u8463' ), ( 0.6381043195724487 , '\u9648\u5955\u8fc5' ), ( 0.6367626190185547 , '\u5f20\u9753\u9896' ), ( 0.6313326358795166 , '\u5f20\u97f6\u6db5' ), ( 0.6271176338195801 , '\u8c22\u9706\u950b' ), ( 0.6188404560089111 , '\u5468\u534e\u5065' ), ( 0.6184280514717102 , '\u6797\u4fca\u6770' ), ( 0.6143589019775391 , '\u738b\u529b\u5b8f' )]","title":"2.4 \u5229\u7528\u90bb\u8fd1\u8bcd\u8fdb\u884c\u6548\u679c\u68c0\u9a8c"},{"location":"05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html#3","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662f\u8bcd\u5411\u91cf\u8fc1\u79fb: \u4f7f\u7528\u5728\u5927\u578b\u8bed\u6599\u5e93\u4e0a\u5df2\u7ecf\u8fdb\u884c\u8bad\u7ec3\u5b8c\u6210\u7684\u8bcd\u5411\u91cf\u6a21\u578b. \u5b66\u4e60\u4e86fasttext\u5de5\u5177\u4e2d\u53ef\u4ee5\u63d0\u4f9b\u7684\u53ef\u8fc1\u79fb\u7684\u8bcd\u5411\u91cf: fasttext\u63d0\u4f9b\u4e86157\u79cd\u8bed\u8a00\u7684\u5728CommonCrawl\u548cWikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528CBOW\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/crawl-vectors.html fasttext\u63d0\u4f9b\u4e86294\u79cd\u8bed\u8a00\u7684\u5728Wikipedia\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u8bcd\u5411\u91cf\u6a21\u578b, \u5b83\u4eec\u91c7\u7528skipgram\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3, \u8bcd\u5411\u91cf\u7ef4\u5ea6\u540c\u6837\u662f300\u7ef4. \u53ef\u901a\u8fc7\u8be5\u5730\u5740\u67e5\u770b\u5177\u4f53\u8bed\u8a00\u8bcd\u5411\u91cf\u6a21\u578b: https://fasttext.cc/docs/en/pretrained-vectors.html \u5982\u4f55\u4f7f\u7528fasttext\u8fdb\u884c\u8bcd\u5411\u91cf\u6a21\u578b\u8fc1\u79fb: \u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u8bcd\u5411\u91cf\u6a21\u578b\u538b\u7f29\u7684bin.gz\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u89e3\u538bbin.gz\u6587\u4ef6\u5230bin\u6587\u4ef6 \u7b2c\u4e09\u6b65: \u52a0\u8f7dbin\u6587\u4ef6\u83b7\u53d6\u8bcd\u5411\u91cf \u7b2c\u56db\u6b65: \u5229\u7528\u90bb\u8fd1\u8bcd\u8fdb\u884c\u6548\u679c\u68c0\u9a8c","title":"3 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/6%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6709\u5173\u6982\u5ff5 \u77e5\u9053\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u8fc1\u79fb\u65b9\u5f0f 1 \u8fc1\u79fb\u5b66\u4e60\u6709\u5173\u6982\u5ff5 \u00b6 \u9884\u8bad\u7ec3\u6a21\u578b \u5fae\u8c03 1.1 \u9884\u8bad\u7ec3\u6a21\u578b(Pretrained model) \u00b6 \u4e00\u822c\u60c5\u51b5\u4e0b\u9884\u8bad\u7ec3\u6a21\u578b\u90fd\u662f\u5927\u578b\u6a21\u578b\uff0c\u5177\u5907\u590d\u6742\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u4f17\u591a\u7684\u53c2\u6570\u91cf\uff0c\u4ee5\u53ca\u5728\u8db3\u591f\u5927\u7684\u6570\u636e\u96c6\u4e0b\u8fdb\u884c\u8bad\u7ec3\u800c\u4ea7\u751f\u7684\u6a21\u578b.\u3002\u5728NLP\u9886\u57df\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5f80\u5f80\u662f\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u662f\u65e0\u76d1\u7763\u7684\uff0c\u53ef\u4ee5\u83b7\u5f97\u5927\u89c4\u6a21\u8bed\u6599\uff0c\u540c\u65f6\u8bed\u8a00\u6a21\u578b\u53c8\u662f\u8bb8\u591a\u5178\u578bNLP\u4efb\u52a1\u7684\u57fa\u7840\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\uff0c\u6587\u672c\u751f\u6210\uff0c\u9605\u8bfb\u7406\u89e3\u7b49\uff0c\u5e38\u89c1\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6709BERT, GPT, roBERTa, transformer-XL\u7b49\u3002 1.2 \u5fae\u8c03(Fine-tuning) \u00b6 \u6839\u636e\u7ed9\u5b9a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6539\u53d8\u5b83\u7684\u90e8\u5206\u53c2\u6570\u6216\u8005\u4e3a\u5176\u65b0\u589e\u90e8\u5206\u8f93\u51fa\u7ed3\u6784\u540e\uff0c\u901a\u8fc7\u5728\u5c0f\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6765\u4f7f\u6574\u4e2a\u6a21\u578b\u66f4\u597d\u7684\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u3002 1.3 \u4e24\u79cd\u8fc1\u79fb\u65b9\u5f0f \u00b6 \u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fdb\u884c\u76f8\u540c\u4efb\u52a1\u7684\u5904\u7406\uff0c\u4e0d\u9700\u8981\u8c03\u6574\u53c2\u6570\u6216\u6a21\u578b\u7ed3\u6784\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f00\u7bb1\u5373\u7528\u3002\u4f46\u662f\u8fd9\u79cd\u60c5\u51b5\u4e00\u822c\u53ea\u9002\u7528\u4e8e\u666e\u9002\u4efb\u52a1, \u5982\uff1afasttest\u5de5\u5177\u5305\u4e2d\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\u6a21\u578b\u3002\u53e6\u5916\uff0c\u5f88\u591a\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u53d1\u8005\u4e3a\u4e86\u8fbe\u5230\u5f00\u7bb1\u5373\u7528\u7684\u6548\u679c\uff0c\u5c06\u6a21\u578b\u7ed3\u6784\u5206\u5404\u4e2a\u90e8\u5206\u4fdd\u5b58\u4e3a\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u4f9b\u5bf9\u5e94\u7684\u52a0\u8f7d\u65b9\u6cd5\u6765\u5b8c\u6210\u7279\u5b9a\u76ee\u6807\u3002 \u66f4\u52a0\u4e3b\u6d41\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u662f\u53d1\u6325\u9884\u8bad\u7ec3\u6a21\u578b\u7279\u5f81\u62bd\u8c61\u7684\u80fd\u529b\uff0c\u7136\u540e\u518d\u901a\u8fc7\u5fae\u8c03\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u8bad\u7ec3\u66f4\u65b0\u5c0f\u90e8\u5206\u53c2\u6570\u4ee5\u6b64\u6765\u9002\u5e94\u4e0d\u540c\u7684\u4efb\u52a1\u3002\u8fd9\u79cd\u8fc1\u79fb\u65b9\u5f0f\u9700\u8981\u63d0\u4f9b\u5c0f\u90e8\u5206\u7684\u6807\u6ce8\u6570\u636e\u6765\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002 \u5173\u4e8e\u8fc1\u79fb\u65b9\u5f0f\u7684\u8bf4\u660e: \u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u5f0f, \u5df2\u7ecf\u5728fasttext\u7684\u8bcd\u5411\u91cf\u8fc1\u79fb\u4e2d\u5b66\u4e60\u3002\u63a5\u4e0b\u6765\u7684\u8fc1\u79fb\u5b66\u4e60\u5b9e\u8df5\u5c06\u4e3b\u8981\u8bb2\u89e3\u901a\u8fc7\u5fae\u8c03\u7684\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002","title":"6 \u8fc1\u79fb\u5b66\u4e60\u6982\u5ff5"},{"location":"05_mkdocs_translearning/6%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5.html#_1","text":"\u4e86\u89e3\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6709\u5173\u6982\u5ff5 \u77e5\u9053\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u8fc1\u79fb\u65b9\u5f0f","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/6%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5.html#1","text":"\u9884\u8bad\u7ec3\u6a21\u578b \u5fae\u8c03","title":"1 \u8fc1\u79fb\u5b66\u4e60\u6709\u5173\u6982\u5ff5"},{"location":"05_mkdocs_translearning/6%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5.html#11-pretrained-model","text":"\u4e00\u822c\u60c5\u51b5\u4e0b\u9884\u8bad\u7ec3\u6a21\u578b\u90fd\u662f\u5927\u578b\u6a21\u578b\uff0c\u5177\u5907\u590d\u6742\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u4f17\u591a\u7684\u53c2\u6570\u91cf\uff0c\u4ee5\u53ca\u5728\u8db3\u591f\u5927\u7684\u6570\u636e\u96c6\u4e0b\u8fdb\u884c\u8bad\u7ec3\u800c\u4ea7\u751f\u7684\u6a21\u578b.\u3002\u5728NLP\u9886\u57df\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5f80\u5f80\u662f\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u662f\u65e0\u76d1\u7763\u7684\uff0c\u53ef\u4ee5\u83b7\u5f97\u5927\u89c4\u6a21\u8bed\u6599\uff0c\u540c\u65f6\u8bed\u8a00\u6a21\u578b\u53c8\u662f\u8bb8\u591a\u5178\u578bNLP\u4efb\u52a1\u7684\u57fa\u7840\uff0c\u5982\u673a\u5668\u7ffb\u8bd1\uff0c\u6587\u672c\u751f\u6210\uff0c\u9605\u8bfb\u7406\u89e3\u7b49\uff0c\u5e38\u89c1\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6709BERT, GPT, roBERTa, transformer-XL\u7b49\u3002","title":"1.1 \u9884\u8bad\u7ec3\u6a21\u578b(Pretrained model)"},{"location":"05_mkdocs_translearning/6%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5.html#12-fine-tuning","text":"\u6839\u636e\u7ed9\u5b9a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6539\u53d8\u5b83\u7684\u90e8\u5206\u53c2\u6570\u6216\u8005\u4e3a\u5176\u65b0\u589e\u90e8\u5206\u8f93\u51fa\u7ed3\u6784\u540e\uff0c\u901a\u8fc7\u5728\u5c0f\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6765\u4f7f\u6574\u4e2a\u6a21\u578b\u66f4\u597d\u7684\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u3002","title":"1.2 \u5fae\u8c03(Fine-tuning)"},{"location":"05_mkdocs_translearning/6%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5.html#13","text":"\u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fdb\u884c\u76f8\u540c\u4efb\u52a1\u7684\u5904\u7406\uff0c\u4e0d\u9700\u8981\u8c03\u6574\u53c2\u6570\u6216\u6a21\u578b\u7ed3\u6784\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f00\u7bb1\u5373\u7528\u3002\u4f46\u662f\u8fd9\u79cd\u60c5\u51b5\u4e00\u822c\u53ea\u9002\u7528\u4e8e\u666e\u9002\u4efb\u52a1, \u5982\uff1afasttest\u5de5\u5177\u5305\u4e2d\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\u6a21\u578b\u3002\u53e6\u5916\uff0c\u5f88\u591a\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u53d1\u8005\u4e3a\u4e86\u8fbe\u5230\u5f00\u7bb1\u5373\u7528\u7684\u6548\u679c\uff0c\u5c06\u6a21\u578b\u7ed3\u6784\u5206\u5404\u4e2a\u90e8\u5206\u4fdd\u5b58\u4e3a\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u4f9b\u5bf9\u5e94\u7684\u52a0\u8f7d\u65b9\u6cd5\u6765\u5b8c\u6210\u7279\u5b9a\u76ee\u6807\u3002 \u66f4\u52a0\u4e3b\u6d41\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u662f\u53d1\u6325\u9884\u8bad\u7ec3\u6a21\u578b\u7279\u5f81\u62bd\u8c61\u7684\u80fd\u529b\uff0c\u7136\u540e\u518d\u901a\u8fc7\u5fae\u8c03\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u8bad\u7ec3\u66f4\u65b0\u5c0f\u90e8\u5206\u53c2\u6570\u4ee5\u6b64\u6765\u9002\u5e94\u4e0d\u540c\u7684\u4efb\u52a1\u3002\u8fd9\u79cd\u8fc1\u79fb\u65b9\u5f0f\u9700\u8981\u63d0\u4f9b\u5c0f\u90e8\u5206\u7684\u6807\u6ce8\u6570\u636e\u6765\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002 \u5173\u4e8e\u8fc1\u79fb\u65b9\u5f0f\u7684\u8bf4\u660e: \u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u5f0f, \u5df2\u7ecf\u5728fasttext\u7684\u8bcd\u5411\u91cf\u8fc1\u79fb\u4e2d\u5b66\u4e60\u3002\u63a5\u4e0b\u6765\u7684\u8fc1\u79fb\u5b66\u4e60\u5b9e\u8df5\u5c06\u4e3b\u8981\u8bb2\u89e3\u901a\u8fc7\u5fae\u8c03\u7684\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002","title":"1.3 \u4e24\u79cd\u8fc1\u79fb\u65b9\u5f0f"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u5f53\u4e0bNLP\u4e2d\u6d41\u884c\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u638c\u63e1\u5982\u4f55\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b. 1 \u5f53\u4e0bNLP\u4e2d\u6d41\u884c\u7684\u9884\u8bad\u7ec3\u6a21\u578b \u00b6 BERT GPT GPT-2 Transformer-XL XLNet XLM RoBERTa DistilBERT ALBERT T5 XLM-RoBERTa 1.1 BERT\u53ca\u5176\u53d8\u4f53 \u00b6 bert-base-uncased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-large-uncased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171340M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-base-cased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-large-cased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171340M\u53c2\u6570\u91cf, \u5728\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-base-multilingual-uncased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684102\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-large-multilingual-uncased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171340M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684102\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-base-chinese: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u7b80\u4f53\u548c\u7e41\u4f53\u4e2d\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.2 GPT \u00b6 openai-gpt: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u7531OpenAI\u5728\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.3 GPT-2\u53ca\u5176\u53d8\u4f53 \u00b6 gpt2: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171117M\u53c2\u6570\u91cf, \u5728OpenAI GPT-2\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. gpt2-xl: \u7f16\u7801\u5668\u5177\u670948\u4e2a\u9690\u5c42, \u8f93\u51fa1600\u7ef4\u5f20\u91cf, 25\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u51711558M\u53c2\u6570\u91cf, \u5728\u5927\u578b\u7684OpenAI GPT-2\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.4 Transformer-XL \u00b6 transfo-xl-wt103: \u7f16\u7801\u5668\u5177\u670918\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171257M\u53c2\u6570\u91cf, \u5728wikitext-103\u82f1\u6587\u8bed\u6599\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.5 XLNet\u53ca\u5176\u53d8\u4f53 \u00b6 xlnet-base-cased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. xlnet-large-cased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171240\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.6 XLM \u00b6 xlm-mlm-en-2048: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa2048\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.7 RoBERTa\u53ca\u5176\u53d8\u4f53 \u00b6 roberta-base: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. roberta-large: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171355M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.8 DistilBERT\u53ca\u5176\u53d8\u4f53 \u00b6 distilbert-base-uncased: \u57fa\u4e8ebert-base-uncased\u7684\u84b8\u998f(\u538b\u7f29)\u6a21\u578b, \u7f16\u7801\u5668\u5177\u67096\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u517166M\u53c2\u6570\u91cf. distilbert-base-multilingual-cased: \u57fa\u4e8ebert-base-multilingual-uncased\u7684\u84b8\u998f(\u538b\u7f29)\u6a21\u578b, \u7f16\u7801\u5668\u5177\u67096\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u517166M\u53c2\u6570\u91cf. 1.9 ALBERT \u00b6 albert-base-v1: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. albert-base-v2: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230, \u76f8\u6bd4v1\u4f7f\u7528\u4e86\u66f4\u591a\u7684\u6570\u636e\u91cf, \u82b1\u8d39\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4. 1.10 T5\u53ca\u5176\u53d8\u4f53 \u00b6 t5-small: \u7f16\u7801\u5668\u5177\u67096\u4e2a\u9690\u5c42, \u8f93\u51fa512\u7ef4\u5f20\u91cf, 8\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u517160M\u53c2\u6570\u91cf, \u5728C4\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. t5-base: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171220M\u53c2\u6570\u91cf, \u5728C4\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. t5-large: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171770M\u53c2\u6570\u91cf, \u5728C4\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 1.1 XLM-RoBERTa\u53ca\u5176\u53d8\u4f53 \u00b6 xlm-roberta-base: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 8\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u57282.5TB\u7684100\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. xlm-roberta-large: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1027\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171355M\u53c2\u6570\u91cf, \u57282.5TB\u7684100\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. 2 \u9884\u8bad\u7ec3\u6a21\u578b\u8bf4\u660e \u00b6 \u6240\u6709\u4e0a\u8ff0\u9884\u8bad\u7ec3\u6a21\u578b\u53ca\u5176\u53d8\u4f53\u90fd\u662f\u4ee5transformer\u4e3a\u57fa\u7840\uff0c\u53ea\u662f\u5728\u6a21\u578b\u7ed3\u6784\u5982\u795e\u7ecf\u5143\u8fde\u63a5\u65b9\u5f0f\uff0c\u7f16\u7801\u5668\u9690\u5c42\u6570\uff0c\u591a\u5934\u6ce8\u610f\u529b\u7684\u5934\u6570\u7b49\u53d1\u751f\u6539\u53d8\uff0c\u8fd9\u4e9b\u6539\u53d8\u65b9\u5f0f\u7684\u5927\u90e8\u5206\u4f9d\u636e\u90fd\u662f\u7531\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u800c\u5b9a\uff0c\u56e0\u6b64\uff0c\u5bf9\u4e8e\u6211\u4eec\u4f7f\u7528\u8005\u800c\u8a00\uff0c\u4e0d\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u6df1\u5ea6\u63a2\u7a76\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7ed3\u6784\u8bbe\u8ba1\u7684\u4f18\u52a3\uff0c\u53ea\u9700\u8981\u5728\u81ea\u5df1\u5904\u7406\u7684\u76ee\u6807\u6570\u636e\u4e0a\uff0c\u5c3d\u91cf\u904d\u5386\u6240\u6709\u53ef\u7528\u7684\u6a21\u578b\u5bf9\u6bd4\u5f97\u5230\u6700\u4f18\u6548\u679c\u5373\u53ef. 3 \u5c0f\u7ed3 \u00b6 \u5f53\u4e0bNLP\u4e2d\u6d41\u884c\u7684\u9884\u8bad\u7ec3\u6a21\u578b: BERT GPT GPT-2 Transformer-XL XLNet XLM RoBERTa DistilBERT ALBERT T5 XLM-RoBERTa","title":"7 NLP\u4e2d\u7684\u5e38\u7528\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#_1","text":"\u4e86\u89e3\u5f53\u4e0bNLP\u4e2d\u6d41\u884c\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u638c\u63e1\u5982\u4f55\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#1-nlp","text":"BERT GPT GPT-2 Transformer-XL XLNet XLM RoBERTa DistilBERT ALBERT T5 XLM-RoBERTa","title":"1 \u5f53\u4e0bNLP\u4e2d\u6d41\u884c\u7684\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#11-bert","text":"bert-base-uncased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-large-uncased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171340M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-base-cased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-large-cased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171340M\u53c2\u6570\u91cf, \u5728\u4e0d\u533a\u5206\u5927\u5c0f\u5199\u7684\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-base-multilingual-uncased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684102\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-large-multilingual-uncased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171340M\u53c2\u6570\u91cf, \u5728\u5c0f\u5199\u7684102\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. bert-base-chinese: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u7b80\u4f53\u548c\u7e41\u4f53\u4e2d\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.1 BERT\u53ca\u5176\u53d8\u4f53"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#12-gpt","text":"openai-gpt: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u7531OpenAI\u5728\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.2 GPT"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#13-gpt-2","text":"gpt2: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171117M\u53c2\u6570\u91cf, \u5728OpenAI GPT-2\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. gpt2-xl: \u7f16\u7801\u5668\u5177\u670948\u4e2a\u9690\u5c42, \u8f93\u51fa1600\u7ef4\u5f20\u91cf, 25\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u51711558M\u53c2\u6570\u91cf, \u5728\u5927\u578b\u7684OpenAI GPT-2\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.3 GPT-2\u53ca\u5176\u53d8\u4f53"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#14-transformer-xl","text":"transfo-xl-wt103: \u7f16\u7801\u5668\u5177\u670918\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171257M\u53c2\u6570\u91cf, \u5728wikitext-103\u82f1\u6587\u8bed\u6599\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.4 Transformer-XL"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#15-xlnet","text":"xlnet-base-cased: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171110M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. xlnet-large-cased: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171240\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.5 XLNet\u53ca\u5176\u53d8\u4f53"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#16-xlm","text":"xlm-mlm-en-2048: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa2048\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.6 XLM"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#17-roberta","text":"roberta-base: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. roberta-large: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171355M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.7 RoBERTa\u53ca\u5176\u53d8\u4f53"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#18-distilbert","text":"distilbert-base-uncased: \u57fa\u4e8ebert-base-uncased\u7684\u84b8\u998f(\u538b\u7f29)\u6a21\u578b, \u7f16\u7801\u5668\u5177\u67096\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u517166M\u53c2\u6570\u91cf. distilbert-base-multilingual-cased: \u57fa\u4e8ebert-base-multilingual-uncased\u7684\u84b8\u998f(\u538b\u7f29)\u6a21\u578b, \u7f16\u7801\u5668\u5177\u67096\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u517166M\u53c2\u6570\u91cf.","title":"1.8 DistilBERT\u53ca\u5176\u53d8\u4f53"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#19-albert","text":"albert-base-v1: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. albert-base-v2: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u5728\u82f1\u6587\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230, \u76f8\u6bd4v1\u4f7f\u7528\u4e86\u66f4\u591a\u7684\u6570\u636e\u91cf, \u82b1\u8d39\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4.","title":"1.9 ALBERT"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#110-t5","text":"t5-small: \u7f16\u7801\u5668\u5177\u67096\u4e2a\u9690\u5c42, \u8f93\u51fa512\u7ef4\u5f20\u91cf, 8\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u517160M\u53c2\u6570\u91cf, \u5728C4\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. t5-base: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 12\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171220M\u53c2\u6570\u91cf, \u5728C4\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. t5-large: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1024\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171770M\u53c2\u6570\u91cf, \u5728C4\u8bed\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.10 T5\u53ca\u5176\u53d8\u4f53"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#11-xlm-roberta","text":"xlm-roberta-base: \u7f16\u7801\u5668\u5177\u670912\u4e2a\u9690\u5c42, \u8f93\u51fa768\u7ef4\u5f20\u91cf, 8\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171125M\u53c2\u6570\u91cf, \u57282.5TB\u7684100\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230. xlm-roberta-large: \u7f16\u7801\u5668\u5177\u670924\u4e2a\u9690\u5c42, \u8f93\u51fa1027\u7ef4\u5f20\u91cf, 16\u4e2a\u81ea\u6ce8\u610f\u529b\u5934, \u5171355M\u53c2\u6570\u91cf, \u57282.5TB\u7684100\u79cd\u8bed\u8a00\u6587\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3\u800c\u5f97\u5230.","title":"1.1 XLM-RoBERTa\u53ca\u5176\u53d8\u4f53"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#2","text":"\u6240\u6709\u4e0a\u8ff0\u9884\u8bad\u7ec3\u6a21\u578b\u53ca\u5176\u53d8\u4f53\u90fd\u662f\u4ee5transformer\u4e3a\u57fa\u7840\uff0c\u53ea\u662f\u5728\u6a21\u578b\u7ed3\u6784\u5982\u795e\u7ecf\u5143\u8fde\u63a5\u65b9\u5f0f\uff0c\u7f16\u7801\u5668\u9690\u5c42\u6570\uff0c\u591a\u5934\u6ce8\u610f\u529b\u7684\u5934\u6570\u7b49\u53d1\u751f\u6539\u53d8\uff0c\u8fd9\u4e9b\u6539\u53d8\u65b9\u5f0f\u7684\u5927\u90e8\u5206\u4f9d\u636e\u90fd\u662f\u7531\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u800c\u5b9a\uff0c\u56e0\u6b64\uff0c\u5bf9\u4e8e\u6211\u4eec\u4f7f\u7528\u8005\u800c\u8a00\uff0c\u4e0d\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u6df1\u5ea6\u63a2\u7a76\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7ed3\u6784\u8bbe\u8ba1\u7684\u4f18\u52a3\uff0c\u53ea\u9700\u8981\u5728\u81ea\u5df1\u5904\u7406\u7684\u76ee\u6807\u6570\u636e\u4e0a\uff0c\u5c3d\u91cf\u904d\u5386\u6240\u6709\u53ef\u7528\u7684\u6a21\u578b\u5bf9\u6bd4\u5f97\u5230\u6700\u4f18\u6548\u679c\u5373\u53ef.","title":"2 \u9884\u8bad\u7ec3\u6a21\u578b\u8bf4\u660e"},{"location":"05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html#3","text":"\u5f53\u4e0bNLP\u4e2d\u6d41\u884c\u7684\u9884\u8bad\u7ec3\u6a21\u578b: BERT GPT GPT-2 Transformer-XL XLNet XLM RoBERTa DistilBERT ALBERT T5 XLM-RoBERTa","title":"3 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u5e76\u638c\u63e1\u7ba1\u9053\u65b9\u5f0f\u5b8c\u6210\u57fa\u672cNLP\u4efb\u52a1 \u4e86\u89e3\u5e76\u638c\u63e1\u81ea\u52a8\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210\u57fa\u672cNLP\u4efb\u52a1 \u4e86\u89e3\u5e76\u638c\u63e1\u5177\u4f53\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210\u57fa\u672cNLP\u4efb\u52a1 1 \u4e86\u89e3Transformers\u5e93 \u00b6 Huggingface\u603b\u90e8\u4f4d\u4e8e\u7ebd\u7ea6\uff0c\u662f\u4e00\u5bb6\u4e13\u6ce8\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u4eba\u5de5\u667a\u80fd\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u521b\u4e1a\u516c\u53f8\u3002\u4ed6\u4eec\u6240\u63d0\u4f9b\u7684\u804a\u5929\u673a\u5668\u4eba\u6280\u672f\u4e00\u76f4\u9887\u53d7\u6b22\u8fce\uff0c\u4f46\u66f4\u51fa\u540d\u7684\u662f\u4ed6\u4eec\u5728NLP\u5f00\u6e90\u793e\u533a\u4e0a\u7684\u8d21\u732e\u3002Huggingface\u4e00\u76f4\u81f4\u529b\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u6280\u672f\u7684\u5e73\u6c11\u5316(democratize)\uff0c\u5e0c\u671b\u6bcf\u4e2a\u4eba\u90fd\u80fd\u7528\u4e0a\u6700\u5148\u8fdb(SOTA, state-of-the-art)\u7684NLP\u6280\u672f\uff0c\u800c\u975e\u56f0\u7a98\u4e8e\u8bad\u7ec3\u8d44\u6e90\u7684\u532e\u4e4f\u3002\u540c\u65f6Hugging Face\u4e13\u6ce8\u4e8eNLP\u6280\u672f\uff0c\u62e5\u6709\u5927\u578b\u7684\u5f00\u6e90\u793e\u533a\u3002\u5c24\u5176\u662f\u5728github\u4e0a\u5f00\u6e90\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5e93 Transformers\uff0c\u5df2\u88ab\u4e0b\u8f7d\u8d85\u8fc7\u4e00\u767e\u4e07\u6b21\uff0cgithub\u4e0a\u8d85\u8fc724000\u4e2astar\u3002 Huggingface Transformers \u662f\u57fa\u4e8e\u4e00\u4e2a\u5f00\u6e90\u57fa\u4e8e transformer \u6a21\u578b\u7ed3\u6784\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u5e93\u3002\u5b83\u652f\u6301 Pytorch\uff0cTensorflow2.0\uff0c\u5e76\u4e14\u652f\u6301\u4e24\u4e2a\u6846\u67b6\u7684\u76f8\u4e92\u8f6c\u6362\u3002Transformers \u63d0\u4f9b\u4e86NLP\u9886\u57df\u5927\u91cfstate-of-art\u7684 \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u7684\u6a21\u578b\u548c\u8c03\u7528\u6846\u67b6\u3002 \u6846\u67b6\u652f\u6301\u4e86\u6700\u65b0\u7684\u5404\u79cdNLP\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u8005\u53ef\u5feb\u901f\u7684\u8fdb\u884c\u6a21\u578b\u8c03\u7528\uff0c\u5e76\u4e14\u652f\u6301\u6a21\u578bfurther pretraining \u548c \u4e0b\u6e38\u4efb\u52a1fine-tuning\u3002\u4e3e\u4e2a\u4f8b\u5b50Transformers \u5e93\u63d0\u4f9b\u4e86\u5f88\u591aSOTA\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6bd4\u5982BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL\u3002 \u793e\u533aTransformer\u7684\u8bbf\u95ee\u5730\u5740\u4e3a\uff1a https://huggingface.co/ \uff0c\u89c1\u4e0b\u56fe\u3002 \u5907\u6ce8 1 \u70b9\u51fb Model\u94fe\u63a5\u53ef\u67e5\u770b\u3001\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u70b9\u51fbDatasets\u94fe\u63a5\u53ef\u67e5\u770b\u3001\u4e0b\u8f7d\u6570\u636e\u96c6\u3002\u70b9\u51fbDocs\u94fe\u63a5\u53ef\u4ee5\u9605\u8bfb\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7f16\u7a0b\u6587\u6863\uff0c\u5341\u5206\u65b9\u4fbf 2 SOTA\uff08state-of-the-art\uff09\u662f\u6307\u76ee\u524d\u5bf9\u67d0\u9879\u4efb\u52a1\u201c\u6700\u597d\u7684\u201d\u7b97\u6cd5\u6216\u6280\u672f 2 Transformers\u5e93\u4e09\u5c42\u5e94\u7528\u7ed3\u6784 \u00b6 \u7ba1\u9053\uff08Pipline\uff09\u65b9\u5f0f\uff1a\u9ad8\u5ea6\u96c6\u6210\u7684\u6781\u7b80\u4f7f\u7528\u65b9\u5f0f\uff0c\u53ea\u9700\u8981\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\u4e00\u4e2aNLP\u4efb\u52a1\u3002 \u81ea\u52a8\u6a21\u578b\uff08AutoMode\uff09\u65b9\u5f0f\uff1a\u53ef\u8f7d\u5165\u5e76\u4f7f\u7528BERTology\u7cfb\u5217\u6a21\u578b\u3002 \u5177\u4f53\u6a21\u578b\uff08SpecificModel\uff09\u65b9\u5f0f\uff1a\u5728\u4f7f\u7528\u65f6\uff0c\u9700\u8981\u660e\u786e\u6307\u5b9a\u5177\u4f53\u7684\u6a21\u578b\uff0c\u5e76\u6309\u7167\u6bcf\u4e2aBERTology\u7cfb\u5217\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u53c2\u6570\u8fdb\u884c\u8c03\u7528\uff0c\u8be5\u65b9\u5f0f\u76f8\u5bf9\u590d\u6742\uff0c\u4f46\u5177\u6709\u8f83\u9ad8\u7684\u7075\u6d3b\u5ea6\u3002 3 \u7ba1\u9053\u65b9\u5f0f\u5b8c\u6210\u591a\u79cdNLP\u4efb\u52a1 \u00b6 \u6ce8\u610f\uff1a\u82e5\u865a\u62df\u673a\u4e2d\u5df2\u7ecf\u5b89\u88c5transformers\uff0c\u4ee5\u4e0b\u5b89\u88c5\u6b65\u9aa4\u4e0d\u9700\u518d\u6b21\u6267\u884c # \u6ce8\u610f\u5728\u6267\u884cclone\u4e4b\u524d\uff0c\u8981\u67e5\u770b\u5f53\u524d\u662f\u5728\u90a3\u4e2a\u76ee\u5f55\u4e0b\uff0c\u6bd4\u5982$HOME/nlpdev/\u76ee\u5f55\u4e0b # \u514b\u9686huggingface\u7684transfomers\u6587\u4ef6 git clone https : // github . com / huggingface / transformers . git # \u8fdb\u884ctransformers\u6587\u4ef6\u5939 cd transformers # \u5207\u6362transformers\u5230\u6307\u5b9a\u7248\u672c git checkout v4 .19.0 # \u5b89\u88c5transformers\u5305 pip install . # \u5b89\u88c5datasets\u6570\u636e\u5e93\uff0c # \u6ce8\u610fworkon xxx\u865a\u62df\u673a\u5f00\u53d1\u73af\u5883\uff0c\u5728\u865a\u62df\u673a\u5f00\u53d1\u73af\u5883\u4e0b\u5b89\u88c5 pip install datasets 3.1 \u6587\u672c\u5206\u7c7b\u4efb\u52a1 \u00b6 \u6587\u672c\u5206\u7c7b\u662f\u6307\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u6587\u672c\u4e2d\u7684\u5185\u5bb9\u6765\u8fdb\u884c\u5206\u7c7b\u3002\u4f8b\u5982\u6839\u636e\u5185\u5bb9\u5bf9\u60c5\u7eea\u8fdb\u884c\u5206\u7c7b\uff0c\u6839\u636e\u5185\u5bb9\u5bf9\u5546\u54c1\u5206\u7c7b\u7b49\u3002\u6587\u672c\u5206\u7c7b\u6a21\u578b\u4e00\u822c\u662f\u901a\u8fc7\u6709\u76d1\u7763\u8bad\u7ec3\u5f97\u5230\u7684\u3002\u5bf9\u6587\u672c\u5185\u5bb9\u7684\u5177\u4f53\u5206\u7c7b\uff0c\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u65f6\u6240\u4f7f\u7528\u7684\u6837\u672c\u6807\u7b7e\u3002 # \u5bfc\u5165\u5de5\u5177\u5305 import torch from transformers import pipeline import numpy as np # \u60c5\u611f\u5206\u7c7b\u4efb\u52a1 def dm01_test_classification (): # 1 \u4f7f\u7528\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578bchinese_sentiment # \u6a21\u578b\u4e0b\u8f7d\u5730\u5740 git clone https://huggingface.co/techthiyanes/chinese_sentiment # 2 \u5b9e\u4f8b\u5316pipeline\u5bf9\u8c61 my_model = pipeline ( task = 'sentiment-analysis' , model = './chinese_sentiment' ) # my_model = pipeline(task='sentiment-analysis', model='./bert-base-chinese') # 3 \u6587\u672c\u9001\u7ed9\u6a21\u578b \u8fdb\u884c\u6587\u672c\u5206\u7c7b output = my_model ( '\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\uff0c\u5929\u5b89\u95e8\u4e0a\u592a\u9633\u5347\u3002' ) print ( 'output--->' , output ) # \u7ed3\u679c\u8f93\u51fa output ---> [{ 'label' : 'star 5' , 'score' : 0.6314294338226318 }] pipeline\u51fd\u6570\u53ef\u4ee5\u81ea\u52a8\u4ece\u5b98\u7f51\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u52a0\u8f7d\u672c\u5730\u7684\u9884\u8bad\u7ec3\u6a21\u578b transformer\u5e93\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u67e5\u627e\u548c\u4e0b\u8f7d 3.2 \u7279\u5f81\u63d0\u53d6\u4efb\u52a1 \u00b6 \u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u53ea\u8fd4\u56de\u6587\u672c\u5904\u7406\u540e\u7684\u7279\u5f81\uff0c\u5c5e\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8303\u7574\u3002\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u7684\u8f93\u51fa\u7ed3\u679c\u9700\u8981\u548c\u5176\u4ed6\u6a21\u578b\u4e00\u8d77\u5de5\u4f5c\u3002 # \u7279\u5f81\u62bd\u53d6\u4efb\u52a1 def dm02_test_feature_extraction (): # 1 \u4e0b\u8f7d\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b git clone https://huggingface.co/bert-base-chinese # 2 \u5b9e\u4f8b\u5316pipeline\u5bf9\u8c61 \u8fd4\u56de\u6a21\u578b\u5bf9\u8c61 my_model = pipeline ( task = 'feature-extraction' , model = './bert-base-chinese' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u63d0\u53d6\u8bed\u53e5\u7279\u5f81 output = my_model ( '\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934' ) print ( 'output--->' , type ( output ), np . array ( output ) . shape ) # \u8f93\u51fa\u7ed3\u679c # output---> <class 'list'> (1, 9, 768) # 7\u4e2a\u5b57\u53d8\u62109\u4e2a\u5b57\u539f\u56e0: [CLS] \u4eba \u751f \u8be5 \u5982 \u4f55 \u8d77 \u5934 [SEP] \u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u5c5e\u4e8e\u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff0c\u672cbert-base-chinese\u6a21\u578b\u76849\u4e2a\u5b57\uff0c\u6bcf\u4e2a\u5b57\u7684\u7279\u5f81\u7ef4\u5ea6\u662f768 \u5e26\u5934\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u5176\u4ed6\u6709\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u6bd4\u5982\u6587\u672c\u5206\u7c7b\uff0c\u5b8c\u578b\u586b\u7a7a\u5c5e\u4e8e\u5e26\u5934\u4efb\u52a1\u8f93\u51fa\uff0c\u4f1a\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7c7b\u578b\u4e0d\u540c\u8f93\u51fa\u4e0d\u540c\u7684\u7ed3\u679c 3.3 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 \u00b6 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1\u53c8\u88ab\u53eb\u505a\u201c\u906e\u853d\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u201d\uff0c\u5b83\u5c5e\u4e8eBERT\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b50\u4efb\u52a1\u3002\u4e0b\u9762\u5b8c\u6210\u4e00\u4e2a\u4e2d\u6587\u573a\u666f\u7684\u5b8c\u578b\u586b\u7a7a\u3002 # \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 def dm03_test_fill_mask (): # 1 \u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b \u5168\u8bcd\u6a21\u578bgit clone https://huggingface.co/hfl/chinese-bert-wwm # 2 \u5b9e\u4f8b\u5316pipeline\u5bf9\u8c61 \u8fd4\u56de\u4e00\u4e2a\u6a21\u578b my_model = pipeline ( task = 'fill-mask' , model = 'chinese-bert-wwm' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u505a\u9884\u6d4b input = '\u6211\u60f3\u660e\u5929\u53bb[MASK]\u5bb6\u5403\u996d\u3002' output = my_model ( input ) # 4 \u8f93\u51fa\u9884\u6d4b\u7ed3\u679c print ( 'output--->' , output ) # \u8f93\u51fa\u7ed3\u679c # output---> # [{'score': 0.34331339597702026, 'token': 1961, 'token_str': '\u5979', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u5979 \u5bb6 \u5403 \u996d.'}, # {'score': 0.2533259987831116, 'token': 872, 'token_str': '\u4f60', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u4f60 \u5bb6 \u5403 \u996d.'}, # {'score': 0.1874391734600067, 'token': 800, 'token_str': '\u4ed6', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u4ed6 \u5bb6 \u5403 \u996d.'}, # {'score': 0.1273055076599121, 'token': 2769, 'token_str': '\u6211', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u6211 \u5bb6 \u5403 \u996d.'}, # {'score': 0.02162978984415531, 'token': 2644, 'token_str': '\u60a8', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u60a8 \u5bb6 \u5403 \u996d.'}] \u53ef\u4ee5\u5728\u5b98\u7f51\u5728\u7ebf\u67e5\u627e\u5b8c\u578b\u586b\u7a7a\u7ed3\u679c 3.4 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1 \u00b6 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u53c8\u79f0\u4e3a\u201c\u62bd\u53d6\u5f0f\u95ee\u7b54\u4efb\u52a1\u201d\uff0c\u5373\u8f93\u5165\u4e00\u6bb5\u6587\u672c\u548c\u4e00\u4e2a\u95ee\u9898\uff0c\u8ba9\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u3002 # \u9605\u8bfb\u7406\u89e3\u4efb\u52a1(\u62bd\u53d6\u5f0f\u95ee\u7b54) def dm04_test_question_answering (): # \u95ee\u7b54\u8bed\u53e5 context = '\u6211\u53eb\u5f20\u4e09\uff0c\u6211\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458\uff0c\u6211\u7684\u559c\u597d\u662f\u6253\u7bee\u7403\u3002' questions = [ '\u6211\u662f\u8c01\uff1f' , '\u6211\u662f\u505a\u4ec0\u4e48\u7684\uff1f' , '\u6211\u7684\u7231\u597d\u662f\u4ec0\u4e48\uff1f' ] # 1 \u4e0b\u8f7d\u6a21\u578b git clone https://huggingface.co/luhua/chinese_pretrain_mrc_roberta_wwm_ext_large # 2 \u5b9e\u4f8b\u5316\u5316pipeline \u8fd4\u56de\u6a21\u578b model = pipeline ( 'question-answering' , model = 'chinese_pretrain_mrc_roberta_wwm_ext_large' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u7684\u9884\u6d4b\u7ed3\u679c print ( model ( context = context , question = questions )) # \u8f93\u51fa\u7ed3\u679c ''' [{'score': 1.2071758523357623e-12, 'start': 2, 'end': 4, 'answer': '\u5f20\u4e09'}, {'score': 2.60890374192968e-06, 'start': 9, 'end': 12, 'answer': '\u7a0b\u5e8f\u5458'}, {'score': 4.1686924134864967e-08, 'start': 18, 'end': 21, 'answer': '\u6253\u7bee\u7403'}] ''' 3.5 \u6587\u672c\u6458\u8981\u4efb\u52a1 \u00b6 \u6458\u8981\u751f\u6210\u4efb\u52a1\u7684\u8f93\u5165\u4e00\u4e00\u6bb5\u6587\u672c\uff0c\u8f93\u51fa\u662f\u4e00\u6bb5\u6982\u51b5\u3001\u7b80\u5355\u7684\u6587\u5b57\u3002 # \u6587\u672c\u6458\u8981\u4efb\u52a1 def dm05_test_summarization (): # 1 \u4e0b\u8f7d\u6a21\u578b git clone https://huggingface.co/sshleifer/distilbart-cnn-12-6 # 2 \u5b9e\u4f8b\u5316pipline \u8fd4\u56de\u6a21\u578b my_model = pipeline ( task = 'summarization' , model = \"distilbart-cnn-12-6\" ) # 3 \u51c6\u5907\u6587\u672c \u9001\u7ed9\u6a21\u578b text = \"BERT is a transformers model pretrained on a large corpus of English data \" \\ \"in a self-supervised fashion. This means it was pretrained on the raw texts \" \\ \"only, with no humans labelling them in any way (which is why it can use lots \" \\ \"of publicly available data) with an automatic process to generate inputs and \" \\ \"labels from those texts. More precisely, it was pretrained with two objectives:Masked \" \\ \"language modeling (MLM): taking a sentence, the model randomly masks 15 % o f the \" \\ \"words in the input then run the entire masked sentence through the model and has \" \\ \"to predict the masked words. This is different from traditional recurrent neural \" \\ \"networks (RNNs) that usually see the words one after the other, or from autoregressive \" \\ \"models like GPT which internally mask the future tokens. It allows the model to learn \" \\ \"a bidirectional representation of the sentence.Next sentence prediction (NSP): the models\" \\ \" concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to \" \\ \"sentences that were next to each other in the original text, sometimes not. The model then \" \\ \"has to predict if the two sentences were following each other or not.\" output = my_model ( text ) # 4 \u6253\u5370\u6458\u8981\u7ed3\u679c print ( 'output--->' , output ) # \u8f93\u51fa\u7ed3\u679c output ---> [{ 'summary_text' : ' BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion . It was pretrained with two objectives: Masked language modeling (MLM) and next sentence prediction (NSP) This allows the model to learn a bidirectional representation of the sentence .' }] 3.6 NER\u4efb\u52a1 \u00b6 \u5b9e\u4f53\u8bcd\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u662fNLP\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\u3002\u5b83\u7528\u4e8e\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u4eba\u540d\uff08PER\uff09\u3001\u5730\u540d\uff08LOC\uff09\u3001\u7ec4\u7ec7\uff08ORG\uff09\u4ee5\u53ca\u5176\u4ed6\u5b9e\u4f53\uff08MISC\uff09\u7b49\u3002\u4f8b\u5982\uff1a(\u738b B-PER) (\u5c0f I-PER) (\u660e I-PER) (\u5728 O) (\u529e B-LOC) (\u516c I-LOC) (\u5ba4 I-LOC)\u3002\u5176\u4e2dO\u8868\u793a\u4e00\u4e2a\u975e\u5b9e\u4f53\uff0cB\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u7684\u5f00\u59cb\uff0cI\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u5757\u7684\u5185\u90e8\u3002 \u5b9e\u4f53\u8bcd\u8bc6\u522b\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1\uff08\u53c8\u53eb\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\uff09\uff0c\u5b9e\u4f53\u8bcd\u8bc6\u522b\u662f\u53e5\u6cd5\u5206\u6790\u7684\u57fa\u7840\uff0c\u800c\u53e5\u6cd5\u5206\u6790\u4f18\u52bfNLP\u4efb\u52a1\u7684\u6838\u5fc3\u3002``` # NER\u4efb\u52a1 def dm06_test_ner (): # 1 \u4e0b\u8f7d\u6a21\u578b git clone https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese # 2 \u5b9e\u4f8b\u5316pipeline \u8fd4\u56de\u6a21\u578b model = pipeline ( 'ner' , model = 'roberta-base-finetuned-cluener2020-chinese' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6253\u5370NER\u7ed3\u679c print ( model ( '\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\uff0c\u5929\u5b89\u95e8\u4e0a\u592a\u9633\u5347\u3002' )) ''' [{'entity': 'B-address', 'score': 0.8838121, 'index': 3, 'word': '\u5317', 'start': 2, 'end': 3}, {'entity': 'I-address', 'score': 0.83543754, 'index': 4, 'word': '\u4eac', 'start': 3, 'end': 4}, {'entity': 'I-address', 'score': 0.4240591, 'index': 5, 'word': '\u5929', 'start': 4, 'end': 5}, {'entity': 'I-address', 'score': 0.7524443, 'index': 6, 'word': '\u5b89', 'start': 5, 'end': 6}, {'entity': 'I-address', 'score': 0.6949866, 'index': 7, 'word': '\u95e8', 'start': 6, 'end': 7}, {'entity': 'B-address', 'score': 0.65552264, 'index': 9, 'word': '\u5929', 'start': 8, 'end': 9}, {'entity': 'I-address', 'score': 0.5376768, 'index': 10, 'word': '\u5b89', 'start': 9, 'end': 10}, {'entity': 'I-address', 'score': 0.510813, 'index': 11, 'word': '\u95e8', 'start': 10, 'end': 11}] ''' 4 \u81ea\u52a8\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210\u591a\u79cdNLP\u4efb\u52a1 \u00b6 4.1 \u6587\u672c\u5206\u7c7b\u4efb\u52a1 \u00b6 \u6587\u672c\u5206\u7c7b\u662f\u6307\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u6587\u672c\u4e2d\u7684\u5185\u5bb9\u6765\u8fdb\u884c\u5206\u7c7b\u3002\u4f8b\u5982\u6839\u636e\u5185\u5bb9\u5bf9\u60c5\u7eea\u8fdb\u884c\u5206\u7c7b\uff0c\u6839\u636e\u5185\u5bb9\u5bf9\u5546\u54c1\u5206\u7c7b\u7b49\u3002\u6587\u672c\u5206\u7c7b\u6a21\u578b\u4e00\u822c\u662f\u901a\u8fc7\u6709\u76d1\u7763\u8bad\u7ec3\u5f97\u5230\u7684\u3002\u5bf9\u6587\u672c\u5185\u5bb9\u7684\u5177\u4f53\u5206\u7c7b\uff0c\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u65f6\u6240\u4f7f\u7528\u7684\u6837\u672c\u6807\u7b7e\u3002 # \u5bfc\u5165\u5de5\u5177\u5305 import torch from transformers import AutoConfig , AutoModel , AutoTokenizer from transformers import AutoModelForSequenceClassification , AutoModelForMaskedLM , AutoModelForQuestionAnswering # AutoModelForSeq2SeqLM\uff1a\u6587\u672c\u6458\u8981 # AutoModelForTokenClassification\uff1aner from transformers import AutoModelForSeq2SeqLM , AutoModelForTokenClassification # \u60c5\u611f\u5206\u7c7b\u4efb\u52a1 def dm01_test_classification (): # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( './chinese_sentiment' ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForSequenceClassification . from_pretrained ( './chinese_sentiment' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf message = '\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934' # 3-1 return_tensors='pt' \u8fd4\u56de\u662f\u4e8c\u7ef4tensor msg_tensor1 = my_tokenizer . encode ( text = message , return_tensors = 'pt' , padding = True , truncation = True , max_length = 20 ) print ( 'msg_tensor1--->' , msg_tensor1 ) # 3-2 \u4e0d\u7528return_tensors='pt'\u662f\u4e00\u7ef4\u5217\u8868 msg_list2 = my_tokenizer . encode ( text = message , padding = True , truncation = True , max_length = 20 ) print ( 'msg_list2--->' , msg_list2 ) msg_tensor2 = torch . tensor ([ msg_list2 ]) print ( 'msg_tensor2--->' , msg_tensor2 ) # 4 \u6570\u636e\u9001\u7ed9\u6a21\u578b # 4-1 my_model . eval () output1 = my_model ( msg_tensor2 ) print ( '\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout1--->' , output1 ) # 4-2 output2 = my_model ( msg_tensor2 , return_dict = False ) print ( '\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout2--->' , output2 ) AutoTokenizer\u3001AutoModelForSequenceClassification\u51fd\u6570\u53ef\u4ee5\u81ea\u52a8\u4ece\u5b98\u7f51\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u52a0\u8f7d\u672c\u5730\u7684\u9884\u8bad\u7ec3\u6a21\u578b AutoModelForSequenceClassification\u7c7b\u7ba1\u7406\u7740\u5206\u7c7b\u4efb\u52a1\uff0c\u4f1a\u6839\u636e\u53c2\u6570\u7684\u8f93\u5165\u9009\u7528\u4e0d\u540c\u7684\u6a21\u578b\u3002 AutoTokenizer\u7684encode()\u51fd\u6570\u4f7f\u7528return_tensors=\u2019pt\u2018\u53c2\u6570\u548c\u4e0d\u4f7f\u7528pt\u53c2\u6570\u5bf9\u6587\u672c\u7f16\u7801\u7684\u7ed3\u679c\u4e0d\u540c AutoTokenizer\u7684encode()\u51fd\u6570\u4f7f\u7528padding='max_length'\u53ef\u4ee5\u6309\u7167\u6700\u5927\u7a0b\u5ea6\u8fdb\u884c\u8865\u9f50\uff0c\u4fd7\u79f0\u6253padding \u8c03\u7528\u6a21\u578b\u7684forward\u51fd\u6570\u8f93\u5165return_dict=False\u53c2\u6570\uff0c\u8fd4\u56de\u7ed3\u679c\u4e5f\u4e0d\u540c \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c msg_tensor1 ---> tensor ([[ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ]]) msg_list2 ---> [ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ] msg_tensor2 ---> tensor ([[ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ]]) \u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout1 ---> SequenceClassifierOutput ( loss = None , logits = tensor ([[ - 2.7387 , - 1.7528 , 0.2273 , 2.0507 , 1.4128 ]], grad_fn =< AddmmBackward > ), hidden_states = None , attentions = None ) \u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout2 ---> ( tensor ([[ - 2.7387 , - 1.7528 , 0.2273 , 2.0507 , 1.4128 ]], grad_fn =< AddmmBackward > ),) #\u6ce81:101\u4ee3\u8868[CLS] 102\u4ee3\u8868[SEP] 4.2 \u7279\u5f81\u63d0\u53d6\u4efb\u52a1 \u00b6 \u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u53ea\u8fd4\u56de\u6587\u672c\u5904\u7406\u540e\u7684\u7279\u5f81\uff0c\u5c5e\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8303\u7574\u3002\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u7684\u8f93\u51fa\u7ed3\u679c\u9700\u8981\u548c\u5176\u4ed6\u6a21\u578b\u4e00\u8d77\u5de5\u4f5c\u3002 # \u7279\u5f81\u63d0\u53d6\u4efb\u52a1-\u4e0d\u5e26\u4efb\u52a1\u8f93\u51fa\u5934\u7684\u4efb\u52a1 def dm02_test_feature_extraction (): # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = './bert-base-chinese' ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModel . from_pretrained ( pretrained_model_name_or_path = './bert-base-chinese' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf message = [ '\u4f60\u662f\u8c01' , '\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934' ] msgs_tensor = my_tokenizer . encode_plus ( text = message , return_tensors = 'pt' , truncation = True , pad_to_max_length = True , max_length = 30 ) print ( 'msgs_tensor--->' , msgs_tensor ) # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e\u63d0\u53d6\u7279\u5f81 my_model . eval () output = my_model ( ** msgs_tensor ) print ( '\u4e0d\u5e26\u6a21\u578b\u5934\u8f93\u51faoutput--->' , output ) print ( 'outputs.last_hidden_state.shape--->' , output . last_hidden_state . shape ) # torch.Size([1, 30, 768]) print ( 'outputs.pooler_output.shape--->' , output . pooler_output . shape ) # torch.Size([1, 768]) \u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u5c5e\u4e8e\u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff0c\u672cbert-base-chinese\u6a21\u578b\u76849\u4e2a\u5b57\uff0c\u6bcf\u4e2a\u5b57\u7684\u7279\u5f81\u7ef4\u5ea6\u662f768 \u5e26\u5934\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u5176\u4ed6\u6709\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u6bd4\u5982\u6587\u672c\u5206\u7c7b\uff0c\u5b8c\u578b\u586b\u7a7a\u5c5e\u4e8e\u5e26\u5934\u4efb\u52a1\u8f93\u51fa\uff0c\u4f1a\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7c7b\u578b\u4e0d\u540c\u8f93\u51fa\u4e0d\u540c\u7684\u7ed3\u679c \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c msgs_tensor ---> # 1 input_ids\u5bf9\u4e24\u4e2a\u53e5\u5b50text2id\u4ee5\u540e\u7684\u7ed3\u679c\uff0c # 101\u8868\u793a\u6bb5\u843d\u5f00\u5934\uff0c\u7b2c\u4e00\u4e2a102\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50\u7ed3\u675f\uff0c\u7b2c\u4e8c\u4e2a102\u70b9\u7b2c\u4e8c\u4e2a\u53e5\u5b50\u7ed3\u675f # \u540e\u9762\u76840\u8868\u793a \u6309\u7167\u7f16\u7801\u8981\u6c42pad_to_max_length=True\u548cmax_length=30\u8865\u5145pad\u96f6 { 'input_ids' : tensor ([[ 101 , 872 , 3221 , 6443 , 102 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), # 2 token_type_ids\u8868\u793a\u6bb5\u843d\u6807\u5fd70\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50\uff0c1\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u53e5\u5b50 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), # 3 attention_mask\u8868\u793a\u6ce8\u610f\u529b\u673a\u5236\u7684\u63a9\u7801\u6570\u636e\uff0c1\u8868\u793a\u6709\u771f\u5b9e\u6570\u636e\uff0c0\u8868\u793a\u662fpad\u6570\u636e\u9700\u8981\u63a9\u7801 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]])} # 1 last_hidden_state\u8868\u793a\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42\u7684\u6570\u636e [1,30,768] # 2 pooler_output\u8868\u793a\u6c60\u5316\uff0c\u4e5f\u5c31\u662f\u5bf9\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42\u518d\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u4ee5\u540e\u5e73\u5747\u6c60\u5316\u7684\u7ed3\u679c\u3002\u5206\u7c7b\u65f6\u5019\u4f7f\u7528\u3002 \u4e0d\u5e26\u6a21\u578b\u5934\u8f93\u51faoutput ---> BaseModelOutputWithPoolingAndCrossAttentions ( last_hidden_state = tensor ([[[ 0.7001 , 0.4651 , 0.2427 , ... , 0.5753 , - 0.4330 , 0.1878 ], [ 0.4017 , 0.1123 , 0.4482 , ... , - 0.2614 , - 0.2649 , - 0.1497 ], [ 1.2000 , - 0.4859 , 1.1970 , ... , 0.7543 , - 0.2405 , - 0.2627 ], ... , [ 0.2074 , 0.4022 , - 0.0448 , ... , - 0.0849 , - 0.0766 , - 0.2134 ], [ 0.0879 , 0.2482 , - 0.2356 , ... , 0.2967 , - 0.2357 , - 0.5138 ], [ 0.4944 , 0.1340 , - 0.2387 , ... , 0.2375 , - 0.1011 , - 0.3314 ]]], grad_fn =< NativeLayerNormBackward > ), pooler_output = tensor ([[ 0.9996 , 1.0000 , 0.9995 , 0.9412 , 0.8629 , 0.9592 , - 0.8144 , - 0.9654 , 0.9892 , - 0.9997 , 1.0000 , 0.9998 , - 0.1187 , - 0.9373 , 0.9999 , - 1.0000 , ... , - 0.9967 , 1.0000 , 0.8626 , - 0.9993 , - 0.9704 , - 0.9993 , - 0.9971 , 0.8522 ]], grad_fn =< TanhBackward > ), hidden_states = None , past_key_values = None , attentions = None , cross_attentions = None ) outputs . last_hidden_state . shape ---> torch . Size ([ 1 , 30 , 768 ]) outputs . pooler_output . shape ---> torch . Size ([ 1 , 768 ]) 4.3 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 \u00b6 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1\u53c8\u88ab\u53eb\u505a\u201c\u906e\u853d\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u201d\uff0c\u5b83\u5c5e\u4e8eBERT\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b50\u4efb\u52a1\u3002\u4e0b\u9762\u5b8c\u6210\u4e00\u4e2a\u4e2d\u6587\u573a\u666f\u7684\u5b8c\u578b\u586b\u7a7a\u3002 # \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 def dm03_test_fill_mask (): # 1 \u52a0\u8f7dtokenizer modename = \"chinese-bert-wwm\" # modename = \"bert-base-chinese\" my_tokenizer = AutoTokenizer . from_pretrained ( modename ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForMaskedLM . from_pretrained ( modename ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf input = my_tokenizer . encode_plus ( '\u6211\u60f3\u660e\u5929\u53bb[MASK]\u5bb6\u5403\u996d.' , return_tensors = 'pt' ) print ( 'input--->' , input ) # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e\u63d0\u53d6\u7279\u5f81 my_model . eval () output = my_model ( ** input ) print ( 'output--->' , output ) print ( 'output.logits--->' , output . logits . shape ) # [1,12,21128] # 5 \u53d6\u6982\u7387\u6700\u9ad8 mask_pred_idx = torch . argmax ( output . logits [ 0 ][ 6 ]) . item () print ( '\u6253\u5370\u6982\u7387\u6700\u9ad8\u7684\u5b57:' , my_tokenizer . convert_ids_to_tokens ([ mask_pred_idx ])) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c # 1 input_ids \u5bf9\u53e5\u5b50text2id\u4ee5\u540e\u7684\u7ed3\u679c # 2 token_type_ids \u53e5\u5b50\u5206\u6bb5\u4fe1\u606f # 3 attention_mask \u53e5\u5b50\u63a9\u7801\u4fe1\u606f input ---> { 'input_ids' : tensor ([[ 101 , 2769 , 2682 , 3209 , 1921 , 1343 , 103 , 2157 , 1391 , 7649 , 119 , 102 ]]), 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]])} # 1 logits\u8868\u793aMASK\u9884\u6d4b\u7684\u7ed3\u679c\uff0c\u4e5f\u662f\u4e00\u79cd\u5206\u7c7b\u6982\u7387 # 2 output.logits\u7684\u5206\u7c7b\u5f62\u72b6 [1, 12, 21128] # 3 \u901a\u8fc7 my_tokenizer.convert_ids_to_tokens()\u51fd\u6570\u5b8c\u6210id2text\u7684\u64cd\u4f5c output ---> MaskedLMOutput ( loss = None , logits = tensor ([[[ - 9.9017 , - 9.6006 , - 9.8032 , ... , - 7.9744 , - 7.7402 , - 8.2912 ], [ - 14.3878 , - 15.0353 , - 14.7893 , ... , - 10.0437 , - 10.5279 , - 9.7544 ], [ - 14.2215 , - 14.1145 , - 14.5770 , ... , - 6.3246 , - 4.1784 , - 4.6072 ], ... , [ - 14.6938 , - 16.8133 , - 15.1296 , ... , - 9.2327 , - 8.1931 , - 15.2430 ], [ - 10.8649 , - 11.4887 , - 11.5731 , ... , - 6.5378 , - 0.8715 , - 5.3870 ], [ - 11.8495 , - 11.8358 , - 12.0314 , ... , - 8.4242 , - 6.2741 , - 8.2787 ]]], grad_fn =< AddBackward0 > ), hidden_states = None , attentions = None ) output . logits ---> torch . Size ([ 1 , 12 , 21128 ]) \u6253\u5370\u6982\u7387\u6700\u9ad8\u7684\u5b57 : [ '\u5979' ] 4.4 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1 \u00b6 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u53c8\u79f0\u4e3a\u201c\u62bd\u53d6\u5f0f\u95ee\u7b54\u4efb\u52a1\u201d\uff0c\u5373\u8f93\u5165\u4e00\u6bb5\u6587\u672c\u548c\u4e00\u4e2a\u95ee\u9898\uff0c\u8ba9\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u3002 # \u9605\u8bfb\u7406\u89e3\u4efb\u52a1(\u62bd\u53d6\u5f0f\u95ee\u7b54) def dm04_test_question_answering (): # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( './chinese_pretrain_mrc_roberta_wwm_ext_large' ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForQuestionAnswering . from_pretrained ( './chinese_pretrain_mrc_roberta_wwm_ext_large' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf # \u6587\u5b57\u4e2d\u7684\u6807\u70b9\u7b26\u53f7\u5982\u679c\u662f\u4e2d\u6587\u7684\u8bdd\uff0c\u4f1a\u5f71\u54cd\u5230\u9884\u6d4b\u7ed3\u679c \u4e5f\u53ef\u4ee5\u53bb\u6389\u6807\u70b9\u7b26\u53f7 context = '\u6211\u53eb\u5f20\u4e09 \u6211\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458 \u6211\u7684\u559c\u597d\u662f\u6253\u7bee\u7403' questions = [ '\u6211\u662f\u8c01\uff1f' , '\u6211\u662f\u505a\u4ec0\u4e48\u7684\uff1f' , '\u6211\u7684\u7231\u597d\u662f\u4ec0\u4e48\uff1f' ] # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6a21\u578b\u505a\u62bd\u53d6\u5f0f\u95ee\u7b54 my_model . eval () for question in questions : input = my_tokenizer . encode_plus ( question , context , return_tensors = 'pt' ) print ( 'input--->' , input ) output = my_model ( ** input ) print ( 'output--->' , output ) start , end = torch . argmax ( output . start_logits ), torch . argmax ( output . end_logits ) + 1 answer = my_tokenizer . convert_ids_to_tokens ( input [ 'input_ids' ][ 0 ][ start : end ] ) print ( 'question:' , question , 'answer:' , answer ) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c\uff1a # input_ids\u8868\u793atext2id\u540e\u7ed3\u679c # token_type_ids\u8868\u793a\u53e5\u5b50\u5206\u6bb5\u4fe1\u606f # attention_mask\u8868\u793a\u53e5\u5b50attention\u63a9\u7801\u4fe1\u606f input ---> { 'input_ids' : tensor ([[ 101 , 2769 , 3221 , 6443 , 8043 , 102 , 2769 , 1373 , 2476 , 676 , 2769 , 3221 , 671 , 702 , 4923 , 2415 , 1447 , 2769 , 4638 , 1599 , 1962 , 3221 , 2802 , 5074 , 4413 , 102 ]]), 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]]), 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]])} # start_logits end_logits\u5206\u5e03\u8868\u793a\u4ece\u539f\u6587\u4e2d\u62bd\u53d6\u7b54\u6848\u7684\u4f4d\u7f6e\u6982\u7387 # \u6bd4\u5982\uff1astart_logits\u7684\u6700\u5927\u503c\u4ee3\u8868\u53e5\u5b50\u7b54\u6848\u6700\u53ef\u80fd\u5f00\u59cb\u7684\u4f4d\u7f6e # \u6bd4\u5982\uff1aend_logits\u7684\u6700\u5927\u503c\u4ee3\u8868\u53e5\u5b50\u7b54\u6848\u53ef\u80fd\u7ed3\u675f\u7684\u4f4d\u7f6e output ---> QuestionAnsweringModelOutput ( loss = None , start_logits = tensor ([[ - 1.9978 , - 11.4788 , - 12.6324 , - 11.8324 , - 12.4148 , - 11.9371 , - 2.7246 , - 6.6402 , 3.9131 , - 2.9533 , - 7.0866 , - 9.5696 , - 4.2775 , - 8.9042 , 0.5753 , - 6.9468 , - 7.0469 , - 8.5334 , - 11.3796 , - 9.3905 , - 11.0242 , - 11.1047 , - 5.7124 , - 2.7293 , - 7.5896 , - 12.6013 ]], grad_fn =< CopyBackwards > ), end_logits = tensor ([[ - 1.3483 , - 12.0141 , - 11.6312 , - 11.6629 , - 11.9607 , - 12.0039 , - 4.6118 , - 7.4034 , - 2.3499 , 4.7159 , - 7.2880 , - 9.5317 , - 6.6742 , - 6.0915 , - 7.0023 , - 4.9691 , 1.4515 , - 7.8329 , - 9.0895 , - 10.3742 , - 8.7482 , - 9.8567 , - 7.2930 , - 5.8163 , - 1.7323 , - 12.2525 ]], grad_fn =< CopyBackwards > ), hidden_states = None , attentions = None ) question : \u6211\u662f\u8c01 \uff1f answer : [ '\u5f20' , '\u4e09' ] question : \u6211\u662f\u505a\u4ec0\u4e48\u7684 \uff1f answer : [ '\u7a0b' , '\u5e8f' , '\u5458' ] question : \u6211\u7684\u7231\u597d\u662f\u4ec0\u4e48 \uff1f answer : [ '\u6253' , '\u7bee' , '\u7403' ] 4.5 \u6587\u672c\u6458\u8981\u4efb\u52a1 \u00b6 \u6458\u8981\u751f\u6210\u4efb\u52a1\u7684\u8f93\u5165\u4e00\u4e00\u6bb5\u6587\u672c\uff0c\u8f93\u51fa\u662f\u4e00\u6bb5\u6982\u51b5\u3001\u7b80\u5355\u7684\u6587\u5b57\u3002 # \u6587\u672c\u6458\u8981\u4efb\u52a1 def dm05_test_summarization (): text = \"BERT is a transformers model pretrained on a large corpus of English data \" \\ \"in a self-supervised fashion. This means it was pretrained on the raw texts \" \\ \"only, with no humans labelling them in any way (which is why it can use lots \" \\ \"of publicly available data) with an automatic process to generate inputs and \" \\ \"labels from those texts. More precisely, it was pretrained with two objectives:Masked \" \\ \"language modeling (MLM): taking a sentence, the model randomly masks 15 % o f the \" \\ \"words in the input then run the entire masked sentence through the model and has \" \\ \"to predict the masked words. This is different from traditional recurrent neural \" \\ \"networks (RNNs) that usually see the words one after the other, or from autoregressive \" \\ \"models like GPT which internally mask the future tokens. It allows the model to learn \" \\ \"a bidirectional representation of the sentence.Next sentence prediction (NSP): the models\" \\ \" concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to \" \\ \"sentences that were next to each other in the original text, sometimes not. The model then \" \\ \"has to predict if the two sentences were following each other or not.\" # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = \"distilbart-cnn-12-6\" ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForSeq2SeqLM . from_pretrained ( pretrained_model_name_or_path = 'distilbart-cnn-12-6' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf input = my_tokenizer ([ text ], return_tensors = 'pt' ) # print('input--->', input) # 4 \u9001\u7ed9\u6a21\u578b\u505a\u6458\u8981 my_model . eval () output = my_model . generate ( input . input_ids ) print ( 'output--->' , output ) # 5 \u5904\u7406\u6458\u8981\u7ed3\u679c # 5-1 decode \u7684 skip_special_tokens \u53c2\u6570\u53ef\u4ee5\u53bb\u9664 token \u524d\u9762\u7684\u7279\u6b8a\u5b57\u7b26 print ([ my_tokenizer . decode ( g , skip_special_tokens = True , clean_up_tokenization_spaces = False ) for g in output ]) # 5-2 convert_ids_to_tokens \u51fd\u6570\u53ea\u80fd\u5c06 ids \u8fd8\u539f\u4e3a token # print(my_tokenizer.convert_ids_to_tokens(output[0])) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c\uff1a output ---> tensor ([[ 2 , 0 , 11126 , 565 , 16 , 10 , 7891 , 268 , 1421 , 11857 , 26492 , 15 , 10 , 739 , 42168 , 9 , 2370 , 414 , 11 , 10 , 1403 , 12 , 16101 , 25376 , 2734 , 479 , 85 , 21 , 11857 , 26492 , 19 , 80 , 10366 , 35 , 31755 , 196 , 2777 , 19039 , 36 , 10537 , 448 , 43 , 8 , 220 , 3645 , 16782 , 36 , 487 , 4186 , 43 , 20 , 3092 , 10146 , 26511 , 1626 , 80 , 24397 , 11305 , 25 , 16584 , 148 , 11857 , 32155 , 479 , 7411 , 51 , 20719 , 7 , 11305 , 14 , 58 , 220 , 7 , 349 , 97 , 11 , 5 , 1461 , 2788 , 6 , 2128 , 45 , 479 , 2 ]]) [ 'BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion . It was pretrained with two objectives: Masked language modeling (MLM) and next sentence prediction (NSP) The models concatenates two masked sentences as inputs during pretraining . Sometimes they correspond to sentences that were next to each other in the original text, sometimes not .' ] 4.6 NER\u4efb\u52a1 \u00b6 \u5b9e\u4f53\u8bcd\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u662fNLP\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\u3002\u5b83\u7528\u4e8e\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u4eba\u540d\uff08PER\uff09\u3001\u5730\u540d\uff08LOC\uff09\u3001\u7ec4\u7ec7\uff08ORG\uff09\u4ee5\u53ca\u5176\u4ed6\u5b9e\u4f53\uff08MISC\uff09\u7b49\u3002\u4f8b\u5982\uff1a(\u738b B-PER) (\u5c0f I-PER) (\u660e I-PER) (\u5728 O) (\u529e B-LOC) (\u516c I-LOC) (\u5ba4 I-LOC)\u3002\u5176\u4e2dO\u8868\u793a\u4e00\u4e2a\u975e\u5b9e\u4f53\uff0cB\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u7684\u5f00\u59cb\uff0cI\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u5757\u7684\u5185\u90e8\u3002 \u5b9e\u4f53\u8bcd\u8bc6\u522b\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1\uff08\u53c8\u53eb\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\uff09\uff0c\u5b9e\u4f53\u8bcd\u8bc6\u522b\u662f\u53e5\u6cd5\u5206\u6790\u7684\u57fa\u7840\uff0c\u800c\u53e5\u6cd5\u5206\u6790\u4f18\u52bfNLP\u4efb\u52a1\u7684\u6838\u5fc3\u3002``` # NER\u4efb\u52a1 def dm06_test_ner (): # 1 \u52a0\u8f7dtokenizer \u52a0\u8f7d\u6a21\u578b \u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 # https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese my_tokenizer = AutoTokenizer . from_pretrained ( 'roberta-base-finetuned-cluener2020-chinese' ) my_model = AutoModelForTokenClassification . from_pretrained ( 'roberta-base-finetuned-cluener2020-chinese' ) config = AutoConfig . from_pretrained ( 'roberta-base-finetuned-cluener2020-chinese' ) # 2 \u6570\u636e\u5f20\u91cf\u5316 inputs = my_tokenizer . encode_plus ( '\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\uff0c\u5929\u5b89\u95e8\u4e0a\u592a\u9633\u5347' , return_tensors = 'pt' ) print ( 'inputs--->' , inputs . input_ids . shape , inputs . input_ids ) # torch.Size([1, 17]) # 3 \u9001\u5165\u6a21\u578b \u9884\u6d4bner\u6982\u7387 \u6bcf\u4e2a\u5b57\u9884\u6d4b\u7684\u6807\u7b7e\u6982\u7387 my_model . eval () logits = my_model ( inputs . input_ids ) . logits print ( 'logits--->' , logits . shape ) # torch.Size([1, 17, 32]) # 4 \u5bf9\u9884\u6d4b\u6570\u636e \u8fdb\u884c\u663e\u793a input_tokens = my_tokenizer . convert_ids_to_tokens ( inputs . input_ids [ 0 ]) print ( 'input_tokens--->' , input_tokens ) outputs = [] for token , value in zip ( input_tokens , logits [ 0 ]): if token in my_tokenizer . all_special_tokens : continue # \u83b7\u5f97\u6bcf\u4e2a\u5b57\u9884\u6d4b\u6982\u7387\u6700\u5927\u7684\u6807\u7b7e\u7d22\u5f15 idx = torch . argmax ( value ) . item () # \u6253\u5370\u7d22\u5f15\u5bf9\u5e94\u6807\u7b7e outputs . append (( token , config . id2label [ idx ])) print ( outputs ) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c inputs ---> torch . Size ([ 1 , 17 ]) tensor ([[ 101 , 2769 , 4263 , 1266 , 776 , 1921 , 2128 , 7305 , 8024 , 1921 , 2128 , 7305 , 677 , 1922 , 7345 , 1285 , 102 ]]) logits ---> torch . Size ([ 1 , 17 , 32 ]) input_tokens ---> [ '[CLS]' , '\u6211' , '\u7231' , '\u5317' , '\u4eac' , '\u5929' , '\u5b89' , '\u95e8' , '\uff0c' , '\u5929' , '\u5b89' , '\u95e8' , '\u4e0a' , '\u592a' , '\u9633' , '\u5347' , '[SEP]' ] [( '\u6211' , 'O' ), ( '\u7231' , 'O' ), ( '\u5317' , 'B-address' ), ( '\u4eac' , 'I-address' ), ( '\u5929' , 'I-address' ), ( '\u5b89' , 'I-address' ), ( '\u95e8' , 'I-address' ), ( '\uff0c' , 'O' ), ( '\u5929' , 'B-address' ), ( '\u5b89' , 'I-address' ), ( '\u95e8' , 'I-address' ), ( '\u4e0a' , 'O' ), ( '\u592a' , 'O' ), ( '\u9633' , 'O' ), ( '\u5347' , 'O' )] 5 \u5177\u4f53\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210NLP\u4efb\u52a1 \u00b6 5.1 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 \u00b6 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1\u53c8\u88ab\u53eb\u505a\u201c\u906e\u853d\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u201d\uff0c\u5b83\u5c5e\u4e8eBERT\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b50\u4efb\u52a1\u3002\u4e0b\u9762\u5b8c\u6210\u4e00\u4e2a\u4e2d\u6587\u573a\u666f\u7684\u5b8c\u578b\u586b\u7a7a\u3002 # \u5177\u4f53\u6a21\u578b\u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 def dm01_test_bert_fill_mask (): # 1 \u52a0\u8f7dtokenizer modename = \"bert-base-chinese\" my_tokenizer = BertTokenizer . from_pretrained ( modename ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = BertForMaskedLM . from_pretrained ( modename ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf input = my_tokenizer . encode_plus ( '\u6211\u60f3\u660e\u5929\u53bb[MASK]\u5bb6\u5403\u996d' , return_tensors = 'pt' ) print ( 'input--->' , input ) # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e\u63d0\u53d6\u7279\u5f81 output = my_model ( ** input ) print ( 'output--->' , output ) print ( 'output.logits--->' , output . logits . shape ) # [1,11,21128] # 5 \u53d6\u6982\u7387\u6700\u9ad8 mask_pred_idx = torch . argmax ( output . logits [ 0 ][ 6 ]) . item () print ( '\u6253\u5370\u6982\u7387\u6700\u9ad8\u7684\u5b57:' , my_tokenizer . convert_ids_to_tokens ([ mask_pred_idx ])) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c # input_ids\u8868\u793atext2id\u540e\u7ed3\u679c # token_type_ids\u8868\u793a\u53e5\u5b50\u5206\u6bb5\u4fe1\u606f # attention_mask\u8868\u793a\u53e5\u5b50attention\u63a9\u7801\u4fe1\u606f input ---> { 'input_ids' : tensor ([[ 101 , 2769 , 2682 , 3209 , 1921 , 1343 , 103 , 2157 , 1391 , 7649 , 102 ]]), 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]])} output ---> MaskedLMOutput ( loss = None , logits = tensor ([[[ - 8.1771 , - 8.1008 , - 8.1191 , ... , - 6.8355 , - 6.9482 , - 6.9834 ], [ - 8.2775 , - 8.1251 , - 8.1655 , ... , - 6.8471 , - 7.4265 , - 6.1365 ], [ - 14.1093 , - 13.1037 , - 14.6324 , ... , - 6.0959 , - 3.7550 , - 5.7456 ], ... , [ - 16.2103 , - 16.7243 , - 15.9876 , ... , - 5.9727 , - 8.2757 , - 7.3852 ], [ - 13.5615 , - 13.7670 , - 13.4497 , ... , - 7.8282 , - 4.9095 , - 9.1699 ], [ - 10.3200 , - 10.1068 , - 10.4439 , ... , - 6.6468 , - 7.0597 , - 7.5027 ]]], grad_fn =< AddBackward0 > ), hidden_states = None , attentions = None ) output . logits ---> torch . Size ([ 1 , 11 , 21128 ]) 6 \u5c0f\u7ed3 \u00b6 \u4e86\u89e3Huggingface Transformers Huggingface Transformers \u662f\u57fa\u4e8e\u4e00\u4e2a\u5f00\u6e90\u57fa\u4e8e transformer \u6a21\u578b\u7ed3\u6784\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u3002\u6846\u67b6\u652f\u6301\u4e86\u6700\u65b0\u7684\u5404\u79cdNLP\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u8005\u53ef\u5feb\u901f\u7684\u8fdb\u884c\u6a21\u578b\u8c03\u7528\uff0c\u5e76\u4e14\u652f\u6301\u6a21\u578bfurther pretraining \u548c \u4e0b\u6e38\u4efb\u52a1fine-tuning \u4f7f\u7528\u7ba1\u9053\u65b9\u5f0f\u5b8c\u6210\u591a\u79cdNLP\u4efb\u52a1 \u6587\u672c\u5206\u7c7b\u4efb\u52a1sentiment-analysis\u3001\u7279\u5f81\u63d0\u53d6\u4efb\u52a1feature-extraction\u3001\u5b8c\u578b\u586b\u7a7a\u4efb\u52a1fill-mask\u3001\u9605\u8bfb\u7406\u89e3\u4efb\u52a1question-answering\u3001\u6587\u672c\u6458\u8981\u4efb\u52a1summarization\u3001NER\u4efb\u52a1ner \u4f7f\u7528\u81ea\u52a8\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210NLP\u4efb\u52a1 \u5bf9Aoto\u7cfb\u5217\u6a21\u578b\u7c7b\u8fdb\u884c\u4f7f\u7528AutoModel\u3001AutoTokenizer\u3001AutoModelForSequenceClassification\u3001AutoModelForMaskedLM\u3001AutoModelForQuestionAnswering\u3001AutoModelForSeq2SeqLM\u3001AutoModelForTokenClassification \u4f7f\u7528\u5177\u4f53\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210NLP\u4efb\u52a1 \u4f7f\u7528\u5177\u4f53\u6a21\u578b\u6bd4\u5982BertTokenizer\u7b49\u8fdb\u884c\u5177\u4f53NLP\u4efb\u52a1\u5f00\u53d1","title":"8 Transformers\u5e93\u4f7f\u7528"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#_1","text":"\u4e86\u89e3\u5e76\u638c\u63e1\u7ba1\u9053\u65b9\u5f0f\u5b8c\u6210\u57fa\u672cNLP\u4efb\u52a1 \u4e86\u89e3\u5e76\u638c\u63e1\u81ea\u52a8\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210\u57fa\u672cNLP\u4efb\u52a1 \u4e86\u89e3\u5e76\u638c\u63e1\u5177\u4f53\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210\u57fa\u672cNLP\u4efb\u52a1","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#1-transformers","text":"Huggingface\u603b\u90e8\u4f4d\u4e8e\u7ebd\u7ea6\uff0c\u662f\u4e00\u5bb6\u4e13\u6ce8\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u4eba\u5de5\u667a\u80fd\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u521b\u4e1a\u516c\u53f8\u3002\u4ed6\u4eec\u6240\u63d0\u4f9b\u7684\u804a\u5929\u673a\u5668\u4eba\u6280\u672f\u4e00\u76f4\u9887\u53d7\u6b22\u8fce\uff0c\u4f46\u66f4\u51fa\u540d\u7684\u662f\u4ed6\u4eec\u5728NLP\u5f00\u6e90\u793e\u533a\u4e0a\u7684\u8d21\u732e\u3002Huggingface\u4e00\u76f4\u81f4\u529b\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u6280\u672f\u7684\u5e73\u6c11\u5316(democratize)\uff0c\u5e0c\u671b\u6bcf\u4e2a\u4eba\u90fd\u80fd\u7528\u4e0a\u6700\u5148\u8fdb(SOTA, state-of-the-art)\u7684NLP\u6280\u672f\uff0c\u800c\u975e\u56f0\u7a98\u4e8e\u8bad\u7ec3\u8d44\u6e90\u7684\u532e\u4e4f\u3002\u540c\u65f6Hugging Face\u4e13\u6ce8\u4e8eNLP\u6280\u672f\uff0c\u62e5\u6709\u5927\u578b\u7684\u5f00\u6e90\u793e\u533a\u3002\u5c24\u5176\u662f\u5728github\u4e0a\u5f00\u6e90\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5e93 Transformers\uff0c\u5df2\u88ab\u4e0b\u8f7d\u8d85\u8fc7\u4e00\u767e\u4e07\u6b21\uff0cgithub\u4e0a\u8d85\u8fc724000\u4e2astar\u3002 Huggingface Transformers \u662f\u57fa\u4e8e\u4e00\u4e2a\u5f00\u6e90\u57fa\u4e8e transformer \u6a21\u578b\u7ed3\u6784\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u5e93\u3002\u5b83\u652f\u6301 Pytorch\uff0cTensorflow2.0\uff0c\u5e76\u4e14\u652f\u6301\u4e24\u4e2a\u6846\u67b6\u7684\u76f8\u4e92\u8f6c\u6362\u3002Transformers \u63d0\u4f9b\u4e86NLP\u9886\u57df\u5927\u91cfstate-of-art\u7684 \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u7684\u6a21\u578b\u548c\u8c03\u7528\u6846\u67b6\u3002 \u6846\u67b6\u652f\u6301\u4e86\u6700\u65b0\u7684\u5404\u79cdNLP\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u8005\u53ef\u5feb\u901f\u7684\u8fdb\u884c\u6a21\u578b\u8c03\u7528\uff0c\u5e76\u4e14\u652f\u6301\u6a21\u578bfurther pretraining \u548c \u4e0b\u6e38\u4efb\u52a1fine-tuning\u3002\u4e3e\u4e2a\u4f8b\u5b50Transformers \u5e93\u63d0\u4f9b\u4e86\u5f88\u591aSOTA\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6bd4\u5982BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL\u3002 \u793e\u533aTransformer\u7684\u8bbf\u95ee\u5730\u5740\u4e3a\uff1a https://huggingface.co/ \uff0c\u89c1\u4e0b\u56fe\u3002 \u5907\u6ce8 1 \u70b9\u51fb Model\u94fe\u63a5\u53ef\u67e5\u770b\u3001\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u70b9\u51fbDatasets\u94fe\u63a5\u53ef\u67e5\u770b\u3001\u4e0b\u8f7d\u6570\u636e\u96c6\u3002\u70b9\u51fbDocs\u94fe\u63a5\u53ef\u4ee5\u9605\u8bfb\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7f16\u7a0b\u6587\u6863\uff0c\u5341\u5206\u65b9\u4fbf 2 SOTA\uff08state-of-the-art\uff09\u662f\u6307\u76ee\u524d\u5bf9\u67d0\u9879\u4efb\u52a1\u201c\u6700\u597d\u7684\u201d\u7b97\u6cd5\u6216\u6280\u672f","title":"1 \u4e86\u89e3Transformers\u5e93"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#2-transformers","text":"\u7ba1\u9053\uff08Pipline\uff09\u65b9\u5f0f\uff1a\u9ad8\u5ea6\u96c6\u6210\u7684\u6781\u7b80\u4f7f\u7528\u65b9\u5f0f\uff0c\u53ea\u9700\u8981\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\u4e00\u4e2aNLP\u4efb\u52a1\u3002 \u81ea\u52a8\u6a21\u578b\uff08AutoMode\uff09\u65b9\u5f0f\uff1a\u53ef\u8f7d\u5165\u5e76\u4f7f\u7528BERTology\u7cfb\u5217\u6a21\u578b\u3002 \u5177\u4f53\u6a21\u578b\uff08SpecificModel\uff09\u65b9\u5f0f\uff1a\u5728\u4f7f\u7528\u65f6\uff0c\u9700\u8981\u660e\u786e\u6307\u5b9a\u5177\u4f53\u7684\u6a21\u578b\uff0c\u5e76\u6309\u7167\u6bcf\u4e2aBERTology\u7cfb\u5217\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u53c2\u6570\u8fdb\u884c\u8c03\u7528\uff0c\u8be5\u65b9\u5f0f\u76f8\u5bf9\u590d\u6742\uff0c\u4f46\u5177\u6709\u8f83\u9ad8\u7684\u7075\u6d3b\u5ea6\u3002","title":"2 Transformers\u5e93\u4e09\u5c42\u5e94\u7528\u7ed3\u6784"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#3-nlp","text":"\u6ce8\u610f\uff1a\u82e5\u865a\u62df\u673a\u4e2d\u5df2\u7ecf\u5b89\u88c5transformers\uff0c\u4ee5\u4e0b\u5b89\u88c5\u6b65\u9aa4\u4e0d\u9700\u518d\u6b21\u6267\u884c # \u6ce8\u610f\u5728\u6267\u884cclone\u4e4b\u524d\uff0c\u8981\u67e5\u770b\u5f53\u524d\u662f\u5728\u90a3\u4e2a\u76ee\u5f55\u4e0b\uff0c\u6bd4\u5982$HOME/nlpdev/\u76ee\u5f55\u4e0b # \u514b\u9686huggingface\u7684transfomers\u6587\u4ef6 git clone https : // github . com / huggingface / transformers . git # \u8fdb\u884ctransformers\u6587\u4ef6\u5939 cd transformers # \u5207\u6362transformers\u5230\u6307\u5b9a\u7248\u672c git checkout v4 .19.0 # \u5b89\u88c5transformers\u5305 pip install . # \u5b89\u88c5datasets\u6570\u636e\u5e93\uff0c # \u6ce8\u610fworkon xxx\u865a\u62df\u673a\u5f00\u53d1\u73af\u5883\uff0c\u5728\u865a\u62df\u673a\u5f00\u53d1\u73af\u5883\u4e0b\u5b89\u88c5 pip install datasets","title":"3 \u7ba1\u9053\u65b9\u5f0f\u5b8c\u6210\u591a\u79cdNLP\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#31","text":"\u6587\u672c\u5206\u7c7b\u662f\u6307\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u6587\u672c\u4e2d\u7684\u5185\u5bb9\u6765\u8fdb\u884c\u5206\u7c7b\u3002\u4f8b\u5982\u6839\u636e\u5185\u5bb9\u5bf9\u60c5\u7eea\u8fdb\u884c\u5206\u7c7b\uff0c\u6839\u636e\u5185\u5bb9\u5bf9\u5546\u54c1\u5206\u7c7b\u7b49\u3002\u6587\u672c\u5206\u7c7b\u6a21\u578b\u4e00\u822c\u662f\u901a\u8fc7\u6709\u76d1\u7763\u8bad\u7ec3\u5f97\u5230\u7684\u3002\u5bf9\u6587\u672c\u5185\u5bb9\u7684\u5177\u4f53\u5206\u7c7b\uff0c\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u65f6\u6240\u4f7f\u7528\u7684\u6837\u672c\u6807\u7b7e\u3002 # \u5bfc\u5165\u5de5\u5177\u5305 import torch from transformers import pipeline import numpy as np # \u60c5\u611f\u5206\u7c7b\u4efb\u52a1 def dm01_test_classification (): # 1 \u4f7f\u7528\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578bchinese_sentiment # \u6a21\u578b\u4e0b\u8f7d\u5730\u5740 git clone https://huggingface.co/techthiyanes/chinese_sentiment # 2 \u5b9e\u4f8b\u5316pipeline\u5bf9\u8c61 my_model = pipeline ( task = 'sentiment-analysis' , model = './chinese_sentiment' ) # my_model = pipeline(task='sentiment-analysis', model='./bert-base-chinese') # 3 \u6587\u672c\u9001\u7ed9\u6a21\u578b \u8fdb\u884c\u6587\u672c\u5206\u7c7b output = my_model ( '\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\uff0c\u5929\u5b89\u95e8\u4e0a\u592a\u9633\u5347\u3002' ) print ( 'output--->' , output ) # \u7ed3\u679c\u8f93\u51fa output ---> [{ 'label' : 'star 5' , 'score' : 0.6314294338226318 }] pipeline\u51fd\u6570\u53ef\u4ee5\u81ea\u52a8\u4ece\u5b98\u7f51\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u52a0\u8f7d\u672c\u5730\u7684\u9884\u8bad\u7ec3\u6a21\u578b transformer\u5e93\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u67e5\u627e\u548c\u4e0b\u8f7d","title":"3.1 \u6587\u672c\u5206\u7c7b\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#32","text":"\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u53ea\u8fd4\u56de\u6587\u672c\u5904\u7406\u540e\u7684\u7279\u5f81\uff0c\u5c5e\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8303\u7574\u3002\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u7684\u8f93\u51fa\u7ed3\u679c\u9700\u8981\u548c\u5176\u4ed6\u6a21\u578b\u4e00\u8d77\u5de5\u4f5c\u3002 # \u7279\u5f81\u62bd\u53d6\u4efb\u52a1 def dm02_test_feature_extraction (): # 1 \u4e0b\u8f7d\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b git clone https://huggingface.co/bert-base-chinese # 2 \u5b9e\u4f8b\u5316pipeline\u5bf9\u8c61 \u8fd4\u56de\u6a21\u578b\u5bf9\u8c61 my_model = pipeline ( task = 'feature-extraction' , model = './bert-base-chinese' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u63d0\u53d6\u8bed\u53e5\u7279\u5f81 output = my_model ( '\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934' ) print ( 'output--->' , type ( output ), np . array ( output ) . shape ) # \u8f93\u51fa\u7ed3\u679c # output---> <class 'list'> (1, 9, 768) # 7\u4e2a\u5b57\u53d8\u62109\u4e2a\u5b57\u539f\u56e0: [CLS] \u4eba \u751f \u8be5 \u5982 \u4f55 \u8d77 \u5934 [SEP] \u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u5c5e\u4e8e\u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff0c\u672cbert-base-chinese\u6a21\u578b\u76849\u4e2a\u5b57\uff0c\u6bcf\u4e2a\u5b57\u7684\u7279\u5f81\u7ef4\u5ea6\u662f768 \u5e26\u5934\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u5176\u4ed6\u6709\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u6bd4\u5982\u6587\u672c\u5206\u7c7b\uff0c\u5b8c\u578b\u586b\u7a7a\u5c5e\u4e8e\u5e26\u5934\u4efb\u52a1\u8f93\u51fa\uff0c\u4f1a\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7c7b\u578b\u4e0d\u540c\u8f93\u51fa\u4e0d\u540c\u7684\u7ed3\u679c","title":"3.2 \u7279\u5f81\u63d0\u53d6\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#33","text":"\u5b8c\u578b\u586b\u7a7a\u4efb\u52a1\u53c8\u88ab\u53eb\u505a\u201c\u906e\u853d\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u201d\uff0c\u5b83\u5c5e\u4e8eBERT\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b50\u4efb\u52a1\u3002\u4e0b\u9762\u5b8c\u6210\u4e00\u4e2a\u4e2d\u6587\u573a\u666f\u7684\u5b8c\u578b\u586b\u7a7a\u3002 # \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 def dm03_test_fill_mask (): # 1 \u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b \u5168\u8bcd\u6a21\u578bgit clone https://huggingface.co/hfl/chinese-bert-wwm # 2 \u5b9e\u4f8b\u5316pipeline\u5bf9\u8c61 \u8fd4\u56de\u4e00\u4e2a\u6a21\u578b my_model = pipeline ( task = 'fill-mask' , model = 'chinese-bert-wwm' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u505a\u9884\u6d4b input = '\u6211\u60f3\u660e\u5929\u53bb[MASK]\u5bb6\u5403\u996d\u3002' output = my_model ( input ) # 4 \u8f93\u51fa\u9884\u6d4b\u7ed3\u679c print ( 'output--->' , output ) # \u8f93\u51fa\u7ed3\u679c # output---> # [{'score': 0.34331339597702026, 'token': 1961, 'token_str': '\u5979', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u5979 \u5bb6 \u5403 \u996d.'}, # {'score': 0.2533259987831116, 'token': 872, 'token_str': '\u4f60', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u4f60 \u5bb6 \u5403 \u996d.'}, # {'score': 0.1874391734600067, 'token': 800, 'token_str': '\u4ed6', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u4ed6 \u5bb6 \u5403 \u996d.'}, # {'score': 0.1273055076599121, 'token': 2769, 'token_str': '\u6211', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u6211 \u5bb6 \u5403 \u996d.'}, # {'score': 0.02162978984415531, 'token': 2644, 'token_str': '\u60a8', 'sequence': '\u6211 \u60f3 \u660e \u5929 \u53bb \u60a8 \u5bb6 \u5403 \u996d.'}] \u53ef\u4ee5\u5728\u5b98\u7f51\u5728\u7ebf\u67e5\u627e\u5b8c\u578b\u586b\u7a7a\u7ed3\u679c","title":"3.3 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#34","text":"\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u53c8\u79f0\u4e3a\u201c\u62bd\u53d6\u5f0f\u95ee\u7b54\u4efb\u52a1\u201d\uff0c\u5373\u8f93\u5165\u4e00\u6bb5\u6587\u672c\u548c\u4e00\u4e2a\u95ee\u9898\uff0c\u8ba9\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u3002 # \u9605\u8bfb\u7406\u89e3\u4efb\u52a1(\u62bd\u53d6\u5f0f\u95ee\u7b54) def dm04_test_question_answering (): # \u95ee\u7b54\u8bed\u53e5 context = '\u6211\u53eb\u5f20\u4e09\uff0c\u6211\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458\uff0c\u6211\u7684\u559c\u597d\u662f\u6253\u7bee\u7403\u3002' questions = [ '\u6211\u662f\u8c01\uff1f' , '\u6211\u662f\u505a\u4ec0\u4e48\u7684\uff1f' , '\u6211\u7684\u7231\u597d\u662f\u4ec0\u4e48\uff1f' ] # 1 \u4e0b\u8f7d\u6a21\u578b git clone https://huggingface.co/luhua/chinese_pretrain_mrc_roberta_wwm_ext_large # 2 \u5b9e\u4f8b\u5316\u5316pipeline \u8fd4\u56de\u6a21\u578b model = pipeline ( 'question-answering' , model = 'chinese_pretrain_mrc_roberta_wwm_ext_large' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u7684\u9884\u6d4b\u7ed3\u679c print ( model ( context = context , question = questions )) # \u8f93\u51fa\u7ed3\u679c ''' [{'score': 1.2071758523357623e-12, 'start': 2, 'end': 4, 'answer': '\u5f20\u4e09'}, {'score': 2.60890374192968e-06, 'start': 9, 'end': 12, 'answer': '\u7a0b\u5e8f\u5458'}, {'score': 4.1686924134864967e-08, 'start': 18, 'end': 21, 'answer': '\u6253\u7bee\u7403'}] '''","title":"3.4 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#35","text":"\u6458\u8981\u751f\u6210\u4efb\u52a1\u7684\u8f93\u5165\u4e00\u4e00\u6bb5\u6587\u672c\uff0c\u8f93\u51fa\u662f\u4e00\u6bb5\u6982\u51b5\u3001\u7b80\u5355\u7684\u6587\u5b57\u3002 # \u6587\u672c\u6458\u8981\u4efb\u52a1 def dm05_test_summarization (): # 1 \u4e0b\u8f7d\u6a21\u578b git clone https://huggingface.co/sshleifer/distilbart-cnn-12-6 # 2 \u5b9e\u4f8b\u5316pipline \u8fd4\u56de\u6a21\u578b my_model = pipeline ( task = 'summarization' , model = \"distilbart-cnn-12-6\" ) # 3 \u51c6\u5907\u6587\u672c \u9001\u7ed9\u6a21\u578b text = \"BERT is a transformers model pretrained on a large corpus of English data \" \\ \"in a self-supervised fashion. This means it was pretrained on the raw texts \" \\ \"only, with no humans labelling them in any way (which is why it can use lots \" \\ \"of publicly available data) with an automatic process to generate inputs and \" \\ \"labels from those texts. More precisely, it was pretrained with two objectives:Masked \" \\ \"language modeling (MLM): taking a sentence, the model randomly masks 15 % o f the \" \\ \"words in the input then run the entire masked sentence through the model and has \" \\ \"to predict the masked words. This is different from traditional recurrent neural \" \\ \"networks (RNNs) that usually see the words one after the other, or from autoregressive \" \\ \"models like GPT which internally mask the future tokens. It allows the model to learn \" \\ \"a bidirectional representation of the sentence.Next sentence prediction (NSP): the models\" \\ \" concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to \" \\ \"sentences that were next to each other in the original text, sometimes not. The model then \" \\ \"has to predict if the two sentences were following each other or not.\" output = my_model ( text ) # 4 \u6253\u5370\u6458\u8981\u7ed3\u679c print ( 'output--->' , output ) # \u8f93\u51fa\u7ed3\u679c output ---> [{ 'summary_text' : ' BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion . It was pretrained with two objectives: Masked language modeling (MLM) and next sentence prediction (NSP) This allows the model to learn a bidirectional representation of the sentence .' }]","title":"3.5 \u6587\u672c\u6458\u8981\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#36-ner","text":"\u5b9e\u4f53\u8bcd\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u662fNLP\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\u3002\u5b83\u7528\u4e8e\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u4eba\u540d\uff08PER\uff09\u3001\u5730\u540d\uff08LOC\uff09\u3001\u7ec4\u7ec7\uff08ORG\uff09\u4ee5\u53ca\u5176\u4ed6\u5b9e\u4f53\uff08MISC\uff09\u7b49\u3002\u4f8b\u5982\uff1a(\u738b B-PER) (\u5c0f I-PER) (\u660e I-PER) (\u5728 O) (\u529e B-LOC) (\u516c I-LOC) (\u5ba4 I-LOC)\u3002\u5176\u4e2dO\u8868\u793a\u4e00\u4e2a\u975e\u5b9e\u4f53\uff0cB\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u7684\u5f00\u59cb\uff0cI\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u5757\u7684\u5185\u90e8\u3002 \u5b9e\u4f53\u8bcd\u8bc6\u522b\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1\uff08\u53c8\u53eb\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\uff09\uff0c\u5b9e\u4f53\u8bcd\u8bc6\u522b\u662f\u53e5\u6cd5\u5206\u6790\u7684\u57fa\u7840\uff0c\u800c\u53e5\u6cd5\u5206\u6790\u4f18\u52bfNLP\u4efb\u52a1\u7684\u6838\u5fc3\u3002``` # NER\u4efb\u52a1 def dm06_test_ner (): # 1 \u4e0b\u8f7d\u6a21\u578b git clone https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese # 2 \u5b9e\u4f8b\u5316pipeline \u8fd4\u56de\u6a21\u578b model = pipeline ( 'ner' , model = 'roberta-base-finetuned-cluener2020-chinese' ) # 3 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6253\u5370NER\u7ed3\u679c print ( model ( '\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\uff0c\u5929\u5b89\u95e8\u4e0a\u592a\u9633\u5347\u3002' )) ''' [{'entity': 'B-address', 'score': 0.8838121, 'index': 3, 'word': '\u5317', 'start': 2, 'end': 3}, {'entity': 'I-address', 'score': 0.83543754, 'index': 4, 'word': '\u4eac', 'start': 3, 'end': 4}, {'entity': 'I-address', 'score': 0.4240591, 'index': 5, 'word': '\u5929', 'start': 4, 'end': 5}, {'entity': 'I-address', 'score': 0.7524443, 'index': 6, 'word': '\u5b89', 'start': 5, 'end': 6}, {'entity': 'I-address', 'score': 0.6949866, 'index': 7, 'word': '\u95e8', 'start': 6, 'end': 7}, {'entity': 'B-address', 'score': 0.65552264, 'index': 9, 'word': '\u5929', 'start': 8, 'end': 9}, {'entity': 'I-address', 'score': 0.5376768, 'index': 10, 'word': '\u5b89', 'start': 9, 'end': 10}, {'entity': 'I-address', 'score': 0.510813, 'index': 11, 'word': '\u95e8', 'start': 10, 'end': 11}] '''","title":"3.6 NER\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#4-nlp","text":"","title":"4 \u81ea\u52a8\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210\u591a\u79cdNLP\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#41","text":"\u6587\u672c\u5206\u7c7b\u662f\u6307\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u6587\u672c\u4e2d\u7684\u5185\u5bb9\u6765\u8fdb\u884c\u5206\u7c7b\u3002\u4f8b\u5982\u6839\u636e\u5185\u5bb9\u5bf9\u60c5\u7eea\u8fdb\u884c\u5206\u7c7b\uff0c\u6839\u636e\u5185\u5bb9\u5bf9\u5546\u54c1\u5206\u7c7b\u7b49\u3002\u6587\u672c\u5206\u7c7b\u6a21\u578b\u4e00\u822c\u662f\u901a\u8fc7\u6709\u76d1\u7763\u8bad\u7ec3\u5f97\u5230\u7684\u3002\u5bf9\u6587\u672c\u5185\u5bb9\u7684\u5177\u4f53\u5206\u7c7b\uff0c\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u65f6\u6240\u4f7f\u7528\u7684\u6837\u672c\u6807\u7b7e\u3002 # \u5bfc\u5165\u5de5\u5177\u5305 import torch from transformers import AutoConfig , AutoModel , AutoTokenizer from transformers import AutoModelForSequenceClassification , AutoModelForMaskedLM , AutoModelForQuestionAnswering # AutoModelForSeq2SeqLM\uff1a\u6587\u672c\u6458\u8981 # AutoModelForTokenClassification\uff1aner from transformers import AutoModelForSeq2SeqLM , AutoModelForTokenClassification # \u60c5\u611f\u5206\u7c7b\u4efb\u52a1 def dm01_test_classification (): # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( './chinese_sentiment' ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForSequenceClassification . from_pretrained ( './chinese_sentiment' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf message = '\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934' # 3-1 return_tensors='pt' \u8fd4\u56de\u662f\u4e8c\u7ef4tensor msg_tensor1 = my_tokenizer . encode ( text = message , return_tensors = 'pt' , padding = True , truncation = True , max_length = 20 ) print ( 'msg_tensor1--->' , msg_tensor1 ) # 3-2 \u4e0d\u7528return_tensors='pt'\u662f\u4e00\u7ef4\u5217\u8868 msg_list2 = my_tokenizer . encode ( text = message , padding = True , truncation = True , max_length = 20 ) print ( 'msg_list2--->' , msg_list2 ) msg_tensor2 = torch . tensor ([ msg_list2 ]) print ( 'msg_tensor2--->' , msg_tensor2 ) # 4 \u6570\u636e\u9001\u7ed9\u6a21\u578b # 4-1 my_model . eval () output1 = my_model ( msg_tensor2 ) print ( '\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout1--->' , output1 ) # 4-2 output2 = my_model ( msg_tensor2 , return_dict = False ) print ( '\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout2--->' , output2 ) AutoTokenizer\u3001AutoModelForSequenceClassification\u51fd\u6570\u53ef\u4ee5\u81ea\u52a8\u4ece\u5b98\u7f51\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u52a0\u8f7d\u672c\u5730\u7684\u9884\u8bad\u7ec3\u6a21\u578b AutoModelForSequenceClassification\u7c7b\u7ba1\u7406\u7740\u5206\u7c7b\u4efb\u52a1\uff0c\u4f1a\u6839\u636e\u53c2\u6570\u7684\u8f93\u5165\u9009\u7528\u4e0d\u540c\u7684\u6a21\u578b\u3002 AutoTokenizer\u7684encode()\u51fd\u6570\u4f7f\u7528return_tensors=\u2019pt\u2018\u53c2\u6570\u548c\u4e0d\u4f7f\u7528pt\u53c2\u6570\u5bf9\u6587\u672c\u7f16\u7801\u7684\u7ed3\u679c\u4e0d\u540c AutoTokenizer\u7684encode()\u51fd\u6570\u4f7f\u7528padding='max_length'\u53ef\u4ee5\u6309\u7167\u6700\u5927\u7a0b\u5ea6\u8fdb\u884c\u8865\u9f50\uff0c\u4fd7\u79f0\u6253padding \u8c03\u7528\u6a21\u578b\u7684forward\u51fd\u6570\u8f93\u5165return_dict=False\u53c2\u6570\uff0c\u8fd4\u56de\u7ed3\u679c\u4e5f\u4e0d\u540c \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c msg_tensor1 ---> tensor ([[ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ]]) msg_list2 ---> [ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ] msg_tensor2 ---> tensor ([[ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ]]) \u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout1 ---> SequenceClassifierOutput ( loss = None , logits = tensor ([[ - 2.7387 , - 1.7528 , 0.2273 , 2.0507 , 1.4128 ]], grad_fn =< AddmmBackward > ), hidden_states = None , attentions = None ) \u60c5\u611f\u5206\u7c7b\u6a21\u578b\u5934\u8f93\u51faoutpout2 ---> ( tensor ([[ - 2.7387 , - 1.7528 , 0.2273 , 2.0507 , 1.4128 ]], grad_fn =< AddmmBackward > ),) #\u6ce81:101\u4ee3\u8868[CLS] 102\u4ee3\u8868[SEP]","title":"4.1 \u6587\u672c\u5206\u7c7b\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#42","text":"\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u53ea\u8fd4\u56de\u6587\u672c\u5904\u7406\u540e\u7684\u7279\u5f81\uff0c\u5c5e\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8303\u7574\u3002\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u7684\u8f93\u51fa\u7ed3\u679c\u9700\u8981\u548c\u5176\u4ed6\u6a21\u578b\u4e00\u8d77\u5de5\u4f5c\u3002 # \u7279\u5f81\u63d0\u53d6\u4efb\u52a1-\u4e0d\u5e26\u4efb\u52a1\u8f93\u51fa\u5934\u7684\u4efb\u52a1 def dm02_test_feature_extraction (): # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = './bert-base-chinese' ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModel . from_pretrained ( pretrained_model_name_or_path = './bert-base-chinese' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf message = [ '\u4f60\u662f\u8c01' , '\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934' ] msgs_tensor = my_tokenizer . encode_plus ( text = message , return_tensors = 'pt' , truncation = True , pad_to_max_length = True , max_length = 30 ) print ( 'msgs_tensor--->' , msgs_tensor ) # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e\u63d0\u53d6\u7279\u5f81 my_model . eval () output = my_model ( ** msgs_tensor ) print ( '\u4e0d\u5e26\u6a21\u578b\u5934\u8f93\u51faoutput--->' , output ) print ( 'outputs.last_hidden_state.shape--->' , output . last_hidden_state . shape ) # torch.Size([1, 30, 768]) print ( 'outputs.pooler_output.shape--->' , output . pooler_output . shape ) # torch.Size([1, 768]) \u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u7279\u5f81\u62bd\u53d6\u4efb\u52a1\u5c5e\u4e8e\u4e0d\u5e26\u4efb\u52a1\u5934\u8f93\u51fa\uff0c\u672cbert-base-chinese\u6a21\u578b\u76849\u4e2a\u5b57\uff0c\u6bcf\u4e2a\u5b57\u7684\u7279\u5f81\u7ef4\u5ea6\u662f768 \u5e26\u5934\u4efb\u52a1\u5934\u8f93\u51fa\uff1a\u5176\u4ed6\u6709\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u6bd4\u5982\u6587\u672c\u5206\u7c7b\uff0c\u5b8c\u578b\u586b\u7a7a\u5c5e\u4e8e\u5e26\u5934\u4efb\u52a1\u8f93\u51fa\uff0c\u4f1a\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7c7b\u578b\u4e0d\u540c\u8f93\u51fa\u4e0d\u540c\u7684\u7ed3\u679c \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c msgs_tensor ---> # 1 input_ids\u5bf9\u4e24\u4e2a\u53e5\u5b50text2id\u4ee5\u540e\u7684\u7ed3\u679c\uff0c # 101\u8868\u793a\u6bb5\u843d\u5f00\u5934\uff0c\u7b2c\u4e00\u4e2a102\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50\u7ed3\u675f\uff0c\u7b2c\u4e8c\u4e2a102\u70b9\u7b2c\u4e8c\u4e2a\u53e5\u5b50\u7ed3\u675f # \u540e\u9762\u76840\u8868\u793a \u6309\u7167\u7f16\u7801\u8981\u6c42pad_to_max_length=True\u548cmax_length=30\u8865\u5145pad\u96f6 { 'input_ids' : tensor ([[ 101 , 872 , 3221 , 6443 , 102 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), # 2 token_type_ids\u8868\u793a\u6bb5\u843d\u6807\u5fd70\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50\uff0c1\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u53e5\u5b50 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), # 3 attention_mask\u8868\u793a\u6ce8\u610f\u529b\u673a\u5236\u7684\u63a9\u7801\u6570\u636e\uff0c1\u8868\u793a\u6709\u771f\u5b9e\u6570\u636e\uff0c0\u8868\u793a\u662fpad\u6570\u636e\u9700\u8981\u63a9\u7801 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]])} # 1 last_hidden_state\u8868\u793a\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42\u7684\u6570\u636e [1,30,768] # 2 pooler_output\u8868\u793a\u6c60\u5316\uff0c\u4e5f\u5c31\u662f\u5bf9\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42\u518d\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u4ee5\u540e\u5e73\u5747\u6c60\u5316\u7684\u7ed3\u679c\u3002\u5206\u7c7b\u65f6\u5019\u4f7f\u7528\u3002 \u4e0d\u5e26\u6a21\u578b\u5934\u8f93\u51faoutput ---> BaseModelOutputWithPoolingAndCrossAttentions ( last_hidden_state = tensor ([[[ 0.7001 , 0.4651 , 0.2427 , ... , 0.5753 , - 0.4330 , 0.1878 ], [ 0.4017 , 0.1123 , 0.4482 , ... , - 0.2614 , - 0.2649 , - 0.1497 ], [ 1.2000 , - 0.4859 , 1.1970 , ... , 0.7543 , - 0.2405 , - 0.2627 ], ... , [ 0.2074 , 0.4022 , - 0.0448 , ... , - 0.0849 , - 0.0766 , - 0.2134 ], [ 0.0879 , 0.2482 , - 0.2356 , ... , 0.2967 , - 0.2357 , - 0.5138 ], [ 0.4944 , 0.1340 , - 0.2387 , ... , 0.2375 , - 0.1011 , - 0.3314 ]]], grad_fn =< NativeLayerNormBackward > ), pooler_output = tensor ([[ 0.9996 , 1.0000 , 0.9995 , 0.9412 , 0.8629 , 0.9592 , - 0.8144 , - 0.9654 , 0.9892 , - 0.9997 , 1.0000 , 0.9998 , - 0.1187 , - 0.9373 , 0.9999 , - 1.0000 , ... , - 0.9967 , 1.0000 , 0.8626 , - 0.9993 , - 0.9704 , - 0.9993 , - 0.9971 , 0.8522 ]], grad_fn =< TanhBackward > ), hidden_states = None , past_key_values = None , attentions = None , cross_attentions = None ) outputs . last_hidden_state . shape ---> torch . Size ([ 1 , 30 , 768 ]) outputs . pooler_output . shape ---> torch . Size ([ 1 , 768 ])","title":"4.2 \u7279\u5f81\u63d0\u53d6\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#43","text":"\u5b8c\u578b\u586b\u7a7a\u4efb\u52a1\u53c8\u88ab\u53eb\u505a\u201c\u906e\u853d\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u201d\uff0c\u5b83\u5c5e\u4e8eBERT\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b50\u4efb\u52a1\u3002\u4e0b\u9762\u5b8c\u6210\u4e00\u4e2a\u4e2d\u6587\u573a\u666f\u7684\u5b8c\u578b\u586b\u7a7a\u3002 # \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 def dm03_test_fill_mask (): # 1 \u52a0\u8f7dtokenizer modename = \"chinese-bert-wwm\" # modename = \"bert-base-chinese\" my_tokenizer = AutoTokenizer . from_pretrained ( modename ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForMaskedLM . from_pretrained ( modename ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf input = my_tokenizer . encode_plus ( '\u6211\u60f3\u660e\u5929\u53bb[MASK]\u5bb6\u5403\u996d.' , return_tensors = 'pt' ) print ( 'input--->' , input ) # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e\u63d0\u53d6\u7279\u5f81 my_model . eval () output = my_model ( ** input ) print ( 'output--->' , output ) print ( 'output.logits--->' , output . logits . shape ) # [1,12,21128] # 5 \u53d6\u6982\u7387\u6700\u9ad8 mask_pred_idx = torch . argmax ( output . logits [ 0 ][ 6 ]) . item () print ( '\u6253\u5370\u6982\u7387\u6700\u9ad8\u7684\u5b57:' , my_tokenizer . convert_ids_to_tokens ([ mask_pred_idx ])) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c # 1 input_ids \u5bf9\u53e5\u5b50text2id\u4ee5\u540e\u7684\u7ed3\u679c # 2 token_type_ids \u53e5\u5b50\u5206\u6bb5\u4fe1\u606f # 3 attention_mask \u53e5\u5b50\u63a9\u7801\u4fe1\u606f input ---> { 'input_ids' : tensor ([[ 101 , 2769 , 2682 , 3209 , 1921 , 1343 , 103 , 2157 , 1391 , 7649 , 119 , 102 ]]), 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]])} # 1 logits\u8868\u793aMASK\u9884\u6d4b\u7684\u7ed3\u679c\uff0c\u4e5f\u662f\u4e00\u79cd\u5206\u7c7b\u6982\u7387 # 2 output.logits\u7684\u5206\u7c7b\u5f62\u72b6 [1, 12, 21128] # 3 \u901a\u8fc7 my_tokenizer.convert_ids_to_tokens()\u51fd\u6570\u5b8c\u6210id2text\u7684\u64cd\u4f5c output ---> MaskedLMOutput ( loss = None , logits = tensor ([[[ - 9.9017 , - 9.6006 , - 9.8032 , ... , - 7.9744 , - 7.7402 , - 8.2912 ], [ - 14.3878 , - 15.0353 , - 14.7893 , ... , - 10.0437 , - 10.5279 , - 9.7544 ], [ - 14.2215 , - 14.1145 , - 14.5770 , ... , - 6.3246 , - 4.1784 , - 4.6072 ], ... , [ - 14.6938 , - 16.8133 , - 15.1296 , ... , - 9.2327 , - 8.1931 , - 15.2430 ], [ - 10.8649 , - 11.4887 , - 11.5731 , ... , - 6.5378 , - 0.8715 , - 5.3870 ], [ - 11.8495 , - 11.8358 , - 12.0314 , ... , - 8.4242 , - 6.2741 , - 8.2787 ]]], grad_fn =< AddBackward0 > ), hidden_states = None , attentions = None ) output . logits ---> torch . Size ([ 1 , 12 , 21128 ]) \u6253\u5370\u6982\u7387\u6700\u9ad8\u7684\u5b57 : [ '\u5979' ]","title":"4.3 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#44","text":"\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u53c8\u79f0\u4e3a\u201c\u62bd\u53d6\u5f0f\u95ee\u7b54\u4efb\u52a1\u201d\uff0c\u5373\u8f93\u5165\u4e00\u6bb5\u6587\u672c\u548c\u4e00\u4e2a\u95ee\u9898\uff0c\u8ba9\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u3002 # \u9605\u8bfb\u7406\u89e3\u4efb\u52a1(\u62bd\u53d6\u5f0f\u95ee\u7b54) def dm04_test_question_answering (): # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( './chinese_pretrain_mrc_roberta_wwm_ext_large' ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForQuestionAnswering . from_pretrained ( './chinese_pretrain_mrc_roberta_wwm_ext_large' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf # \u6587\u5b57\u4e2d\u7684\u6807\u70b9\u7b26\u53f7\u5982\u679c\u662f\u4e2d\u6587\u7684\u8bdd\uff0c\u4f1a\u5f71\u54cd\u5230\u9884\u6d4b\u7ed3\u679c \u4e5f\u53ef\u4ee5\u53bb\u6389\u6807\u70b9\u7b26\u53f7 context = '\u6211\u53eb\u5f20\u4e09 \u6211\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5458 \u6211\u7684\u559c\u597d\u662f\u6253\u7bee\u7403' questions = [ '\u6211\u662f\u8c01\uff1f' , '\u6211\u662f\u505a\u4ec0\u4e48\u7684\uff1f' , '\u6211\u7684\u7231\u597d\u662f\u4ec0\u4e48\uff1f' ] # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6a21\u578b\u505a\u62bd\u53d6\u5f0f\u95ee\u7b54 my_model . eval () for question in questions : input = my_tokenizer . encode_plus ( question , context , return_tensors = 'pt' ) print ( 'input--->' , input ) output = my_model ( ** input ) print ( 'output--->' , output ) start , end = torch . argmax ( output . start_logits ), torch . argmax ( output . end_logits ) + 1 answer = my_tokenizer . convert_ids_to_tokens ( input [ 'input_ids' ][ 0 ][ start : end ] ) print ( 'question:' , question , 'answer:' , answer ) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c\uff1a # input_ids\u8868\u793atext2id\u540e\u7ed3\u679c # token_type_ids\u8868\u793a\u53e5\u5b50\u5206\u6bb5\u4fe1\u606f # attention_mask\u8868\u793a\u53e5\u5b50attention\u63a9\u7801\u4fe1\u606f input ---> { 'input_ids' : tensor ([[ 101 , 2769 , 3221 , 6443 , 8043 , 102 , 2769 , 1373 , 2476 , 676 , 2769 , 3221 , 671 , 702 , 4923 , 2415 , 1447 , 2769 , 4638 , 1599 , 1962 , 3221 , 2802 , 5074 , 4413 , 102 ]]), 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]]), 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]])} # start_logits end_logits\u5206\u5e03\u8868\u793a\u4ece\u539f\u6587\u4e2d\u62bd\u53d6\u7b54\u6848\u7684\u4f4d\u7f6e\u6982\u7387 # \u6bd4\u5982\uff1astart_logits\u7684\u6700\u5927\u503c\u4ee3\u8868\u53e5\u5b50\u7b54\u6848\u6700\u53ef\u80fd\u5f00\u59cb\u7684\u4f4d\u7f6e # \u6bd4\u5982\uff1aend_logits\u7684\u6700\u5927\u503c\u4ee3\u8868\u53e5\u5b50\u7b54\u6848\u53ef\u80fd\u7ed3\u675f\u7684\u4f4d\u7f6e output ---> QuestionAnsweringModelOutput ( loss = None , start_logits = tensor ([[ - 1.9978 , - 11.4788 , - 12.6324 , - 11.8324 , - 12.4148 , - 11.9371 , - 2.7246 , - 6.6402 , 3.9131 , - 2.9533 , - 7.0866 , - 9.5696 , - 4.2775 , - 8.9042 , 0.5753 , - 6.9468 , - 7.0469 , - 8.5334 , - 11.3796 , - 9.3905 , - 11.0242 , - 11.1047 , - 5.7124 , - 2.7293 , - 7.5896 , - 12.6013 ]], grad_fn =< CopyBackwards > ), end_logits = tensor ([[ - 1.3483 , - 12.0141 , - 11.6312 , - 11.6629 , - 11.9607 , - 12.0039 , - 4.6118 , - 7.4034 , - 2.3499 , 4.7159 , - 7.2880 , - 9.5317 , - 6.6742 , - 6.0915 , - 7.0023 , - 4.9691 , 1.4515 , - 7.8329 , - 9.0895 , - 10.3742 , - 8.7482 , - 9.8567 , - 7.2930 , - 5.8163 , - 1.7323 , - 12.2525 ]], grad_fn =< CopyBackwards > ), hidden_states = None , attentions = None ) question : \u6211\u662f\u8c01 \uff1f answer : [ '\u5f20' , '\u4e09' ] question : \u6211\u662f\u505a\u4ec0\u4e48\u7684 \uff1f answer : [ '\u7a0b' , '\u5e8f' , '\u5458' ] question : \u6211\u7684\u7231\u597d\u662f\u4ec0\u4e48 \uff1f answer : [ '\u6253' , '\u7bee' , '\u7403' ]","title":"4.4 \u9605\u8bfb\u7406\u89e3\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#45","text":"\u6458\u8981\u751f\u6210\u4efb\u52a1\u7684\u8f93\u5165\u4e00\u4e00\u6bb5\u6587\u672c\uff0c\u8f93\u51fa\u662f\u4e00\u6bb5\u6982\u51b5\u3001\u7b80\u5355\u7684\u6587\u5b57\u3002 # \u6587\u672c\u6458\u8981\u4efb\u52a1 def dm05_test_summarization (): text = \"BERT is a transformers model pretrained on a large corpus of English data \" \\ \"in a self-supervised fashion. This means it was pretrained on the raw texts \" \\ \"only, with no humans labelling them in any way (which is why it can use lots \" \\ \"of publicly available data) with an automatic process to generate inputs and \" \\ \"labels from those texts. More precisely, it was pretrained with two objectives:Masked \" \\ \"language modeling (MLM): taking a sentence, the model randomly masks 15 % o f the \" \\ \"words in the input then run the entire masked sentence through the model and has \" \\ \"to predict the masked words. This is different from traditional recurrent neural \" \\ \"networks (RNNs) that usually see the words one after the other, or from autoregressive \" \\ \"models like GPT which internally mask the future tokens. It allows the model to learn \" \\ \"a bidirectional representation of the sentence.Next sentence prediction (NSP): the models\" \\ \" concatenates two masked sentences as inputs during pretraining. Sometimes they correspond to \" \\ \"sentences that were next to each other in the original text, sometimes not. The model then \" \\ \"has to predict if the two sentences were following each other or not.\" # 1 \u52a0\u8f7dtokenizer my_tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = \"distilbart-cnn-12-6\" ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = AutoModelForSeq2SeqLM . from_pretrained ( pretrained_model_name_or_path = 'distilbart-cnn-12-6' ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf input = my_tokenizer ([ text ], return_tensors = 'pt' ) # print('input--->', input) # 4 \u9001\u7ed9\u6a21\u578b\u505a\u6458\u8981 my_model . eval () output = my_model . generate ( input . input_ids ) print ( 'output--->' , output ) # 5 \u5904\u7406\u6458\u8981\u7ed3\u679c # 5-1 decode \u7684 skip_special_tokens \u53c2\u6570\u53ef\u4ee5\u53bb\u9664 token \u524d\u9762\u7684\u7279\u6b8a\u5b57\u7b26 print ([ my_tokenizer . decode ( g , skip_special_tokens = True , clean_up_tokenization_spaces = False ) for g in output ]) # 5-2 convert_ids_to_tokens \u51fd\u6570\u53ea\u80fd\u5c06 ids \u8fd8\u539f\u4e3a token # print(my_tokenizer.convert_ids_to_tokens(output[0])) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c\uff1a output ---> tensor ([[ 2 , 0 , 11126 , 565 , 16 , 10 , 7891 , 268 , 1421 , 11857 , 26492 , 15 , 10 , 739 , 42168 , 9 , 2370 , 414 , 11 , 10 , 1403 , 12 , 16101 , 25376 , 2734 , 479 , 85 , 21 , 11857 , 26492 , 19 , 80 , 10366 , 35 , 31755 , 196 , 2777 , 19039 , 36 , 10537 , 448 , 43 , 8 , 220 , 3645 , 16782 , 36 , 487 , 4186 , 43 , 20 , 3092 , 10146 , 26511 , 1626 , 80 , 24397 , 11305 , 25 , 16584 , 148 , 11857 , 32155 , 479 , 7411 , 51 , 20719 , 7 , 11305 , 14 , 58 , 220 , 7 , 349 , 97 , 11 , 5 , 1461 , 2788 , 6 , 2128 , 45 , 479 , 2 ]]) [ 'BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion . It was pretrained with two objectives: Masked language modeling (MLM) and next sentence prediction (NSP) The models concatenates two masked sentences as inputs during pretraining . Sometimes they correspond to sentences that were next to each other in the original text, sometimes not .' ]","title":"4.5 \u6587\u672c\u6458\u8981\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#46-ner","text":"\u5b9e\u4f53\u8bcd\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u662fNLP\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\u3002\u5b83\u7528\u4e8e\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u4eba\u540d\uff08PER\uff09\u3001\u5730\u540d\uff08LOC\uff09\u3001\u7ec4\u7ec7\uff08ORG\uff09\u4ee5\u53ca\u5176\u4ed6\u5b9e\u4f53\uff08MISC\uff09\u7b49\u3002\u4f8b\u5982\uff1a(\u738b B-PER) (\u5c0f I-PER) (\u660e I-PER) (\u5728 O) (\u529e B-LOC) (\u516c I-LOC) (\u5ba4 I-LOC)\u3002\u5176\u4e2dO\u8868\u793a\u4e00\u4e2a\u975e\u5b9e\u4f53\uff0cB\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u7684\u5f00\u59cb\uff0cI\u8868\u793a\u4e00\u4e2a\u5b9e\u4f53\u5757\u7684\u5185\u90e8\u3002 \u5b9e\u4f53\u8bcd\u8bc6\u522b\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1\uff08\u53c8\u53eb\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\uff09\uff0c\u5b9e\u4f53\u8bcd\u8bc6\u522b\u662f\u53e5\u6cd5\u5206\u6790\u7684\u57fa\u7840\uff0c\u800c\u53e5\u6cd5\u5206\u6790\u4f18\u52bfNLP\u4efb\u52a1\u7684\u6838\u5fc3\u3002``` # NER\u4efb\u52a1 def dm06_test_ner (): # 1 \u52a0\u8f7dtokenizer \u52a0\u8f7d\u6a21\u578b \u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 # https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese my_tokenizer = AutoTokenizer . from_pretrained ( 'roberta-base-finetuned-cluener2020-chinese' ) my_model = AutoModelForTokenClassification . from_pretrained ( 'roberta-base-finetuned-cluener2020-chinese' ) config = AutoConfig . from_pretrained ( 'roberta-base-finetuned-cluener2020-chinese' ) # 2 \u6570\u636e\u5f20\u91cf\u5316 inputs = my_tokenizer . encode_plus ( '\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\uff0c\u5929\u5b89\u95e8\u4e0a\u592a\u9633\u5347' , return_tensors = 'pt' ) print ( 'inputs--->' , inputs . input_ids . shape , inputs . input_ids ) # torch.Size([1, 17]) # 3 \u9001\u5165\u6a21\u578b \u9884\u6d4bner\u6982\u7387 \u6bcf\u4e2a\u5b57\u9884\u6d4b\u7684\u6807\u7b7e\u6982\u7387 my_model . eval () logits = my_model ( inputs . input_ids ) . logits print ( 'logits--->' , logits . shape ) # torch.Size([1, 17, 32]) # 4 \u5bf9\u9884\u6d4b\u6570\u636e \u8fdb\u884c\u663e\u793a input_tokens = my_tokenizer . convert_ids_to_tokens ( inputs . input_ids [ 0 ]) print ( 'input_tokens--->' , input_tokens ) outputs = [] for token , value in zip ( input_tokens , logits [ 0 ]): if token in my_tokenizer . all_special_tokens : continue # \u83b7\u5f97\u6bcf\u4e2a\u5b57\u9884\u6d4b\u6982\u7387\u6700\u5927\u7684\u6807\u7b7e\u7d22\u5f15 idx = torch . argmax ( value ) . item () # \u6253\u5370\u7d22\u5f15\u5bf9\u5e94\u6807\u7b7e outputs . append (( token , config . id2label [ idx ])) print ( outputs ) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c inputs ---> torch . Size ([ 1 , 17 ]) tensor ([[ 101 , 2769 , 4263 , 1266 , 776 , 1921 , 2128 , 7305 , 8024 , 1921 , 2128 , 7305 , 677 , 1922 , 7345 , 1285 , 102 ]]) logits ---> torch . Size ([ 1 , 17 , 32 ]) input_tokens ---> [ '[CLS]' , '\u6211' , '\u7231' , '\u5317' , '\u4eac' , '\u5929' , '\u5b89' , '\u95e8' , '\uff0c' , '\u5929' , '\u5b89' , '\u95e8' , '\u4e0a' , '\u592a' , '\u9633' , '\u5347' , '[SEP]' ] [( '\u6211' , 'O' ), ( '\u7231' , 'O' ), ( '\u5317' , 'B-address' ), ( '\u4eac' , 'I-address' ), ( '\u5929' , 'I-address' ), ( '\u5b89' , 'I-address' ), ( '\u95e8' , 'I-address' ), ( '\uff0c' , 'O' ), ( '\u5929' , 'B-address' ), ( '\u5b89' , 'I-address' ), ( '\u95e8' , 'I-address' ), ( '\u4e0a' , 'O' ), ( '\u592a' , 'O' ), ( '\u9633' , 'O' ), ( '\u5347' , 'O' )]","title":"4.6 NER\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#5-nlp","text":"","title":"5 \u5177\u4f53\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210NLP\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#51","text":"\u5b8c\u578b\u586b\u7a7a\u4efb\u52a1\u53c8\u88ab\u53eb\u505a\u201c\u906e\u853d\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u201d\uff0c\u5b83\u5c5e\u4e8eBERT\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b50\u4efb\u52a1\u3002\u4e0b\u9762\u5b8c\u6210\u4e00\u4e2a\u4e2d\u6587\u573a\u666f\u7684\u5b8c\u578b\u586b\u7a7a\u3002 # \u5177\u4f53\u6a21\u578b\u5b8c\u578b\u586b\u7a7a\u4efb\u52a1 def dm01_test_bert_fill_mask (): # 1 \u52a0\u8f7dtokenizer modename = \"bert-base-chinese\" my_tokenizer = BertTokenizer . from_pretrained ( modename ) # 2 \u52a0\u8f7d\u6a21\u578b my_model = BertForMaskedLM . from_pretrained ( modename ) # 3 \u6587\u672c\u8f6c\u5f20\u91cf input = my_tokenizer . encode_plus ( '\u6211\u60f3\u660e\u5929\u53bb[MASK]\u5bb6\u5403\u996d' , return_tensors = 'pt' ) print ( 'input--->' , input ) # 4 \u7ed9\u6a21\u578b\u9001\u6570\u636e\u63d0\u53d6\u7279\u5f81 output = my_model ( ** input ) print ( 'output--->' , output ) print ( 'output.logits--->' , output . logits . shape ) # [1,11,21128] # 5 \u53d6\u6982\u7387\u6700\u9ad8 mask_pred_idx = torch . argmax ( output . logits [ 0 ][ 6 ]) . item () print ( '\u6253\u5370\u6982\u7387\u6700\u9ad8\u7684\u5b57:' , my_tokenizer . convert_ids_to_tokens ([ mask_pred_idx ])) \u7a0b\u5e8f\u8fd0\u884c\u7ed3\u679c # input_ids\u8868\u793atext2id\u540e\u7ed3\u679c # token_type_ids\u8868\u793a\u53e5\u5b50\u5206\u6bb5\u4fe1\u606f # attention_mask\u8868\u793a\u53e5\u5b50attention\u63a9\u7801\u4fe1\u606f input ---> { 'input_ids' : tensor ([[ 101 , 2769 , 2682 , 3209 , 1921 , 1343 , 103 , 2157 , 1391 , 7649 , 102 ]]), 'token_type_ids' : tensor ([[ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]]), 'attention_mask' : tensor ([[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]])} output ---> MaskedLMOutput ( loss = None , logits = tensor ([[[ - 8.1771 , - 8.1008 , - 8.1191 , ... , - 6.8355 , - 6.9482 , - 6.9834 ], [ - 8.2775 , - 8.1251 , - 8.1655 , ... , - 6.8471 , - 7.4265 , - 6.1365 ], [ - 14.1093 , - 13.1037 , - 14.6324 , ... , - 6.0959 , - 3.7550 , - 5.7456 ], ... , [ - 16.2103 , - 16.7243 , - 15.9876 , ... , - 5.9727 , - 8.2757 , - 7.3852 ], [ - 13.5615 , - 13.7670 , - 13.4497 , ... , - 7.8282 , - 4.9095 , - 9.1699 ], [ - 10.3200 , - 10.1068 , - 10.4439 , ... , - 6.6468 , - 7.0597 , - 7.5027 ]]], grad_fn =< AddBackward0 > ), hidden_states = None , attentions = None ) output . logits ---> torch . Size ([ 1 , 11 , 21128 ])","title":"5.1 \u5b8c\u578b\u586b\u7a7a\u4efb\u52a1"},{"location":"05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html#6","text":"\u4e86\u89e3Huggingface Transformers Huggingface Transformers \u662f\u57fa\u4e8e\u4e00\u4e2a\u5f00\u6e90\u57fa\u4e8e transformer \u6a21\u578b\u7ed3\u6784\u63d0\u4f9b\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u3002\u6846\u67b6\u652f\u6301\u4e86\u6700\u65b0\u7684\u5404\u79cdNLP\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u8005\u53ef\u5feb\u901f\u7684\u8fdb\u884c\u6a21\u578b\u8c03\u7528\uff0c\u5e76\u4e14\u652f\u6301\u6a21\u578bfurther pretraining \u548c \u4e0b\u6e38\u4efb\u52a1fine-tuning \u4f7f\u7528\u7ba1\u9053\u65b9\u5f0f\u5b8c\u6210\u591a\u79cdNLP\u4efb\u52a1 \u6587\u672c\u5206\u7c7b\u4efb\u52a1sentiment-analysis\u3001\u7279\u5f81\u63d0\u53d6\u4efb\u52a1feature-extraction\u3001\u5b8c\u578b\u586b\u7a7a\u4efb\u52a1fill-mask\u3001\u9605\u8bfb\u7406\u89e3\u4efb\u52a1question-answering\u3001\u6587\u672c\u6458\u8981\u4efb\u52a1summarization\u3001NER\u4efb\u52a1ner \u4f7f\u7528\u81ea\u52a8\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210NLP\u4efb\u52a1 \u5bf9Aoto\u7cfb\u5217\u6a21\u578b\u7c7b\u8fdb\u884c\u4f7f\u7528AutoModel\u3001AutoTokenizer\u3001AutoModelForSequenceClassification\u3001AutoModelForMaskedLM\u3001AutoModelForQuestionAnswering\u3001AutoModelForSeq2SeqLM\u3001AutoModelForTokenClassification \u4f7f\u7528\u5177\u4f53\u6a21\u578b\u65b9\u5f0f\u5b8c\u6210NLP\u4efb\u52a1 \u4f7f\u7528\u5177\u4f53\u6a21\u578b\u6bd4\u5982BertTokenizer\u7b49\u8fdb\u884c\u5177\u4f53NLP\u4efb\u52a1\u5f00\u53d1","title":"6 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5de5\u5177. \u638c\u63e1\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8fc7\u7a0b. 1 \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5de5\u5177 \u00b6 \u5728\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528Transformers\u5de5\u5177\u5305\u8fdb\u884c\u6a21\u578b\u7684\u52a0\u8f7d\u548c\u4f7f\u7528. \u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u7531\u4e16\u754c\u5148\u8fdb\u7684NLP\u7814\u53d1\u56e2\u961fhuggingface\u63d0\u4f9b. \u6ce8\u610f: \u4e0b\u9762\u4f7f\u7528\u7684\u4ee3\u7801\u9700\u8981\u56fd\u5916\u670d\u52a1\u5668\u7684\u8d44\u6e90, \u5728\u56fd\u5185\u4f7f\u7528\u7684\u65f6\u5019, \u56fd\u5185\u7684\u7f51\u7ad9\u4e0b\u8f7d\u53ef\u80fd\u4f1a\u51fa\u73b0\u5728\u539f\u5730\u5361\u6b7b\u4e0d\u52a8, \u6216\u662f\u7f51\u7edc\u8fde\u63a5\u8d85\u65f6\u7b49\u4e00\u4e9b\u7f51\u7edc\u62a5\u9519, \u5747\u662f\u7f51\u7edc\u95ee\u9898, \u4e0d\u662f\u4ee3\u7801\u95ee\u9898, \u8fd9\u4e2a\u53ef\u4ee5\u5148\u884c\u8df3\u8fc7, \u628a\u4e3b\u8981\u903b\u8f91\u68b3\u7406\u5b8c\u6210\u5373\u53ef 2 \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65: \u786e\u5b9a\u9700\u8981\u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u5b89\u88c5\u4f9d\u8d56\u5305. \u7b2c\u4e8c\u6b65: \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6620\u5c04\u5668tokenizer. \u7b2c\u4e09\u6b65: \u52a0\u8f7d\u5e26/\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa\u7ed3\u679c. 2.1 \u786e\u5b9a\u9700\u8981\u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u5b89\u88c5\u4f9d\u8d56\u5305 \u00b6 \u80fd\u591f\u52a0\u8f7d\u54ea\u4e9b\u6a21\u578b\u53ef\u4ee5\u53c2\u8003\u524d\u4e00\u5c0f\u7ed3\u4e2d\u7684\u5e38\u7528\u9884\u8bad\u7ec3\u6a21\u578b \u8fd9\u91cc\u5047\u8bbe\u6211\u4eec\u5904\u7406\u7684\u662f\u4e2d\u6587\u6587\u672c\u4efb\u52a1, \u9700\u8981\u52a0\u8f7d\u7684\u6a21\u578b\u662fBERT\u7684\u4e2d\u6587\u6a21\u578b: bert-base-chinese \u5728\u4f7f\u7528\u5de5\u5177\u52a0\u8f7d\u6a21\u578b\u524d\u9700\u8981\u5b89\u88c5\u5fc5\u5907\u7684\u4f9d\u8d56\u5305: pip install tqdm boto3 requests regex sentencepiece sacremoses 2.2 \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6620\u5c04\u5668tokenizer \u00b6 import torch from transformers import AutoTokenizer , AutoModel , AutoModelForMaskedLM , AutoModelForSequenceClassification , AutoModelForQuestionAnswering mirror = 'https://mirrors.tuna.tsinghua.edu.cn/help/hugging-face-models/' def demo24_1_load_tokenizer (): tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = 'https://mirrors.tuna.tsinghua.edu.cn/help/hugging-face-models/' ) print ( \"tokenizer--->\" , tokenizer ) demo24_1_load_tokenizer () 2.3 \u52a0\u8f7d\u5e26/\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b \u00b6 \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5e26\u5934\u6216\u8005\u4e0d\u5e26\u5934\u7684\u6a21\u578b \u8fd9\u91cc\u7684'\u5934'\u662f\u6307\u6a21\u578b\u7684\u4efb\u52a1\u8f93\u51fa\u5c42, \u9009\u62e9\u52a0\u8f7d\u4e0d\u5e26\u5934\u7684\u6a21\u578b, \u76f8\u5f53\u4e8e\u4f7f\u7528\u6a21\u578b\u5bf9\u8f93\u5165\u6587\u672c\u8fdb\u884c\u7279\u5f81\u8868\u793a. \u9009\u62e9\u52a0\u8f7d\u5e26\u5934\u7684\u6a21\u578b\u65f6, \u6709\u4e09\u79cd\u7c7b\u578b\u7684'\u5934'\u53ef\u4f9b\u9009\u62e9, AutoModelForMaskedLM (\u8bed\u8a00\u6a21\u578b\u5934), AutoModelForSequenceClassification (\u5206\u7c7b\u6a21\u578b\u5934), AutoModelForQuestionAnswering (\u95ee\u7b54\u6a21\u578b\u5934) \u4e0d\u540c\u7c7b\u578b\u7684'\u5934', \u53ef\u4ee5\u4f7f\u9884\u8bad\u7ec3\u6a21\u578b\u8f93\u51fa\u6307\u5b9a\u7684\u5f20\u91cf\u7ef4\u5ea6. \u5982\u4f7f\u7528'\u5206\u7c7b\u6a21\u578b\u5934', \u5219\u8f93\u51fa\u5c3a\u5bf8\u4e3a(1,2)\u7684\u5f20\u91cf, \u7528\u4e8e\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u5224\u5b9a\u7ed3\u679c. # \u52a0\u8f7d\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b def demo24_2_load_model (): # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' print ( '\u52a0\u8f7d\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) model = AutoModel . from_pretrained ( model_name ) print ( 'model--->' , model ) # \u52a0\u8f7d\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b print ( '\u52a0\u8f7d\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) lm_model = AutoModelForMaskedLM . from_pretrained ( model_name ) print ( 'lm_model--->' , lm_model ) # \u52a0\u8f7d\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b print ( '\u52a0\u8f7d\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) classification_model = AutoModelForSequenceClassification . from_pretrained ( model_name ) print ( 'classification_model--->' , classification_model ) # \u52a0\u8f7d\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b print ( '\u52a0\u8f7d\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) qa_model = AutoModelForQuestionAnswering . from_pretrained ( model_name ) print ( 'qa_model--->' , qa_model ) demo24_2_load_model () 2.4 \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa\u7ed3\u679c \u00b6 1 \u4f7f\u7528\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa \u00b6 def demo24_3_load_AutoModel (): # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel model = AutoModel . from_pretrained ( model_name ) # 3 \u4f7f\u7528tokenizer \u6587\u672c\u6570\u503c\u5316 # \u8f93\u5165\u7684\u4e2d\u6587\u6587\u672c input_text = \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" # \u4f7f\u7528tokenizer\u8fdb\u884c\u6570\u503c\u6620\u5c04 indexed_tokens = tokenizer . encode ( input_text ) # \u6253\u5370\u6620\u5c04\u540e\u7684\u7ed3\u6784 print ( \"indexed_tokens:\" , indexed_tokens ) # \u5c06\u6620\u5c04\u7ed3\u6784\u8f6c\u5316\u4e3a\u5f20\u91cf\u8f93\u9001\u7ed9\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b tokens_tensor = torch . tensor ([ indexed_tokens ]) # 4 \u4f7f\u7528\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): encoded_layers , _ = model ( tokens_tensor , return_dict = False ) # encoded_layers, _ = model(tokens_tensor) print ( \"\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , encoded_layers ) print ( \"\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , encoded_layers . shape ) demo24_3_load_AutoModel () \u8f93\u51fa\u6548\u679c: # tokenizer\u6620\u5c04\u540e\u7684\u7ed3\u679c, 101\u548c102\u662f\u8d77\u6b62\u7b26, # \u4e2d\u95f4\u7684\u6bcf\u4e2a\u6570\u5b57\u5bf9\u5e94\"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\"\u7684\u6bcf\u4e2a\u5b57. indexed_tokens : [ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ] \u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c : tensor ([[[ 0.5421 , 0.4526 , - 0.0179 , ... , 1.0447 , - 0.1140 , 0.0068 ], [ - 0.1343 , 0.2785 , 0.1602 , ... , - 0.0345 , - 0.1646 , - 0.2186 ], [ 0.9960 , - 0.5121 , - 0.6229 , ... , 1.4173 , 0.5533 , - 0.2681 ], ... , [ 0.0115 , 0.2150 , - 0.0163 , ... , 0.6445 , 0.2452 , - 0.3749 ], [ 0.8649 , 0.4337 , - 0.1867 , ... , 0.7397 , - 0.2636 , 0.2144 ], [ - 0.6207 , 0.1668 , 0.1561 , ... , 1.1218 , - 0.0985 , - 0.0937 ]]]) # \u8f93\u51fa\u5c3a\u5bf8\u4e3a1x9x768, \u5373\u6bcf\u4e2a\u5b57\u5df2\u7ecf\u4f7f\u7528768\u7ef4\u7684\u5411\u91cf\u8fdb\u884c\u4e86\u8868\u793a, # \u6211\u4eec\u53ef\u4ee5\u57fa\u4e8e\u6b64\u7f16\u7801\u7ed3\u679c\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c, \u5982: \u7f16\u5199\u81ea\u5df1\u7684\u5fae\u8c03\u7f51\u7edc\u8fdb\u884c\u6700\u7ec8\u8f93\u51fa. \u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8 : torch . Size ([ 1 , 9 , 768 ]) 2 \u4f7f\u7528\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa \u00b6 def demo24_4_load_AutoLM (): # 1 \u52a0\u8f7d tokenizer # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel lm_model = AutoModelForMaskedLM . from_pretrained ( model_name ) # 3 \u4f7f\u7528tokenizer \u6587\u672c\u6570\u503c\u5316 # \u8f93\u5165\u7684\u4e2d\u6587\u6587\u672c input_text = \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" # \u4f7f\u7528tokenizer\u8fdb\u884c\u6570\u503c\u6620\u5c04 indexed_tokens = tokenizer . encode ( input_text ) # \u6253\u5370\u6620\u5c04\u540e\u7684\u7ed3\u6784 print ( \"indexed_tokens:\" , indexed_tokens ) # \u5c06\u6620\u5c04\u7ed3\u6784\u8f6c\u5316\u4e3a\u5f20\u91cf\u8f93\u9001\u7ed9\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b tokens_tensor = torch . tensor ([ indexed_tokens ]) # \u4f7f\u7528\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): lm_output = lm_model ( tokens_tensor , return_dict = False ) print ( \"\u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , lm_output ) print ( \"\u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , lm_output [ 0 ] . shape ) demo24_4_load_AutoLM () \u8f93\u51fa\u6548\u679c: \u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c: (tensor([[[ -7.9706, -7.9119, -7.9317, ..., -7.2174, -7.0263, -7.3746], [ -8.2097, -8.1810, -8.0645, ..., -7.2349, -6.9283, -6.9856], [-13.7458, -13.5978, -12.6076, ..., -7.6817, -9.5642, -11.9928], ..., [ -9.0928, -8.6857, -8.4648, ..., -8.2368, -7.5684, -10.2419], [ -8.9458, -8.5784, -8.6325, ..., -7.0547, -5.3288, -7.8077], [ -8.4154, -8.5217, -8.5379, ..., -6.7102, -5.9782, -7.6909]]]),) # \u8f93\u51fa\u5c3a\u5bf8\u4e3a1x9x21128, \u5373\u6bcf\u4e2a\u5b57\u5df2\u7ecf\u4f7f\u752821128\u7ef4\u7684\u5411\u91cf\u8fdb\u884c\u4e86\u8868\u793a, # \u540c\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u4e00\u6837, \u6211\u4eec\u53ef\u4ee5\u57fa\u4e8e\u6b64\u7f16\u7801\u7ed3\u679c\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c, \u5982: \u7f16\u5199\u81ea\u5df1\u7684\u5fae\u8c03\u7f51\u7edc\u8fdb\u884c\u6700\u7ec8\u8f93\u51fa. \u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8: torch.Size([1, 9, 21128]) 3 \u4f7f\u7528\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa \u00b6 def demo24_5_load_AutoSeqC (): # 1 \u52a0\u8f7d tokenizer # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel classification_model = AutoModelForSequenceClassification . from_pretrained ( model_name ) # 3 \u4f7f\u7528tokenizer \u6587\u672c\u6570\u503c\u5316 # \u8f93\u5165\u7684\u4e2d\u6587\u6587\u672c input_text = \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" # \u4f7f\u7528tokenizer\u8fdb\u884c\u6570\u503c\u6620\u5c04 indexed_tokens = tokenizer . encode ( input_text ) # \u6253\u5370\u6620\u5c04\u540e\u7684\u7ed3\u6784 print ( \"indexed_tokens:\" , indexed_tokens ) # \u5c06\u6620\u5c04\u7ed3\u6784\u8f6c\u5316\u4e3a\u5f20\u91cf\u8f93\u9001\u7ed9\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b tokens_tensor = torch . tensor ([ indexed_tokens ]) # \u4f7f\u7528\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): classification_output = classification_model ( tokens_tensor ) print ( \"\u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , classification_output ) print ( \"\u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , classification_output [ 0 ] . shape ) demo24_5_load_AutoSeqC () \u8f93\u51fa\u6548\u679c: \u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c: (tensor([[-0.0649, -0.1593]]),) # \u8f93\u51fa\u5c3a\u5bf8\u4e3a1x2, \u53ef\u76f4\u63a5\u7528\u4e8e\u6587\u672c\u4e8c\u5206\u95ee\u9898\u7684\u8f93\u51fa \u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8: torch.Size([1, 2]) 4 \u4f7f\u7528\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa \u00b6 def demo24_6_load_AutoQA (): # 1 \u52a0\u8f7d tokenizer # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel qa_model = AutoModelForQuestionAnswering . from_pretrained ( model_name ) # 3 \u4f7f\u7528 # \u4f7f\u7528\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa\u65f6, \u9700\u8981\u4f7f\u8f93\u5165\u7684\u5f62\u5f0f\u4e3a\u53e5\u5b50\u5bf9 # \u7b2c\u4e00\u6761\u53e5\u5b50\u662f\u5bf9\u5ba2\u89c2\u4e8b\u7269\u7684\u9648\u8ff0 # \u7b2c\u4e8c\u6761\u53e5\u5b50\u662f\u9488\u5bf9\u7b2c\u4e00\u6761\u53e5\u5b50\u63d0\u51fa\u7684\u95ee\u9898 # \u95ee\u7b54\u6a21\u578b\u6700\u7ec8\u5c06\u5f97\u5230\u4e24\u4e2a\u5f20\u91cf, # \u6bcf\u4e2a\u5f20\u91cf\u4e2d\u6700\u5927\u503c\u5bf9\u5e94\u7d22\u5f15\u7684\u5206\u522b\u4ee3\u8868\u7b54\u6848\u7684\u5728\u6587\u672c\u4e2d\u7684\u8d77\u59cb\u4f4d\u7f6e\u548c\u7ec8\u6b62\u4f4d\u7f6e input_text1 = \"\u6211\u5bb6\u7684\u5c0f\u72d7\u662f\u9ed1\u8272\u7684\" input_text2 = \"\u6211\u5bb6\u7684\u5c0f\u72d7\u662f\u4ec0\u4e48\u989c\u8272\u7684\u5462?\" # \u6620\u5c04\u4e24\u4e2a\u53e5\u5b50 indexed_tokens = tokenizer . encode ( input_text1 , input_text2 ) print ( \"\u53e5\u5b50\u5bf9\u7684indexed_tokens:\" , indexed_tokens ) # \u8f93\u51fa\u7ed3\u679c: [101, 2769, 2157, 4638, 2207, 4318, 3221, 7946, 5682, 4638, 102, 2769, 2157, 4638, 2207, 4318, 3221, 784, 720, 7582, 5682, 4638, 1450, 136, 102] # # \u75280\uff0c1\u6765\u533a\u5206\u7b2c\u4e00\u6761\u548c\u7b2c\u4e8c\u6761\u53e5\u5b50 segments_ids = [ 0 ] * 11 + [ 1 ] * 14 # \u8f6c\u5316\u5f20\u91cf\u5f62\u5f0f segments_tensors = torch . tensor ([ segments_ids ]) tokens_tensor = torch . tensor ([ indexed_tokens ]) # \u4f7f\u7528\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): start_logits , end_logits = qa_model ( tokens_tensor , token_type_ids = segments_tensors , return_dict = False ) print ( \"\u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , ( start_logits , end_logits )) print ( \"\u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , ( start_logits . shape , end_logits . shape )) # (torch.Size([1, 25]), torch.Size([1, 25])) demo24_6_load_AutoQA () \u8f93\u51fa\u6548\u679c: \u53e5\u5b50\u5bf9\u7684indexed_tokens: [101, 2769, 2157, 4638, 2207, 4318, 3221, 7946, 5682, 4638, 102, 2769, 2157, 4638, 2207, 4318, 3221, 784, 720, 7582, 5682, 4638, 1450, 136, 102] \u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c: (tensor([[ 0.2574, -0.0293, -0.8337, -0.5135, -0.3645, -0.2216, -0.1625, -0.2768, -0.8368, -0.2581, 0.0131, -0.1736, -0.5908, -0.4104, -0.2155, -0.0307, -0.1639, -0.2691, -0.4640, -0.1696, -0.4943, -0.0976, -0.6693, 0.2426, 0.0131]]), tensor([[-0.3788, -0.2393, -0.5264, -0.4911, -0.7277, -0.5425, -0.6280, -0.9800, -0.6109, -0.2379, -0.0042, -0.2309, -0.4894, -0.5438, -0.6717, -0.5371, -0.1701, 0.0826, 0.1411, -0.1180, -0.4732, -0.1541, 0.2543, 0.2163, -0.0042]])) # \u8f93\u51fa\u4e3a\u4e24\u4e2a\u5f62\u72b61x25\u7684\u5f20\u91cf, \u4ed6\u4eec\u662f\u4e24\u6761\u53e5\u5b50\u5408\u5e76\u957f\u5ea6\u7684\u6982\u7387\u5206\u5e03, # \u7b2c\u4e00\u4e2a\u5f20\u91cf\u4e2d\u6700\u5927\u503c\u6240\u5728\u7684\u7d22\u5f15\u4ee3\u8868\u7b54\u6848\u51fa\u73b0\u7684\u8d77\u59cb\u7d22\u5f15, # \u7b2c\u4e8c\u4e2a\u5f20\u91cf\u4e2d\u6700\u5927\u503c\u6240\u5728\u7684\u7d22\u5f15\u4ee3\u8868\u7b54\u6848\u51fa\u73b0\u7684\u7ec8\u6b62\u7d22\u5f15. \u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8: (torch.Size([1, 25]), torch.Size([1, 25])) 3 \u5c0f\u7ed3 \u00b6 \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5de5\u5177: \u5728\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528transformers\u5de5\u5177\u8fdb\u884c\u6a21\u578b\u7684\u52a0\u8f7d\u548c\u4f7f\u7528. \u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u7531\u4e16\u754c\u5148\u8fdb\u7684NLP\u7814\u53d1\u56e2\u961fhuggingface\u63d0\u4f9b. \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u786e\u5b9a\u9700\u8981\u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u5b89\u88c5\u4f9d\u8d56\u5305. \u7b2c\u4e8c\u6b65: \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6620\u5c04\u5668tokenizer. \u7b2c\u4e09\u6b65: \u52a0\u8f7d\u5e26/\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa\u7ed3\u679c.","title":"8 \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b old"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#_1","text":"\u4e86\u89e3\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5de5\u5177. \u638c\u63e1\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#1","text":"\u5728\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528Transformers\u5de5\u5177\u5305\u8fdb\u884c\u6a21\u578b\u7684\u52a0\u8f7d\u548c\u4f7f\u7528. \u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u7531\u4e16\u754c\u5148\u8fdb\u7684NLP\u7814\u53d1\u56e2\u961fhuggingface\u63d0\u4f9b. \u6ce8\u610f: \u4e0b\u9762\u4f7f\u7528\u7684\u4ee3\u7801\u9700\u8981\u56fd\u5916\u670d\u52a1\u5668\u7684\u8d44\u6e90, \u5728\u56fd\u5185\u4f7f\u7528\u7684\u65f6\u5019, \u56fd\u5185\u7684\u7f51\u7ad9\u4e0b\u8f7d\u53ef\u80fd\u4f1a\u51fa\u73b0\u5728\u539f\u5730\u5361\u6b7b\u4e0d\u52a8, \u6216\u662f\u7f51\u7edc\u8fde\u63a5\u8d85\u65f6\u7b49\u4e00\u4e9b\u7f51\u7edc\u62a5\u9519, \u5747\u662f\u7f51\u7edc\u95ee\u9898, \u4e0d\u662f\u4ee3\u7801\u95ee\u9898, \u8fd9\u4e2a\u53ef\u4ee5\u5148\u884c\u8df3\u8fc7, \u628a\u4e3b\u8981\u903b\u8f91\u68b3\u7406\u5b8c\u6210\u5373\u53ef","title":"1 \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5de5\u5177"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#2","text":"\u7b2c\u4e00\u6b65: \u786e\u5b9a\u9700\u8981\u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u5b89\u88c5\u4f9d\u8d56\u5305. \u7b2c\u4e8c\u6b65: \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6620\u5c04\u5668tokenizer. \u7b2c\u4e09\u6b65: \u52a0\u8f7d\u5e26/\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa\u7ed3\u679c.","title":"2 \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6b65\u9aa4"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#21","text":"\u80fd\u591f\u52a0\u8f7d\u54ea\u4e9b\u6a21\u578b\u53ef\u4ee5\u53c2\u8003\u524d\u4e00\u5c0f\u7ed3\u4e2d\u7684\u5e38\u7528\u9884\u8bad\u7ec3\u6a21\u578b \u8fd9\u91cc\u5047\u8bbe\u6211\u4eec\u5904\u7406\u7684\u662f\u4e2d\u6587\u6587\u672c\u4efb\u52a1, \u9700\u8981\u52a0\u8f7d\u7684\u6a21\u578b\u662fBERT\u7684\u4e2d\u6587\u6a21\u578b: bert-base-chinese \u5728\u4f7f\u7528\u5de5\u5177\u52a0\u8f7d\u6a21\u578b\u524d\u9700\u8981\u5b89\u88c5\u5fc5\u5907\u7684\u4f9d\u8d56\u5305: pip install tqdm boto3 requests regex sentencepiece sacremoses","title":"2.1 \u786e\u5b9a\u9700\u8981\u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u5b89\u88c5\u4f9d\u8d56\u5305"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#22-tokenizer","text":"import torch from transformers import AutoTokenizer , AutoModel , AutoModelForMaskedLM , AutoModelForSequenceClassification , AutoModelForQuestionAnswering mirror = 'https://mirrors.tuna.tsinghua.edu.cn/help/hugging-face-models/' def demo24_1_load_tokenizer (): tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = 'https://mirrors.tuna.tsinghua.edu.cn/help/hugging-face-models/' ) print ( \"tokenizer--->\" , tokenizer ) demo24_1_load_tokenizer ()","title":"2.2 \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6620\u5c04\u5668tokenizer"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#23","text":"\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u5e26\u5934\u6216\u8005\u4e0d\u5e26\u5934\u7684\u6a21\u578b \u8fd9\u91cc\u7684'\u5934'\u662f\u6307\u6a21\u578b\u7684\u4efb\u52a1\u8f93\u51fa\u5c42, \u9009\u62e9\u52a0\u8f7d\u4e0d\u5e26\u5934\u7684\u6a21\u578b, \u76f8\u5f53\u4e8e\u4f7f\u7528\u6a21\u578b\u5bf9\u8f93\u5165\u6587\u672c\u8fdb\u884c\u7279\u5f81\u8868\u793a. \u9009\u62e9\u52a0\u8f7d\u5e26\u5934\u7684\u6a21\u578b\u65f6, \u6709\u4e09\u79cd\u7c7b\u578b\u7684'\u5934'\u53ef\u4f9b\u9009\u62e9, AutoModelForMaskedLM (\u8bed\u8a00\u6a21\u578b\u5934), AutoModelForSequenceClassification (\u5206\u7c7b\u6a21\u578b\u5934), AutoModelForQuestionAnswering (\u95ee\u7b54\u6a21\u578b\u5934) \u4e0d\u540c\u7c7b\u578b\u7684'\u5934', \u53ef\u4ee5\u4f7f\u9884\u8bad\u7ec3\u6a21\u578b\u8f93\u51fa\u6307\u5b9a\u7684\u5f20\u91cf\u7ef4\u5ea6. \u5982\u4f7f\u7528'\u5206\u7c7b\u6a21\u578b\u5934', \u5219\u8f93\u51fa\u5c3a\u5bf8\u4e3a(1,2)\u7684\u5f20\u91cf, \u7528\u4e8e\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u5224\u5b9a\u7ed3\u679c. # \u52a0\u8f7d\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b def demo24_2_load_model (): # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' print ( '\u52a0\u8f7d\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) model = AutoModel . from_pretrained ( model_name ) print ( 'model--->' , model ) # \u52a0\u8f7d\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b print ( '\u52a0\u8f7d\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) lm_model = AutoModelForMaskedLM . from_pretrained ( model_name ) print ( 'lm_model--->' , lm_model ) # \u52a0\u8f7d\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b print ( '\u52a0\u8f7d\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) classification_model = AutoModelForSequenceClassification . from_pretrained ( model_name ) print ( 'classification_model--->' , classification_model ) # \u52a0\u8f7d\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b print ( '\u52a0\u8f7d\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b' ) qa_model = AutoModelForQuestionAnswering . from_pretrained ( model_name ) print ( 'qa_model--->' , qa_model ) demo24_2_load_model ()","title":"2.3 \u52a0\u8f7d\u5e26/\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#24","text":"","title":"2.4 \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa\u7ed3\u679c"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#1_1","text":"def demo24_3_load_AutoModel (): # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel model = AutoModel . from_pretrained ( model_name ) # 3 \u4f7f\u7528tokenizer \u6587\u672c\u6570\u503c\u5316 # \u8f93\u5165\u7684\u4e2d\u6587\u6587\u672c input_text = \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" # \u4f7f\u7528tokenizer\u8fdb\u884c\u6570\u503c\u6620\u5c04 indexed_tokens = tokenizer . encode ( input_text ) # \u6253\u5370\u6620\u5c04\u540e\u7684\u7ed3\u6784 print ( \"indexed_tokens:\" , indexed_tokens ) # \u5c06\u6620\u5c04\u7ed3\u6784\u8f6c\u5316\u4e3a\u5f20\u91cf\u8f93\u9001\u7ed9\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b tokens_tensor = torch . tensor ([ indexed_tokens ]) # 4 \u4f7f\u7528\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): encoded_layers , _ = model ( tokens_tensor , return_dict = False ) # encoded_layers, _ = model(tokens_tensor) print ( \"\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , encoded_layers ) print ( \"\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , encoded_layers . shape ) demo24_3_load_AutoModel () \u8f93\u51fa\u6548\u679c: # tokenizer\u6620\u5c04\u540e\u7684\u7ed3\u679c, 101\u548c102\u662f\u8d77\u6b62\u7b26, # \u4e2d\u95f4\u7684\u6bcf\u4e2a\u6570\u5b57\u5bf9\u5e94\"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\"\u7684\u6bcf\u4e2a\u5b57. indexed_tokens : [ 101 , 782 , 4495 , 6421 , 1963 , 862 , 6629 , 1928 , 102 ] \u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c : tensor ([[[ 0.5421 , 0.4526 , - 0.0179 , ... , 1.0447 , - 0.1140 , 0.0068 ], [ - 0.1343 , 0.2785 , 0.1602 , ... , - 0.0345 , - 0.1646 , - 0.2186 ], [ 0.9960 , - 0.5121 , - 0.6229 , ... , 1.4173 , 0.5533 , - 0.2681 ], ... , [ 0.0115 , 0.2150 , - 0.0163 , ... , 0.6445 , 0.2452 , - 0.3749 ], [ 0.8649 , 0.4337 , - 0.1867 , ... , 0.7397 , - 0.2636 , 0.2144 ], [ - 0.6207 , 0.1668 , 0.1561 , ... , 1.1218 , - 0.0985 , - 0.0937 ]]]) # \u8f93\u51fa\u5c3a\u5bf8\u4e3a1x9x768, \u5373\u6bcf\u4e2a\u5b57\u5df2\u7ecf\u4f7f\u7528768\u7ef4\u7684\u5411\u91cf\u8fdb\u884c\u4e86\u8868\u793a, # \u6211\u4eec\u53ef\u4ee5\u57fa\u4e8e\u6b64\u7f16\u7801\u7ed3\u679c\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c, \u5982: \u7f16\u5199\u81ea\u5df1\u7684\u5fae\u8c03\u7f51\u7edc\u8fdb\u884c\u6700\u7ec8\u8f93\u51fa. \u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8 : torch . Size ([ 1 , 9 , 768 ])","title":"1 \u4f7f\u7528\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#2_1","text":"def demo24_4_load_AutoLM (): # 1 \u52a0\u8f7d tokenizer # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel lm_model = AutoModelForMaskedLM . from_pretrained ( model_name ) # 3 \u4f7f\u7528tokenizer \u6587\u672c\u6570\u503c\u5316 # \u8f93\u5165\u7684\u4e2d\u6587\u6587\u672c input_text = \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" # \u4f7f\u7528tokenizer\u8fdb\u884c\u6570\u503c\u6620\u5c04 indexed_tokens = tokenizer . encode ( input_text ) # \u6253\u5370\u6620\u5c04\u540e\u7684\u7ed3\u6784 print ( \"indexed_tokens:\" , indexed_tokens ) # \u5c06\u6620\u5c04\u7ed3\u6784\u8f6c\u5316\u4e3a\u5f20\u91cf\u8f93\u9001\u7ed9\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b tokens_tensor = torch . tensor ([ indexed_tokens ]) # \u4f7f\u7528\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): lm_output = lm_model ( tokens_tensor , return_dict = False ) print ( \"\u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , lm_output ) print ( \"\u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , lm_output [ 0 ] . shape ) demo24_4_load_AutoLM () \u8f93\u51fa\u6548\u679c: \u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c: (tensor([[[ -7.9706, -7.9119, -7.9317, ..., -7.2174, -7.0263, -7.3746], [ -8.2097, -8.1810, -8.0645, ..., -7.2349, -6.9283, -6.9856], [-13.7458, -13.5978, -12.6076, ..., -7.6817, -9.5642, -11.9928], ..., [ -9.0928, -8.6857, -8.4648, ..., -8.2368, -7.5684, -10.2419], [ -8.9458, -8.5784, -8.6325, ..., -7.0547, -5.3288, -7.8077], [ -8.4154, -8.5217, -8.5379, ..., -6.7102, -5.9782, -7.6909]]]),) # \u8f93\u51fa\u5c3a\u5bf8\u4e3a1x9x21128, \u5373\u6bcf\u4e2a\u5b57\u5df2\u7ecf\u4f7f\u752821128\u7ef4\u7684\u5411\u91cf\u8fdb\u884c\u4e86\u8868\u793a, # \u540c\u4e0d\u5e26\u5934\u7684\u6a21\u578b\u4e00\u6837, \u6211\u4eec\u53ef\u4ee5\u57fa\u4e8e\u6b64\u7f16\u7801\u7ed3\u679c\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c, \u5982: \u7f16\u5199\u81ea\u5df1\u7684\u5fae\u8c03\u7f51\u7edc\u8fdb\u884c\u6700\u7ec8\u8f93\u51fa. \u5e26\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8: torch.Size([1, 9, 21128])","title":"2 \u4f7f\u7528\u5e26\u6709\u8bed\u8a00\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#3","text":"def demo24_5_load_AutoSeqC (): # 1 \u52a0\u8f7d tokenizer # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel classification_model = AutoModelForSequenceClassification . from_pretrained ( model_name ) # 3 \u4f7f\u7528tokenizer \u6587\u672c\u6570\u503c\u5316 # \u8f93\u5165\u7684\u4e2d\u6587\u6587\u672c input_text = \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" # \u4f7f\u7528tokenizer\u8fdb\u884c\u6570\u503c\u6620\u5c04 indexed_tokens = tokenizer . encode ( input_text ) # \u6253\u5370\u6620\u5c04\u540e\u7684\u7ed3\u6784 print ( \"indexed_tokens:\" , indexed_tokens ) # \u5c06\u6620\u5c04\u7ed3\u6784\u8f6c\u5316\u4e3a\u5f20\u91cf\u8f93\u9001\u7ed9\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b tokens_tensor = torch . tensor ([ indexed_tokens ]) # \u4f7f\u7528\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): classification_output = classification_model ( tokens_tensor ) print ( \"\u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , classification_output ) print ( \"\u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , classification_output [ 0 ] . shape ) demo24_5_load_AutoSeqC () \u8f93\u51fa\u6548\u679c: \u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c: (tensor([[-0.0649, -0.1593]]),) # \u8f93\u51fa\u5c3a\u5bf8\u4e3a1x2, \u53ef\u76f4\u63a5\u7528\u4e8e\u6587\u672c\u4e8c\u5206\u95ee\u9898\u7684\u8f93\u51fa \u5e26\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8: torch.Size([1, 2])","title":"3 \u4f7f\u7528\u5e26\u6709\u5206\u7c7b\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#4","text":"def demo24_6_load_AutoQA (): # 1 \u52a0\u8f7d tokenizer # \u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u540d\u5b57 model_name = 'bert-base-chinese' tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-chinese\" , mirror = mirror ) # 2 \u52a0\u8f7dmodel qa_model = AutoModelForQuestionAnswering . from_pretrained ( model_name ) # 3 \u4f7f\u7528 # \u4f7f\u7528\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa\u65f6, \u9700\u8981\u4f7f\u8f93\u5165\u7684\u5f62\u5f0f\u4e3a\u53e5\u5b50\u5bf9 # \u7b2c\u4e00\u6761\u53e5\u5b50\u662f\u5bf9\u5ba2\u89c2\u4e8b\u7269\u7684\u9648\u8ff0 # \u7b2c\u4e8c\u6761\u53e5\u5b50\u662f\u9488\u5bf9\u7b2c\u4e00\u6761\u53e5\u5b50\u63d0\u51fa\u7684\u95ee\u9898 # \u95ee\u7b54\u6a21\u578b\u6700\u7ec8\u5c06\u5f97\u5230\u4e24\u4e2a\u5f20\u91cf, # \u6bcf\u4e2a\u5f20\u91cf\u4e2d\u6700\u5927\u503c\u5bf9\u5e94\u7d22\u5f15\u7684\u5206\u522b\u4ee3\u8868\u7b54\u6848\u7684\u5728\u6587\u672c\u4e2d\u7684\u8d77\u59cb\u4f4d\u7f6e\u548c\u7ec8\u6b62\u4f4d\u7f6e input_text1 = \"\u6211\u5bb6\u7684\u5c0f\u72d7\u662f\u9ed1\u8272\u7684\" input_text2 = \"\u6211\u5bb6\u7684\u5c0f\u72d7\u662f\u4ec0\u4e48\u989c\u8272\u7684\u5462?\" # \u6620\u5c04\u4e24\u4e2a\u53e5\u5b50 indexed_tokens = tokenizer . encode ( input_text1 , input_text2 ) print ( \"\u53e5\u5b50\u5bf9\u7684indexed_tokens:\" , indexed_tokens ) # \u8f93\u51fa\u7ed3\u679c: [101, 2769, 2157, 4638, 2207, 4318, 3221, 7946, 5682, 4638, 102, 2769, 2157, 4638, 2207, 4318, 3221, 784, 720, 7582, 5682, 4638, 1450, 136, 102] # # \u75280\uff0c1\u6765\u533a\u5206\u7b2c\u4e00\u6761\u548c\u7b2c\u4e8c\u6761\u53e5\u5b50 segments_ids = [ 0 ] * 11 + [ 1 ] * 14 # \u8f6c\u5316\u5f20\u91cf\u5f62\u5f0f segments_tensors = torch . tensor ([ segments_ids ]) tokens_tensor = torch . tensor ([ indexed_tokens ]) # \u4f7f\u7528\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u7ed3\u679c with torch . no_grad (): start_logits , end_logits = qa_model ( tokens_tensor , token_type_ids = segments_tensors , return_dict = False ) print ( \"\u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c:\" , ( start_logits , end_logits )) print ( \"\u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8:\" , ( start_logits . shape , end_logits . shape )) # (torch.Size([1, 25]), torch.Size([1, 25])) demo24_6_load_AutoQA () \u8f93\u51fa\u6548\u679c: \u53e5\u5b50\u5bf9\u7684indexed_tokens: [101, 2769, 2157, 4638, 2207, 4318, 3221, 7946, 5682, 4638, 102, 2769, 2157, 4638, 2207, 4318, 3221, 784, 720, 7582, 5682, 4638, 1450, 136, 102] \u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c: (tensor([[ 0.2574, -0.0293, -0.8337, -0.5135, -0.3645, -0.2216, -0.1625, -0.2768, -0.8368, -0.2581, 0.0131, -0.1736, -0.5908, -0.4104, -0.2155, -0.0307, -0.1639, -0.2691, -0.4640, -0.1696, -0.4943, -0.0976, -0.6693, 0.2426, 0.0131]]), tensor([[-0.3788, -0.2393, -0.5264, -0.4911, -0.7277, -0.5425, -0.6280, -0.9800, -0.6109, -0.2379, -0.0042, -0.2309, -0.4894, -0.5438, -0.6717, -0.5371, -0.1701, 0.0826, 0.1411, -0.1180, -0.4732, -0.1541, 0.2543, 0.2163, -0.0042]])) # \u8f93\u51fa\u4e3a\u4e24\u4e2a\u5f62\u72b61x25\u7684\u5f20\u91cf, \u4ed6\u4eec\u662f\u4e24\u6761\u53e5\u5b50\u5408\u5e76\u957f\u5ea6\u7684\u6982\u7387\u5206\u5e03, # \u7b2c\u4e00\u4e2a\u5f20\u91cf\u4e2d\u6700\u5927\u503c\u6240\u5728\u7684\u7d22\u5f15\u4ee3\u8868\u7b54\u6848\u51fa\u73b0\u7684\u8d77\u59cb\u7d22\u5f15, # \u7b2c\u4e8c\u4e2a\u5f20\u91cf\u4e2d\u6700\u5927\u503c\u6240\u5728\u7684\u7d22\u5f15\u4ee3\u8868\u7b54\u6848\u51fa\u73b0\u7684\u7ec8\u6b62\u7d22\u5f15. \u5e26\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u7684\u5c3a\u5bf8: (torch.Size([1, 25]), torch.Size([1, 25]))","title":"4 \u4f7f\u7528\u5e26\u6709\u95ee\u7b54\u6a21\u578b\u5934\u7684\u6a21\u578b\u8fdb\u884c\u8f93\u51fa"},{"location":"05_mkdocs_translearning/8%20%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-old.html#3_1","text":"\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5de5\u5177: \u5728\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528transformers\u5de5\u5177\u8fdb\u884c\u6a21\u578b\u7684\u52a0\u8f7d\u548c\u4f7f\u7528. \u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u7531\u4e16\u754c\u5148\u8fdb\u7684NLP\u7814\u53d1\u56e2\u961fhuggingface\u63d0\u4f9b. \u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u786e\u5b9a\u9700\u8981\u52a0\u8f7d\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u5b89\u88c5\u4f9d\u8d56\u5305. \u7b2c\u4e8c\u6b65: \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6620\u5c04\u5668tokenizer. \u7b2c\u4e09\u6b65: \u52a0\u8f7d\u5e26/\u4e0d\u5e26\u5934\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u7b2c\u56db\u6b65: \u4f7f\u7528\u6a21\u578b\u83b7\u5f97\u8f93\u51fa\u7ed3\u679c.","title":"3 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u5e76\u638c\u63e1\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u4f7f\u7528\u65b9\u6cd5. \u4e86\u89e3\u5e76\u638c\u63e1\u901a\u8fc7\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u540e\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u6cd5. \u638c\u63e1\u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b\u5b9e\u73b0\u8fc7\u7a0b. 1 \u5fae\u8c03\u811a\u672c\u4ecb\u7ecd \u00b6 \u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c: * huggingface\u7814\u7a76\u673a\u6784\u5411\u6211\u4eec\u63d0\u4f9b\u4e86\u9488\u5bf9GLUE\u6570\u636e\u96c6\u5408\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c, \u8fd9\u4e9b\u5fae\u8c03\u811a\u672c\u7684\u6838\u5fc3\u90fd\u662f\u5fae\u8c03\u6a21\u578b\u7684\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42. * \u901a\u8fc7\u7b80\u5355\u7684\u53c2\u6570\u914d\u7f6e\u6765\u6307\u5b9aGLUE\u4e2d\u5b58\u5728\u4efb\u52a1\u7c7b\u578b(\u5982: CoLA\u5bf9\u5e94\u6587\u672c\u4e8c\u5206\u7c7b, MRPC\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u4e8c\u5206\u7c7b, STS-B\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u591a\u5206\u7c7b), \u4ee5\u53ca\u6307\u5b9a\u9700\u8981\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u6a21\u578b. 2 \u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u4f7f\u7528\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u5fae\u8c03\u811a\u672c\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u914d\u7f6e\u5fae\u8c03\u811a\u672c\u53c2\u6570 \u7b2c\u4e09\u6b65: \u8fd0\u884c\u5e76\u68c0\u9a8c\u6548\u679c 2.1 \u4e0b\u8f7d\u5fae\u8c03\u811a\u672c\u6587\u4ef6 \u00b6 \u6ce8\u610f\uff1a\u865a\u62df\u673a\u4e2d\u5df2\u7ecf\u5b89\u88c5transformers\uff0c\u4ee5\u4e0b\u5b89\u88c5\u6b65\u9aa4\u4e0d\u9700\u518d\u6b21\u6267\u884c # \u514b\u9686huggingface\u7684transfomers\u6587\u4ef6 git clone https://github.com/huggingface/transformers.git # \u8fdb\u884ctransformers\u6587\u4ef6\u5939 cd transformers # \u5207\u6362transformers\u5230\u6307\u5b9a\u7248\u672c git checkout v4.17.0 # \u5b89\u88c5transformers\u5305 pip install . # \u8fdb\u5165\u5fae\u8c03\u811a\u672c\u6240\u5728\u8def\u5f84\u5e76\u67e5\u770b cd examples / pytorch / text - classification ls # \u5176\u4e2drun_glue.py\u5c31\u662f\u9488\u5bf9GLUE\u6570\u636e\u96c6\u5408\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c 2.2 \u914d\u7f6e\u5fae\u8c03\u811a\u672c\u53c2\u6570 \u00b6 \u5728run_glue.py\u540c\u7ea7\u76ee\u5f55\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 # \u5b9a\u4e49DATA_DIR: \u5fae\u8c03\u6570\u636e\u6240\u5728\u8def\u5f84, \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528glue_data\u4e2d\u7684\u6570\u636e\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e export DATA_DIR = \"/root/data/glue_data\" # \u5b9a\u4e49SAVE_DIR: \u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84, \u6211\u4eec\u5c06\u6a21\u578b\u4fdd\u5b58\u5728\u5f53\u524d\u76ee\u5f55\u7684bert_finetuning_test\u6587\u4ef6\u4e2d export SAVE_DIR = \"./bert_finetuning_test/\" # \u4f7f\u7528python\u8fd0\u884c\u5fae\u8c03\u811a\u672c # --model_name_or_path: \u9009\u62e9\u5177\u4f53\u7684\u6a21\u578b\u6216\u8005\u53d8\u4f53, \u8fd9\u91cc\u662f\u5728\u82f1\u6587\u8bed\u6599\u4e0a\u5fae\u8c03, \u56e0\u6b64\u9009\u62e9bert-base-uncased # --task_name: \u5b83\u5c06\u4ee3\u8868\u5bf9\u5e94\u7684\u4efb\u52a1\u7c7b\u578b, \u5982MRPC\u4ee3\u8868\u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 # --do_train: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u8bad\u7ec3 # --do_eval: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u9a8c\u8bc1 # --max_seq_length: \u8f93\u5165\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u8d85\u8fc7\u5219\u622a\u65ad, \u4e0d\u8db3\u5219\u8865\u9f50 # --learning_rate: \u5b66\u4e60\u7387 # --num_train_epochs: \u8bad\u7ec3\u8f6e\u6570 # --output_dir $SAVE_DIR: \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u4fdd\u5b58\u8def\u5f84 # --overwrite_output_dir: \u518d\u6b21\u8bad\u7ec3\u65f6\u5c06\u6e05\u7a7a\u4e4b\u524d\u7684\u4fdd\u5b58\u8def\u5f84\u5185\u5bb9\u91cd\u65b0\u5199\u5165 # \u56e0\u4e3a\u7a7a\u95f4\u7684\u6709\u9650\uff0c\u6240\u4ee5\u865a\u62df\u673a\u4e2d\u7f13\u5b58\u4e86\u4e09\u4e2a\u6a21\u578bbert-base-uncased bert-base-chinese bert-base-cased # \u56e0\u4e3a\u7f51\u7edc\u539f\u56e0\uff0c\u5982\u679c\u9700\u8981\u5176\u4ed6\u6a21\u578b\uff0c\u9700\u8981\u79d1\u5b66\u4e0a\u7f51\u624d\u80fd\u4e0b\u8f7d # \u865a\u62df\u673a\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8017\u65f6\u8f83\u957f\uff0c\u5efa\u8bae\u5728\u6709GPU\u7684\u4e3b\u673a\u4e0a\u6267\u884c # \u6a21\u578b1\uff0c\u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-uncased \\ --task_name mrpc \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-uncased-finetuning \\ --overwrite_output_dir # \u6a21\u578b2\uff0c\u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-chinese \\ --task_name mrpc \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-chinese-finetuning \\ --overwrite_output_dir # \u6a21\u578b3\uff0c\u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-cased \\ --task_name mrpc \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-cased-finetuning \\ --overwrite_output_dir 2.3 \u68c0\u9a8c\u6548\u679c \u00b6 1 \u8f93\u51fa\u6548\u679c \u00b6 # \u6700\u7ec8\u6253\u5370\u6a21\u578b\u7684\u9a8c\u8bc1\u7ed3\u679c: 01/05/2020 23:59:53 - INFO - __main__ - Saving features into cached file ../../glue_data/MRPC/cached_dev_bert-base-uncased_128_mrpc 01/05/2020 23:59:53 - INFO - __main__ - ***** Running evaluation ***** 01/05/2020 23:59:53 - INFO - __main__ - Num examples = 408 01/05/2020 23:59:53 - INFO - __main__ - Batch size = 8 Evaluating: 100%|\u2588| 51/51 [00:23<00:00, 2.20it/s] 01/06/2020 00:00:16 - INFO - __main__ - ***** Eval results ***** 01/06/2020 00:00:16 - INFO - __main__ - acc = 0.7671568627450981 01/06/2020 00:00:16 - INFO - __main__ - acc_and_f1 = 0.8073344506341863 01/06/2020 00:00:16 - INFO - __main__ - f1 = 0.8475120385232745 2 \u67e5\u770b\u6587\u4ef6\u5185\u5bb9 \u00b6 added_tokens.json checkpoint-450 checkpoint-400 checkpoint-350 checkpoint-200 checkpoint-300 checkpoint-250 checkpoint-200 checkpoint-150 checkpoint-100 checkpoint-50 pytorch_model.bin training_args.bin config.json special_tokens_map.json vocab.txt eval_results.txt tokenizer_config.json 3 \u6587\u4ef6\u8bf4\u660e \u00b6 pytorch_model.bin\u4ee3\u8868\u6a21\u578b\u53c2\u6570\uff0c\u53ef\u4ee5\u4f7f\u7528torch.load\u52a0\u8f7d\u67e5\u770b\uff1b traning_args.bin\u4ee3\u8868\u6a21\u578b\u8bad\u7ec3\u65f6\u7684\u8d85\u53c2\uff0c\u5982batch_size\uff0cepoch\u7b49\uff0c\u4ecd\u53ef\u4f7f\u7528torch.load\u67e5\u770b\uff1b config.json\u662f\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\uff0c\u5982\u591a\u5934\u6ce8\u610f\u529b\u7684\u5934\u6570\uff0c\u7f16\u7801\u5668\u7684\u5c42\u6570\u7b49\uff0c\u4ee3\u8868\u5178\u578b\u7684\u6a21\u578b\u7ed3\u6784\uff0c\u5982bert\uff0cxlnet\uff0c\u4e00\u822c\u4e0d\u66f4\u6539\uff1b added_token.json\u8bb0\u5f55\u5728\u8bad\u7ec3\u65f6\u901a\u8fc7\u4ee3\u7801\u6dfb\u52a0\u7684\u81ea\u5b9a\u4e49token\u5bf9\u5e94\u7684\u6570\u503c\uff0c\u5373\u5728\u4ee3\u7801\u4e2d\u4f7f\u7528add_token\u65b9\u6cd5\u6dfb\u52a0\u7684\u81ea\u5b9a\u4e49\u8bcd\u6c47\uff1b special_token_map.json\u5f53\u6dfb\u52a0\u7684token\u5177\u6709\u7279\u6b8a\u542b\u4e49\u65f6\uff0c\u5982\u5206\u9694\u7b26\uff0c\u8be5\u6587\u4ef6\u5b58\u50a8\u7279\u6b8a\u5b57\u7b26\u7684\u53ca\u5176\u5bf9\u5e94\u7684\u542b\u4e49\uff0c\u4f7f\u6587\u672c\u4e2d\u51fa\u73b0\u7684\u7279\u6b8a\u5b57\u7b26\u5148\u6620\u5c04\u6210\u5176\u542b\u4e49\uff0c\u4e4b\u540e\u7279\u6b8a\u5b57\u7b26\u7684\u542b\u4e49\u4ecd\u7136\u4f7f\u7528add_token\u65b9\u6cd5\u6620\u5c04\u3002 checkpoint: \u82e5\u5e72\u6b65\u9aa4\u4fdd\u5b58\u7684\u6a21\u578b\u53c2\u6570\u6587\u4ef6(\u4e5f\u53eb\u68c0\u6d4b\u70b9\u6587\u4ef6)\u3002 2.4 \u4f7f\u7528\u672c\u5730\u5fae\u8c03\u6a21\u578b \u00b6 import torch from transformers import AutoModel , AutoTokenizer # 1 \u901a\u8fc7git clone\u4e0b\u6a21\u578b\u5305, \u7136\u540e\u518d\u4f7f\u7528 # 2 \u76f4\u63a5\u672c\u5730\u52a0\u8f7d\u6a21\u578b mypathname = '/root/transformers/examples/pytorch/text-classification/bert-base-uncased-finetuning' tokenizer = AutoTokenizer . from_pretrained ( mypathname ) model = AutoModel . from_pretrained ( mypathname ) index = tokenizer . encode ( \"Talk is cheap\" , \"Please show me your code!\" ) # 102\u662fbert\u6a21\u578b\u4e2d\u7684\u95f4\u9694(\u7ed3\u675f)\u7b26\u53f7\u7684\u6570\u503c\u6620\u5c04 mark = 102 # \u627e\u5230\u7b2c\u4e00\u4e2a102\u7684\u7d22\u5f15, \u5373\u53e5\u5b50\u5bf9\u7684\u95f4\u9694\u7b26\u53f7 k = index . index ( mark ) # \u53e5\u5b50\u5bf9\u5206\u5272id\u5217\u8868, \u75310\uff0c1\u7ec4\u6210, 0\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50, 1\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u53e5\u5b50 segments_ids = [ 0 ] * ( k + 1 ) + [ 1 ] * ( len ( index ) - k - 1 ) # \u8f6c\u5316\u4e3atensor tokens_tensor = torch . tensor ([ index ]) segments_tensors = torch . tensor ([ segments_ids ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor , token_type_ids = segments_tensors ) # \u6253\u5370\u9884\u6d4b\u7ed3\u679c\u4ee5\u53ca\u5f20\u91cf\u5c3a\u5bf8 print ( result ) print ( result [ 0 ] . shape ) \u8f93\u51fa\u6548\u679c: (tensor([[[-0.1591, 0.0816, 0.4366, ..., 0.0307, -0.0419, 0.3326], [-0.3387, -0.0445, 0.9261, ..., -0.0232, -0.0023, 0.2407], [-0.0427, -0.1688, 0.5533, ..., -0.1092, 0.1071, 0.4287], ..., [-0.1800, -0.3889, -0.1001, ..., -0.1369, 0.0469, 0.9429], [-0.2970, -0.0023, 0.1976, ..., 0.3776, -0.0069, 0.2029], [ 0.7061, 0.0102, -0.4738, ..., 0.2246, -0.7604, -0.2503]]]), tensor([[-3.5925e-01, 2.0294e-02, -2.3487e-01, 4.5763e-01, -6.1821e-02, 2.4697e-02, 3.8172e-01, -1.8212e-01, 3.4533e-01, -9.7177e-01, 1.1063e-01, 7.8944e-02, 8.2582e-01, 1.9020e-01, 6.5513e-01, -1.8114e-01, 3.9617e-02, -5.6230e-02, 1.5207e-01, -3.2552e-01, ... 1.4417e-01, 3.0337e-01, -6.6146e-01, -9.6959e-02, 8.9790e-02, 1.2345e-01, -5.9831e-02, 2.2399e-01, 8.2549e-02, 6.7749e-01, 1.4473e-01, 5.4490e-01, 5.9272e-01, 3.4453e-01, -8.9982e-02, -1.2631e-01, -1.9465e-01, 6.5992e-01]])) torch.Size([1, 12, 768]) 3 \u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b \u00b6 \u7c7b\u578b\u4e00: \u4f7f\u7528\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u7c7b\u578b\u4e8c: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c. \u8bf4\u660e: \u6240\u6709\u7c7b\u578b\u7684\u5b9e\u6218\u6f14\u793a, \u90fd\u5c06\u9488\u5bf9\u4e2d\u6587\u6587\u672c\u8fdb\u884c. 3.1 \u7c7b\u578b\u4e00\u5b9e\u6218\u6f14\u793a \u00b6 1 \u4ecb\u7ecd \u00b6 \u4f7f\u7528\u6587\u672c\u4e8c\u5206\u7c7b\u7684\u4efb\u52a1\u7c7b\u578bSST-2\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u5206\u7c7b\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u76ee\u6807\u662f\u5224\u65ad\u53e5\u5b50\u7684\u60c5\u611f\u503e\u5411. \u51c6\u5907\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u7684\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8bed\u6599\u6837\u5f0f\u4e0eSST-2\u6570\u636e\u96c6\u76f8\u540c, \u6807\u7b7e0\u4ee3\u8868\u5dee\u8bc4, \u6807\u7b7e1\u597d\u8bc4. \u8bed\u6599\u5b58\u653e\u5728\u4e0eglue_data/\u540c\u7ea7\u76ee\u5f55cn_data/\u4e0b, \u5176\u4e2d\u7684SST-2\u76ee\u5f55\u5305\u542btrain.tsv\u548cdev.tsv train.tsv sentence label \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d,\u9910\u5385\u4e0d\u5206\u5438\u70df\u533a.\u623f\u95f4\u4e0d\u5206\u6709\u65e0\u70df\u623f. 0 \u53bb\u7684\u65f6\u5019 ,\u9152\u5e97\u5927\u5385\u548c\u9910\u5385\u5728\u88c5\u4fee,\u611f\u89c9\u5927\u5385\u6709\u70b9\u6324.\u7531\u4e8e\u9910\u5385\u88c5\u4fee\u672c\u6765\u8be5\u4eab\u53d7\u7684\u65e9\u996d,\u4e5f\u6ca1\u6709\u4eab\u53d7(\u4ed6\u4eec\u662f8\u70b9\u5f00\u59cb\u6bcf\u4e2a\u623f\u95f4\u9001,\u4f46\u662f\u6211\u65f6\u95f4\u6765\u4e0d\u53ca\u4e86)\u4e0d\u8fc7\u524d\u53f0\u670d\u52a1\u5458\u6001\u5ea6\u597d! 1 \u6709\u5f88\u957f\u65f6\u95f4\u6ca1\u6709\u5728\u897f\u85cf\u5927\u53a6\u4f4f\u4e86\uff0c\u4ee5\u524d\u53bb\u5317\u4eac\u5728\u8fd9\u91cc\u4f4f\u7684\u8f83\u591a\u3002\u8fd9\u6b21\u4f4f\u8fdb\u6765\u53d1\u73b0\u6362\u4e86\u6db2\u6676\u7535\u89c6\uff0c\u4f46\u7f51\u7edc\u4e0d\u662f\u5f88\u597d\uff0c\u4ed6\u4eec\u81ea\u5df1\u8bf4\u662f\u6536\u8d39\u7684\u539f\u56e0\u9020\u6210\u7684\u3002\u5176\u5b83\u8fd8\u597d\u3002 1 \u975e\u5e38\u597d\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u4f4f\u7684\u662f\u8c6a\u534e\u6d77\u666f\u623f\uff0c\u6253\u5f00\u7a97\u6237\u5c31\u53ef\u4ee5\u770b\u89c1\u6808\u6865\u548c\u6d77\u666f\u3002\u8bb0\u5f97\u5f88\u65e9\u4ee5\u524d\u4e5f\u4f4f\u8fc7\uff0c\u73b0\u5728\u91cd\u65b0\u88c5\u4fee\u4e86\u3002\u603b\u7684\u6765\u8bf4\u6bd4\u8f83\u6ee1\u610f\uff0c\u4ee5\u540e\u8fd8\u4f1a\u4f4f 1 \u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u623f\u95f4\u5c0f\u4e86\u4e00\u70b9\uff0c\u4f46\u662f\u5e72\u51c0\u6574\u6d01\uff0c\u5f88\u6709\u9999\u6e2f\u7684\u7279\u8272\uff0c\u6027\u4ef7\u6bd4\u8f83\u9ad8\uff0c\u63a8\u8350\u4e00\u4e0b\u54e6 1 \u9152\u5e97\u7684\u88c5\u4fee\u6bd4\u8f83\u9648\u65e7\uff0c\u623f\u95f4\u7684\u9694\u97f3\uff0c\u4e3b\u8981\u662f\u536b\u751f\u95f4\u7684\u9694\u97f3\u975e\u5e38\u5dee\uff0c\u53ea\u80fd\u7b97\u662f\u4e00\u822c\u7684 0 \u9152\u5e97\u6709\u70b9\u65e7\uff0c\u623f\u95f4\u6bd4\u8f83\u5c0f\uff0c\u4f46\u9152\u5e97\u7684\u4f4d\u5b50\u4e0d\u9519\uff0c\u5c31\u5728\u6d77\u8fb9\uff0c\u53ef\u4ee5\u76f4\u63a5\u53bb\u6e38\u6cf3\u30028\u697c\u7684\u6d77\u666f\u6253\u5f00\u7a97\u6237\u5c31\u662f\u6d77\u3002\u5982\u679c\u60f3\u4f4f\u5728\u70ed\u95f9\u7684\u5730\u5e26\uff0c\u8fd9\u91cc\u4e0d\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\uff0c\u4e0d\u8fc7\u5a01\u6d77\u57ce\u5e02\u771f\u7684\u6bd4\u8f83\u5c0f\uff0c\u6253\u8f66\u8fd8\u662f\u76f8\u5f53\u4fbf\u5b9c\u7684\u3002\u665a\u4e0a\u9152\u5e97\u95e8\u53e3\u51fa\u79df\u8f66\u6bd4\u8f83\u5c11\u3002 1 \u4f4d\u7f6e\u5f88\u597d\uff0c\u8d70\u8def\u5230\u6587\u5e99\u3001\u6e05\u51c9\u5bfa5\u5206\u949f\u90fd\u7528\u4e0d\u4e86\uff0c\u5468\u8fb9\u516c\u4ea4\u8f66\u5f88\u591a\u5f88\u65b9\u4fbf\uff0c\u5c31\u662f\u51fa\u79df\u8f66\u4e0d\u592a\u7231\u53bb\uff08\u8001\u57ce\u533a\u8def\u7a84\u7231\u5835\u8f66\uff09\uff0c\u56e0\u4e3a\u662f\u8001\u5bbe\u9986\u6240\u4ee5\u8bbe\u65bd\u8981\u9648\u65e7\u4e9b\uff0c 1 \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 0 dev.tsv sentence label \u623f\u95f4\u91cc\u6709\u7535\u8111\uff0c\u867d\u7136\u623f\u95f4\u7684\u6761\u4ef6\u7565\u663e\u7b80\u964b\uff0c\u4f46\u73af\u5883\u3001\u670d\u52a1\u8fd8\u6709\u996d\u83dc\u90fd\u8fd8\u662f\u5f88\u4e0d\u9519\u7684\u3002\u5982\u679c\u4e0b\u6b21\u53bb\u65e0\u9521\uff0c\u6211\u8fd8\u662f\u4f1a\u9009\u62e9\u8fd9\u91cc\u7684\u3002 1 \u6211\u4eec\u662f5\u67081\u65e5\u901a\u8fc7\u643a\u7a0b\u7f51\u5165\u4f4f\u7684\uff0c\u6761\u4ef6\u662f\u592a\u5dee\u4e86\uff0c\u6839\u672c\u8fbe\u4e0d\u5230\u56db\u661f\u7ea7\u7684\u6807\u51c6\uff0c\u6240\u6709\u7684\u4e1c\u897f\u90fd\u5f88\u9648\u65e7\uff0c\u536b\u751f\u95f4\u6c34\u9f99\u5934\u7528\u5b8c\u7adf\u5173\u4e0d\u4e0a\uff0c\u6d74\u7f38\u7684\u6f06\u9762\u90fd\u6389\u4e86\uff0c\u4f30\u8ba1\u662f\u5341\u5e74\u524d\u7684\u56db\u661f\u7ea7\u5427\uff0c\u603b\u4e4b\u4e0b\u6b21\u662f\u4e0d\u4f1a\u5165\u4f4f\u4e86\u3002 0 \u79bb\u706b\u8f66\u7ad9\u5f88\u8fd1\u5f88\u65b9\u4fbf\u3002\u4f4f\u5728\u4e1c\u697c\u6807\u95f4\uff0c\u76f8\u6bd4\u8f83\u5728\u4e5d\u6c5f\u4f4f\u7684\u53e6\u4e00\u5bb6\u9152\u5e97\uff0c\u623f\u95f4\u6bd4\u8f83\u5927\u3002\u536b\u751f\u95f4\u8bbe\u65bd\u7565\u65e7\u3002\u670d\u52a1\u8fd8\u597d\u300210\u5143\u4e2d\u5f0f\u65e9\u9910\u4e5f\u4e0d\u9519\uff0c\u5f88\u4e30\u5bcc\uff0c\u5c45\u7136\u8fd8\u6709\u9752\u83dc\u8089\u7247\u6c64\u3002 1 \u5750\u843d\u5728\u9999\u6e2f\u7684\u8001\u57ce\u533a\uff0c\u53ef\u4ee5\u4f53\u9a8c\u9999\u6e2f\u5c45\u6c11\u751f\u6d3b\uff0c\u95e8\u53e3\u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u5982\u679c\u65f6\u95f4\u4e0d\u7d27\uff0c\u5750\u53ee\u5f53\u8f66\u5f88\u597d\u5440\uff01\u5468\u56f4\u6709\u5f88\u591a\u5c0f\u9910\u9986\uff0c\u65e9\u9910\u5c31\u5728\u4e2d\u8fdc\u540e\u9762\u7684\u5357\u5317\u56bc\u5403\u7684\uff0c\u4e1c\u897f\u5f88\u4e0d\u9519\u3002\u6211\u4eec\u5b9a\u7684\u5927\u5e8a\u623f\uff0c\u633a\u5b89\u9759\u7684\uff0c\u603b\u4f53\u6765\u8bf4\u4e0d\u9519\u3002\u524d\u53f0\u7ed3\u8d26\u6ca1\u6709\u94f6\u8054\uff01 1 \u9152\u5e97\u524d\u53f0\u670d\u52a1\u5dee\uff0c\u5bf9\u5f85\u5ba2\u4eba\u4e0d\u70ed\u60c5\u3002\u53f7\u79f0\u643a\u7a0b\u6ca1\u6709\u9884\u5b9a\u3002\u611f\u89c9\u662f\u5ba2\u4eba\u5728\u6c42\u4ed6\u4eec\uff0c\u6211\u4eec\u4e00\u5b9a\u5f97\u4f4f\u3002\u8fd9\u6837\u7684\u5bbe\u9986\u4e0b\u6b21\u4e0d\u4f1a\u5165\u4f4f\uff01 0 \u4ef7\u683c\u786e\u5b9e\u6bd4\u8f83\u9ad8\uff0c\u800c\u4e14\u8fd8\u6ca1\u6709\u65e9\u9910\u63d0\u4f9b\u3002 1 \u662f\u4e00\u5bb6\u5f88\u5b9e\u60e0\u7684\u9152\u5e97\uff0c\u4ea4\u901a\u65b9\u4fbf\uff0c\u623f\u95f4\u4e5f\u5bbd\u655e\uff0c\u665a\u4e0a\u6ca1\u6709\u7535\u8bdd\u9a9a\u6270\uff0c\u4f4f\u4e86\u4e24\u6b21\uff0c\u6709\u4e00\u6b21\u4f4f\uff15\uff10\uff11\u623f\u95f4\uff0c\u6d17\u6fa1\u95f4\u6392\u6c34\u4e0d\u7545\u901a\uff0c\u4e5f\u8bb8\u662f\u4e2a\u522b\u95ee\u9898\uff0e\u670d\u52a1\u8d28\u91cf\u5f88\u597d\uff0c\u521a\u5165\u4f4f\u65f6\u6ca1\u6709\u8c03\u597d\u5bbd\u5e26\uff0c\u670d\u52a1\u5458\u5f88\u5feb\u5c31\u5e2e\u5fd9\u89e3\u51b3\u4e86\uff0e 1 \u4f4d\u7f6e\u975e\u5e38\u597d\uff0c\u5c31\u5728\u897f\u8857\u7684\u8857\u53e3\uff0c\u4f46\u662f\u5374\u95f9\u4e2d\u53d6\u9759\uff0c\u73af\u5883\u5f88\u6e05\u65b0\u4f18\u96c5\u3002 1 \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. 1 2 \u8fd0\u884c\u4ee3\u7801 \u00b6 \u5728run_glue.py\u540c\u7ea7\u76ee\u5f55\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 # \u4f7f\u7528python\u8fd0\u884c\u5fae\u8c03\u811a\u672c # --model_name_or_path: \u9009\u62e9bert-base-chinese # --task_name: \u53e5\u5b50\u4e8c\u5206\u7c7b\u4efb\u52a1sst2 # --do_train: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u8bad\u7ec3 # --do_eval: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u9a8c\u8bc1 # --max_seq_length: 128\uff0c\u8f93\u5165\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6 # \u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-chinese \\ --task_name sst2 \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-chinese-sst2-finetuning \u68c0\u9a8c\u6548\u679c # \u6700\u7ec8\u6253\u5370\u6a21\u578b\u7684\u9a8c\u8bc1\u7ed3\u679c, \u51c6\u786e\u7387\u9ad8\u8fbe0.88. 01/06/2020 14:22:36 - INFO - __main__ - Saving features into cached file ../../cn_data/SST-2/cached_dev_bert-base-chinese_128_sst-2 01/06/2020 14:22:36 - INFO - __main__ - ***** Running evaluation ***** 01/06/2020 14:22:36 - INFO - __main__ - Num examples = 1000 01/06/2020 14:22:36 - INFO - __main__ - Batch size = 8 Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:56<00:00, 2.20it/s] 01/06/2020 14:23:33 - INFO - __main__ - ***** Eval results ***** 01/06/2020 14:23:33 - INFO - __main__ - acc = 0.88 3 \u67e5\u770b\u6587\u4ef6\u5185\u5bb9: \u00b6 added_tokens.json checkpoint-350 checkpoint-200 checkpoint-300 checkpoint-250 checkpoint-200 checkpoint-150 checkpoint-100 checkpoint-50 pytorch_model.bin training_args.bin config.json special_tokens_map.json vocab.txt eval_results.txt tokenizer_config.json 4 \u4f7f\u7528\u672c\u5730\u5fae\u8c03\u6a21\u578b \u00b6 import torch # 0 \u627e\u5230\u81ea\u5df1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84 mymodelname = '/root/transformers/examples/pytorch/text-classification/bert-base-chinese-sst2-finetuning' print ( mymodelname ) # 1 \u672c\u5730\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684tokenizer tokenizer = AutoTokenizer . from_pretrained ( mymodelname ) # 2 \u672c\u5730\u52a0\u8f7d \u9884\u8bad\u7ec3\u6a21\u578b \u5e26\u5206\u7c7b\u6a21\u578b\u5934 model = AutoModelForSequenceClassification . from_pretrained ( mymodelname ) # model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2) # 3 \u9ed8\u8ba4\u60c5\u51b5\u4e0b \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e0d\u5e26\u5934 # model = AutoModel.from_pretrained('./transformers/examples/pytorch/text-classification/bert_finetuning_test_hug') text = \"\u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d\" index = tokenizer . encode ( text ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( result [ 0 ]) predicted_label = torch . argmax ( result [ 0 ]) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) text1 = \"\u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519.\" index = tokenizer . encode ( text1 ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( result [ 0 ]) predicted_label = torch . argmax ( result [ 0 ]) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) \u8f93\u51fa\u6548\u679c: \u8f93\u5165\u6587\u672c\u4e3a: \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d \u9884\u6d4b\u6807\u7b7e\u4e3a: 0 \u8f93\u5165\u6587\u672c\u4e3a: \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. \u9884\u6d4b\u6807\u7b7e\u4e3a: 1 3.2 \u7c7b\u578b\u4e8c\u5b9e\u6218\u6f14\u793a \u00b6 1 \u4ecb\u7ecd \u00b6 \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c. \u4f7f\u7528\u8bed\u6599\u548c\u5b8c\u6210\u7684\u76ee\u6807\u4e0e\u7c7b\u578b\u4e00\u5b9e\u6218\u76f8\u540c. 2 \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b \u00b6 \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a: import torch # \u8fdb\u884c\u53e5\u5b50\u7684\u622a\u65ad\u8865\u9f50(\u89c4\u8303\u957f\u5ea6) from keras.preprocessing import sequence # \u56e0\u4e3a\u7a7a\u95f4\u539f\u56e0\uff0c\u865a\u62df\u673a\u4e2d\u4e4b\u7f13\u5b58\u4e86huggingface/pytorch-transformers\u6a21\u578b # \u4ece\u672c\u5730\u52a0\u8f7d source = '/root/.cache/torch/hub/huggingface_pytorch-transformers_master' # \u4ecegithub\u52a0\u8f7d # source = 'huggingface/pytorch-transformers' # \u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u7684bert\u4e2d\u6587\u6a21\u578b model_name = 'bert-base-chinese' # \u901a\u8fc7torch.hub\u83b7\u5f97\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684bert-base-chinese\u6a21\u578b model = torch . hub . load ( source , 'model' , model_name , source = 'local' ) # \u4ecegithub\u52a0\u8f7d # model = torch.hub.load(source, 'model', model_name, source='github') # \u83b7\u5f97\u5bf9\u5e94\u7684\u5b57\u7b26\u6620\u5c04\u5668, \u5b83\u5c06\u628a\u4e2d\u6587\u7684\u6bcf\u4e2a\u5b57\u6620\u5c04\u6210\u4e00\u4e2a\u6570\u5b57 tokenizer = torch . hub . load ( source , 'tokenizer' , model_name , source = 'local' ) # \u4ecegithub\u52a0\u8f7d # tokenizer = torch.hub.load(source, 'tokenizer', model_name, source='github') # \u53e5\u5b50\u89c4\u8303\u957f\u5ea6 cutlen = 32 def get_bert_encode ( text ): \"\"\" description: \u4f7f\u7528bert-chinese\u7f16\u7801\u4e2d\u6587\u6587\u672c :param text: \u8981\u8fdb\u884c\u7f16\u7801\u7684\u6587\u672c :return: \u4f7f\u7528bert\u7f16\u7801\u540e\u7684\u6587\u672c\u5f20\u91cf\u8868\u793a \"\"\" # \u9996\u5148\u4f7f\u7528\u5b57\u7b26\u6620\u5c04\u5668\u5bf9\u6bcf\u4e2a\u6c49\u5b57\u8fdb\u884c\u6620\u5c04 # \u8fd9\u91cc\u9700\u8981\u6ce8\u610f, bert\u7684tokenizer\u6620\u5c04\u540e\u4f1a\u4e3a\u7ed3\u679c\u524d\u540e\u6dfb\u52a0\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u8bb0\u5373101\u548c102 # \u8fd9\u5bf9\u4e8e\u591a\u6bb5\u6587\u672c\u7684\u7f16\u7801\u662f\u6709\u610f\u4e49\u7684, \u4f46\u5728\u6211\u4eec\u8fd9\u91cc\u6ca1\u6709\u610f\u4e49, \u56e0\u6b64\u4f7f\u7528[1:-1]\u5bf9\u5934\u548c\u5c3e\u8fdb\u884c\u5207\u7247 indexed_tokens = tokenizer . encode ( text [: cutlen ])[ 1 : - 1 ] # \u5bf9\u6620\u5c04\u540e\u7684\u53e5\u5b50\u8fdb\u884c\u622a\u65ad\u8865\u9f50 indexed_tokens = sequence . pad_sequences ([ indexed_tokens ], cutlen ) # \u4e4b\u540e\u5c06\u5217\u8868\u7ed3\u6784\u8f6c\u5316\u4e3atensor tokens_tensor = torch . LongTensor ( indexed_tokens ) # \u4f7f\u6a21\u578b\u4e0d\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6 with torch . no_grad (): # \u8c03\u7528\u6a21\u578b\u83b7\u5f97\u9690\u5c42\u8f93\u51fa encoded_layers = model ( tokens_tensor ) # \u8f93\u51fa\u7684\u9690\u5c42\u662f\u4e00\u4e2a\u4e09\u7ef4\u5f20\u91cf, \u6700\u5916\u5c42\u4e00\u7ef4\u662f1, \u6211\u4eec\u4f7f\u7528[0]\u964d\u53bb\u5b83. encoded_layers = encoded_layers [ 0 ] return encoded_layers \u8c03\u7528: if __name__ == \"__main__\" : text = \"\u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d\" encoded_layers = get_bert_encode ( text ) print ( encoded_layers ) print ( encoded_layers . shape ) \u8f93\u51fa\u6548\u679c: tensor([[[-0.4078, 0.8188, -0.6263, ..., -0.0878, -0.3879, 0.1973], [-0.1980, 0.4741, 0.1832, ..., 0.1118, -0.2924, 0.0820], [ 0.6442, 0.7331, -1.0680, ..., 0.2806, -0.1484, 0.7688], ..., [ 1.2418, -0.0812, -0.3268, ..., 1.0782, 0.1485, -1.1028], [ 0.2462, -0.5323, 0.0962, ..., -0.8405, 0.8222, -0.1156], [ 0.6589, -0.0304, -0.7150, ..., -0.4237, 0.3504, -0.7093]]]) torch.Size([1, 32, 768]) 3 \u81ea\u5b9a\u4e49\u5355\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc \u00b6 \u81ea\u5b9a\u4e49\u5355\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc\u4f5c\u4e3a\u5fae\u8c03\u7f51\u7edc\u3002\u6839\u636e\u5b9e\u9645\u7ecf\u9a8c, \u81ea\u5b9a\u4e49\u7684\u5fae\u8c03\u7f51\u7edc\u53c2\u6570\u603b\u6570\u5e94\u5927\u4e8e0.5\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u5c0f\u4e8e10\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u8fd9\u6837\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u6536\u655b. import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): \"\"\"\u5b9a\u4e49\u5fae\u8c03\u7f51\u7edc\u7684\u7c7b\"\"\" def __init__ ( self , char_size = 32 , embedding_size = 768 ): \"\"\" :param char_size: \u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u5b57\u7b26\u6570\u91cf, \u5373\u8f93\u5165\u53e5\u5b50\u89c4\u8303\u540e\u7684\u957f\u5ea6128. :param embedding_size: \u5b57\u5d4c\u5165\u7684\u7ef4\u5ea6, \u56e0\u4e3a\u4f7f\u7528\u7684bert\u4e2d\u6587\u6a21\u578b\u5d4c\u5165\u7ef4\u5ea6\u662f768, \u56e0\u6b64embedding_size\u4e3a768 \"\"\" super ( Net , self ) . __init__ () # \u5c06char_size\u548cembedding_size\u4f20\u5165\u5176\u4e2d self . char_size = char_size self . embedding_size = embedding_size # \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 self . fc1 = nn . Linear ( char_size * embedding_size , 2 ) def forward ( self , x ): # \u5bf9\u8f93\u5165\u7684\u5f20\u91cf\u5f62\u72b6\u8fdb\u884c\u53d8\u6362, \u4ee5\u6ee1\u8db3\u63a5\u4e0b\u6765\u5c42\u7684\u8f93\u5165\u8981\u6c42 x = x . view ( - 1 , self . char_size * self . embedding_size ) # \u4f7f\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 x = self . fc1 ( x ) return x \u8c03\u7528: if __name__ == \"__main__\" : # \u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u5165\u53c2\u6570 x = torch . randn ( 1 , 32 , 768 ) # \u5b9e\u4f8b\u5316\u7f51\u7edc\u7ed3\u6784, \u6240\u6709\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4\u503c net = Net () nr = net ( x ) print ( nr ) \u8f93\u51fa\u6548\u679c: tensor([[0.3279, 0.2519]], grad_fn=<ReluBackward0>) 4 \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u6570\u636e\u6279\u6b21\u751f\u6210\u5668 \u00b6 import pandas as pd from collections import Counter from functools import reduce from sklearn.utils import shuffle def data_loader ( train_data_path , valid_data_path , batch_size ): \"\"\" description: \u4ece\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6570\u636e :param train_data_path: \u8bad\u7ec3\u6570\u636e\u8def\u5f84 :param valid_data_path: \u9a8c\u8bc1\u6570\u636e\u8def\u5f84 :param batch_size: \u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6279\u6b21\u5927\u5c0f :return: \u8bad\u7ec3\u6570\u636e\u751f\u6210\u5668, \u9a8c\u8bc1\u6570\u636e\u751f\u6210\u5668, \u8bad\u7ec3\u6570\u636e\u6570\u91cf, \u9a8c\u8bc1\u6570\u636e\u6570\u91cf \"\"\" # \u4f7f\u7528pd\u8fdb\u884ccsv\u6570\u636e\u7684\u8bfb\u53d6, \u5e76\u53bb\u9664\u7b2c\u4e00\u884c\u7684\u5217\u540d train_data = pd . read_csv ( train_data_path , header = None , sep = \" \\t \" ) . drop ([ 0 ]) valid_data = pd . read_csv ( valid_data_path , header = None , sep = \" \\t \" ) . drop ([ 0 ]) # \u6253\u5370\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf print ( \"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf:\" ) print ( dict ( Counter ( train_data [ 1 ] . values ))) print ( \"\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf:\" ) print ( dict ( Counter ( valid_data [ 1 ] . values ))) # \u9a8c\u8bc1\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u603b\u6570\u81f3\u5c11\u80fd\u591f\u6ee1\u8db3\u4e00\u4e2a\u6279\u6b21 if len ( valid_data ) < batch_size : raise ( \"Batch size or split not match!\" ) def _loader_generator ( data ): \"\"\" description: \u83b7\u5f97\u8bad\u7ec3\u96c6/\u9a8c\u8bc1\u96c6\u7684\u6bcf\u4e2a\u6279\u6b21\u6570\u636e\u7684\u751f\u6210\u5668 :param data: \u8bad\u7ec3\u6570\u636e\u6216\u9a8c\u8bc1\u6570\u636e :return: \u4e00\u4e2a\u6279\u6b21\u7684\u8bad\u7ec3\u6570\u636e\u6216\u9a8c\u8bc1\u6570\u636e\u7684\u751f\u6210\u5668 \"\"\" # \u4ee5\u6bcf\u4e2a\u6279\u6b21\u7684\u95f4\u9694\u904d\u5386\u6570\u636e\u96c6 for batch in range ( 0 , len ( data ), batch_size ): # \u5b9a\u4e49batch\u6570\u636e\u7684\u5f20\u91cf\u5217\u8868 batch_encoded = [] batch_labels = [] # \u5c06\u4e00\u4e2abitch_size\u5927\u5c0f\u7684\u6570\u636e\u8f6c\u6362\u6210\u5217\u8868\u5f62\u5f0f, \u5e76\u8fdb\u884c\u9010\u6761\u904d\u5386 for item in shuffle ( data . values . tolist ())[ batch : batch + batch_size ]: # \u4f7f\u7528bert\u4e2d\u6587\u6a21\u578b\u8fdb\u884c\u7f16\u7801 encoded = get_bert_encode ( item [ 0 ]) # \u5c06\u7f16\u7801\u540e\u7684\u6bcf\u6761\u6570\u636e\u88c5\u8fdb\u9884\u5148\u5b9a\u4e49\u597d\u7684\u5217\u8868\u4e2d batch_encoded . append ( encoded ) # \u540c\u6837\u5c06\u5bf9\u5e94\u7684\u8be5batch\u7684\u6807\u7b7e\u88c5\u8fdblabels\u5217\u8868\u4e2d batch_labels . append ([ int ( item [ 1 ])]) # \u4f7f\u7528reduce\u9ad8\u9636\u51fd\u6570\u5c06\u5217\u8868\u4e2d\u7684\u6570\u636e\u8f6c\u6362\u6210\u6a21\u578b\u9700\u8981\u7684\u5f20\u91cf\u5f62\u5f0f # encoded\u7684\u5f62\u72b6\u662f(batch_size*max_len, embedding_size) encoded = reduce ( lambda x , y : torch . cat (( x , y ), dim = 0 ), batch_encoded ) labels = torch . tensor ( reduce ( lambda x , y : x + y , batch_labels )) # \u4ee5\u751f\u6210\u5668\u7684\u65b9\u5f0f\u8fd4\u56de\u6570\u636e\u548c\u6807\u7b7e yield ( encoded , labels ) # \u5bf9\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u5206\u522b\u4f7f\u7528_loader_generator\u51fd\u6570, \u8fd4\u56de\u5bf9\u5e94\u7684\u751f\u6210\u5668 # \u6700\u540e\u8fd8\u8981\u8fd4\u56de\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u6837\u672c\u6570\u91cf return _loader_generator ( train_data ), _loader_generator ( valid_data ), len ( train_data ), len ( valid_data ) \u8c03\u7528: if __name__ == \"__main__\" : train_data_path = \"/root/data/glue_data/SST-2/train.tsv\" valid_data_path = \"/root/data/glue_data/SST-2/dev.tsv\" batch_size = 16 train_data_labels , valid_data_labels , \\ train_data_len , valid_data_len = data_loader ( train_data_path , valid_data_path , batch_size ) print ( next ( train_data_labels )) print ( next ( valid_data_labels )) print ( \"train_data_len:\" , train_data_len ) print ( \"valid_data_len:\" , valid_data_len ) \u8f93\u51fa\u6548\u679c: \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf: {'0': 29780, '1': 37569} \u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf: {'1': 444, '0': 428} (tensor([[[-0.6303, 1.1318, -0.3418, ..., 1.6460, -0.1171, 0.7541], [-0.5715, 0.9577, -0.4190, ..., 1.5169, -0.0387, 0.6166], [-0.5301, 0.7905, -0.2580, ..., 1.5954, -0.0559, 0.6453], ..., [-0.3087, 0.6281, 0.1010, ..., 1.5620, -0.1870, 0.5816], [-0.2482, 0.6478, 0.0386, ..., 1.4672, -0.2018, 0.6288], [ 0.0115, 0.8074, 0.3172, ..., 1.8373, -0.0368, 0.5223]], ..., [[-0.7761, 1.2271, -0.1928, ..., 1.3955, -0.4057, 0.7237], [-0.6987, 1.2270, -0.2225, ..., 1.4247, -0.3673, 0.6321], [-0.6177, 1.0689, -0.0544, ..., 1.5243, -0.4109, 0.6564], ..., [-0.2122, 0.7630, -0.1084, ..., 1.5221, -0.0703, 0.4527], [-0.5035, 0.7712, -0.2957, ..., 1.4507, -0.1208, 0.5033], [-0.3215, 0.7201, -0.0899, ..., 1.4875, -0.1781, 0.6034]]]), tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0])) train_data_len: 67349 valid_data_len: 872 5 \u7f16\u5199\u8bad\u7ec3\u548c\u9a8c\u8bc1\u51fd\u6570 \u00b6 import torch.optim as optim def train ( train_data_labels ): \"\"\" description: \u8bad\u7ec3\u51fd\u6570, \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5c06\u66f4\u65b0\u6a21\u578b\u53c2\u6570, \u5e76\u6536\u96c6\u51c6\u786e\u7387\u548c\u635f\u5931 :param train_data_labels: \u8bad\u7ec3\u6570\u636e\u548c\u6807\u7b7e\u7684\u751f\u6210\u5668\u5bf9\u8c61 :return: \u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5e73\u5747\u635f\u5931\u4e4b\u548c\u4ee5\u53ca\u6b63\u786e\u6807\u7b7e\u7684\u7d2f\u52a0\u6570 \"\"\" # \u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\u7684\u521d\u59cb\u635f\u5931\u548c\u51c6\u786e\u7387\u7d2f\u52a0\u6570 train_running_loss = 0.0 train_running_acc = 0.0 # \u5faa\u73af\u904d\u5386\u8bad\u7ec3\u6570\u636e\u548c\u6807\u7b7e\u751f\u6210\u5668, \u6bcf\u4e2a\u6279\u6b21\u66f4\u65b0\u4e00\u6b21\u6a21\u578b\u53c2\u6570 for train_tensor , train_labels in train_data_labels : # \u521d\u59cb\u5316\u8be5\u6279\u6b21\u7684\u4f18\u5316\u5668 optimizer . zero_grad () # \u4f7f\u7528\u5fae\u8c03\u7f51\u7edc\u83b7\u5f97\u8f93\u51fa train_outputs = net ( train_tensor ) # \u5f97\u5230\u8be5\u6279\u6b21\u4e0b\u7684\u5e73\u5747\u635f\u5931 train_loss = criterion ( train_outputs , train_labels ) # \u5c06\u8be5\u6279\u6b21\u7684\u5e73\u5747\u635f\u5931\u52a0\u5230train_running_loss\u4e2d train_running_loss += train_loss . item () # \u635f\u5931\u53cd\u5411\u4f20\u64ad train_loss . backward () # \u4f18\u5316\u5668\u66f4\u65b0\u6a21\u578b\u53c2\u6570 optimizer . step () # \u5c06\u8be5\u6279\u6b21\u4e2d\u6b63\u786e\u7684\u6807\u7b7e\u6570\u91cf\u8fdb\u884c\u7d2f\u52a0, \u4ee5\u4fbf\u4e4b\u540e\u8ba1\u7b97\u51c6\u786e\u7387 train_running_acc += ( train_outputs . argmax ( 1 ) == train_labels ) . sum () . item () return train_running_loss , train_running_acc def valid ( valid_data_labels ): \"\"\" description: \u9a8c\u8bc1\u51fd\u6570, \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5c06\u9a8c\u8bc1\u6a21\u578b\u7684\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u6807\u7b7e, \u6536\u96c6\u635f\u5931\u548c\u51c6\u786e\u7387 :param valid_data_labels: \u9a8c\u8bc1\u6570\u636e\u548c\u6807\u7b7e\u7684\u751f\u6210\u5668\u5bf9\u8c61 :return: \u6574\u4e2a\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u5e73\u5747\u635f\u5931\u4e4b\u548c\u4ee5\u53ca\u6b63\u786e\u6807\u7b7e\u7684\u7d2f\u52a0\u6570 \"\"\" # \u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\u7684\u521d\u59cb\u635f\u5931\u548c\u51c6\u786e\u7387\u7d2f\u52a0\u6570 valid_running_loss = 0.0 valid_running_acc = 0.0 # \u5faa\u73af\u904d\u5386\u9a8c\u8bc1\u6570\u636e\u548c\u6807\u7b7e\u751f\u6210\u5668 for valid_tensor , valid_labels in valid_data_labels : # \u4e0d\u81ea\u52a8\u66f4\u65b0\u68af\u5ea6 with torch . no_grad (): # \u4f7f\u7528\u5fae\u8c03\u7f51\u7edc\u83b7\u5f97\u8f93\u51fa valid_outputs = net ( valid_tensor ) # \u5f97\u5230\u8be5\u6279\u6b21\u4e0b\u7684\u5e73\u5747\u635f\u5931 valid_loss = criterion ( valid_outputs , valid_labels ) # \u5c06\u8be5\u6279\u6b21\u7684\u5e73\u5747\u635f\u5931\u52a0\u5230valid_running_loss\u4e2d valid_running_loss += valid_loss . item () # \u5c06\u8be5\u6279\u6b21\u4e2d\u6b63\u786e\u7684\u6807\u7b7e\u6570\u91cf\u8fdb\u884c\u7d2f\u52a0, \u4ee5\u4fbf\u4e4b\u540e\u8ba1\u7b97\u51c6\u786e\u7387 valid_running_acc += ( valid_outputs . argmax ( 1 ) == valid_labels ) . sum () . item () return valid_running_loss , valid_running_acc 6 \u8c03\u7528\u5e76\u4fdd\u5b58\u6a21\u578b \u00b6 if __name__ == \"__main__\" : # \u8bbe\u5b9a\u6570\u636e\u8def\u5f84 train_data_path = \"/root/data/glue_data/SST-2/train.tsv\" valid_data_path = \"/root/data/glue_data/SST-2/dev.tsv\" # \u5b9a\u4e49\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 criterion = nn . CrossEntropyLoss () # \u5b9a\u4e49SGD\u4f18\u5316\u65b9\u6cd5 optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) # \u5b9a\u4e49\u8bad\u7ec3\u8f6e\u6570 epochs = 4 # \u5b9a\u4e49\u6279\u6b21\u6837\u672c\u6570\u91cf batch_size = 16 # \u8fdb\u884c\u6307\u5b9a\u8f6e\u6b21\u7684\u8bad\u7ec3 for epoch in range ( epochs ): # \u6253\u5370\u8f6e\u6b21 print ( \"Epoch:\" , epoch + 1 ) # \u901a\u8fc7\u6570\u636e\u52a0\u8f7d\u5668\u83b7\u5f97\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\u751f\u6210\u5668, \u4ee5\u53ca\u5bf9\u5e94\u7684\u6837\u672c\u6570\u91cf train_data_labels , valid_data_labels , train_data_len , \\ valid_data_len = data_loader ( train_data_path , valid_data_path , batch_size ) # \u8c03\u7528\u8bad\u7ec3\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3 train_running_loss , train_running_acc = train ( train_data_labels ) # \u8c03\u7528\u9a8c\u8bc1\u51fd\u6570\u8fdb\u884c\u9a8c\u8bc1 valid_running_loss , valid_running_acc = valid ( valid_data_labels ) # \u8ba1\u7b97\u6bcf\u4e00\u8f6e\u7684\u5e73\u5747\u635f\u5931, train_running_loss\u548cvalid_running_loss\u662f\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u635f\u5931\u4e4b\u548c # \u56e0\u6b64\u5c06\u5b83\u4eec\u4e58\u4ee5batch_size\u5c31\u5f97\u5230\u4e86\u8be5\u8f6e\u7684\u603b\u635f\u5931, \u9664\u4ee5\u6837\u672c\u6570\u5373\u8be5\u8f6e\u6b21\u7684\u5e73\u5747\u635f\u5931 train_average_loss = train_running_loss * batch_size / train_data_len valid_average_loss = valid_running_loss * batch_size / valid_data_len # train_running_acc\u548cvalid_running_acc\u662f\u6bcf\u4e2a\u6279\u6b21\u7684\u6b63\u786e\u6807\u7b7e\u7d2f\u52a0\u548c, # \u56e0\u6b64\u53ea\u9700\u9664\u4ee5\u5bf9\u5e94\u6837\u672c\u603b\u6570\u5373\u662f\u8be5\u8f6e\u6b21\u7684\u51c6\u786e\u7387 train_average_acc = train_running_acc / train_data_len valid_average_acc = valid_running_acc / valid_data_len # \u6253\u5370\u8be5\u8f6e\u6b21\u4e0b\u7684\u8bad\u7ec3\u635f\u5931\u548c\u51c6\u786e\u7387\u4ee5\u53ca\u9a8c\u8bc1\u635f\u5931\u548c\u51c6\u786e\u7387 print ( \"Train Loss:\" , train_average_loss , \"|\" , \"Train Acc:\" , train_average_acc ) print ( \"Valid Loss:\" , valid_average_loss , \"|\" , \"Valid Acc:\" , valid_average_acc ) print ( 'Finished Training' ) # \u4fdd\u5b58\u8def\u5f84 MODEL_PATH = './BERT_net.pth' # \u4fdd\u5b58\u6a21\u578b\u53c2\u6570 torch . save ( net . state_dict (), MODEL_PATH ) print ( 'Finished Saving' ) \u8f93\u51fa\u6548\u679c: Epoch: 1 Train Loss: 2.144986984236597 | Train Acc: 0.7347972972972973 Valid Loss: 2.1898122818128902 | Valid Acc: 0.704 Epoch: 2 Train Loss: 1.3592962406135032 | Train Acc: 0.8435810810810811 Valid Loss: 1.8816152956699324 | Valid Acc: 0.784 Epoch: 3 Train Loss: 1.5507876996199943 | Train Acc: 0.8439189189189189 Valid Loss: 1.8626576719331536 | Valid Acc: 0.795 Epoch: 4 Train Loss: 0.7825378059198299 | Train Acc: 0.9081081081081082 Valid Loss: 2.121698483480899 | Valid Acc: 0.803 Finished Training Finished Saving 7 \u52a0\u8f7d\u6a21\u578b\u8fdb\u884c\u4f7f\u7528 \u00b6 if __name__ == \"__main__\" : MODEL_PATH = './BERT_net.pth' # \u52a0\u8f7d\u6a21\u578b\u53c2\u6570 net . load_state_dict ( torch . load ( MODEL_PATH )) # text = \"\u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002\" text = \"\u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519.\" print ( \"\u8f93\u5165\u6587\u672c\u4e3a:\" , text ) with torch . no_grad (): output = net ( get_bert_encode ( text )) # \u4eceoutput\u4e2d\u53d6\u51fa\u6700\u5927\u503c\u5bf9\u5e94\u7684\u7d22\u5f15 print ( \"\u9884\u6d4b\u6807\u7b7e\u4e3a:\" , torch . argmax ( output ) . item ()) \u8f93\u51fa\u6548\u679c: \u8f93\u5165\u6587\u672c\u4e3a: \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. \u9884\u6d4b\u6807\u7b7e\u4e3a: 1 \u8f93\u5165\u6587\u672c\u4e3a: \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 \u9884\u6d4b\u6807\u7b7e\u4e3a: 0 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c: huggingface\u7814\u7a76\u673a\u6784\u5411\u6211\u4eec\u63d0\u4f9b\u4e86\u9488\u5bf9GLUE\u6570\u636e\u96c6\u5408\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c, \u8fd9\u4e9b\u5fae\u8c03\u811a\u672c\u7684\u6838\u5fc3\u90fd\u662f\u5fae\u8c03\u6a21\u578b\u7684\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42. \u901a\u8fc7\u7b80\u5355\u7684\u53c2\u6570\u914d\u7f6e\u6765\u6307\u5b9aGLUE\u4e2d\u5b58\u5728\u4efb\u52a1\u7c7b\u578b(\u5982: CoLA\u5bf9\u5e94\u6587\u672c\u4e8c\u5206\u7c7b, MRPC\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u4e8c\u5206\u7c7b, STS-B\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u591a\u5206\u7c7b), \u4ee5\u53ca\u6307\u5b9a\u9700\u8981\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u5b66\u4e60\u4e86\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u4f7f\u7528\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u5fae\u8c03\u811a\u672c\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u914d\u7f6e\u5fae\u8c03\u811a\u672c\u53c2\u6570 \u7b2c\u4e09\u6b65: \u8fd0\u884c\u5e76\u68c0\u9a8c\u6548\u679c \u5b66\u4e60\u4e86\u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b: \u7c7b\u578b\u4e00: \u4f7f\u7528\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u7c7b\u578b\u4e8c: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c. \u5b66\u4e60\u4e86\u7c7b\u578b\u4e00\u5b9e\u6218\u6f14\u793a: \u4f7f\u7528\u6587\u672c\u4e8c\u5206\u7c7b\u7684\u4efb\u52a1\u7c7b\u578bSST-2\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u5206\u7c7b\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u76ee\u6807\u662f\u5224\u65ad\u53e5\u5b50\u7684\u60c5\u611f\u503e\u5411. \u51c6\u5907\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u7684\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8bed\u6599\u6837\u5f0f\u4e0eSST-2\u6570\u636e\u96c6\u76f8\u540c, \u6807\u7b7e0\u4ee3\u8868\u5dee\u8bc4, \u6807\u7b7e1\u597d\u8bc4. \u5b66\u4e60\u4e86\u7c7b\u578b\u4e8c\u5b9e\u6218\u6f14\u793a: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c.","title":"9 \u8fc1\u79fb\u5b66\u4e60\u5b9e\u8df5 old"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#_1","text":"\u4e86\u89e3\u5e76\u638c\u63e1\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u4f7f\u7528\u65b9\u6cd5. \u4e86\u89e3\u5e76\u638c\u63e1\u901a\u8fc7\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u540e\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u6cd5. \u638c\u63e1\u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b\u5b9e\u73b0\u8fc7\u7a0b.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#1","text":"\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c: * huggingface\u7814\u7a76\u673a\u6784\u5411\u6211\u4eec\u63d0\u4f9b\u4e86\u9488\u5bf9GLUE\u6570\u636e\u96c6\u5408\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c, \u8fd9\u4e9b\u5fae\u8c03\u811a\u672c\u7684\u6838\u5fc3\u90fd\u662f\u5fae\u8c03\u6a21\u578b\u7684\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42. * \u901a\u8fc7\u7b80\u5355\u7684\u53c2\u6570\u914d\u7f6e\u6765\u6307\u5b9aGLUE\u4e2d\u5b58\u5728\u4efb\u52a1\u7c7b\u578b(\u5982: CoLA\u5bf9\u5e94\u6587\u672c\u4e8c\u5206\u7c7b, MRPC\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u4e8c\u5206\u7c7b, STS-B\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u591a\u5206\u7c7b), \u4ee5\u53ca\u6307\u5b9a\u9700\u8981\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u6a21\u578b.","title":"1 \u5fae\u8c03\u811a\u672c\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#2","text":"\u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u5fae\u8c03\u811a\u672c\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u914d\u7f6e\u5fae\u8c03\u811a\u672c\u53c2\u6570 \u7b2c\u4e09\u6b65: \u8fd0\u884c\u5e76\u68c0\u9a8c\u6548\u679c","title":"2 \u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u4f7f\u7528\u6b65\u9aa4"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#21","text":"\u6ce8\u610f\uff1a\u865a\u62df\u673a\u4e2d\u5df2\u7ecf\u5b89\u88c5transformers\uff0c\u4ee5\u4e0b\u5b89\u88c5\u6b65\u9aa4\u4e0d\u9700\u518d\u6b21\u6267\u884c # \u514b\u9686huggingface\u7684transfomers\u6587\u4ef6 git clone https://github.com/huggingface/transformers.git # \u8fdb\u884ctransformers\u6587\u4ef6\u5939 cd transformers # \u5207\u6362transformers\u5230\u6307\u5b9a\u7248\u672c git checkout v4.17.0 # \u5b89\u88c5transformers\u5305 pip install . # \u8fdb\u5165\u5fae\u8c03\u811a\u672c\u6240\u5728\u8def\u5f84\u5e76\u67e5\u770b cd examples / pytorch / text - classification ls # \u5176\u4e2drun_glue.py\u5c31\u662f\u9488\u5bf9GLUE\u6570\u636e\u96c6\u5408\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c","title":"2.1 \u4e0b\u8f7d\u5fae\u8c03\u811a\u672c\u6587\u4ef6"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#22","text":"\u5728run_glue.py\u540c\u7ea7\u76ee\u5f55\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 # \u5b9a\u4e49DATA_DIR: \u5fae\u8c03\u6570\u636e\u6240\u5728\u8def\u5f84, \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528glue_data\u4e2d\u7684\u6570\u636e\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e export DATA_DIR = \"/root/data/glue_data\" # \u5b9a\u4e49SAVE_DIR: \u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84, \u6211\u4eec\u5c06\u6a21\u578b\u4fdd\u5b58\u5728\u5f53\u524d\u76ee\u5f55\u7684bert_finetuning_test\u6587\u4ef6\u4e2d export SAVE_DIR = \"./bert_finetuning_test/\" # \u4f7f\u7528python\u8fd0\u884c\u5fae\u8c03\u811a\u672c # --model_name_or_path: \u9009\u62e9\u5177\u4f53\u7684\u6a21\u578b\u6216\u8005\u53d8\u4f53, \u8fd9\u91cc\u662f\u5728\u82f1\u6587\u8bed\u6599\u4e0a\u5fae\u8c03, \u56e0\u6b64\u9009\u62e9bert-base-uncased # --task_name: \u5b83\u5c06\u4ee3\u8868\u5bf9\u5e94\u7684\u4efb\u52a1\u7c7b\u578b, \u5982MRPC\u4ee3\u8868\u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 # --do_train: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u8bad\u7ec3 # --do_eval: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u9a8c\u8bc1 # --max_seq_length: \u8f93\u5165\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u8d85\u8fc7\u5219\u622a\u65ad, \u4e0d\u8db3\u5219\u8865\u9f50 # --learning_rate: \u5b66\u4e60\u7387 # --num_train_epochs: \u8bad\u7ec3\u8f6e\u6570 # --output_dir $SAVE_DIR: \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u4fdd\u5b58\u8def\u5f84 # --overwrite_output_dir: \u518d\u6b21\u8bad\u7ec3\u65f6\u5c06\u6e05\u7a7a\u4e4b\u524d\u7684\u4fdd\u5b58\u8def\u5f84\u5185\u5bb9\u91cd\u65b0\u5199\u5165 # \u56e0\u4e3a\u7a7a\u95f4\u7684\u6709\u9650\uff0c\u6240\u4ee5\u865a\u62df\u673a\u4e2d\u7f13\u5b58\u4e86\u4e09\u4e2a\u6a21\u578bbert-base-uncased bert-base-chinese bert-base-cased # \u56e0\u4e3a\u7f51\u7edc\u539f\u56e0\uff0c\u5982\u679c\u9700\u8981\u5176\u4ed6\u6a21\u578b\uff0c\u9700\u8981\u79d1\u5b66\u4e0a\u7f51\u624d\u80fd\u4e0b\u8f7d # \u865a\u62df\u673a\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8017\u65f6\u8f83\u957f\uff0c\u5efa\u8bae\u5728\u6709GPU\u7684\u4e3b\u673a\u4e0a\u6267\u884c # \u6a21\u578b1\uff0c\u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-uncased \\ --task_name mrpc \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-uncased-finetuning \\ --overwrite_output_dir # \u6a21\u578b2\uff0c\u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-chinese \\ --task_name mrpc \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-chinese-finetuning \\ --overwrite_output_dir # \u6a21\u578b3\uff0c\u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-cased \\ --task_name mrpc \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-cased-finetuning \\ --overwrite_output_dir","title":"2.2 \u914d\u7f6e\u5fae\u8c03\u811a\u672c\u53c2\u6570"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#23","text":"","title":"2.3 \u68c0\u9a8c\u6548\u679c"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#1_1","text":"# \u6700\u7ec8\u6253\u5370\u6a21\u578b\u7684\u9a8c\u8bc1\u7ed3\u679c: 01/05/2020 23:59:53 - INFO - __main__ - Saving features into cached file ../../glue_data/MRPC/cached_dev_bert-base-uncased_128_mrpc 01/05/2020 23:59:53 - INFO - __main__ - ***** Running evaluation ***** 01/05/2020 23:59:53 - INFO - __main__ - Num examples = 408 01/05/2020 23:59:53 - INFO - __main__ - Batch size = 8 Evaluating: 100%|\u2588| 51/51 [00:23<00:00, 2.20it/s] 01/06/2020 00:00:16 - INFO - __main__ - ***** Eval results ***** 01/06/2020 00:00:16 - INFO - __main__ - acc = 0.7671568627450981 01/06/2020 00:00:16 - INFO - __main__ - acc_and_f1 = 0.8073344506341863 01/06/2020 00:00:16 - INFO - __main__ - f1 = 0.8475120385232745","title":"1 \u8f93\u51fa\u6548\u679c"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#2_1","text":"added_tokens.json checkpoint-450 checkpoint-400 checkpoint-350 checkpoint-200 checkpoint-300 checkpoint-250 checkpoint-200 checkpoint-150 checkpoint-100 checkpoint-50 pytorch_model.bin training_args.bin config.json special_tokens_map.json vocab.txt eval_results.txt tokenizer_config.json","title":"2 \u67e5\u770b\u6587\u4ef6\u5185\u5bb9"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#3","text":"pytorch_model.bin\u4ee3\u8868\u6a21\u578b\u53c2\u6570\uff0c\u53ef\u4ee5\u4f7f\u7528torch.load\u52a0\u8f7d\u67e5\u770b\uff1b traning_args.bin\u4ee3\u8868\u6a21\u578b\u8bad\u7ec3\u65f6\u7684\u8d85\u53c2\uff0c\u5982batch_size\uff0cepoch\u7b49\uff0c\u4ecd\u53ef\u4f7f\u7528torch.load\u67e5\u770b\uff1b config.json\u662f\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\uff0c\u5982\u591a\u5934\u6ce8\u610f\u529b\u7684\u5934\u6570\uff0c\u7f16\u7801\u5668\u7684\u5c42\u6570\u7b49\uff0c\u4ee3\u8868\u5178\u578b\u7684\u6a21\u578b\u7ed3\u6784\uff0c\u5982bert\uff0cxlnet\uff0c\u4e00\u822c\u4e0d\u66f4\u6539\uff1b added_token.json\u8bb0\u5f55\u5728\u8bad\u7ec3\u65f6\u901a\u8fc7\u4ee3\u7801\u6dfb\u52a0\u7684\u81ea\u5b9a\u4e49token\u5bf9\u5e94\u7684\u6570\u503c\uff0c\u5373\u5728\u4ee3\u7801\u4e2d\u4f7f\u7528add_token\u65b9\u6cd5\u6dfb\u52a0\u7684\u81ea\u5b9a\u4e49\u8bcd\u6c47\uff1b special_token_map.json\u5f53\u6dfb\u52a0\u7684token\u5177\u6709\u7279\u6b8a\u542b\u4e49\u65f6\uff0c\u5982\u5206\u9694\u7b26\uff0c\u8be5\u6587\u4ef6\u5b58\u50a8\u7279\u6b8a\u5b57\u7b26\u7684\u53ca\u5176\u5bf9\u5e94\u7684\u542b\u4e49\uff0c\u4f7f\u6587\u672c\u4e2d\u51fa\u73b0\u7684\u7279\u6b8a\u5b57\u7b26\u5148\u6620\u5c04\u6210\u5176\u542b\u4e49\uff0c\u4e4b\u540e\u7279\u6b8a\u5b57\u7b26\u7684\u542b\u4e49\u4ecd\u7136\u4f7f\u7528add_token\u65b9\u6cd5\u6620\u5c04\u3002 checkpoint: \u82e5\u5e72\u6b65\u9aa4\u4fdd\u5b58\u7684\u6a21\u578b\u53c2\u6570\u6587\u4ef6(\u4e5f\u53eb\u68c0\u6d4b\u70b9\u6587\u4ef6)\u3002","title":"3 \u6587\u4ef6\u8bf4\u660e"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#24","text":"import torch from transformers import AutoModel , AutoTokenizer # 1 \u901a\u8fc7git clone\u4e0b\u6a21\u578b\u5305, \u7136\u540e\u518d\u4f7f\u7528 # 2 \u76f4\u63a5\u672c\u5730\u52a0\u8f7d\u6a21\u578b mypathname = '/root/transformers/examples/pytorch/text-classification/bert-base-uncased-finetuning' tokenizer = AutoTokenizer . from_pretrained ( mypathname ) model = AutoModel . from_pretrained ( mypathname ) index = tokenizer . encode ( \"Talk is cheap\" , \"Please show me your code!\" ) # 102\u662fbert\u6a21\u578b\u4e2d\u7684\u95f4\u9694(\u7ed3\u675f)\u7b26\u53f7\u7684\u6570\u503c\u6620\u5c04 mark = 102 # \u627e\u5230\u7b2c\u4e00\u4e2a102\u7684\u7d22\u5f15, \u5373\u53e5\u5b50\u5bf9\u7684\u95f4\u9694\u7b26\u53f7 k = index . index ( mark ) # \u53e5\u5b50\u5bf9\u5206\u5272id\u5217\u8868, \u75310\uff0c1\u7ec4\u6210, 0\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e00\u4e2a\u53e5\u5b50, 1\u7684\u4f4d\u7f6e\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u53e5\u5b50 segments_ids = [ 0 ] * ( k + 1 ) + [ 1 ] * ( len ( index ) - k - 1 ) # \u8f6c\u5316\u4e3atensor tokens_tensor = torch . tensor ([ index ]) segments_tensors = torch . tensor ([ segments_ids ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor , token_type_ids = segments_tensors ) # \u6253\u5370\u9884\u6d4b\u7ed3\u679c\u4ee5\u53ca\u5f20\u91cf\u5c3a\u5bf8 print ( result ) print ( result [ 0 ] . shape ) \u8f93\u51fa\u6548\u679c: (tensor([[[-0.1591, 0.0816, 0.4366, ..., 0.0307, -0.0419, 0.3326], [-0.3387, -0.0445, 0.9261, ..., -0.0232, -0.0023, 0.2407], [-0.0427, -0.1688, 0.5533, ..., -0.1092, 0.1071, 0.4287], ..., [-0.1800, -0.3889, -0.1001, ..., -0.1369, 0.0469, 0.9429], [-0.2970, -0.0023, 0.1976, ..., 0.3776, -0.0069, 0.2029], [ 0.7061, 0.0102, -0.4738, ..., 0.2246, -0.7604, -0.2503]]]), tensor([[-3.5925e-01, 2.0294e-02, -2.3487e-01, 4.5763e-01, -6.1821e-02, 2.4697e-02, 3.8172e-01, -1.8212e-01, 3.4533e-01, -9.7177e-01, 1.1063e-01, 7.8944e-02, 8.2582e-01, 1.9020e-01, 6.5513e-01, -1.8114e-01, 3.9617e-02, -5.6230e-02, 1.5207e-01, -3.2552e-01, ... 1.4417e-01, 3.0337e-01, -6.6146e-01, -9.6959e-02, 8.9790e-02, 1.2345e-01, -5.9831e-02, 2.2399e-01, 8.2549e-02, 6.7749e-01, 1.4473e-01, 5.4490e-01, 5.9272e-01, 3.4453e-01, -8.9982e-02, -1.2631e-01, -1.9465e-01, 6.5992e-01]])) torch.Size([1, 12, 768])","title":"2.4 \u4f7f\u7528\u672c\u5730\u5fae\u8c03\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#3_1","text":"\u7c7b\u578b\u4e00: \u4f7f\u7528\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u7c7b\u578b\u4e8c: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c. \u8bf4\u660e: \u6240\u6709\u7c7b\u578b\u7684\u5b9e\u6218\u6f14\u793a, \u90fd\u5c06\u9488\u5bf9\u4e2d\u6587\u6587\u672c\u8fdb\u884c.","title":"3 \u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#31","text":"","title":"3.1 \u7c7b\u578b\u4e00\u5b9e\u6218\u6f14\u793a"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#1_2","text":"\u4f7f\u7528\u6587\u672c\u4e8c\u5206\u7c7b\u7684\u4efb\u52a1\u7c7b\u578bSST-2\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u5206\u7c7b\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u76ee\u6807\u662f\u5224\u65ad\u53e5\u5b50\u7684\u60c5\u611f\u503e\u5411. \u51c6\u5907\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u7684\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8bed\u6599\u6837\u5f0f\u4e0eSST-2\u6570\u636e\u96c6\u76f8\u540c, \u6807\u7b7e0\u4ee3\u8868\u5dee\u8bc4, \u6807\u7b7e1\u597d\u8bc4. \u8bed\u6599\u5b58\u653e\u5728\u4e0eglue_data/\u540c\u7ea7\u76ee\u5f55cn_data/\u4e0b, \u5176\u4e2d\u7684SST-2\u76ee\u5f55\u5305\u542btrain.tsv\u548cdev.tsv train.tsv sentence label \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d,\u9910\u5385\u4e0d\u5206\u5438\u70df\u533a.\u623f\u95f4\u4e0d\u5206\u6709\u65e0\u70df\u623f. 0 \u53bb\u7684\u65f6\u5019 ,\u9152\u5e97\u5927\u5385\u548c\u9910\u5385\u5728\u88c5\u4fee,\u611f\u89c9\u5927\u5385\u6709\u70b9\u6324.\u7531\u4e8e\u9910\u5385\u88c5\u4fee\u672c\u6765\u8be5\u4eab\u53d7\u7684\u65e9\u996d,\u4e5f\u6ca1\u6709\u4eab\u53d7(\u4ed6\u4eec\u662f8\u70b9\u5f00\u59cb\u6bcf\u4e2a\u623f\u95f4\u9001,\u4f46\u662f\u6211\u65f6\u95f4\u6765\u4e0d\u53ca\u4e86)\u4e0d\u8fc7\u524d\u53f0\u670d\u52a1\u5458\u6001\u5ea6\u597d! 1 \u6709\u5f88\u957f\u65f6\u95f4\u6ca1\u6709\u5728\u897f\u85cf\u5927\u53a6\u4f4f\u4e86\uff0c\u4ee5\u524d\u53bb\u5317\u4eac\u5728\u8fd9\u91cc\u4f4f\u7684\u8f83\u591a\u3002\u8fd9\u6b21\u4f4f\u8fdb\u6765\u53d1\u73b0\u6362\u4e86\u6db2\u6676\u7535\u89c6\uff0c\u4f46\u7f51\u7edc\u4e0d\u662f\u5f88\u597d\uff0c\u4ed6\u4eec\u81ea\u5df1\u8bf4\u662f\u6536\u8d39\u7684\u539f\u56e0\u9020\u6210\u7684\u3002\u5176\u5b83\u8fd8\u597d\u3002 1 \u975e\u5e38\u597d\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u4f4f\u7684\u662f\u8c6a\u534e\u6d77\u666f\u623f\uff0c\u6253\u5f00\u7a97\u6237\u5c31\u53ef\u4ee5\u770b\u89c1\u6808\u6865\u548c\u6d77\u666f\u3002\u8bb0\u5f97\u5f88\u65e9\u4ee5\u524d\u4e5f\u4f4f\u8fc7\uff0c\u73b0\u5728\u91cd\u65b0\u88c5\u4fee\u4e86\u3002\u603b\u7684\u6765\u8bf4\u6bd4\u8f83\u6ee1\u610f\uff0c\u4ee5\u540e\u8fd8\u4f1a\u4f4f 1 \u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u623f\u95f4\u5c0f\u4e86\u4e00\u70b9\uff0c\u4f46\u662f\u5e72\u51c0\u6574\u6d01\uff0c\u5f88\u6709\u9999\u6e2f\u7684\u7279\u8272\uff0c\u6027\u4ef7\u6bd4\u8f83\u9ad8\uff0c\u63a8\u8350\u4e00\u4e0b\u54e6 1 \u9152\u5e97\u7684\u88c5\u4fee\u6bd4\u8f83\u9648\u65e7\uff0c\u623f\u95f4\u7684\u9694\u97f3\uff0c\u4e3b\u8981\u662f\u536b\u751f\u95f4\u7684\u9694\u97f3\u975e\u5e38\u5dee\uff0c\u53ea\u80fd\u7b97\u662f\u4e00\u822c\u7684 0 \u9152\u5e97\u6709\u70b9\u65e7\uff0c\u623f\u95f4\u6bd4\u8f83\u5c0f\uff0c\u4f46\u9152\u5e97\u7684\u4f4d\u5b50\u4e0d\u9519\uff0c\u5c31\u5728\u6d77\u8fb9\uff0c\u53ef\u4ee5\u76f4\u63a5\u53bb\u6e38\u6cf3\u30028\u697c\u7684\u6d77\u666f\u6253\u5f00\u7a97\u6237\u5c31\u662f\u6d77\u3002\u5982\u679c\u60f3\u4f4f\u5728\u70ed\u95f9\u7684\u5730\u5e26\uff0c\u8fd9\u91cc\u4e0d\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\uff0c\u4e0d\u8fc7\u5a01\u6d77\u57ce\u5e02\u771f\u7684\u6bd4\u8f83\u5c0f\uff0c\u6253\u8f66\u8fd8\u662f\u76f8\u5f53\u4fbf\u5b9c\u7684\u3002\u665a\u4e0a\u9152\u5e97\u95e8\u53e3\u51fa\u79df\u8f66\u6bd4\u8f83\u5c11\u3002 1 \u4f4d\u7f6e\u5f88\u597d\uff0c\u8d70\u8def\u5230\u6587\u5e99\u3001\u6e05\u51c9\u5bfa5\u5206\u949f\u90fd\u7528\u4e0d\u4e86\uff0c\u5468\u8fb9\u516c\u4ea4\u8f66\u5f88\u591a\u5f88\u65b9\u4fbf\uff0c\u5c31\u662f\u51fa\u79df\u8f66\u4e0d\u592a\u7231\u53bb\uff08\u8001\u57ce\u533a\u8def\u7a84\u7231\u5835\u8f66\uff09\uff0c\u56e0\u4e3a\u662f\u8001\u5bbe\u9986\u6240\u4ee5\u8bbe\u65bd\u8981\u9648\u65e7\u4e9b\uff0c 1 \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 0 dev.tsv sentence label \u623f\u95f4\u91cc\u6709\u7535\u8111\uff0c\u867d\u7136\u623f\u95f4\u7684\u6761\u4ef6\u7565\u663e\u7b80\u964b\uff0c\u4f46\u73af\u5883\u3001\u670d\u52a1\u8fd8\u6709\u996d\u83dc\u90fd\u8fd8\u662f\u5f88\u4e0d\u9519\u7684\u3002\u5982\u679c\u4e0b\u6b21\u53bb\u65e0\u9521\uff0c\u6211\u8fd8\u662f\u4f1a\u9009\u62e9\u8fd9\u91cc\u7684\u3002 1 \u6211\u4eec\u662f5\u67081\u65e5\u901a\u8fc7\u643a\u7a0b\u7f51\u5165\u4f4f\u7684\uff0c\u6761\u4ef6\u662f\u592a\u5dee\u4e86\uff0c\u6839\u672c\u8fbe\u4e0d\u5230\u56db\u661f\u7ea7\u7684\u6807\u51c6\uff0c\u6240\u6709\u7684\u4e1c\u897f\u90fd\u5f88\u9648\u65e7\uff0c\u536b\u751f\u95f4\u6c34\u9f99\u5934\u7528\u5b8c\u7adf\u5173\u4e0d\u4e0a\uff0c\u6d74\u7f38\u7684\u6f06\u9762\u90fd\u6389\u4e86\uff0c\u4f30\u8ba1\u662f\u5341\u5e74\u524d\u7684\u56db\u661f\u7ea7\u5427\uff0c\u603b\u4e4b\u4e0b\u6b21\u662f\u4e0d\u4f1a\u5165\u4f4f\u4e86\u3002 0 \u79bb\u706b\u8f66\u7ad9\u5f88\u8fd1\u5f88\u65b9\u4fbf\u3002\u4f4f\u5728\u4e1c\u697c\u6807\u95f4\uff0c\u76f8\u6bd4\u8f83\u5728\u4e5d\u6c5f\u4f4f\u7684\u53e6\u4e00\u5bb6\u9152\u5e97\uff0c\u623f\u95f4\u6bd4\u8f83\u5927\u3002\u536b\u751f\u95f4\u8bbe\u65bd\u7565\u65e7\u3002\u670d\u52a1\u8fd8\u597d\u300210\u5143\u4e2d\u5f0f\u65e9\u9910\u4e5f\u4e0d\u9519\uff0c\u5f88\u4e30\u5bcc\uff0c\u5c45\u7136\u8fd8\u6709\u9752\u83dc\u8089\u7247\u6c64\u3002 1 \u5750\u843d\u5728\u9999\u6e2f\u7684\u8001\u57ce\u533a\uff0c\u53ef\u4ee5\u4f53\u9a8c\u9999\u6e2f\u5c45\u6c11\u751f\u6d3b\uff0c\u95e8\u53e3\u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u5982\u679c\u65f6\u95f4\u4e0d\u7d27\uff0c\u5750\u53ee\u5f53\u8f66\u5f88\u597d\u5440\uff01\u5468\u56f4\u6709\u5f88\u591a\u5c0f\u9910\u9986\uff0c\u65e9\u9910\u5c31\u5728\u4e2d\u8fdc\u540e\u9762\u7684\u5357\u5317\u56bc\u5403\u7684\uff0c\u4e1c\u897f\u5f88\u4e0d\u9519\u3002\u6211\u4eec\u5b9a\u7684\u5927\u5e8a\u623f\uff0c\u633a\u5b89\u9759\u7684\uff0c\u603b\u4f53\u6765\u8bf4\u4e0d\u9519\u3002\u524d\u53f0\u7ed3\u8d26\u6ca1\u6709\u94f6\u8054\uff01 1 \u9152\u5e97\u524d\u53f0\u670d\u52a1\u5dee\uff0c\u5bf9\u5f85\u5ba2\u4eba\u4e0d\u70ed\u60c5\u3002\u53f7\u79f0\u643a\u7a0b\u6ca1\u6709\u9884\u5b9a\u3002\u611f\u89c9\u662f\u5ba2\u4eba\u5728\u6c42\u4ed6\u4eec\uff0c\u6211\u4eec\u4e00\u5b9a\u5f97\u4f4f\u3002\u8fd9\u6837\u7684\u5bbe\u9986\u4e0b\u6b21\u4e0d\u4f1a\u5165\u4f4f\uff01 0 \u4ef7\u683c\u786e\u5b9e\u6bd4\u8f83\u9ad8\uff0c\u800c\u4e14\u8fd8\u6ca1\u6709\u65e9\u9910\u63d0\u4f9b\u3002 1 \u662f\u4e00\u5bb6\u5f88\u5b9e\u60e0\u7684\u9152\u5e97\uff0c\u4ea4\u901a\u65b9\u4fbf\uff0c\u623f\u95f4\u4e5f\u5bbd\u655e\uff0c\u665a\u4e0a\u6ca1\u6709\u7535\u8bdd\u9a9a\u6270\uff0c\u4f4f\u4e86\u4e24\u6b21\uff0c\u6709\u4e00\u6b21\u4f4f\uff15\uff10\uff11\u623f\u95f4\uff0c\u6d17\u6fa1\u95f4\u6392\u6c34\u4e0d\u7545\u901a\uff0c\u4e5f\u8bb8\u662f\u4e2a\u522b\u95ee\u9898\uff0e\u670d\u52a1\u8d28\u91cf\u5f88\u597d\uff0c\u521a\u5165\u4f4f\u65f6\u6ca1\u6709\u8c03\u597d\u5bbd\u5e26\uff0c\u670d\u52a1\u5458\u5f88\u5feb\u5c31\u5e2e\u5fd9\u89e3\u51b3\u4e86\uff0e 1 \u4f4d\u7f6e\u975e\u5e38\u597d\uff0c\u5c31\u5728\u897f\u8857\u7684\u8857\u53e3\uff0c\u4f46\u662f\u5374\u95f9\u4e2d\u53d6\u9759\uff0c\u73af\u5883\u5f88\u6e05\u65b0\u4f18\u96c5\u3002 1 \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. 1","title":"1 \u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#2_2","text":"\u5728run_glue.py\u540c\u7ea7\u76ee\u5f55\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 # \u4f7f\u7528python\u8fd0\u884c\u5fae\u8c03\u811a\u672c # --model_name_or_path: \u9009\u62e9bert-base-chinese # --task_name: \u53e5\u5b50\u4e8c\u5206\u7c7b\u4efb\u52a1sst2 # --do_train: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u8bad\u7ec3 # --do_eval: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u9a8c\u8bc1 # --max_seq_length: 128\uff0c\u8f93\u5165\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6 # \u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue.py \\ --model_name_or_path bert-base-chinese \\ --task_name sst2 \\ --do_train \\ --do_eval \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir bert-base-chinese-sst2-finetuning \u68c0\u9a8c\u6548\u679c # \u6700\u7ec8\u6253\u5370\u6a21\u578b\u7684\u9a8c\u8bc1\u7ed3\u679c, \u51c6\u786e\u7387\u9ad8\u8fbe0.88. 01/06/2020 14:22:36 - INFO - __main__ - Saving features into cached file ../../cn_data/SST-2/cached_dev_bert-base-chinese_128_sst-2 01/06/2020 14:22:36 - INFO - __main__ - ***** Running evaluation ***** 01/06/2020 14:22:36 - INFO - __main__ - Num examples = 1000 01/06/2020 14:22:36 - INFO - __main__ - Batch size = 8 Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [00:56<00:00, 2.20it/s] 01/06/2020 14:23:33 - INFO - __main__ - ***** Eval results ***** 01/06/2020 14:23:33 - INFO - __main__ - acc = 0.88","title":"2 \u8fd0\u884c\u4ee3\u7801"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#3_2","text":"added_tokens.json checkpoint-350 checkpoint-200 checkpoint-300 checkpoint-250 checkpoint-200 checkpoint-150 checkpoint-100 checkpoint-50 pytorch_model.bin training_args.bin config.json special_tokens_map.json vocab.txt eval_results.txt tokenizer_config.json","title":"3 \u67e5\u770b\u6587\u4ef6\u5185\u5bb9:"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#4","text":"import torch # 0 \u627e\u5230\u81ea\u5df1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84 mymodelname = '/root/transformers/examples/pytorch/text-classification/bert-base-chinese-sst2-finetuning' print ( mymodelname ) # 1 \u672c\u5730\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684tokenizer tokenizer = AutoTokenizer . from_pretrained ( mymodelname ) # 2 \u672c\u5730\u52a0\u8f7d \u9884\u8bad\u7ec3\u6a21\u578b \u5e26\u5206\u7c7b\u6a21\u578b\u5934 model = AutoModelForSequenceClassification . from_pretrained ( mymodelname ) # model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2) # 3 \u9ed8\u8ba4\u60c5\u51b5\u4e0b \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e0d\u5e26\u5934 # model = AutoModel.from_pretrained('./transformers/examples/pytorch/text-classification/bert_finetuning_test_hug') text = \"\u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d\" index = tokenizer . encode ( text ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( result [ 0 ]) predicted_label = torch . argmax ( result [ 0 ]) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) text1 = \"\u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519.\" index = tokenizer . encode ( text1 ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( result [ 0 ]) predicted_label = torch . argmax ( result [ 0 ]) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) \u8f93\u51fa\u6548\u679c: \u8f93\u5165\u6587\u672c\u4e3a: \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d \u9884\u6d4b\u6807\u7b7e\u4e3a: 0 \u8f93\u5165\u6587\u672c\u4e3a: \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. \u9884\u6d4b\u6807\u7b7e\u4e3a: 1","title":"4 \u4f7f\u7528\u672c\u5730\u5fae\u8c03\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#32","text":"","title":"3.2 \u7c7b\u578b\u4e8c\u5b9e\u6218\u6f14\u793a"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#1_3","text":"\u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c. \u4f7f\u7528\u8bed\u6599\u548c\u5b8c\u6210\u7684\u76ee\u6807\u4e0e\u7c7b\u578b\u4e00\u5b9e\u6218\u76f8\u540c.","title":"1 \u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#2_3","text":"\u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a: import torch # \u8fdb\u884c\u53e5\u5b50\u7684\u622a\u65ad\u8865\u9f50(\u89c4\u8303\u957f\u5ea6) from keras.preprocessing import sequence # \u56e0\u4e3a\u7a7a\u95f4\u539f\u56e0\uff0c\u865a\u62df\u673a\u4e2d\u4e4b\u7f13\u5b58\u4e86huggingface/pytorch-transformers\u6a21\u578b # \u4ece\u672c\u5730\u52a0\u8f7d source = '/root/.cache/torch/hub/huggingface_pytorch-transformers_master' # \u4ecegithub\u52a0\u8f7d # source = 'huggingface/pytorch-transformers' # \u76f4\u63a5\u4f7f\u7528\u9884\u8bad\u7ec3\u7684bert\u4e2d\u6587\u6a21\u578b model_name = 'bert-base-chinese' # \u901a\u8fc7torch.hub\u83b7\u5f97\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684bert-base-chinese\u6a21\u578b model = torch . hub . load ( source , 'model' , model_name , source = 'local' ) # \u4ecegithub\u52a0\u8f7d # model = torch.hub.load(source, 'model', model_name, source='github') # \u83b7\u5f97\u5bf9\u5e94\u7684\u5b57\u7b26\u6620\u5c04\u5668, \u5b83\u5c06\u628a\u4e2d\u6587\u7684\u6bcf\u4e2a\u5b57\u6620\u5c04\u6210\u4e00\u4e2a\u6570\u5b57 tokenizer = torch . hub . load ( source , 'tokenizer' , model_name , source = 'local' ) # \u4ecegithub\u52a0\u8f7d # tokenizer = torch.hub.load(source, 'tokenizer', model_name, source='github') # \u53e5\u5b50\u89c4\u8303\u957f\u5ea6 cutlen = 32 def get_bert_encode ( text ): \"\"\" description: \u4f7f\u7528bert-chinese\u7f16\u7801\u4e2d\u6587\u6587\u672c :param text: \u8981\u8fdb\u884c\u7f16\u7801\u7684\u6587\u672c :return: \u4f7f\u7528bert\u7f16\u7801\u540e\u7684\u6587\u672c\u5f20\u91cf\u8868\u793a \"\"\" # \u9996\u5148\u4f7f\u7528\u5b57\u7b26\u6620\u5c04\u5668\u5bf9\u6bcf\u4e2a\u6c49\u5b57\u8fdb\u884c\u6620\u5c04 # \u8fd9\u91cc\u9700\u8981\u6ce8\u610f, bert\u7684tokenizer\u6620\u5c04\u540e\u4f1a\u4e3a\u7ed3\u679c\u524d\u540e\u6dfb\u52a0\u5f00\u59cb\u548c\u7ed3\u675f\u6807\u8bb0\u5373101\u548c102 # \u8fd9\u5bf9\u4e8e\u591a\u6bb5\u6587\u672c\u7684\u7f16\u7801\u662f\u6709\u610f\u4e49\u7684, \u4f46\u5728\u6211\u4eec\u8fd9\u91cc\u6ca1\u6709\u610f\u4e49, \u56e0\u6b64\u4f7f\u7528[1:-1]\u5bf9\u5934\u548c\u5c3e\u8fdb\u884c\u5207\u7247 indexed_tokens = tokenizer . encode ( text [: cutlen ])[ 1 : - 1 ] # \u5bf9\u6620\u5c04\u540e\u7684\u53e5\u5b50\u8fdb\u884c\u622a\u65ad\u8865\u9f50 indexed_tokens = sequence . pad_sequences ([ indexed_tokens ], cutlen ) # \u4e4b\u540e\u5c06\u5217\u8868\u7ed3\u6784\u8f6c\u5316\u4e3atensor tokens_tensor = torch . LongTensor ( indexed_tokens ) # \u4f7f\u6a21\u578b\u4e0d\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6 with torch . no_grad (): # \u8c03\u7528\u6a21\u578b\u83b7\u5f97\u9690\u5c42\u8f93\u51fa encoded_layers = model ( tokens_tensor ) # \u8f93\u51fa\u7684\u9690\u5c42\u662f\u4e00\u4e2a\u4e09\u7ef4\u5f20\u91cf, \u6700\u5916\u5c42\u4e00\u7ef4\u662f1, \u6211\u4eec\u4f7f\u7528[0]\u964d\u53bb\u5b83. encoded_layers = encoded_layers [ 0 ] return encoded_layers \u8c03\u7528: if __name__ == \"__main__\" : text = \"\u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d\" encoded_layers = get_bert_encode ( text ) print ( encoded_layers ) print ( encoded_layers . shape ) \u8f93\u51fa\u6548\u679c: tensor([[[-0.4078, 0.8188, -0.6263, ..., -0.0878, -0.3879, 0.1973], [-0.1980, 0.4741, 0.1832, ..., 0.1118, -0.2924, 0.0820], [ 0.6442, 0.7331, -1.0680, ..., 0.2806, -0.1484, 0.7688], ..., [ 1.2418, -0.0812, -0.3268, ..., 1.0782, 0.1485, -1.1028], [ 0.2462, -0.5323, 0.0962, ..., -0.8405, 0.8222, -0.1156], [ 0.6589, -0.0304, -0.7150, ..., -0.4237, 0.3504, -0.7093]]]) torch.Size([1, 32, 768])","title":"2 \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#3_3","text":"\u81ea\u5b9a\u4e49\u5355\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc\u4f5c\u4e3a\u5fae\u8c03\u7f51\u7edc\u3002\u6839\u636e\u5b9e\u9645\u7ecf\u9a8c, \u81ea\u5b9a\u4e49\u7684\u5fae\u8c03\u7f51\u7edc\u53c2\u6570\u603b\u6570\u5e94\u5927\u4e8e0.5\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u5c0f\u4e8e10\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u8fd9\u6837\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u6536\u655b. import torch.nn as nn import torch.nn.functional as F class Net ( nn . Module ): \"\"\"\u5b9a\u4e49\u5fae\u8c03\u7f51\u7edc\u7684\u7c7b\"\"\" def __init__ ( self , char_size = 32 , embedding_size = 768 ): \"\"\" :param char_size: \u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u5b57\u7b26\u6570\u91cf, \u5373\u8f93\u5165\u53e5\u5b50\u89c4\u8303\u540e\u7684\u957f\u5ea6128. :param embedding_size: \u5b57\u5d4c\u5165\u7684\u7ef4\u5ea6, \u56e0\u4e3a\u4f7f\u7528\u7684bert\u4e2d\u6587\u6a21\u578b\u5d4c\u5165\u7ef4\u5ea6\u662f768, \u56e0\u6b64embedding_size\u4e3a768 \"\"\" super ( Net , self ) . __init__ () # \u5c06char_size\u548cembedding_size\u4f20\u5165\u5176\u4e2d self . char_size = char_size self . embedding_size = embedding_size # \u5b9e\u4f8b\u5316\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 self . fc1 = nn . Linear ( char_size * embedding_size , 2 ) def forward ( self , x ): # \u5bf9\u8f93\u5165\u7684\u5f20\u91cf\u5f62\u72b6\u8fdb\u884c\u53d8\u6362, \u4ee5\u6ee1\u8db3\u63a5\u4e0b\u6765\u5c42\u7684\u8f93\u5165\u8981\u6c42 x = x . view ( - 1 , self . char_size * self . embedding_size ) # \u4f7f\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42 x = self . fc1 ( x ) return x \u8c03\u7528: if __name__ == \"__main__\" : # \u968f\u673a\u521d\u59cb\u5316\u4e00\u4e2a\u8f93\u5165\u53c2\u6570 x = torch . randn ( 1 , 32 , 768 ) # \u5b9e\u4f8b\u5316\u7f51\u7edc\u7ed3\u6784, \u6240\u6709\u53c2\u6570\u4f7f\u7528\u9ed8\u8ba4\u503c net = Net () nr = net ( x ) print ( nr ) \u8f93\u51fa\u6548\u679c: tensor([[0.3279, 0.2519]], grad_fn=<ReluBackward0>)","title":"3 \u81ea\u5b9a\u4e49\u5355\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#4_1","text":"import pandas as pd from collections import Counter from functools import reduce from sklearn.utils import shuffle def data_loader ( train_data_path , valid_data_path , batch_size ): \"\"\" description: \u4ece\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6570\u636e :param train_data_path: \u8bad\u7ec3\u6570\u636e\u8def\u5f84 :param valid_data_path: \u9a8c\u8bc1\u6570\u636e\u8def\u5f84 :param batch_size: \u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6279\u6b21\u5927\u5c0f :return: \u8bad\u7ec3\u6570\u636e\u751f\u6210\u5668, \u9a8c\u8bc1\u6570\u636e\u751f\u6210\u5668, \u8bad\u7ec3\u6570\u636e\u6570\u91cf, \u9a8c\u8bc1\u6570\u636e\u6570\u91cf \"\"\" # \u4f7f\u7528pd\u8fdb\u884ccsv\u6570\u636e\u7684\u8bfb\u53d6, \u5e76\u53bb\u9664\u7b2c\u4e00\u884c\u7684\u5217\u540d train_data = pd . read_csv ( train_data_path , header = None , sep = \" \\t \" ) . drop ([ 0 ]) valid_data = pd . read_csv ( valid_data_path , header = None , sep = \" \\t \" ) . drop ([ 0 ]) # \u6253\u5370\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf print ( \"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf:\" ) print ( dict ( Counter ( train_data [ 1 ] . values ))) print ( \"\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf:\" ) print ( dict ( Counter ( valid_data [ 1 ] . values ))) # \u9a8c\u8bc1\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u603b\u6570\u81f3\u5c11\u80fd\u591f\u6ee1\u8db3\u4e00\u4e2a\u6279\u6b21 if len ( valid_data ) < batch_size : raise ( \"Batch size or split not match!\" ) def _loader_generator ( data ): \"\"\" description: \u83b7\u5f97\u8bad\u7ec3\u96c6/\u9a8c\u8bc1\u96c6\u7684\u6bcf\u4e2a\u6279\u6b21\u6570\u636e\u7684\u751f\u6210\u5668 :param data: \u8bad\u7ec3\u6570\u636e\u6216\u9a8c\u8bc1\u6570\u636e :return: \u4e00\u4e2a\u6279\u6b21\u7684\u8bad\u7ec3\u6570\u636e\u6216\u9a8c\u8bc1\u6570\u636e\u7684\u751f\u6210\u5668 \"\"\" # \u4ee5\u6bcf\u4e2a\u6279\u6b21\u7684\u95f4\u9694\u904d\u5386\u6570\u636e\u96c6 for batch in range ( 0 , len ( data ), batch_size ): # \u5b9a\u4e49batch\u6570\u636e\u7684\u5f20\u91cf\u5217\u8868 batch_encoded = [] batch_labels = [] # \u5c06\u4e00\u4e2abitch_size\u5927\u5c0f\u7684\u6570\u636e\u8f6c\u6362\u6210\u5217\u8868\u5f62\u5f0f, \u5e76\u8fdb\u884c\u9010\u6761\u904d\u5386 for item in shuffle ( data . values . tolist ())[ batch : batch + batch_size ]: # \u4f7f\u7528bert\u4e2d\u6587\u6a21\u578b\u8fdb\u884c\u7f16\u7801 encoded = get_bert_encode ( item [ 0 ]) # \u5c06\u7f16\u7801\u540e\u7684\u6bcf\u6761\u6570\u636e\u88c5\u8fdb\u9884\u5148\u5b9a\u4e49\u597d\u7684\u5217\u8868\u4e2d batch_encoded . append ( encoded ) # \u540c\u6837\u5c06\u5bf9\u5e94\u7684\u8be5batch\u7684\u6807\u7b7e\u88c5\u8fdblabels\u5217\u8868\u4e2d batch_labels . append ([ int ( item [ 1 ])]) # \u4f7f\u7528reduce\u9ad8\u9636\u51fd\u6570\u5c06\u5217\u8868\u4e2d\u7684\u6570\u636e\u8f6c\u6362\u6210\u6a21\u578b\u9700\u8981\u7684\u5f20\u91cf\u5f62\u5f0f # encoded\u7684\u5f62\u72b6\u662f(batch_size*max_len, embedding_size) encoded = reduce ( lambda x , y : torch . cat (( x , y ), dim = 0 ), batch_encoded ) labels = torch . tensor ( reduce ( lambda x , y : x + y , batch_labels )) # \u4ee5\u751f\u6210\u5668\u7684\u65b9\u5f0f\u8fd4\u56de\u6570\u636e\u548c\u6807\u7b7e yield ( encoded , labels ) # \u5bf9\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u5206\u522b\u4f7f\u7528_loader_generator\u51fd\u6570, \u8fd4\u56de\u5bf9\u5e94\u7684\u751f\u6210\u5668 # \u6700\u540e\u8fd8\u8981\u8fd4\u56de\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u7684\u6837\u672c\u6570\u91cf return _loader_generator ( train_data ), _loader_generator ( valid_data ), len ( train_data ), len ( valid_data ) \u8c03\u7528: if __name__ == \"__main__\" : train_data_path = \"/root/data/glue_data/SST-2/train.tsv\" valid_data_path = \"/root/data/glue_data/SST-2/dev.tsv\" batch_size = 16 train_data_labels , valid_data_labels , \\ train_data_len , valid_data_len = data_loader ( train_data_path , valid_data_path , batch_size ) print ( next ( train_data_labels )) print ( next ( valid_data_labels )) print ( \"train_data_len:\" , train_data_len ) print ( \"valid_data_len:\" , valid_data_len ) \u8f93\u51fa\u6548\u679c: \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf: {'0': 29780, '1': 37569} \u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6b63\u8d1f\u6837\u672c\u6570\u91cf: {'1': 444, '0': 428} (tensor([[[-0.6303, 1.1318, -0.3418, ..., 1.6460, -0.1171, 0.7541], [-0.5715, 0.9577, -0.4190, ..., 1.5169, -0.0387, 0.6166], [-0.5301, 0.7905, -0.2580, ..., 1.5954, -0.0559, 0.6453], ..., [-0.3087, 0.6281, 0.1010, ..., 1.5620, -0.1870, 0.5816], [-0.2482, 0.6478, 0.0386, ..., 1.4672, -0.2018, 0.6288], [ 0.0115, 0.8074, 0.3172, ..., 1.8373, -0.0368, 0.5223]], ..., [[-0.7761, 1.2271, -0.1928, ..., 1.3955, -0.4057, 0.7237], [-0.6987, 1.2270, -0.2225, ..., 1.4247, -0.3673, 0.6321], [-0.6177, 1.0689, -0.0544, ..., 1.5243, -0.4109, 0.6564], ..., [-0.2122, 0.7630, -0.1084, ..., 1.5221, -0.0703, 0.4527], [-0.5035, 0.7712, -0.2957, ..., 1.4507, -0.1208, 0.5033], [-0.3215, 0.7201, -0.0899, ..., 1.4875, -0.1781, 0.6034]]]), tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0])) train_data_len: 67349 valid_data_len: 872","title":"4 \u6784\u5efa\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u6570\u636e\u6279\u6b21\u751f\u6210\u5668"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#5","text":"import torch.optim as optim def train ( train_data_labels ): \"\"\" description: \u8bad\u7ec3\u51fd\u6570, \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5c06\u66f4\u65b0\u6a21\u578b\u53c2\u6570, \u5e76\u6536\u96c6\u51c6\u786e\u7387\u548c\u635f\u5931 :param train_data_labels: \u8bad\u7ec3\u6570\u636e\u548c\u6807\u7b7e\u7684\u751f\u6210\u5668\u5bf9\u8c61 :return: \u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5e73\u5747\u635f\u5931\u4e4b\u548c\u4ee5\u53ca\u6b63\u786e\u6807\u7b7e\u7684\u7d2f\u52a0\u6570 \"\"\" # \u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\u7684\u521d\u59cb\u635f\u5931\u548c\u51c6\u786e\u7387\u7d2f\u52a0\u6570 train_running_loss = 0.0 train_running_acc = 0.0 # \u5faa\u73af\u904d\u5386\u8bad\u7ec3\u6570\u636e\u548c\u6807\u7b7e\u751f\u6210\u5668, \u6bcf\u4e2a\u6279\u6b21\u66f4\u65b0\u4e00\u6b21\u6a21\u578b\u53c2\u6570 for train_tensor , train_labels in train_data_labels : # \u521d\u59cb\u5316\u8be5\u6279\u6b21\u7684\u4f18\u5316\u5668 optimizer . zero_grad () # \u4f7f\u7528\u5fae\u8c03\u7f51\u7edc\u83b7\u5f97\u8f93\u51fa train_outputs = net ( train_tensor ) # \u5f97\u5230\u8be5\u6279\u6b21\u4e0b\u7684\u5e73\u5747\u635f\u5931 train_loss = criterion ( train_outputs , train_labels ) # \u5c06\u8be5\u6279\u6b21\u7684\u5e73\u5747\u635f\u5931\u52a0\u5230train_running_loss\u4e2d train_running_loss += train_loss . item () # \u635f\u5931\u53cd\u5411\u4f20\u64ad train_loss . backward () # \u4f18\u5316\u5668\u66f4\u65b0\u6a21\u578b\u53c2\u6570 optimizer . step () # \u5c06\u8be5\u6279\u6b21\u4e2d\u6b63\u786e\u7684\u6807\u7b7e\u6570\u91cf\u8fdb\u884c\u7d2f\u52a0, \u4ee5\u4fbf\u4e4b\u540e\u8ba1\u7b97\u51c6\u786e\u7387 train_running_acc += ( train_outputs . argmax ( 1 ) == train_labels ) . sum () . item () return train_running_loss , train_running_acc def valid ( valid_data_labels ): \"\"\" description: \u9a8c\u8bc1\u51fd\u6570, \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5c06\u9a8c\u8bc1\u6a21\u578b\u7684\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u6807\u7b7e, \u6536\u96c6\u635f\u5931\u548c\u51c6\u786e\u7387 :param valid_data_labels: \u9a8c\u8bc1\u6570\u636e\u548c\u6807\u7b7e\u7684\u751f\u6210\u5668\u5bf9\u8c61 :return: \u6574\u4e2a\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u5e73\u5747\u635f\u5931\u4e4b\u548c\u4ee5\u53ca\u6b63\u786e\u6807\u7b7e\u7684\u7d2f\u52a0\u6570 \"\"\" # \u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\u7684\u521d\u59cb\u635f\u5931\u548c\u51c6\u786e\u7387\u7d2f\u52a0\u6570 valid_running_loss = 0.0 valid_running_acc = 0.0 # \u5faa\u73af\u904d\u5386\u9a8c\u8bc1\u6570\u636e\u548c\u6807\u7b7e\u751f\u6210\u5668 for valid_tensor , valid_labels in valid_data_labels : # \u4e0d\u81ea\u52a8\u66f4\u65b0\u68af\u5ea6 with torch . no_grad (): # \u4f7f\u7528\u5fae\u8c03\u7f51\u7edc\u83b7\u5f97\u8f93\u51fa valid_outputs = net ( valid_tensor ) # \u5f97\u5230\u8be5\u6279\u6b21\u4e0b\u7684\u5e73\u5747\u635f\u5931 valid_loss = criterion ( valid_outputs , valid_labels ) # \u5c06\u8be5\u6279\u6b21\u7684\u5e73\u5747\u635f\u5931\u52a0\u5230valid_running_loss\u4e2d valid_running_loss += valid_loss . item () # \u5c06\u8be5\u6279\u6b21\u4e2d\u6b63\u786e\u7684\u6807\u7b7e\u6570\u91cf\u8fdb\u884c\u7d2f\u52a0, \u4ee5\u4fbf\u4e4b\u540e\u8ba1\u7b97\u51c6\u786e\u7387 valid_running_acc += ( valid_outputs . argmax ( 1 ) == valid_labels ) . sum () . item () return valid_running_loss , valid_running_acc","title":"5 \u7f16\u5199\u8bad\u7ec3\u548c\u9a8c\u8bc1\u51fd\u6570"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#6","text":"if __name__ == \"__main__\" : # \u8bbe\u5b9a\u6570\u636e\u8def\u5f84 train_data_path = \"/root/data/glue_data/SST-2/train.tsv\" valid_data_path = \"/root/data/glue_data/SST-2/dev.tsv\" # \u5b9a\u4e49\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570 criterion = nn . CrossEntropyLoss () # \u5b9a\u4e49SGD\u4f18\u5316\u65b9\u6cd5 optimizer = optim . SGD ( net . parameters (), lr = 0.001 , momentum = 0.9 ) # \u5b9a\u4e49\u8bad\u7ec3\u8f6e\u6570 epochs = 4 # \u5b9a\u4e49\u6279\u6b21\u6837\u672c\u6570\u91cf batch_size = 16 # \u8fdb\u884c\u6307\u5b9a\u8f6e\u6b21\u7684\u8bad\u7ec3 for epoch in range ( epochs ): # \u6253\u5370\u8f6e\u6b21 print ( \"Epoch:\" , epoch + 1 ) # \u901a\u8fc7\u6570\u636e\u52a0\u8f7d\u5668\u83b7\u5f97\u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\u751f\u6210\u5668, \u4ee5\u53ca\u5bf9\u5e94\u7684\u6837\u672c\u6570\u91cf train_data_labels , valid_data_labels , train_data_len , \\ valid_data_len = data_loader ( train_data_path , valid_data_path , batch_size ) # \u8c03\u7528\u8bad\u7ec3\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3 train_running_loss , train_running_acc = train ( train_data_labels ) # \u8c03\u7528\u9a8c\u8bc1\u51fd\u6570\u8fdb\u884c\u9a8c\u8bc1 valid_running_loss , valid_running_acc = valid ( valid_data_labels ) # \u8ba1\u7b97\u6bcf\u4e00\u8f6e\u7684\u5e73\u5747\u635f\u5931, train_running_loss\u548cvalid_running_loss\u662f\u6bcf\u4e2a\u6279\u6b21\u7684\u5e73\u5747\u635f\u5931\u4e4b\u548c # \u56e0\u6b64\u5c06\u5b83\u4eec\u4e58\u4ee5batch_size\u5c31\u5f97\u5230\u4e86\u8be5\u8f6e\u7684\u603b\u635f\u5931, \u9664\u4ee5\u6837\u672c\u6570\u5373\u8be5\u8f6e\u6b21\u7684\u5e73\u5747\u635f\u5931 train_average_loss = train_running_loss * batch_size / train_data_len valid_average_loss = valid_running_loss * batch_size / valid_data_len # train_running_acc\u548cvalid_running_acc\u662f\u6bcf\u4e2a\u6279\u6b21\u7684\u6b63\u786e\u6807\u7b7e\u7d2f\u52a0\u548c, # \u56e0\u6b64\u53ea\u9700\u9664\u4ee5\u5bf9\u5e94\u6837\u672c\u603b\u6570\u5373\u662f\u8be5\u8f6e\u6b21\u7684\u51c6\u786e\u7387 train_average_acc = train_running_acc / train_data_len valid_average_acc = valid_running_acc / valid_data_len # \u6253\u5370\u8be5\u8f6e\u6b21\u4e0b\u7684\u8bad\u7ec3\u635f\u5931\u548c\u51c6\u786e\u7387\u4ee5\u53ca\u9a8c\u8bc1\u635f\u5931\u548c\u51c6\u786e\u7387 print ( \"Train Loss:\" , train_average_loss , \"|\" , \"Train Acc:\" , train_average_acc ) print ( \"Valid Loss:\" , valid_average_loss , \"|\" , \"Valid Acc:\" , valid_average_acc ) print ( 'Finished Training' ) # \u4fdd\u5b58\u8def\u5f84 MODEL_PATH = './BERT_net.pth' # \u4fdd\u5b58\u6a21\u578b\u53c2\u6570 torch . save ( net . state_dict (), MODEL_PATH ) print ( 'Finished Saving' ) \u8f93\u51fa\u6548\u679c: Epoch: 1 Train Loss: 2.144986984236597 | Train Acc: 0.7347972972972973 Valid Loss: 2.1898122818128902 | Valid Acc: 0.704 Epoch: 2 Train Loss: 1.3592962406135032 | Train Acc: 0.8435810810810811 Valid Loss: 1.8816152956699324 | Valid Acc: 0.784 Epoch: 3 Train Loss: 1.5507876996199943 | Train Acc: 0.8439189189189189 Valid Loss: 1.8626576719331536 | Valid Acc: 0.795 Epoch: 4 Train Loss: 0.7825378059198299 | Train Acc: 0.9081081081081082 Valid Loss: 2.121698483480899 | Valid Acc: 0.803 Finished Training Finished Saving","title":"6 \u8c03\u7528\u5e76\u4fdd\u5b58\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#7","text":"if __name__ == \"__main__\" : MODEL_PATH = './BERT_net.pth' # \u52a0\u8f7d\u6a21\u578b\u53c2\u6570 net . load_state_dict ( torch . load ( MODEL_PATH )) # text = \"\u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002\" text = \"\u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519.\" print ( \"\u8f93\u5165\u6587\u672c\u4e3a:\" , text ) with torch . no_grad (): output = net ( get_bert_encode ( text )) # \u4eceoutput\u4e2d\u53d6\u51fa\u6700\u5927\u503c\u5bf9\u5e94\u7684\u7d22\u5f15 print ( \"\u9884\u6d4b\u6807\u7b7e\u4e3a:\" , torch . argmax ( output ) . item ()) \u8f93\u51fa\u6548\u679c: \u8f93\u5165\u6587\u672c\u4e3a: \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. \u9884\u6d4b\u6807\u7b7e\u4e3a: 1 \u8f93\u5165\u6587\u672c\u4e3a: \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 \u9884\u6d4b\u6807\u7b7e\u4e3a: 0","title":"7 \u52a0\u8f7d\u6a21\u578b\u8fdb\u884c\u4f7f\u7528"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-old.html#4_2","text":"\u5b66\u4e60\u4e86\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c: huggingface\u7814\u7a76\u673a\u6784\u5411\u6211\u4eec\u63d0\u4f9b\u4e86\u9488\u5bf9GLUE\u6570\u636e\u96c6\u5408\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c, \u8fd9\u4e9b\u5fae\u8c03\u811a\u672c\u7684\u6838\u5fc3\u90fd\u662f\u5fae\u8c03\u6a21\u578b\u7684\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42. \u901a\u8fc7\u7b80\u5355\u7684\u53c2\u6570\u914d\u7f6e\u6765\u6307\u5b9aGLUE\u4e2d\u5b58\u5728\u4efb\u52a1\u7c7b\u578b(\u5982: CoLA\u5bf9\u5e94\u6587\u672c\u4e8c\u5206\u7c7b, MRPC\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u4e8c\u5206\u7c7b, STS-B\u5bf9\u5e94\u53e5\u5b50\u5bf9\u6587\u672c\u591a\u5206\u7c7b), \u4ee5\u53ca\u6307\u5b9a\u9700\u8981\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u6a21\u578b. \u5b66\u4e60\u4e86\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u4f7f\u7528\u6b65\u9aa4: \u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u5fae\u8c03\u811a\u672c\u6587\u4ef6 \u7b2c\u4e8c\u6b65: \u914d\u7f6e\u5fae\u8c03\u811a\u672c\u53c2\u6570 \u7b2c\u4e09\u6b65: \u8fd0\u884c\u5e76\u68c0\u9a8c\u6548\u679c \u5b66\u4e60\u4e86\u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b: \u7c7b\u578b\u4e00: \u4f7f\u7528\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u7c7b\u578b\u4e8c: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c. \u5b66\u4e60\u4e86\u7c7b\u578b\u4e00\u5b9e\u6218\u6f14\u793a: \u4f7f\u7528\u6587\u672c\u4e8c\u5206\u7c7b\u7684\u4efb\u52a1\u7c7b\u578bSST-2\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u5206\u7c7b\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u76ee\u6807\u662f\u5224\u65ad\u53e5\u5b50\u7684\u60c5\u611f\u503e\u5411. \u51c6\u5907\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u7684\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8bed\u6599\u6837\u5f0f\u4e0eSST-2\u6570\u636e\u96c6\u76f8\u540c, \u6807\u7b7e0\u4ee3\u8868\u5dee\u8bc4, \u6807\u7b7e1\u597d\u8bc4. \u5b66\u4e60\u4e86\u7c7b\u578b\u4e8c\u5b9e\u6218\u6f14\u793a: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c.","title":"4 \u5c0f\u7ed3"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u5e76\u638c\u63e1\u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u5206\u7c7b\u4efb\u52a1\u5f00\u53d1 \u4e86\u89e3\u5e76\u638c\u63e1\u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u586b\u7a7a\u4efb\u52a1\u5f00\u53d1 \u4e86\u89e3\u5e76\u638c\u63e1\u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1 \u4e86\u89e3\u901a\u8fc7\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u540e\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u6cd5 1 \u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b \u00b6 \u7c7b\u578b\u4e00: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c \u7c7b\u578b\u4e8c: \u4f7f\u7528\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c \u8bf4\u660e: \u6240\u6709\u7c7b\u578b\u7684\u5b9e\u6218\u6f14\u793a, \u90fd\u5c06\u9488\u5bf9\u4e2d\u6587\u6587\u672c\u8fdb\u884c 2 \u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u5206\u7c7b \u00b6 1 \u4efb\u52a1\u4ecb\u7ecd \u00b6 \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c 2 \u6570\u636e\u4ecb\u7ecd \u00b6 \u6570\u636e\u6587\u4ef6\u6709\u4e09\u4e2atrain.csv\uff0ctest.csv\uff0cvalidation.csv\uff0c\u6570\u636e\u6837\u5f0f\u90fd\u662f\u4e00\u6837\u7684\u3002 label , text 1 , \u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf \uff0c \u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9 \uff0c \u5468\u56f4\u9910\u9986 \u3001 \u98df\u5eca \u3001 \u5546\u573a \u3001 \u8d85\u5e02 \u3001 \u644a\u4f4d\u4e00\u5e94\u4ff1\u5168 \u3002 \u9152\u5e97\u88c5\u4fee\u4e00\u822c \uff0c \u4f46\u8fd8\u7b97\u6574\u6d01 \u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876 \uff0c \u56e0\u6b64\u5f88\u5c0f \uff0c \u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22 \u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684 \uff0c \u8fd8\u7b97\u4e30\u5bcc \u3002 \u670d\u52a1\u5417 \uff0c \u4e00\u822c 1 , 15.4 \u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d \uff0c \u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86 \uff0c \u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8 \uff0c \u8f93\u6570\u5b57\u7279\u65b9\u4fbf \uff0c \u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2 \uff0c \u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519 0 , \u623f\u95f4\u592a\u5c0f \u3002 \u5176\u4ed6\u7684\u90fd\u4e00\u822c \u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002 0 , \"1.\u63a5\u7535\u6e90\u6ca1\u6709\u51e0\u5206\u949f,\u7535\u6e90\u9002\u914d\u5668\u70ed\u7684\u4e0d\u884c. 2.\u6444\u50cf\u5934\u7528\u4e0d\u8d77\u6765. 3.\u673a\u76d6\u7684\u94a2\u7434\u6f06\uff0c\u624b\u4e0d\u80fd\u6478\uff0c\u4e00\u6478\u4e00\u4e2a\u5370. 4.\u786c\u76d8\u5206\u533a\u4e0d\u597d\u529e.\" 1 , \"\u4eca\u5929\u624d\u77e5\u9053\u8fd9\u4e66\u8fd8\u6709\u7b2c6\u5377,\u771f\u6709\u70b9\u90c1\u95f7:\u4e3a\u4ec0\u4e48\u540c\u4e00\u5957\u4e66\u6709\u4e24\u79cd\u7248\u672c\u5462?\u5f53\u5f53\u7f51\u662f\u4e0d\u662f\u8be5\u8ddf\u51fa\u7248\u793e\u5546\u91cf\u5546\u91cf,\u5355\u72ec\u51fa\u4e2a\u7b2c6\u5377,\u8ba9\u6211\u4eec\u7684\u5b69\u5b50\u4e0d\u4f1a\u6709\u6240\u9057\u61be\u3002\" \u901a\u8fc7huggingface\u7684datasets\u5de5\u5177\uff0c\u52a0\u8f7d\u4fe1\u606f\u6587\u4ef6\u4fe1\u606f\u5982\u4e0b \u52a0\u8f7d\u8bad\u7ec3\u96c6 dataset_train ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 9600 }) { 'label' : [ 1 , 1 , 0 ], 'text' : [ '\u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf\uff0c\u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9\uff0c\u5468\u56f4\u9910\u9986\u3001\u98df\u5eca\u3001\u5546\u573a\u3001\u8d85\u5e02\u3001\u644a\u4f4d\u4e00\u5e94\u4ff1\u5168\u3002\u9152\u5e97\u88c5\u4fee\u4e00\u822c\uff0c\u4f46\u8fd8\u7b97\u6574\u6d01\u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876\uff0c\u56e0\u6b64\u5f88\u5c0f\uff0c\u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22\u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684\uff0c\u8fd8\u7b97\u4e30\u5bcc\u3002 \u670d\u52a1\u5417\uff0c\u4e00\u822c' , '15.4\u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d\uff0c\u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86\uff0c\u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8\uff0c\u8f93\u6570\u5b57\u7279\u65b9\u4fbf\uff0c\u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2\uff0c\u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519' , '\u623f\u95f4\u592a\u5c0f\u3002\u5176\u4ed6\u7684\u90fd\u4e00\u822c\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002' ]} \u52a0\u8f7d\u6d4b\u8bd5\u96c6 my_dataset_test ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 1200 }) { 'label' : [ 1 , 0 , 0 ], 'text' : [ '\u8fd9\u4e2a\u5bbe\u9986\u6bd4\u8f83\u9648\u65e7\u4e86\uff0c\u7279\u4ef7\u7684\u623f\u95f4\u4e5f\u5f88\u4e00\u822c\u3002\u603b\u4f53\u6765\u8bf4\u4e00\u822c' , '\u6000\u7740\u5341\u5206\u6fc0\u52a8\u7684\u5fc3\u60c5\u653e\u6620\uff0c\u53ef\u662f\u770b\u7740\u770b\u7740\u53d1\u73b0\uff0c\u5728\u653e\u6620\u5b8c\u6bd5\u540e\uff0c\u51fa\u73b0\u4e00\u96c6\u7c73\u8001\u9f20\u7684\u52a8\u753b\u7247\uff01\u5f00\u59cb\u8fd8\u6000\u7591\u662f\u4e0d\u662f\u8d60\u9001\u7684\u4e2a\u522b\u73b0\u8c61\uff0c\u53ef\u662f\u540e\u6765\u53d1\u73b0\u6bcf\u5f20DVD\u540e\u9762\u90fd\u6709\uff01\u771f\u4e0d\u77e5\u9053\u751f\u4ea7\u5546\u600e\u4e48\u60f3\u7684\uff0c\u6211\u60f3\u770b\u7684\u662f\u732b\u548c\u8001\u9f20\uff0c\u4e0d\u662f\u7c73\u8001\u9f20\uff01\u5982\u679c\u5382\u5bb6\u662f\u60f3\u8d60\u9001\u7684\u8bdd\uff0c\u90a3\u5c31\u5168\u5957\u7c73\u8001\u9f20\u548c\u5510\u8001\u9e2d\u90fd\u8d60\u9001\uff0c\u53ea\u5728\u6bcf\u5f20DVD\u540e\u9762\u6dfb\u52a0\u4e00\u96c6\u7b97\u4ec0\u4e48\uff1f\uff1f\u7b80\u76f4\u662f\u753b\u86c7\u6dfb\u8db3\uff01\uff01' , '\u8fd8\u7a0d\u5fae\u91cd\u4e86\u70b9\uff0c\u53ef\u80fd\u662f\u786c\u76d8\u5927\u7684\u539f\u6545\uff0c\u8fd8\u8981\u518d\u8f7b\u534a\u65a4\u5c31\u597d\u4e86\u3002\u5176\u4ed6\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\u8d34\u7684\u51e0\u79cd\u819c\u6c14\u6ce1\u8f83\u591a\uff0c\u7528\u4e0d\u4e86\u591a\u4e45\u5c31\u8981\u66f4\u6362\u4e86\uff0c\u5c4f\u5e55\u819c\u7a0d\u597d\u70b9\uff0c\u4f46\u6bd4\u6ca1\u6709\u8981\u5f3a\u591a\u4e86\u3002\u5efa\u8bae\u914d\u8d60\u51e0\u5f20\u819c\u8ba9\u7528\u7528\u6237\u81ea\u5df1\u8d34\u3002' ]} \u52a0\u8f7d\u9a8c\u8bc1\u96c6 my_dataset_validation ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 1200 }) { 'label' : [ 1 , 1 , 0 ], 'text' : [ '\u9019\u9593\u9152\u5e97\u74b0\u5883\u548c\u670d\u52d9\u614b\u5ea6\u4ea6\u7b97\u4e0d\u932f,\u4f46\u623f\u9593\u7a7a\u9593\u592a\u5c0f~~\u4e0d\u5ba3\u5bb9\u7d0d\u592a\u5927\u4ef6\u884c\u674e~~\u4e14\u623f\u9593\u683c\u8abf\u9084\u53ef\u4ee5~~ \u4e2d\u9910\u5ef3\u7684\u5ee3\u6771\u9ede\u5fc3\u4e0d\u592a\u597d\u5403~~\u8981\u6539\u5584\u4e4b~~~~\u4f46\u7b97\u50f9\u9322\u5e73\u5b9c~~\u53ef\u63a5\u53d7~~ \u897f\u9910\u5ef3\u683c\u8abf\u90fd\u5f88\u597d~~\u4f46\u5403\u7684\u5473\u9053\u4e00\u822c\u4e14\u4ee4\u4eba\u7b49\u5f97\u592a\u8010\u4e86~~\u8981\u6539\u5584\u4e4b~~' , '<\u8350\u4e66> \u63a8\u8350\u6240\u6709\u559c\u6b22<\u7ea2\u697c>\u7684\u7ea2\u8ff7\u4eec\u4e00\u5b9a\u8981\u6536\u85cf\u8fd9\u672c\u4e66,\u8981\u77e5\u9053\u5f53\u5e74\u6211\u542c\u8bf4\u8fd9\u672c\u4e66\u7684\u65f6\u5019\u82b1\u5f88\u957f\u65f6\u95f4\u53bb\u56fe\u4e66\u9986\u627e\u548c\u501f\u90fd\u6ca1\u80fd\u5982\u613f,\u6240\u4ee5\u8fd9\u6b21\u4e00\u770b\u5230\u5f53\u5f53\u6709,\u9a6c\u4e0a\u4e70\u4e86,\u7ea2\u8ff7\u4eec\u4e5f\u8981\u8bb0\u5f97\u5907\u8d27\u54e6!' , '\u5546\u54c1\u7684\u4e0d\u8db3\u6682\u65f6\u8fd8\u6ca1\u53d1\u73b0\uff0c\u4eac\u4e1c\u7684\u8ba2\u5355\u5904\u7406\u901f\u5ea6\u5b9e\u5728.......\u5468\u4e8c\u5c31\u6253\u5305\u5b8c\u6210\uff0c\u5468\u4e94\u624d\u53d1\u8d27...' ]} \u901a\u8fc7huggingface\u7684datasets\u5de5\u5177\uff0c\u52a0\u8f7d\u4ee3\u7801\u5982\u4e0b def dm_file2dataset (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train print ( ' \\n \u52a0\u8f7d\u8bad\u7ec3\u96c6' ) my_dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = 'train' ) print ( 'dataset_train--->' , my_dataset_train ) print ( my_dataset_train [ 0 : 3 ]) # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test print ( ' \\n \u52a0\u8f7d\u6d4b\u8bd5\u96c6' ) my_dataset_test = load_dataset ( 'csv' , data_files = './mydata1/test.csv' , split = 'train' ) print ( 'my_dataset_test--->' , my_dataset_test ) print ( my_dataset_test [ 0 : 3 ]) print ( ' \\n \u52a0\u8f7d\u9a8c\u8bc1\u96c6' ) # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train my_dataset_validation = load_dataset ( 'csv' , data_files = './mydata1/validation.csv' , split = \"train\" ) print ( 'my_dataset_validation--->' , my_dataset_validation ) print ( my_dataset_validation [ 0 : 3 ]) \u5bfc\u5165\u5de5\u5177\u5305\u548c\u8f85\u52a9\u5de5\u5177\u5b9e\u4f8b\u5316\u5bf9\u8c61 # \u5bfc\u5165\u5de5\u5177\u5305 import torch from datasets import load_dataset from transformers import BertTokenizer , BertModel from transformers import AdamW import time # \u52a0\u8f7d\u5b57\u5178\u548c\u5206\u8bcd\u5de5\u5177 \u5b9e\u4f8b\u5316\u5206\u8bcd\u5de5\u5177 my_tokenizer = BertTokenizer . from_pretrained ( 'bert-base-chinese' ) # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b \u5b9e\u4f8b\u5316\u9884\u8bad\u7ec3\u6a21\u578b my_model_pretrained = BertModel . from_pretrained ( 'bert-base-chinese' ) 3 \u6570\u636e\u9884\u5904\u7406 \u00b6 \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42\u3002 \u6570\u636e\u9884\u5904\u7406\u548c\u76f8\u5173\u6d4b\u8bd5\u51fd\u6570 # \u6570\u636e\u96c6\u5904\u7406\u81ea\u5b9a\u4e49\u51fd\u6570 def collate_fn1 ( data ): # data\u4f20\u8fc7\u6765\u7684\u6570\u636e\u662flist eg: \u6279\u6b21\u65708\uff0c8\u4e2a\u5b57\u5178 # [{'text':'xxxx','label':0} , {'text':'xxxx','label':1}, ...] sents = [ i [ 'text' ] for i in data ] labels = [ i [ 'label' ] for i in data ] # \u7f16\u7801text2id \u5bf9\u591a\u53e5\u8bdd\u8fdb\u884c\u7f16\u7801\u7528batch_encode_plus\u51fd\u6570 data = my_tokenizer . batch_encode_plus ( batch_text_or_text_pairs = sents , truncation = True , padding = 'max_length' , max_length = 500 , return_tensors = 'pt' , return_length = True ) # input_ids:\u7f16\u7801\u4e4b\u540e\u7684\u6570\u5b57 # attention_mask:\u662f\u8865\u96f6\u7684\u4f4d\u7f6e\u662f0,\u5176\u4ed6\u4f4d\u7f6e\u662f1 input_ids = data [ 'input_ids' ] attention_mask = data [ 'attention_mask' ] token_type_ids = data [ 'token_type_ids' ] labels = torch . LongTensor ( labels ) # \u8fd4\u56detext2id\u4fe1\u606f \u63a9\u7801\u4fe1\u606f \u53e5\u5b50\u5206\u6bb5\u4fe1\u606f \u6807\u7b7ey return input_ids , attention_mask , token_type_ids , labels # \u6d4b\u8bd5\u6570\u636e def dm01_test_dataset (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90 \u901a\u8fc7\u8bad\u7ec3\u6587\u4ef6 dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) print ( 'dataset_train--->' , dataset_train ) # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668 mydataloader mydataloader = torch . utils . data . DataLoader ( dataset_train , batch_size = 8 , collate_fn = collate_fn1 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , len ( mydataloader )) # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( len ( mydataloader )) print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) # \u6253\u5370\u53e5\u5b50text2id\u540e\u7684\u4fe1\u606f print ( 'input_ids' , input_ids ) # \u6253\u5370\u53e5\u5b50attention\u63a9\u7801\u4fe1\u606f print ( 'attention_mask' , attention_mask ) # \u6253\u5370\u53e5\u5b50\u5206\u6bb5\u4fe1\u606f print ( 'token_type_ids' , token_type_ids ) # \u6253\u5370\u76ee\u6807y\u4fe1\u606f print ( 'labels' , labels ) break \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c # \u663e\u793a\u8bad\u7ec3\u96c6\u5b57\u6bb5\u548c\u6837\u672c\u6570\u76ee dataset_train ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 9600 }) mydataloader ---> 1200 # \u663e\u793a\u5904\u7406\u540e\u9001\u7ed9\u6a21\u578b\u7684\u6570\u636e\u4fe1\u606f torch . Size ([ 8 , 500 ]) torch . Size ([ 8 , 500 ]) torch . Size ([ 8 , 500 ]) tensor ([ 1 , 1 , 0 , 0 , 1 , 0 , 0 , 0 ]) # \u53e5\u5b50text2id\u540e\u7684\u4fe1\u606f input_ids tensor ([[ 101 , 6848 , 2885 , ... , 0 , 0 , 0 ], [ 101 , 8115 , 119 , ... , 0 , 0 , 0 ], [ 101 , 2791 , 7313 , ... , 0 , 0 , 0 ], ... , [ 101 , 3322 , 1690 , ... , 0 , 0 , 0 ], [ 101 , 1457 , 1457 , ... , 0 , 0 , 0 ], [ 101 , 6821 , 3315 , ... , 0 , 0 , 0 ]]) # \u53e5\u5b50\u6ce8\u610f\u529b\u673a\u5236\u63a9\u7801\u4fe1\u606f attention_mask tensor ([[ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], ... , [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ]]) # \u53e5\u5b50\u5206\u6bb5\u4fe1\u606f token_type_ids tensor ([[ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], ... , [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ]]) # \u53e5\u5b50\u7684\u6807\u7b7e\u4fe1\u606f labels tensor ([ 1 , 1 , 0 , 0 , 1 , 0 , 0 , 0 ]) 4 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b \u00b6 \u81ea\u5b9a\u4e49\u5355\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc\u4f5c\u4e3a\u5fae\u8c03\u7f51\u7edc\u3002\u6839\u636e\u5b9e\u9645\u7ecf\u9a8c, \u81ea\u5b9a\u4e49\u7684\u5fae\u8c03\u7f51\u7edc\u53c2\u6570\u603b\u6570\u5e94\u5927\u4e8e0.5\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u5c0f\u4e8e10\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u8fd9\u6837\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u6536\u655b \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b # \u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b class MyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () # \u5b9a\u4e49\u5168\u8fde\u63a5\u5c42 self . fc = torch . nn . Linear ( 768 , 2 ) def forward ( self , input_ids , attention_mask , token_type_ids ): # \u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u8bad\u7ec3 \u53ea\u8fdb\u884c\u7279\u5f81\u62bd\u53d6 [8,500] ---> [8,768] with torch . no_grad (): out = my_model_pretrained ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3 \u6570\u636e\u7ecf\u8fc7\u5168\u8fde\u63a5\u5c42 [8,768] --> [8,2] out = self . fc ( out . last_hidden_state [:, 0 ]) # \u6570\u636e\u8fdb\u884csoftmax\u5f52\u4e00\u5316 \u5206\u7c7b\u6982\u7387\u503c out = out . softmax ( dim = 1 ) return out \u6a21\u578b\u6d4b\u8bd5 # \u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u8f93\u5165\u548c\u8f93\u51fa\u6d4b\u8bd5 def dm02_test_mymodel (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90 \u901a\u8fc7\u8bad\u7ec3\u6587\u4ef6 dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) # print('dataset_train--->', dataset_train) # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668 mydataloader mydataloader = torch . utils . data . DataLoader ( dataset_train , batch_size = 8 , collate_fn = collate_fn1 , shuffle = False , drop_last = True ) # print('mydataloader--->', len(mydataloader)) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b mymodel = MyModel () print ( 'mymodel--->' , mymodel ) # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( len ( mydataloader )) # print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels) # \u6570\u636e\u9001\u7ed9\u6a21\u578b y_out = mymodel ( input_ids , attention_mask , token_type_ids ) print ( 'y_out---->' , y_out . shape , y_out ) break \u8f93\u51fa\u6548\u679c # \u6a21\u578b\u4fe1\u606f\u6253\u5370 mymodel---> MyModel( (fc): Linear(in_features=768, out_features=2, bias=True) ) # \u6a21\u578b\u8fd0\u7b97\u540e\u5206\u7c7b\u7ed3\u679c\u5c55\u793a y_out----> torch.Size([8, 2]) tensor([[0.4062, 0.5938], [0.2788, 0.7212], [0.3671, 0.6329], [0.2496, 0.7504], [0.2995, 0.7005], [0.2566, 0.7434], [0.2537, 0.7463], [0.3832, 0.6168]], grad_fn=<SoftmaxBackward>) 5 \u6a21\u578b\u8bad\u7ec3 \u00b6 # \u6a21\u578b\u8bad\u7ec3 def dm03_train_model (): # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model my_model = MyModel () # \u5b9e\u4f8b\u5316\u4f18\u5316\u5668my_optimizer my_optimizer = AdamW ( my_model . parameters (), lr = 5e-4 ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570my_criterion my_criterion = torch . nn . CrossEntropyLoss () # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train my_dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) print ( 'dataset_train--->' , my_dataset_train ) # \u4e0d\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b \u53ea\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6570\u636e\u7279\u5f81 \u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 epochs = 3 # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b my_model . train () # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for eporch_idx in range ( epochs ): # \u6bcf\u6b21\u8f6e\u6b21\u5f00\u59cb\u8ba1\u7b97\u65f6\u95f4 starttime = ( int )( time . time ()) # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61my_dataloader my_dataloader = torch . utils . data . DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn1 , shuffle = True , drop_last = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_dataloader , start = 1 ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,500] --> [8,2] my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8ba1\u7b97\u635f\u5931 my_loss = my_criterion ( my_out , labels ) # \u68af\u5ea6\u6e05\u96f6 my_optimizer . zero_grad () # \u53cd\u5411\u4f20\u64ad my_loss . backward () # \u68af\u5ea6\u66f4\u65b0 my_optimizer . step () # \u6bcf5\u6b21\u8fed\u4ee3 \u7b97\u4e00\u4e0b\u51c6\u786e\u7387 if i % 5 == 0 : out = my_out . argmax ( dim = 1 ) # [8,2] --> (8,) accuracy = ( out == labels ) . sum () . item () / len ( labels ) print ( '\u8f6e\u6b21: %d \u8fed\u4ee3\u6570: %d \u635f\u5931: %.6f \u51c6\u786e\u7387 %.3f \u65f6\u95f4 %d ' \\ % ( eporch_idx , i , my_loss . item (), accuracy , ( int )( time . time ()) - starttime )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_model . state_dict (), './my_model_ %d .bin' % ( eporch_idx + 1 )) \u6a21\u578b\u8bad\u7ec3\u6548\u679c\u8f93\u51fa \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 5 \u635f\u5931 : 0.735494 \u51c6\u786e\u73870 .250 \u65f6\u95f440 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 10 \u635f\u5931 : 0.614211 \u51c6\u786e\u73870 .875 \u65f6\u95f481 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 15 \u635f\u5931 : 0.635408 \u51c6\u786e\u73870 .750 \u65f6\u95f4119 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 20 \u635f\u5931 : 0.575522 \u51c6\u786e\u73871 .000 \u65f6\u95f4157 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 25 \u635f\u5931 : 0.661196 \u51c6\u786e\u73870 .625 \u65f6\u95f4196 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 30 \u635f\u5931 : 0.546462 \u51c6\u786e\u73870 .875 \u65f6\u95f4234 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 35 \u635f\u5931 : 0.609517 \u51c6\u786e\u73870 .875 \u65f6\u95f4272 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 40 \u635f\u5931 : 0.529246 \u51c6\u786e\u73871 .000 \u65f6\u95f4310 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 45 \u635f\u5931 : 0.474820 \u51c6\u786e\u73871 .000 \u65f6\u95f4348 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 50 \u635f\u5931 : 0.540127 \u51c6\u786e\u73870 .875 \u65f6\u95f4387 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 55 \u635f\u5931 : 0.575326 \u51c6\u786e\u73870 .625 \u65f6\u95f4426 # \u4ece\u4ee5\u4e0a\u7684\u8bad\u7ec3\u8f93\u51fa\u6548\u679c\u6765\u770b\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u5341\u5206\u5f3a\u5927\u7684\uff0c\u53ea\u9700\u8981\u77ed\u77ed\u7684\u51e0\u6b21\u8fed\u4ee3\uff0c\u5c31\u53ef\u4ee5\u8ba9\u51c6\u786e\u7387\u4e0a88% 6 \u6a21\u578b\u8bc4\u4f30 \u00b6 # \u6a21\u578b\u6d4b\u8bd5 def dm04_evaluate_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test print ( ' \\n \u52a0\u8f7d\u6d4b\u8bd5\u96c6' ) my_dataset_test = load_dataset ( 'csv' , data_files = './mydata1/test.csv' , split = 'train' ) print ( 'my_dataset_test--->' , my_dataset_test ) # print(my_dataset_test[0:3]) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model path = './my_model_3.bin' my_model = MyModel () my_model . load_state_dict ( torch . load ( path )) print ( 'my_model-->' , my_model ) # \u8bbe\u7f6e\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f my_model . eval () # \u8bbe\u7f6e\u8bc4\u4f30\u53c2\u6570 correct = 0 total = 0 # \u5b9e\u4f8b\u5316\u5316dataloader my_loader_test = torch . utils . data . DataLoader ( my_dataset_test , batch_size = 8 , collate_fn = collate_fn1 , shuffle = True , drop_last = True ) # \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_loader_test ): # \u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u62bd\u53d6 with torch . no_grad (): my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8d2a\u5fc3\u7b97\u6cd5\u6c42\u9884\u6d4b\u7ed3\u679c out = my_out . argmax ( dim = 1 ) # \u8ba1\u7b97\u51c6\u786e\u7387 correct += ( out == labels ) . sum () . item () total += len ( labels ) # \u6bcf5\u6b21\u8fed\u4ee3\u6253\u5370\u4e00\u6b21\u51c6\u786e\u7387 if i % 5 == 0 : print ( correct / total , end = \" \" ) print ( my_tokenizer . decode ( input_ids [ 0 ], skip_special_tokens = True ), end = \" \" ) print ( '\u9884\u6d4b\u503c \u771f\u5b9e\u503c:' , out [ 0 ] . item (), labels [ 0 ] . item ()) \u8f93\u51fa\u6548\u679c: 0.875 \u6211 \u6ca1 \u6709 \u6536 \u5230 \u8fd9 \u672c \u4e66 \uff0c \u6211 \u660e \u660e \u548c \u53e6 \u5916 \u4e00 \u672c \u4e00 \u8d77 \u4e70 \u7684 \uff0c \u670d \u52a1 \u4e5f \u4e0d \u597d \u3002 \u81f3 \u5c11 \u5e94 \u8be5 \u8ba9 \u6211 \u77e5 \u9053 \u8fd9 \u4e2a \u60c5 \u51b5 \u5728 \uff01 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 0 0 0.8125 \u4e5f \u8bb8 \u8fd9 \u4e0d \u7b97 \u4e00 \u4e2a \u5f88 \u597d \u7684 \u7406 \u7531, \u4f46 \u662f \u6211 \u4e4b \u6240 \u4ee5 \u559c \u6b22 \u8bfb \u4e66 \u800c \u4e0d \u662f \u770b \u7f51 \u4e0a \u7684 \u8d44 \u6599 \u4ec0 \u4e48 \u7684, \u5c31 \u662f \u559c \u6b22 \u95fb \u7740 \u4e66 \u9999. \u8fd9 \u672c \u4e66 \u53ef \u80fd \u662f \u5370 \u5237 \u7684 \u6cb9 \u58a8 \u4e0d \u597d \u8fd8 \u662f \u4ec0 \u4e48 \u539f \u56e0, \u611f \u89c9 \u81ed \u81ed \u7684 \u4e0d \u597d \u95fb. \u91cc \u9762 \u662f \u4e00 \u4e9b \u5173 \u4e8e \u4e2d \u5f0f \u82f1 \u8bed \u7684 \u5c0f \u8da3 \u95fb, \u6709 \u4e9b \u5c0f \u4e50 \u8da3, \u4f46 \u611f \u89c9 \u5bf9 \u4e8e \u6709 \u6d53 \u91cd \u4e2d \u5f0f \u601d \u7ef4 \u4e60 \u60ef \u8bf4 \u82f1 \u8bf4 \u7684 \u4eba \u6765 \u8bf4 \u624d \u6bd4 \u8f83 \u6709 \u70b9 \u7528 \u5904. \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 1 0 0.8409090909090909 1. \u6709 \u6025 \u4e8b \u51fa \u53bb \uff0c \u8981 \u4eec \u7ae5 \u53eb \u51fa \u79df \u8f66 \uff0c \u4ed6 \u4eec \u5c31 \u53eb \u9152 \u5e97 \u91cc \u7684 \u9ed1 \u8f66 \uff0c \u4ef7 \u683c \u662f \u666e \u901a \u51fa \u79df \u4ef7 \u7684 \u4e24 \u500d \u3002 \u4f60 \u63d0 \u51fa \u4e0d \u8981 \u9152 \u5e97 \u7684 \u9ed1 \u8f66 \u65f6 \uff0c \u4ed6 \u4eec \u5c31 \u544a \u8bc9 \u4f60 \u5916 \u9762 \u62e6 \u4e0d \u5230 \u51fa \u79df \u8f66 \uff0c \u6211 \u4eec \u81ea \u5df1 \u8d70 \u51fa \u53bb \u65f6 \uff0c \u5916 \u9762 \u51fa \u79df \u8f66 \u968f \u65f6 \u53ef \u4ee5 \u62e6 \u5230 \u3002 \u4f4f \u5e97 \u671f \u95f4 \u4e0d \u6b62 \u4e00 \u6b21 \u53d1 \u751f \u3002 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 0 0 0.828125 \u8fd9 \u672c \u4e66 \u4e2d \u7684 \u56fe \u7247 \u5f88 \u8ba9 \u4eba \u89e6 \u52a8 \uff0c \u6bd4 \u5982 \u4e3a \u52b3 \u62c9 \u7948 \u7977 \u7684 \u6821 \u53cb \uff0c \u60e0 \u7279 \u5c3c \u548c \u9a6c \u7279 \u7684 \u7b11 \u5bb9 \uff0c \u5f88 \u52a8 \u4eba \u3002 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 1 1 0.8511904761904762 \u8fd9 \u662f \u6211 \u4f4f \u8fc7 \u7684 \u6700 \u5dee \u7684 \u9152 \u5e97 \uff0c \u623f \u95f4 \u6c14 \u5473 \u96be \u95fb \uff0c \u521a \u6253 \u4e86 \u706d \u868a \u836f \u6c34 \uff0c \u6362 \u4e86 \u4e09 \u4e2a \u623f \u95f4 \u8fd8 \u662f \u5982 \u6b64 \uff0c \u670d \u52a1 \u5458 \u8bf4 \uff1a \u4f4f \u4e45 \u4e86 \u5c31 \u4e60 \u60ef \u4e86 \uff0c \u6bcf \u4e2a \u5bbe \u9986 \u90fd \u6709 \u81ea \u5df1 \u7684 \u5473 \u9053 \u3002 \u8003 \uff01 \u6211 \u53c8 \u4e0d \u662f \u6765 \u4f53 \u9a8c \u751f \u6d3b \u7684 \u3002 \u5468 \u56f4 \u73af \u5883 \u590d \u6742 \uff0c \u810f \u4e71 \u5dee \u3002 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 0 0 3 \u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u586b\u7a7a \u00b6 1 \u4efb\u52a1\u4ecb\u7ecd \u00b6 # \u8f93\u5165\u4e00\u53e5\u8bdd\uff0cMASK\u4e00\u4e2a\u5b57\uff0c\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u586b\u7a7a [ CLS ] \u9009 \u62e9 \u73e0 \u6c5f \u82b1 \u56ed \u7684 \u539f \u56e0 \u5c31 \u662f \u65b9 \u4fbf \uff0c \u6709 [ MASK ] \u52a8 \u6276 \u68af \u76f4 \u63a5 \u5230 \u8fbe \u6d77 \u8fb9 \uff0c \u5468 \u56f4 \u9910 \u9986 [ SEP ] # \u672c\u53e5MASK\u7684\u5b57\u4e3a\u201c\u7535\u201d 2 \u6570\u636e\u4ecb\u7ecd \u00b6 \u6570\u636e\u6587\u4ef6\u6709\u4e09\u4e2atrain.csv\uff0ctest.csv\uff0cvalidation.csv\uff0c\u6570\u636e\u6837\u5f0f\u90fd\u662f\u4e00\u6837\u7684\u3002 label , text 1 , \u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf \uff0c \u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9 \uff0c \u5468\u56f4\u9910\u9986 \u3001 \u98df\u5eca \u3001 \u5546\u573a \u3001 \u8d85\u5e02 \u3001 \u644a\u4f4d\u4e00\u5e94\u4ff1\u5168 \u3002 \u9152\u5e97\u88c5\u4fee\u4e00\u822c \uff0c \u4f46\u8fd8\u7b97\u6574\u6d01 \u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876 \uff0c \u56e0\u6b64\u5f88\u5c0f \uff0c \u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22 \u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684 \uff0c \u8fd8\u7b97\u4e30\u5bcc \u3002 \u670d\u52a1\u5417 \uff0c \u4e00\u822c 1 , 15.4 \u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d \uff0c \u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86 \uff0c \u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8 \uff0c \u8f93\u6570\u5b57\u7279\u65b9\u4fbf \uff0c \u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2 \uff0c \u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519 0 , \u623f\u95f4\u592a\u5c0f \u3002 \u5176\u4ed6\u7684\u90fd\u4e00\u822c \u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002 0 , \"1.\u63a5\u7535\u6e90\u6ca1\u6709\u51e0\u5206\u949f,\u7535\u6e90\u9002\u914d\u5668\u70ed\u7684\u4e0d\u884c. 2.\u6444\u50cf\u5934\u7528\u4e0d\u8d77\u6765. 3.\u673a\u76d6\u7684\u94a2\u7434\u6f06\uff0c\u624b\u4e0d\u80fd\u6478\uff0c\u4e00\u6478\u4e00\u4e2a\u5370. 4.\u786c\u76d8\u5206\u533a\u4e0d\u597d\u529e.\" 1 , \"\u4eca\u5929\u624d\u77e5\u9053\u8fd9\u4e66\u8fd8\u6709\u7b2c6\u5377,\u771f\u6709\u70b9\u90c1\u95f7:\u4e3a\u4ec0\u4e48\u540c\u4e00\u5957\u4e66\u6709\u4e24\u79cd\u7248\u672c\u5462?\u5f53\u5f53\u7f51\u662f\u4e0d\u662f\u8be5\u8ddf\u51fa\u7248\u793e\u5546\u91cf\u5546\u91cf,\u5355\u72ec\u51fa\u4e2a\u7b2c6\u5377,\u8ba9\u6211\u4eec\u7684\u5b69\u5b50\u4e0d\u4f1a\u6709\u6240\u9057\u61be\u3002\" \u5bfc\u5165\u5de5\u5177\u5305\u548c\u8f85\u52a9\u5de5\u5177\u5b9e\u4f8b\u5316\u5bf9\u8c61 import torch from torch.utils.data import DataLoader from datasets import load_dataset from transformers import BertTokenizer , BertModel from transformers import AdamW import time # \u52a0\u8f7d\u5b57\u5178\u548c\u5206\u8bcd\u5de5\u5177 my_tokenizer = BertTokenizer . from_pretrained ( 'bert-base-chinese' ) # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b my_model_pretrained = BertModel . from_pretrained ( 'bert-base-chinese' ) 3 \u6570\u636e\u9884\u5904\u7406 \u00b6 \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42\u3002 \u6570\u636e\u9884\u5904\u7406\u548c\u76f8\u5173\u6d4b\u8bd5\u51fd\u6570 # \u6570\u636e\u96c6\u5904\u7406\u81ea\u5b9a\u4e49\u51fd\u6570 def collate_fn2 ( data ): sents = [ i [ 'text' ] for i in data ] # \u6587\u672c\u6570\u503c\u5316 data = my_tokenizer . batch_encode_plus ( batch_text_or_text_pairs = sents , truncation = True , padding = 'max_length' , max_length = 32 , return_tensors = 'pt' , return_length = True ) # input_ids \u7f16\u7801\u4e4b\u540e\u7684\u6570\u5b57 # attention_mask \u662f\u8865\u96f6\u7684\u4f4d\u7f6e\u662f0,\u5176\u4ed6\u4f4d\u7f6e\u662f1 input_ids = data [ 'input_ids' ] attention_mask = data [ 'attention_mask' ] token_type_ids = data [ 'token_type_ids' ] # \u628a\u7b2c16\u4e2a\u8bcd\u56fa\u5b9a\u66ff\u6362\u4e3amask labels = input_ids [:, 16 ] . reshape ( - 1 ) . clone () # \u53d6\u51fa\u6570\u636e8\u53e5\u8bdd \u5728\u7b2c16\u4e2a\u4f4d\u7f6eclone\u51fa\u6765 \u505a\u6807\u7b7e input_ids [:, 16 ] = my_tokenizer . get_vocab ()[ my_tokenizer . mask_token ] labels = torch . LongTensor ( labels ) # tmpa = input_ids[:, 16] # print('tmpa--->', tmpa, tmpa.shape) # torch.Size([8] # print('labels-->', labels.shape, labels) # torch.Size([8] return input_ids , attention_mask , token_type_ids , labels # \u6570\u636e\u6e90 \u6570\u636e\u8fed\u4ee3\u5668 \u6d4b\u8bd5 def dm01_test_dataset (): # \u751f\u6210\u6570\u636e\u6e90dataset\u5bf9\u8c61 dataset_train_tmp = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) # print('dataset_train_tmp--->', dataset_train_tmp) # \u6309\u7167\u6761\u4ef6\u8fc7\u6ee4\u6570\u636e\u6e90\u5bf9\u8c61 my_dataset_train = dataset_train_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) # print('my_dataset_train--->', my_dataset_train) # print('my_dataset_train[0:3]-->', my_dataset_train[0:3]) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) # \u4e0d\u8bad\u7ec3,\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) print ( ' \\n \u7b2c1\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 0 ])) print ( my_tokenizer . decode ( labels [ 0 ])) print ( ' \\n \u7b2c2\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 1 ])) print ( my_tokenizer . decode ( labels [ 1 ])) break \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c mydataloader ---> < torch . utils . data . dataloader . DataLoader object at 0x7fbe28611cd0 > torch . Size ([ 8 , 32 ]) torch . Size ([ 8 , 32 ]) torch . Size ([ 8 , 32 ]) tensor ([ 3160 , 1905 , 8152 , 7415 , 7231 , 1331 , 2360 , 5831 ]) \u7b2c1\u53e5mask\u7684\u4fe1\u606f [ CLS ] \u6587 \u7ae0 \u603b \u4f53 \u4e0d \u9519 \uff0c \u4e0d \u8fc7 \u592a \u591a \u7684 \u5386 \u53f2 \u8d44 [ MASK ] \u91cd \u590d \uff0c \u611f \u89c9 \u6ca1 \u6709 \u6b65 \u6b65 \u597d \u770b \u4e86 \uff0c \u4f5c [ SEP ] \u6599 \u7b2c2\u53e5mask\u7684\u4fe1\u606f [ CLS ] \u7b2c \u4e00 \u6563 \u70ed \u4e00 \u822c \u3002 \u3002 \u3002 \u7528 \u4e86 \u4e00 \u4f1a \u6258 \u8155 [ MASK ] \u5c31 \u53d1 \u70eb \u4e86 ? \u7b2c \u4e8c \u6db2 \u6676 \u5c4f \u548c \u4e3b \u673a \u90e8 [ SEP ] \u5904 \u8fc1\u79fb\u5b66\u4e60 \u4e2d\u6587\u586b\u7a7a End 4 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b \u00b6 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b # \u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b class MyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () # \u5b9a\u4e49\u5168\u8fde\u63a5\u5c42 self . decoder = torch . nn . Linear ( 768 , my_tokenizer . vocab_size , bias = False ) # \u8bbe\u7f6e\u5168\u8fde\u63a5\u5c42\u504f\u7f6e\u4e3a\u96f6 self . decoder . bias = torch . nn . Parameter ( torch . zeros ( my_tokenizer . vocab_size )) def forward ( self , input_ids , attention_mask , token_type_ids ): # \u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u8fdb\u884c\u8bad\u7ec3 with torch . no_grad (): out = my_model_pretrained ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3 \u5f62\u72b6[8,768] ---> [8, 21128] out = self . decoder ( out . last_hidden_state [:, 16 ]) # \u8fd4\u56de return out \u6a21\u578b\u6d4b\u8bd5 # \u6a21\u578b\u8f93\u5165\u548c\u8f93\u51fa\u6d4b\u8bd5 def dm02_test_mymodel (): # \u751f\u6210\u6570\u636e\u6e90dataset\u5bf9\u8c61 dataset_train_tmp = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) # print('dataset_train_tmp--->', dataset_train_tmp) # \u6309\u7167\u6761\u4ef6\u8fc7\u6ee4\u6570\u636e\u6e90\u5bf9\u8c61 my_dataset_train = dataset_train_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) # print('my_dataset_train--->', my_dataset_train) # print('my_dataset_train[0:3]-->', my_dataset_train[0:3]) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) # \u4e0d\u8bad\u7ec3,\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b mymodel = MyModel () # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) print ( ' \\n \u7b2c1\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 0 ])) print ( my_tokenizer . decode ( labels [ 0 ])) print ( ' \\n \u7b2c2\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 1 ])) print ( my_tokenizer . decode ( labels [ 1 ])) # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,768] ---> [8,21128] \u586b\u7a7a\u5c31\u662f\u5206\u7c7b 21128\u4e2a\u5355\u8bcd\u4e2d\u627e\u4e00\u4e2a\u5355\u8bcd myout = mymodel ( input_ids , attention_mask , token_type_ids ) print ( 'myout--->' , myout . shape , myout ) break \u8f93\u51fa\u6548\u679c mydataloader---> <torch.utils.data.dataloader.DataLoader object at 0x7fd7c0765c10> torch.Size([8, 32]) torch.Size([8, 32]) torch.Size([8, 32]) tensor([ 702, 6381, 857, 8024, 2218, 1961, 3175, 2141]) \u7b2c1\u53e5mask\u7684\u4fe1\u606f [CLS] \u5f88 \u91cd, \u80cc \u51fa \u53bb \u5f88 \u7d2f. \u597d \u8c61 \u5c4f \u5e55 \u4e0a \u6709 [MASK] \u4eae \u70b9, \u4e0d \u8fc7 \u5f88 \u5c0f ( \u5c4f \u672c \u6765 \u5c31 \u5f88 \u5c0f [SEP] \u4e2a \u7b2c2\u53e5mask\u7684\u4fe1\u606f [CLS] \u6211 \u4e8e 11 \u6708 22 \u65e5 \u8ba2 \u8d2d \u4e86 \u300a \u675c \u62c9 \u62c9 \u5347 \u804c [MASK] \u300b \u5e76 \u5df2 \u901a \u8fc7 \u94f6 \u884c \u4ed8 \u6b3e \uff0c \u4e3a \u4ec0 \u4e48 \u8ba2 [SEP] \u8bb0 myout---> torch.Size([8, 21128]) tensor([[-0.3201, 0.3877, 0.1041, ..., 0.2262, 0.5397, 0.5053], [ 0.0626, 0.1335, 0.7057, ..., -0.6277, -0.2287, -0.0532], [ 0.3807, 0.2024, 0.0514, ..., -0.0113, 0.3084, 0.4678], ..., [ 0.3452, 0.1774, 0.0127, ..., -0.3960, 0.2417, -0.0260], [-0.4155, 0.2038, 0.2512, ..., -0.4112, -0.1052, 0.3574], [ 0.3464, 0.3439, 0.6628, ..., -0.1706, 0.1020, 0.4141]], grad_fn=<AddmmBackward>) 5 \u6a21\u578b\u8bad\u7ec3 \u00b6 # \u6a21\u578b\u8bad\u7ec3 - \u586b\u7a7a def dm03_train_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train dataset_train_tmp = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) my_dataset_train = dataset_train_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) print ( 'my_dataset_train--->' , my_dataset_train ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model my_model = MyModel () # \u5b9e\u4f8b\u5316\u4f18\u5316\u5668my_optimizer my_optimizer = AdamW ( my_model . parameters (), lr = 5e-4 ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570my_criterion my_criterion = torch . nn . CrossEntropyLoss () # \u4e0d\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b \u53ea\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6570\u636e\u7279\u5f81 \u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 epochs = 3 # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b my_model . train () # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for eporch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61my_dataloader my_dataloader = torch . utils . data . DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) starttime = ( int )( time . time ()) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_dataloader , start = 1 ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,32] --> [8,21128] my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8ba1\u7b97\u635f\u5931 my_loss = my_criterion ( my_out , labels ) # \u68af\u5ea6\u6e05\u96f6 my_optimizer . zero_grad () # \u53cd\u5411\u4f20\u64ad my_loss . backward () # \u68af\u5ea6\u66f4\u65b0 my_optimizer . step () # \u6bcf5\u6b21\u8fed\u4ee3 \u7b97\u4e00\u4e0b\u51c6\u786e\u7387 if i % 20 == 0 : out = my_out . argmax ( dim = 1 ) # [8,21128] --> (8,) accuracy = ( out == labels ) . sum () . item () / len ( labels ) print ( '\u8f6e\u6b21: %d \u8fed\u4ee3\u6570: %d \u635f\u5931: %.6f \u51c6\u786e\u7387 %.3f \u65f6\u95f4 %d ' \\ % ( eporch_idx , i , my_loss . item (), accuracy , ( int )( time . time ()) - starttime )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_model . state_dict (), './my_model_mask_ %d .bin' % ( eporch_idx + 1 )) \u6a21\u578b\u8bad\u7ec3\u6548\u679c\u8f93\u51fa \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 680 \u635f\u5931 : 0.324525 \u51c6\u786e\u73870 .875 \u65f6\u95f4370 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 700 \u635f\u5931 : 0.776103 \u51c6\u786e\u73870 .750 \u65f6\u95f4381 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 720 \u635f\u5931 : 0.681674 \u51c6\u786e\u73870 .875 \u65f6\u95f4392 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 740 \u635f\u5931 : 0.654384 \u51c6\u786e\u73870 .750 \u65f6\u95f4402 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 760 \u635f\u5931 : 0.612616 \u51c6\u786e\u73870 .875 \u65f6\u95f4413 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 780 \u635f\u5931 : 0.918874 \u51c6\u786e\u73870 .625 \u65f6\u95f4424 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 800 \u635f\u5931 : 0.640087 \u51c6\u786e\u73870 .750 \u65f6\u95f4435 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 820 \u635f\u5931 : 0.410612 \u51c6\u786e\u73871 .000 \u65f6\u95f4446 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 840 \u635f\u5931 : 0.395016 \u51c6\u786e\u73871 .000 \u65f6\u95f4457 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 860 \u635f\u5931 : 0.313001 \u51c6\u786e\u73870 .875 \u65f6\u95f4468 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 880 \u635f\u5931 : 1.534165 \u51c6\u786e\u73870 .750 \u65f6\u95f4479 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 900 \u635f\u5931 : 0.384014 \u51c6\u786e\u73870 .875 \u65f6\u95f4490 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 920 \u635f\u5931 : 0.386283 \u51c6\u786e\u73871 .000 \u65f6\u95f4501 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 940 \u635f\u5931 : 1.168535 \u51c6\u786e\u73870 .750 \u65f6\u95f4512 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 960 \u635f\u5931 : 0.568617 \u51c6\u786e\u73870 .875 \u65f6\u95f4522 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 980 \u635f\u5931 : 1.178597 \u51c6\u786e\u73870 .750 \u65f6\u95f4533 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1000 \u635f\u5931 : 0.534274 \u51c6\u786e\u73870 .875 \u65f6\u95f4544 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1020 \u635f\u5931 : 0.371301 \u51c6\u786e\u73871 .000 \u65f6\u95f4555 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1040 \u635f\u5931 : 0.106059 \u51c6\u786e\u73871 .000 \u65f6\u95f4566 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1060 \u635f\u5931 : 1.153722 \u51c6\u786e\u73870 .750 \u65f6\u95f4577 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1080 \u635f\u5931 : 0.352150 \u51c6\u786e\u73871 .000 \u65f6\u95f4588 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1100 \u635f\u5931 : 1.242567 \u51c6\u786e\u73870 .625 \u65f6\u95f4598 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1120 \u635f\u5931 : 0.548795 \u51c6\u786e\u73870 .875 \u65f6\u95f4609 # \u53ea\u9700\u89813\u4e2a\u8f6e\u6b21\uff0c\u5c31\u53ef\u4ee5\u586b\u7a7a\u51c6\u786e\u7387\u8fbe\u523088%\u4ee5\u4e0a 6 \u6a21\u578b\u8bc4\u4f30 \u00b6 # \u6a21\u578b\u6d4b\u8bd5\uff1a\u586b\u7a7a def dm04_evaluate_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test print ( ' \\n \u52a0\u8f7d\u6d4b\u8bd5\u96c6' ) my_dataset_tmp = load_dataset ( 'csv' , data_files = './mydata1/test.csv' , split = 'train' ) my_dataset_test = my_dataset_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) print ( 'my_dataset_test--->' , my_dataset_test ) # print(my_dataset_test[0:3]) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model path = './my_model_mask_3.bin' my_model = MyModel () my_model . load_state_dict ( torch . load ( path )) print ( 'my_model-->' , my_model ) # \u8bbe\u7f6e\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f my_model . eval () # \u8bbe\u7f6e\u8bc4\u4f30\u53c2\u6570 correct = 0 total = 0 # \u5b9e\u4f8b\u5316\u5316dataloader my_loader_test = torch . utils . data . DataLoader ( my_dataset_test , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) # \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_loader_test ): with torch . no_grad (): my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) out = my_out . argmax ( dim = 1 ) correct += ( out == labels ) . sum () . item () total += len ( labels ) if i % 25 == 0 : print ( i + 1 , my_tokenizer . decode ( input_ids [ 0 ])) print ( '\u9884\u6d4b\u503c:' , my_tokenizer . decode ( out [ 0 ]), ' \\t \u771f\u5b9e\u503c:' , my_tokenizer . decode ( labels [ 0 ])) print ( correct / total ) \u8f93\u51fa\u6548\u679c: 26 [CLS] \u5927 \u5802 \u662f \u55a7 \u95f9 \u7684 \uff0c \u623f \u95f4 \u5730 \u6bef \u662f \u810f \u7684 \uff0c [MASK] \u5ba4 \u6dcb \u6d74 \u9f99 \u5934 \u662f \u574f \u7684 \uff0c \u6bdb \u5dfe \u662f \u9ec4 \u989c [SEP] \u9884\u6d4b\u503c: \u5bdd \u771f\u5b9e\u503c: \u6d74 0.62 51 [CLS] \u5916 \u89c2 \u6f02 \u4eae \uff0c 13 \u5bf8 \u9ec4 \u91d1 \u6bd4 \u4f8b \uff0c \u60ac \u6d6e \u952e [MASK] \u8bbe \u8ba1 \uff0c \u566a \u97f3 \u5f88 \u4f4e \uff0c \u548c \u6211 \u516c \u53f8 \u53d1 \u7684 [SEP] \u9884\u6d4b\u503c: \u76d8 \u771f\u5b9e\u503c: \u76d8 0.6625 76 [CLS] \u67d0 \u4e9b \u9152 \u5e97 \u4eba \u5458 \u5bf9 \u5f85 \u987e \u5ba2 \u4e0d \u8bda \u6073 \u3002 \u8bf4 [MASK] \u4f60 \u6362 \u623f \u95f4 \uff0c \u9a97 \u4f60 \u8bf4 \u662f \u4ef7 \u94b1 \u8d35 \u7684 \u623f [SEP] \u9884\u6d4b\u503c: \u8ba9 \u771f\u5b9e\u503c: \u7ed9 0.68 101 [CLS] \u770b \u8fd9 \u672c \u4e66 \u6709 \u79cd \u611f \u89c9 \u5c31 \u662f \u4f5c \u8005 \u5728 \u82e6 \u53e3 [MASK] \u5fc3 \u7684 \u544a \u6212 \u8bfb \u8005 \u5982 \u4f55 \u5982 \u4f55 \uff0c \u4e3a \u4e86 \u8bc1 [SEP] \u9884\u6d4b\u503c: \u5a46 \u771f\u5b9e\u503c: \u5a46 0.6775 126 [CLS] \u623f \u95f4 \u5f88 \u8212 \u9002 \u4e5f \u5f88 \u5e72 \u51c0 \uff0c \u5468 \u56f4 \u867d \u7136 \u4e0d [MASK] \u95f9 \u4f46 \u5f88 \u5b89 \u9759 \uff0c \u79bb \u8679 \u6865 \u673a \u573a \u5f88 \u9759 \uff0c [SEP] \u9884\u6d4b\u503c: \u5435 \u771f\u5b9e\u503c: \u70ed 0.681 4 \u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb \u00b6 1 \u4efb\u52a1\u4ecb\u7ecd \u00b6 \u4e0b\u4e00\u53e5\u8bdd\u4efb\u52a1NSP ( Next Sentence Prediction ) \u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1 \u3002 \u8f93\u51652\u53e5\u8bdd \uff0c \u5224\u65ad\u7b2c\u4e8c\u53e5\u662f\u5426\u4e3a\u7b2c\u4e00\u53e5\u7684\u4e0b\u534a\u53e5 \u3002 2 \u6570\u636e\u4ecb\u7ecd \u00b6 \u6570\u636e\u6587\u4ef6\u6709\u4e09\u4e2atrain.csv\uff0ctest.csv\uff0cvalidation.csv\uff0c\u6570\u636e\u6837\u5f0f\u90fd\u662f\u4e00\u6837\u7684\u3002 label , text 1 , \u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf \uff0c \u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9 \uff0c \u5468\u56f4\u9910\u9986 \u3001 \u98df\u5eca \u3001 \u5546\u573a \u3001 \u8d85\u5e02 \u3001 \u644a\u4f4d\u4e00\u5e94\u4ff1\u5168 \u3002 \u9152\u5e97\u88c5\u4fee\u4e00\u822c \uff0c \u4f46\u8fd8\u7b97\u6574\u6d01 \u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876 \uff0c \u56e0\u6b64\u5f88\u5c0f \uff0c \u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22 \u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684 \uff0c \u8fd8\u7b97\u4e30\u5bcc \u3002 \u670d\u52a1\u5417 \uff0c \u4e00\u822c 1 , 15.4 \u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d \uff0c \u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86 \uff0c \u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8 \uff0c \u8f93\u6570\u5b57\u7279\u65b9\u4fbf \uff0c \u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2 \uff0c \u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519 0 , \u623f\u95f4\u592a\u5c0f \u3002 \u5176\u4ed6\u7684\u90fd\u4e00\u822c \u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002 0 , \"1.\u63a5\u7535\u6e90\u6ca1\u6709\u51e0\u5206\u949f,\u7535\u6e90\u9002\u914d\u5668\u70ed\u7684\u4e0d\u884c. 2.\u6444\u50cf\u5934\u7528\u4e0d\u8d77\u6765. 3.\u673a\u76d6\u7684\u94a2\u7434\u6f06\uff0c\u624b\u4e0d\u80fd\u6478\uff0c\u4e00\u6478\u4e00\u4e2a\u5370. 4.\u786c\u76d8\u5206\u533a\u4e0d\u597d\u529e.\" 1 , \"\u4eca\u5929\u624d\u77e5\u9053\u8fd9\u4e66\u8fd8\u6709\u7b2c6\u5377,\u771f\u6709\u70b9\u90c1\u95f7:\u4e3a\u4ec0\u4e48\u540c\u4e00\u5957\u4e66\u6709\u4e24\u79cd\u7248\u672c\u5462?\u5f53\u5f53\u7f51\u662f\u4e0d\u662f\u8be5\u8ddf\u51fa\u7248\u793e\u5546\u91cf\u5546\u91cf,\u5355\u72ec\u51fa\u4e2a\u7b2c6\u5377,\u8ba9\u6211\u4eec\u7684\u5b69\u5b50\u4e0d\u4f1a\u6709\u6240\u9057\u61be\u3002\" \u5bfc\u5165\u5de5\u5177\u5305\u548c\u8f85\u52a9\u5de5\u5177\u5b9e\u4f8b\u5316\u5bf9\u8c61 import torch from torch.utils.data import DataLoader from datasets import load_dataset from transformers import BertTokenizer , BertModel from transformers import AdamW import time # \u52a0\u8f7d\u5b57\u5178\u548c\u5206\u8bcd\u5de5\u5177 my_tokenizer = BertTokenizer . from_pretrained ( 'bert-base-chinese' ) # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b my_model_pretrained = BertModel . from_pretrained ( 'bert-base-chinese' ) 3 \u6570\u636e\u9884\u5904\u7406 \u00b6 \u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42\u3002 \u6570\u636e\u9884\u5904\u7406\u548c\u76f8\u5173\u6d4b\u8bd5\u51fd\u6570 # \u5b9a\u4e49\u6570\u636e\u6e90\u7c7b class MyDataSet ( Dataset ): def __init__ ( self , data_csv_files ): # \u751f\u6210\u6570\u636e\u6e90dataset\u5bf9\u8c61 my_dataset_temp = load_dataset ( 'csv' , data_files = data_csv_files , split = \"train\" ) # print('my_dataset_temp--->', my_dataset_temp) # \u6309\u7167\u6761\u4ef6\u8fc7\u6ee4\u6570\u636e\u6e90\u5bf9\u8c61 self . my_dataset = my_dataset_temp . filter ( lambda x : len ( x [ 'text' ]) > 44 ) # print('self.my_dataset--->', self.my_dataset) # print('self.my_dataset[0:3]-->', self.my_dataset[0:3]) def __len__ ( self ): self . len = len ( self . my_dataset ) return self . len # 7472 def __getitem__ ( self , index ): # 1\u662f\u4e0b\u4e00\u53e5\u8bdd 0\u4e0d\u662f\u4e0b\u4e00\u53e5 label = 1 text = self . my_dataset [ index ][ 'text' ] sentence1 = text [ 0 : 22 ] sentence2 = text [ 22 : 44 ] # \u4ea7\u751f\u8d1f\u6837\u672c: \u968f\u673a\u4ea7\u751f0\u548c1 \u4e00\u822c\u6982\u7387\u9009\u4e2d0, \u66ff\u6362\u4e3a\u65e0\u5173\u7684\u4e00\u53e5\u8bdd if random . randint ( 0 , 1 ) == 0 : j = random . randint ( 0 , self . len - 1 ) sentence2 = self . my_dataset [ j ][ 'text' ][ 22 : 44 ] label = 0 # \u8fd4\u56de\u4e24\u53e5\u8bdd \u548c\u4e24\u53e5\u8bdd\u4e4b\u95f4\u7684\u5173\u7cfb return sentence1 , sentence2 , label # \u6570\u636e\u96c6\u5904\u7406\u81ea\u5b9a\u4e49\u51fd\u6570 def collate_fn3 ( data ): sents = [ i [: 2 ] for i in data ] labels = [ i [ 2 ] for i in data ] # \u6587\u672c\u6570\u503c\u5316 data = my_tokenizer . batch_encode_plus ( batch_text_or_text_pairs = sents , truncation = True , padding = 'max_length' , max_length = 50 , # 44+cls+sep+sep+other = 44+3=47 return_tensors = 'pt' , return_length = True ) # input_ids \u7f16\u7801\u4e4b\u540e\u7684\u6570\u5b57 # attention_mask \u662f\u8865\u96f6\u7684\u4f4d\u7f6e\u662f0,\u5176\u4ed6\u4f4d\u7f6e\u662f1 input_ids = data [ 'input_ids' ] attention_mask = data [ 'attention_mask' ] token_type_ids = data [ 'token_type_ids' ] # \u6ce8\u610flabels\u4e0d\u8981\u5fd8\u8bb0\u9700\u8981\u8f6c\u6210tensor 1\u7ef4\u6570\u7ec4 labels = torch . LongTensor ( labels ) return input_ids , attention_mask , token_type_ids , labels \u6570\u636e\u6e90 \u6570\u636e\u8fed\u4ee3\u5668\u6d4b\u8bd5 # \u5b9a\u4e49\u6570\u636e\u6e90 DataSet def dm01_test_dataset (): data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) print ( 'mydataset-->' , mydataset , len ( mydataset )) # print(mydataset[3]) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , Tshuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) for ( input_ids , attention_mask , token_type_ids , labels ) in mydataloader : print ( my_tokenizer . decode ( input_ids [ 0 ])) # \u6253\u5370\u6bcf\u4e2a\u6279\u6b21\u7684\u7b2c1\u53e5\u8bdd print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) break \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c mydataset --> < __main__ . MyDataSet object at 0x7fb4a8471ad0 > 7472 mydataloader ---> < torch . utils . data . dataloader . DataLoader object at 0x7fb44835d8d0 > # \u4e0b\u97622\u53e5\u8bdd \u4e0d\u662f\u4e0a\u4e0b\u6587\u5173\u7cfb [ CLS ] \u9009 \u62e9 \u73e0 \u6c5f \u82b1 \u56ed \u7684 \u539f \u56e0 \u5c31 \u662f \u65b9 \u4fbf \uff0c \u6709 \u7535 \u52a8 \u6276 \u68af \u76f4 \u63a5 \u5230 [ SEP ] \u8fbe \u6d77 \u8fb9 \uff0c \u5468 \u56f4 \u9910 \u9986 \u3001 \u98df \u5eca \u3001 \u5546 \u573a \u3001 \u8d85 \u5e02 \u3001 \u644a \u4f4d \u4e00 \u5e94 [ SEP ] [ PAD ] [ PAD ] [ PAD ] torch . Size ([ 8 , 50 ]) torch . Size ([ 8 , 50 ]) torch . Size ([ 8 , 50 ]) tensor ([ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 0 ]) # \u4e0b\u97622\u53e5\u8bdd \u662f\u4e0a\u7ebf\u6587\u5173\u7cfb [ CLS ] \u9009 \u62e9 \u73e0 \u6c5f \u82b1 \u56ed \u7684 \u539f \u56e0 \u5c31 \u662f \u65b9 \u4fbf \uff0c \u6709 \u7535 \u52a8 \u6276 \u68af \u76f4 \u63a5 \u5230 [ SEP ] \u5fd9 \u5fd9 \u788c \u788c \u9547 \u548c \u9752 \u86d9 \u5f17 \u6d1b \u683c \u7684 \u6210 \u957f \u6545 \u4e8b \uff0c \u90a3 \u51e0 \u672c \u4e66 \u6211 [ SEP ] [ PAD ] [ PAD ] [ PAD ] 4 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b \u00b6 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b # \u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bNSP class MyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () # \u5b9a\u4e49\u5168\u8fde\u63a5\u5c42 self . fc = torch . nn . Linear ( 768 , 2 ) def forward ( self , input_ids , attention_mask , token_type_ids ): # \u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u8bad\u7ec3 with torch . no_grad (): out = my_model_pretrained ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3 \u6570\u636e\u7ecf\u8fc7\u5168\u8fde\u63a5\u5c42 out = self . fc ( out . last_hidden_state [:, 0 ]) # \u6570\u636e\u8fdb\u884csoftmax\u5f52\u4e00\u5316 \u5206\u7c7b\u6982\u7387\u503c out = out . softmax ( dim = 1 ) return out \u6a21\u578b\u6d4b\u8bd5 # NSP\u6a21\u578b\u8f93\u5165\u548c\u8f93\u51fa\u6d4b\u8bd5 def dm02_test_mymodel (): data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) print ( 'mydataset-->' , mydataset , len ( mydataset )) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b mymodel = MyModel () # \u7ed9\u6a21\u578b\u5582\u6570\u636e for ( input_ids , attention_mask , token_type_ids , labels ) in mydataloader : print ( my_tokenizer . decode ( input_ids [ 0 ])) print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,768] ---> [8,2] nsp\u4efb\u52a1\u662f\u4e8c\u5206\u7c7b myout = mymodel ( input_ids , attention_mask , token_type_ids ) print ( 'myout--->' , myout . shape , myout ) break \u8f93\u51fa\u6548\u679c mydataset--> <__main__.MyDataSet object at 0x7fe4b86b8610> 7472 mydataloader---> <torch.utils.data.dataloader.DataLoader object at 0x7fe4d8b4ad50> # \u8fd92\u53e5\u8bdd \u7b2c2\u53e5\u8bdd\u662f\u7b2c1\u53e5\u8bdd\u7684\u4e0b\u6587 [CLS] \u6211 \u5bb6 \u5c0f \u5b9d \u540c \u5b66 \u7279 \u522b \u559c \u7231 \u5361 \u6885 \u62c9 \u7cfb \u5217 \u6545 \u4e8b, \u6bcf \u5929 \u665a \u4e0a [SEP] \u7761 \u89c9 \u524d \u975e \u8bb2 \u4e0d \u53ef, \u90a3 \u4e9b \u53ef \u7231 \u7684 \u6545 \u4e8b \u4f1a \u966a \u7740 \u4ed6 \u8fdb \u5165 \u68a6 [SEP] [PAD] [PAD] [PAD] torch.Size([8, 50]) torch.Size([8, 50]) torch.Size([8, 50]) tensor([1, 0, 1, 1, 1, 1, 0, 1]) myout---> torch.Size([8, 2]) tensor([[0.8149, 0.1851], [0.4211, 0.5789], [0.7687, 0.2313], [0.7724, 0.2276], [0.7527, 0.2473], [0.7036, 0.2964], [0.7887, 0.2113], [0.7259, 0.2741]], grad_fn=<SoftmaxBackward>) # \u8fd92\u53e5\u8bdd \u7b2c2\u53e5\u8bdd\u4e0d\u662f\u7b2c1\u53e5\u8bdd\u7684\u4e0b\u6587 [CLS] \u6628 \u5929 \u6211 \u628a \u8fd9 \u5957 \u4e66 \u770b \u5b8c \u4e86 \uff0c \u7ed3 \u5c3e \u6211 \u4e0d \u662f \u5f88 \u559c \u6b22 \u6709 \u70b9 \u592a [SEP] \u4e00 \u90e8 \u5206 \u6ca1 \u610f \u601d, \u800c \u4e14 \u4e0d \u503c \u90a3 \u4e48 \u591a \u94b1, \u4e0a \u5f53 \u4e86, \u73b0 \u5728 [SEP] [PAD] [PAD] [PAD] 5 \u6a21\u578b\u8bad\u7ec3 \u00b6 # \u6a21\u578b\u8bad\u7ec3NSP def dm03_train_model (): data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) # print('mydataset-->', mydataset, len(mydataset)) # print(mydataset[3]) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model my_model = MyModel () # \u5b9e\u4f8b\u5316\u4f18\u5316\u5668my_optimizer my_optimizer = AdamW ( my_model . parameters (), lr = 5e-4 ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570my_criterion my_criterion = torch . nn . CrossEntropyLoss () # \u4e0d\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b \u53ea\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6570\u636e\u7279\u5f81 \u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 epochs = 3 # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b my_model . train () # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for eporch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61my_dataloader my_dataloader = torch . utils . data . DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , shuffle = True , drop_last = True ) starttime = ( int )( time . time ()) # \u5185\u5b58for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_dataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,50] --> [8,2] my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8ba1\u7b97\u635f\u5931 my_loss = my_criterion ( my_out , labels ) # \u68af\u5ea6\u6e05\u96f6 my_optimizer . zero_grad () # \u53cd\u5411\u4f20\u64ad my_loss . backward () # \u68af\u5ea6\u66f4\u65b0 my_optimizer . step () # \u6bcf5\u6b21\u8fed\u4ee3 \u7b97\u4e00\u4e0b\u51c6\u786e\u7387 if i % 20 == 0 : out = my_out . argmax ( dim = 1 ) # [8,2] --> (8,) accuracy = ( out == labels ) . sum () . item () / len ( labels ) print ( '\u8f6e\u6b21: %d \u8fed\u4ee3\u6570: %d \u635f\u5931: %.6f \u51c6\u786e\u7387 %.3f \u65f6\u95f4 %d ' \\ % ( eporch_idx , i , my_loss . item (), accuracy , ( int )( time . time ()) - starttime )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_model . state_dict (), './my_model_nsp_ %d .bin' % ( eporch_idx + 1 )) \u6a21\u578b\u8bad\u7ec3\u6548\u679c\u8f93\u51fa \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 20 \u635f\u5931 : 0.535745 \u51c6\u786e\u73870 .750 \u65f6\u95f413 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 40 \u635f\u5931 : 0.389325 \u51c6\u786e\u73871 .000 \u65f6\u95f426 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 60 \u635f\u5931 : 0.361616 \u51c6\u786e\u73871 .000 \u65f6\u95f439 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 80 \u635f\u5931 : 0.417564 \u51c6\u786e\u73870 .875 \u65f6\u95f452 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 100 \u635f\u5931 : 0.375896 \u51c6\u786e\u73871 .000 \u65f6\u95f465 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 120 \u635f\u5931 : 0.402824 \u51c6\u786e\u73870 .875 \u65f6\u95f478 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 140 \u635f\u5931 : 0.650159 \u51c6\u786e\u73870 .625 \u65f6\u95f491 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 160 \u635f\u5931 : 0.541370 \u51c6\u786e\u73870 .625 \u65f6\u95f4104 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 180 \u635f\u5931 : 0.613911 \u51c6\u786e\u73870 .625 \u65f6\u95f4117 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 200 \u635f\u5931 : 0.449493 \u51c6\u786e\u73870 .875 \u65f6\u95f4130 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 220 \u635f\u5931 : 0.545564 \u51c6\u786e\u73870 .750 \u65f6\u95f4143 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 240 \u635f\u5931 : 0.314993 \u51c6\u786e\u73871 .000 \u65f6\u95f4156 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 260 \u635f\u5931 : 0.409358 \u51c6\u786e\u73871 .000 \u65f6\u95f4169 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 280 \u635f\u5931 : 0.513622 \u51c6\u786e\u73870 .750 \u65f6\u95f4182 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 300 \u635f\u5931 : 0.360398 \u51c6\u786e\u73871 .000 \u65f6\u95f4197 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 320 \u635f\u5931 : 0.430326 \u51c6\u786e\u73870 .875 \u65f6\u95f4210 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 340 \u635f\u5931 : 0.379781 \u51c6\u786e\u73871 .000 \u65f6\u95f4224 6 \u6a21\u578b\u8bc4\u4f30 \u00b6 # \u6a21\u578b\u6d4b\u8bd5 def dm04_evaluate_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model path = './my_model_nsp_3.bin' my_model = MyModel () my_model . load_state_dict ( torch . load ( path )) print ( 'my_model-->' , my_model ) # \u8bbe\u7f6e\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f my_model . eval () # \u8bbe\u7f6e\u8bc4\u4f30\u53c2\u6570 correct = 0 total = 0 # \u5b9e\u4f8b\u5316\u5316dataloader my_loader_test = torch . utils . data . DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , shuffle = True , drop_last = True ) # \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_loader_test ): with torch . no_grad (): my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) out = my_out . argmax ( dim = 1 ) correct += ( out == labels ) . sum () . item () total += len ( labels ) if i % 20 == 0 : print ( correct / total ) \u8f93\u51fa\u6548\u679c: # \u6bcf20\u6b21\u8fed\u4ee3 \u6253\u5370\u4e00\u4e0b\u51c6\u786e\u7387 0.875 0.9047619047619048 0.8871951219512195 0.8954918032786885 0.8981481481481481 0.8960396039603961 0.8915289256198347 0.8874113475177305 0.8843167701863354 0.8867403314917127 0.8868159203980099 0.8829185520361991 0.8858921161825726 0.8874521072796935 0.8883451957295374 0.8899501661129569 5 \u8fc1\u79fb\u5b66\u4e60-\u5fae\u8c03\u811a\u672c\u4e2d\u6587\u5206\u7c7b \u00b6 1 \u6570\u636e\u4ecb\u7ecd \u00b6 \u4f7f\u7528\u6587\u672c\u4e8c\u5206\u7c7b\u7684\u4efb\u52a1\u7c7b\u578bSST-2\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u5206\u7c7b\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u76ee\u6807\u662f\u5224\u65ad\u53e5\u5b50\u7684\u60c5\u611f\u503e\u5411 \u51c6\u5907\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u7684\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8bed\u6599\u6837\u5f0f\u4e0eSST-2\u6570\u636e\u96c6\u76f8\u540c, \u6807\u7b7e0\u4ee3\u8868\u5dee\u8bc4, \u6807\u7b7e1\u597d\u8bc4 \u8bed\u6599\u5b58\u653e\u5728\u4e0eglue_data/\u540c\u7ea7\u76ee\u5f55cn_data/\u4e0b, \u5176\u4e2d\u7684SST-2\u76ee\u5f55\u5305\u542btrain.csv\u548cdev.csv train.csv sentence label \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d,\u9910\u5385\u4e0d\u5206\u5438\u70df\u533a.\u623f\u95f4\u4e0d\u5206\u6709\u65e0\u70df\u623f. 0 \u53bb\u7684\u65f6\u5019 ,\u9152\u5e97\u5927\u5385\u548c\u9910\u5385\u5728\u88c5\u4fee,\u611f\u89c9\u5927\u5385\u6709\u70b9\u6324.\u7531\u4e8e\u9910\u5385\u88c5\u4fee\u672c\u6765\u8be5\u4eab\u53d7\u7684\u65e9\u996d,\u4e5f\u6ca1\u6709\u4eab\u53d7(\u4ed6\u4eec\u662f8\u70b9\u5f00\u59cb\u6bcf\u4e2a\u623f\u95f4\u9001,\u4f46\u662f\u6211\u65f6\u95f4\u6765\u4e0d\u53ca\u4e86)\u4e0d\u8fc7\u524d\u53f0\u670d\u52a1\u5458\u6001\u5ea6\u597d! 1 \u6709\u5f88\u957f\u65f6\u95f4\u6ca1\u6709\u5728\u897f\u85cf\u5927\u53a6\u4f4f\u4e86\uff0c\u4ee5\u524d\u53bb\u5317\u4eac\u5728\u8fd9\u91cc\u4f4f\u7684\u8f83\u591a\u3002\u8fd9\u6b21\u4f4f\u8fdb\u6765\u53d1\u73b0\u6362\u4e86\u6db2\u6676\u7535\u89c6\uff0c\u4f46\u7f51\u7edc\u4e0d\u662f\u5f88\u597d\uff0c\u4ed6\u4eec\u81ea\u5df1\u8bf4\u662f\u6536\u8d39\u7684\u539f\u56e0\u9020\u6210\u7684\u3002\u5176\u5b83\u8fd8\u597d\u3002 1 \u975e\u5e38\u597d\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u4f4f\u7684\u662f\u8c6a\u534e\u6d77\u666f\u623f\uff0c\u6253\u5f00\u7a97\u6237\u5c31\u53ef\u4ee5\u770b\u89c1\u6808\u6865\u548c\u6d77\u666f\u3002\u8bb0\u5f97\u5f88\u65e9\u4ee5\u524d\u4e5f\u4f4f\u8fc7\uff0c\u73b0\u5728\u91cd\u65b0\u88c5\u4fee\u4e86\u3002\u603b\u7684\u6765\u8bf4\u6bd4\u8f83\u6ee1\u610f\uff0c\u4ee5\u540e\u8fd8\u4f1a\u4f4f 1 \u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u623f\u95f4\u5c0f\u4e86\u4e00\u70b9\uff0c\u4f46\u662f\u5e72\u51c0\u6574\u6d01\uff0c\u5f88\u6709\u9999\u6e2f\u7684\u7279\u8272\uff0c\u6027\u4ef7\u6bd4\u8f83\u9ad8\uff0c\u63a8\u8350\u4e00\u4e0b\u54e6 1 \u9152\u5e97\u7684\u88c5\u4fee\u6bd4\u8f83\u9648\u65e7\uff0c\u623f\u95f4\u7684\u9694\u97f3\uff0c\u4e3b\u8981\u662f\u536b\u751f\u95f4\u7684\u9694\u97f3\u975e\u5e38\u5dee\uff0c\u53ea\u80fd\u7b97\u662f\u4e00\u822c\u7684 0 \u9152\u5e97\u6709\u70b9\u65e7\uff0c\u623f\u95f4\u6bd4\u8f83\u5c0f\uff0c\u4f46\u9152\u5e97\u7684\u4f4d\u5b50\u4e0d\u9519\uff0c\u5c31\u5728\u6d77\u8fb9\uff0c\u53ef\u4ee5\u76f4\u63a5\u53bb\u6e38\u6cf3\u30028\u697c\u7684\u6d77\u666f\u6253\u5f00\u7a97\u6237\u5c31\u662f\u6d77\u3002\u5982\u679c\u60f3\u4f4f\u5728\u70ed\u95f9\u7684\u5730\u5e26\uff0c\u8fd9\u91cc\u4e0d\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\uff0c\u4e0d\u8fc7\u5a01\u6d77\u57ce\u5e02\u771f\u7684\u6bd4\u8f83\u5c0f\uff0c\u6253\u8f66\u8fd8\u662f\u76f8\u5f53\u4fbf\u5b9c\u7684\u3002\u665a\u4e0a\u9152\u5e97\u95e8\u53e3\u51fa\u79df\u8f66\u6bd4\u8f83\u5c11\u3002 1 \u4f4d\u7f6e\u5f88\u597d\uff0c\u8d70\u8def\u5230\u6587\u5e99\u3001\u6e05\u51c9\u5bfa5\u5206\u949f\u90fd\u7528\u4e0d\u4e86\uff0c\u5468\u8fb9\u516c\u4ea4\u8f66\u5f88\u591a\u5f88\u65b9\u4fbf\uff0c\u5c31\u662f\u51fa\u79df\u8f66\u4e0d\u592a\u7231\u53bb\uff08\u8001\u57ce\u533a\u8def\u7a84\u7231\u5835\u8f66\uff09\uff0c\u56e0\u4e3a\u662f\u8001\u5bbe\u9986\u6240\u4ee5\u8bbe\u65bd\u8981\u9648\u65e7\u4e9b\uff0c 1 \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 0 dev.csv sentence label \u623f\u95f4\u91cc\u6709\u7535\u8111\uff0c\u867d\u7136\u623f\u95f4\u7684\u6761\u4ef6\u7565\u663e\u7b80\u964b\uff0c\u4f46\u73af\u5883\u3001\u670d\u52a1\u8fd8\u6709\u996d\u83dc\u90fd\u8fd8\u662f\u5f88\u4e0d\u9519\u7684\u3002\u5982\u679c\u4e0b\u6b21\u53bb\u65e0\u9521\uff0c\u6211\u8fd8\u662f\u4f1a\u9009\u62e9\u8fd9\u91cc\u7684\u3002 1 \u6211\u4eec\u662f5\u67081\u65e5\u901a\u8fc7\u643a\u7a0b\u7f51\u5165\u4f4f\u7684\uff0c\u6761\u4ef6\u662f\u592a\u5dee\u4e86\uff0c\u6839\u672c\u8fbe\u4e0d\u5230\u56db\u661f\u7ea7\u7684\u6807\u51c6\uff0c\u6240\u6709\u7684\u4e1c\u897f\u90fd\u5f88\u9648\u65e7\uff0c\u536b\u751f\u95f4\u6c34\u9f99\u5934\u7528\u5b8c\u7adf\u5173\u4e0d\u4e0a\uff0c\u6d74\u7f38\u7684\u6f06\u9762\u90fd\u6389\u4e86\uff0c\u4f30\u8ba1\u662f\u5341\u5e74\u524d\u7684\u56db\u661f\u7ea7\u5427\uff0c\u603b\u4e4b\u4e0b\u6b21\u662f\u4e0d\u4f1a\u5165\u4f4f\u4e86\u3002 0 \u79bb\u706b\u8f66\u7ad9\u5f88\u8fd1\u5f88\u65b9\u4fbf\u3002\u4f4f\u5728\u4e1c\u697c\u6807\u95f4\uff0c\u76f8\u6bd4\u8f83\u5728\u4e5d\u6c5f\u4f4f\u7684\u53e6\u4e00\u5bb6\u9152\u5e97\uff0c\u623f\u95f4\u6bd4\u8f83\u5927\u3002\u536b\u751f\u95f4\u8bbe\u65bd\u7565\u65e7\u3002\u670d\u52a1\u8fd8\u597d\u300210\u5143\u4e2d\u5f0f\u65e9\u9910\u4e5f\u4e0d\u9519\uff0c\u5f88\u4e30\u5bcc\uff0c\u5c45\u7136\u8fd8\u6709\u9752\u83dc\u8089\u7247\u6c64\u3002 1 \u5750\u843d\u5728\u9999\u6e2f\u7684\u8001\u57ce\u533a\uff0c\u53ef\u4ee5\u4f53\u9a8c\u9999\u6e2f\u5c45\u6c11\u751f\u6d3b\uff0c\u95e8\u53e3\u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u5982\u679c\u65f6\u95f4\u4e0d\u7d27\uff0c\u5750\u53ee\u5f53\u8f66\u5f88\u597d\u5440\uff01\u5468\u56f4\u6709\u5f88\u591a\u5c0f\u9910\u9986\uff0c\u65e9\u9910\u5c31\u5728\u4e2d\u8fdc\u540e\u9762\u7684\u5357\u5317\u56bc\u5403\u7684\uff0c\u4e1c\u897f\u5f88\u4e0d\u9519\u3002\u6211\u4eec\u5b9a\u7684\u5927\u5e8a\u623f\uff0c\u633a\u5b89\u9759\u7684\uff0c\u603b\u4f53\u6765\u8bf4\u4e0d\u9519\u3002\u524d\u53f0\u7ed3\u8d26\u6ca1\u6709\u94f6\u8054\uff01 1 \u9152\u5e97\u524d\u53f0\u670d\u52a1\u5dee\uff0c\u5bf9\u5f85\u5ba2\u4eba\u4e0d\u70ed\u60c5\u3002\u53f7\u79f0\u643a\u7a0b\u6ca1\u6709\u9884\u5b9a\u3002\u611f\u89c9\u662f\u5ba2\u4eba\u5728\u6c42\u4ed6\u4eec\uff0c\u6211\u4eec\u4e00\u5b9a\u5f97\u4f4f\u3002\u8fd9\u6837\u7684\u5bbe\u9986\u4e0b\u6b21\u4e0d\u4f1a\u5165\u4f4f\uff01 0 \u4ef7\u683c\u786e\u5b9e\u6bd4\u8f83\u9ad8\uff0c\u800c\u4e14\u8fd8\u6ca1\u6709\u65e9\u9910\u63d0\u4f9b\u3002 1 \u662f\u4e00\u5bb6\u5f88\u5b9e\u60e0\u7684\u9152\u5e97\uff0c\u4ea4\u901a\u65b9\u4fbf\uff0c\u623f\u95f4\u4e5f\u5bbd\u655e\uff0c\u665a\u4e0a\u6ca1\u6709\u7535\u8bdd\u9a9a\u6270\uff0c\u4f4f\u4e86\u4e24\u6b21\uff0c\u6709\u4e00\u6b21\u4f4f\uff15\uff10\uff11\u623f\u95f4\uff0c\u6d17\u6fa1\u95f4\u6392\u6c34\u4e0d\u7545\u901a\uff0c\u4e5f\u8bb8\u662f\u4e2a\u522b\u95ee\u9898\uff0e\u670d\u52a1\u8d28\u91cf\u5f88\u597d\uff0c\u521a\u5165\u4f4f\u65f6\u6ca1\u6709\u8c03\u597d\u5bbd\u5e26\uff0c\u670d\u52a1\u5458\u5f88\u5feb\u5c31\u5e2e\u5fd9\u89e3\u51b3\u4e86\uff0e 1 \u4f4d\u7f6e\u975e\u5e38\u597d\uff0c\u5c31\u5728\u897f\u8857\u7684\u8857\u53e3\uff0c\u4f46\u662f\u5374\u95f9\u4e2d\u53d6\u9759\uff0c\u73af\u5883\u5f88\u6e05\u65b0\u4f18\u96c5\u3002 1 \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. 1 \u8bed\u6599\u5728\u670d\u52a1\u5668\u4e0a\u5b58\u653e\u8def\u5f84\u5c55\u793a 2 \u8fd0\u884c\u4ee3\u7801 \u00b6 \u5728run_glue.py\u540c\u7ea7\u76ee\u5f55\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 # \u5b9a\u4e49DATA_DIR: \u5fae\u8c03\u6570\u636e\u6240\u5728\u8def\u5f84, \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528glue_data\u4e2d\u7684\u6570\u636e\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e # export DATA_DIR=\"/root/data/glue_data\" # \u76ee\u524d\u7248\u672c\u6682\u4e0d\u9700\u8981 # \u5b9a\u4e49SAVE_DIR: \u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84, \u6211\u4eec\u5c06\u6a21\u578b\u4fdd\u5b58\u5728\u5f53\u524d\u76ee\u5f55\u7684bert_finetuning_test\u6587\u4ef6\u4e2d export SAVE_DIR = \"./bert_finetuning_sst2_test/\" # \u4f7f\u7528python\u8fd0\u884c\u5fae\u8c03\u811a\u672c # --model_name_or_path: \u9009\u62e9\u5177\u4f53\u7684\u6a21\u578b\u6216\u8005\u53d8\u4f53, \u4e2d\u6587\u8bed\u6599\u4e0a\u5fae\u8c03, \u9009\u62e9bert-base-uncased # --task_name: \u5b83\u5c06\u4ee3\u8868\u5bf9\u5e94\u7684\u4efb\u52a1\u7c7b\u578b, \u5982MRPC\u4ee3\u8868\u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 # --do_train: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u8bad\u7ec3 # --do_eval: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u9a8c\u8bc1 # --max_seq_length: \u8f93\u5165\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u8d85\u8fc7\u5219\u622a\u65ad, \u4e0d\u8db3\u5219\u8865\u9f50 # --learning_rate: \u5b66\u4e60\u7387 # --num_train_epochs: \u8bad\u7ec3\u8f6e\u6570 # --output_dir $SAVE_DIR: \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u4fdd\u5b58\u8def\u5f84 # --overwrite_output_dir: \u518d\u6b21\u8bad\u7ec3\u65f6\u5c06\u6e05\u7a7a\u4e4b\u524d\u7684\u4fdd\u5b58\u8def\u5f84\u5185\u5bb9\u91cd\u65b0\u5199\u5165 # \u6ce81 \u865a\u62df\u673a\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8017\u65f6\u8f83\u957f\uff0c\u5efa\u8bae\u5728\u6709GPU\u7684\u4e3b\u673a\u4e0a\u6267\u884c # \u6ce82 \u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue_forcnsst2.py \\ --task_name sst2 \\ --model_name_or_path bert-base-chinese \\ --do_train \\ --do_eval \\ --train_file \"/Users/xxxxxx/transformers/examples/pytorch/text-classification/cn_data/SST-2/train.csv\" \\ --validation_file \"/Users/xxxxxx/transformers/examples/pytorch/text-classification/cn_data/SST-2/dev.csv\" \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir \"bert-base-uncased-finetuning\" \\ --overwrite_output_dir # \u6ce83 \u672c\u547d\u4ee4\u5df2\u5f62\u6210mycnsst2.sh shell\u6587\u4ef6\uff0c\u4f7f\u7528\u547d\u4ee4\u76f4\u63a5\u6267\u884c\u5373\u53ef\u3002\u6bd4\u5982\uff1a sh mycnnsst2.sh \u68c0\u9a8c\u6548\u679c # \u627e\u5230mycnnsst2.sh\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u6bd4\u5982\uff1a/Users/xxxxxx/transformers/examples/pytorch/text-classification (nlp382) xxxxxx@bogon text-classification % sh mycnsst2.sh # \u6700\u7ec8\u6253\u5370\u6a21\u578b\u7684\u9a8c\u8bc1\u7ed3\u679c, \u51c6\u786e\u7387\u9ad8\u8fbe0.88. [INFO|trainer.py:2463] 2022-05-15 11:50:30,323 >> ***** Running Evaluation ***** [INFO|trainer.py:2465] 2022-05-15 11:50:30,323 >> Num examples = 1000 [INFO|trainer.py:2468] 2022-05-15 11:50:30,324 >> Batch size = 8 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [04:06<00:00, 1.99s/it]05/15/2022 11:54:39 - INFO - datasets.metric - Removing /Users/xxxxxx/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [04:06<00:00, 1.97s/it] ***** eval metrics ***** epoch = 1.0 eval_accuracy = 0.885 eval_loss = 0.3328 eval_runtime = 0:04:08.99 eval_samples = 1000 eval_samples_per_second = 4.016 eval_steps_per_second = 0.502 3 \u67e5\u770b\u6a21\u578b\u6587\u4ef6\u5185\u5bb9 \u00b6 -rw-r--r-- 1 1390 5 15 11:54 README.md -rw-r--r-- 1 389 5 15 11:54 all_results.json -rw-r--r-- 1 216 5 15 11:54 eval_results.json -rw-r--r-- 1 590 5 15 11:50 trainer_state.json -rw-r--r-- 1 193 5 15 11:50 train_results.json -rw-r--r-- 1 3119 5 15 11:50 training_args.bin -rw-r--r-- 1 439390 5 15 11:50 tokenizer.json -rw-r--r-- 1 109540 5 15 11:50 vocab.txt -rw-r--r-- 1 112 5 15 11:50 special_tokens_map.json -rw-r--r-- 1 322 5 15 11:50 tokenizer_config.json -rw-r--r-- 1 409150611 5 15 11:50 pytorch_model.bin -rw-r--r-- 1 1005 5 15 11:50 config.json 4 \u4f7f\u7528\u672c\u5730\u5fae\u8c03\u6a21\u578b \u00b6 import torch from transformers import AutoTokenizer , AutoModelForSequenceClassification , AutoModel def dm01_use_model (): # 0 \u627e\u5230\u81ea\u5df1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84 mymodelname = '/Users/xxxx/transformers/examples/pytorch/text-classification/bert-base-uncased-finetuning' print ( mymodelname ) # 1 \u672c\u5730\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684tokenizer tokenizer = AutoTokenizer . from_pretrained ( mymodelname ) # 2 \u672c\u5730\u52a0\u8f7d \u9884\u8bad\u7ec3\u6a21\u578b \u5e26\u5206\u7c7b\u6a21\u578b\u5934 model = AutoModelForSequenceClassification . from_pretrained ( mymodelname ) # model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2) # 3 \u9ed8\u8ba4\u60c5\u51b5\u4e0b \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e0d\u5e26\u5934 # model2 = AutoModel.from_pretrained(mymodelname) text = \"\u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d\" index = tokenizer . encode ( text ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( 'result.logits--->' , result . logits ) predicted_label = torch . argmax ( result . logits ) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) text1 = \"\u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519.\" index = tokenizer . encode ( text1 ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( 'result.logits--->' , result . logits ) predicted_label = torch . argmax ( result [ 0 ]) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) \u8f93\u51fa\u6548\u679c: \u8f93\u5165\u6587\u672c\u4e3a: \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d \u9884\u6d4b\u6807\u7b7e\u4e3a: 0 \u8f93\u5165\u6587\u672c\u4e3a: \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. \u9884\u6d4b\u6807\u7b7e\u4e3a: 1 6 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u4e2d\u6587\u5206\u7c7b\u4efb\u52a1\u5f00\u53d1 \u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6e90\u5c01\u88c5\u3001\u6570\u636e\u8fed\u4ee3\u5668\u7684\u4f7f\u7528 \u642d\u5efa\u4e2d\u6587\u5206\u7c7b\u4efb\u52a1\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3001\u6a21\u578b\u9884\u6d4b \u5b66\u4e60\u4e86\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u4e2d\u6587\u586b\u7a7a\u4efb\u52a1\u5f00\u53d1 \u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6e90\u5c01\u88c5\u3001\u6570\u636e\u8fed\u4ee3\u5668\u7684\u4f7f\u7528 \u642d\u5efa\u4e2d\u6587\u586b\u7a7a\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3001\u6a21\u578b\u9884\u6d4b \u5b66\u4e60\u4e86\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1\u5f00\u53d1 \u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6e90\u5c01\u88c5\u3001\u6570\u636e\u8fed\u4ee3\u5668\u7684\u4f7f\u7528 \u642d\u5efa\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3001\u6a21\u578b\u9884\u6d4b","title":"9 \u8fc1\u79fb\u5b66\u4e60\u5b9e\u8df5"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#_1","text":"\u4e86\u89e3\u5e76\u638c\u63e1\u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u5206\u7c7b\u4efb\u52a1\u5f00\u53d1 \u4e86\u89e3\u5e76\u638c\u63e1\u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u586b\u7a7a\u4efb\u52a1\u5f00\u53d1 \u4e86\u89e3\u5e76\u638c\u63e1\u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1 \u4e86\u89e3\u901a\u8fc7\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u540e\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u6cd5","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#1","text":"\u7c7b\u578b\u4e00: \u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c \u7c7b\u578b\u4e8c: \u4f7f\u7528\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c \u8bf4\u660e: \u6240\u6709\u7c7b\u578b\u7684\u5b9e\u6218\u6f14\u793a, \u90fd\u5c06\u9488\u5bf9\u4e2d\u6587\u6587\u672c\u8fdb\u884c","title":"1 \u901a\u8fc7\u5fae\u8c03\u65b9\u5f0f\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u7684\u4e24\u79cd\u7c7b\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#2-","text":"","title":"2 \u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u5206\u7c7b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#1_1","text":"\u76f4\u63a5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8f93\u5165\u6587\u672c\u7684\u7279\u5f81\u8868\u793a, \u540e\u63a5\u81ea\u5b9a\u4e49\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u8f93\u51fa\u7ed3\u679c","title":"1 \u4efb\u52a1\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#2","text":"\u6570\u636e\u6587\u4ef6\u6709\u4e09\u4e2atrain.csv\uff0ctest.csv\uff0cvalidation.csv\uff0c\u6570\u636e\u6837\u5f0f\u90fd\u662f\u4e00\u6837\u7684\u3002 label , text 1 , \u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf \uff0c \u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9 \uff0c \u5468\u56f4\u9910\u9986 \u3001 \u98df\u5eca \u3001 \u5546\u573a \u3001 \u8d85\u5e02 \u3001 \u644a\u4f4d\u4e00\u5e94\u4ff1\u5168 \u3002 \u9152\u5e97\u88c5\u4fee\u4e00\u822c \uff0c \u4f46\u8fd8\u7b97\u6574\u6d01 \u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876 \uff0c \u56e0\u6b64\u5f88\u5c0f \uff0c \u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22 \u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684 \uff0c \u8fd8\u7b97\u4e30\u5bcc \u3002 \u670d\u52a1\u5417 \uff0c \u4e00\u822c 1 , 15.4 \u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d \uff0c \u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86 \uff0c \u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8 \uff0c \u8f93\u6570\u5b57\u7279\u65b9\u4fbf \uff0c \u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2 \uff0c \u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519 0 , \u623f\u95f4\u592a\u5c0f \u3002 \u5176\u4ed6\u7684\u90fd\u4e00\u822c \u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002 0 , \"1.\u63a5\u7535\u6e90\u6ca1\u6709\u51e0\u5206\u949f,\u7535\u6e90\u9002\u914d\u5668\u70ed\u7684\u4e0d\u884c. 2.\u6444\u50cf\u5934\u7528\u4e0d\u8d77\u6765. 3.\u673a\u76d6\u7684\u94a2\u7434\u6f06\uff0c\u624b\u4e0d\u80fd\u6478\uff0c\u4e00\u6478\u4e00\u4e2a\u5370. 4.\u786c\u76d8\u5206\u533a\u4e0d\u597d\u529e.\" 1 , \"\u4eca\u5929\u624d\u77e5\u9053\u8fd9\u4e66\u8fd8\u6709\u7b2c6\u5377,\u771f\u6709\u70b9\u90c1\u95f7:\u4e3a\u4ec0\u4e48\u540c\u4e00\u5957\u4e66\u6709\u4e24\u79cd\u7248\u672c\u5462?\u5f53\u5f53\u7f51\u662f\u4e0d\u662f\u8be5\u8ddf\u51fa\u7248\u793e\u5546\u91cf\u5546\u91cf,\u5355\u72ec\u51fa\u4e2a\u7b2c6\u5377,\u8ba9\u6211\u4eec\u7684\u5b69\u5b50\u4e0d\u4f1a\u6709\u6240\u9057\u61be\u3002\" \u901a\u8fc7huggingface\u7684datasets\u5de5\u5177\uff0c\u52a0\u8f7d\u4fe1\u606f\u6587\u4ef6\u4fe1\u606f\u5982\u4e0b \u52a0\u8f7d\u8bad\u7ec3\u96c6 dataset_train ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 9600 }) { 'label' : [ 1 , 1 , 0 ], 'text' : [ '\u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf\uff0c\u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9\uff0c\u5468\u56f4\u9910\u9986\u3001\u98df\u5eca\u3001\u5546\u573a\u3001\u8d85\u5e02\u3001\u644a\u4f4d\u4e00\u5e94\u4ff1\u5168\u3002\u9152\u5e97\u88c5\u4fee\u4e00\u822c\uff0c\u4f46\u8fd8\u7b97\u6574\u6d01\u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876\uff0c\u56e0\u6b64\u5f88\u5c0f\uff0c\u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22\u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684\uff0c\u8fd8\u7b97\u4e30\u5bcc\u3002 \u670d\u52a1\u5417\uff0c\u4e00\u822c' , '15.4\u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d\uff0c\u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86\uff0c\u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8\uff0c\u8f93\u6570\u5b57\u7279\u65b9\u4fbf\uff0c\u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2\uff0c\u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519' , '\u623f\u95f4\u592a\u5c0f\u3002\u5176\u4ed6\u7684\u90fd\u4e00\u822c\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002' ]} \u52a0\u8f7d\u6d4b\u8bd5\u96c6 my_dataset_test ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 1200 }) { 'label' : [ 1 , 0 , 0 ], 'text' : [ '\u8fd9\u4e2a\u5bbe\u9986\u6bd4\u8f83\u9648\u65e7\u4e86\uff0c\u7279\u4ef7\u7684\u623f\u95f4\u4e5f\u5f88\u4e00\u822c\u3002\u603b\u4f53\u6765\u8bf4\u4e00\u822c' , '\u6000\u7740\u5341\u5206\u6fc0\u52a8\u7684\u5fc3\u60c5\u653e\u6620\uff0c\u53ef\u662f\u770b\u7740\u770b\u7740\u53d1\u73b0\uff0c\u5728\u653e\u6620\u5b8c\u6bd5\u540e\uff0c\u51fa\u73b0\u4e00\u96c6\u7c73\u8001\u9f20\u7684\u52a8\u753b\u7247\uff01\u5f00\u59cb\u8fd8\u6000\u7591\u662f\u4e0d\u662f\u8d60\u9001\u7684\u4e2a\u522b\u73b0\u8c61\uff0c\u53ef\u662f\u540e\u6765\u53d1\u73b0\u6bcf\u5f20DVD\u540e\u9762\u90fd\u6709\uff01\u771f\u4e0d\u77e5\u9053\u751f\u4ea7\u5546\u600e\u4e48\u60f3\u7684\uff0c\u6211\u60f3\u770b\u7684\u662f\u732b\u548c\u8001\u9f20\uff0c\u4e0d\u662f\u7c73\u8001\u9f20\uff01\u5982\u679c\u5382\u5bb6\u662f\u60f3\u8d60\u9001\u7684\u8bdd\uff0c\u90a3\u5c31\u5168\u5957\u7c73\u8001\u9f20\u548c\u5510\u8001\u9e2d\u90fd\u8d60\u9001\uff0c\u53ea\u5728\u6bcf\u5f20DVD\u540e\u9762\u6dfb\u52a0\u4e00\u96c6\u7b97\u4ec0\u4e48\uff1f\uff1f\u7b80\u76f4\u662f\u753b\u86c7\u6dfb\u8db3\uff01\uff01' , '\u8fd8\u7a0d\u5fae\u91cd\u4e86\u70b9\uff0c\u53ef\u80fd\u662f\u786c\u76d8\u5927\u7684\u539f\u6545\uff0c\u8fd8\u8981\u518d\u8f7b\u534a\u65a4\u5c31\u597d\u4e86\u3002\u5176\u4ed6\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\u8d34\u7684\u51e0\u79cd\u819c\u6c14\u6ce1\u8f83\u591a\uff0c\u7528\u4e0d\u4e86\u591a\u4e45\u5c31\u8981\u66f4\u6362\u4e86\uff0c\u5c4f\u5e55\u819c\u7a0d\u597d\u70b9\uff0c\u4f46\u6bd4\u6ca1\u6709\u8981\u5f3a\u591a\u4e86\u3002\u5efa\u8bae\u914d\u8d60\u51e0\u5f20\u819c\u8ba9\u7528\u7528\u6237\u81ea\u5df1\u8d34\u3002' ]} \u52a0\u8f7d\u9a8c\u8bc1\u96c6 my_dataset_validation ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 1200 }) { 'label' : [ 1 , 1 , 0 ], 'text' : [ '\u9019\u9593\u9152\u5e97\u74b0\u5883\u548c\u670d\u52d9\u614b\u5ea6\u4ea6\u7b97\u4e0d\u932f,\u4f46\u623f\u9593\u7a7a\u9593\u592a\u5c0f~~\u4e0d\u5ba3\u5bb9\u7d0d\u592a\u5927\u4ef6\u884c\u674e~~\u4e14\u623f\u9593\u683c\u8abf\u9084\u53ef\u4ee5~~ \u4e2d\u9910\u5ef3\u7684\u5ee3\u6771\u9ede\u5fc3\u4e0d\u592a\u597d\u5403~~\u8981\u6539\u5584\u4e4b~~~~\u4f46\u7b97\u50f9\u9322\u5e73\u5b9c~~\u53ef\u63a5\u53d7~~ \u897f\u9910\u5ef3\u683c\u8abf\u90fd\u5f88\u597d~~\u4f46\u5403\u7684\u5473\u9053\u4e00\u822c\u4e14\u4ee4\u4eba\u7b49\u5f97\u592a\u8010\u4e86~~\u8981\u6539\u5584\u4e4b~~' , '<\u8350\u4e66> \u63a8\u8350\u6240\u6709\u559c\u6b22<\u7ea2\u697c>\u7684\u7ea2\u8ff7\u4eec\u4e00\u5b9a\u8981\u6536\u85cf\u8fd9\u672c\u4e66,\u8981\u77e5\u9053\u5f53\u5e74\u6211\u542c\u8bf4\u8fd9\u672c\u4e66\u7684\u65f6\u5019\u82b1\u5f88\u957f\u65f6\u95f4\u53bb\u56fe\u4e66\u9986\u627e\u548c\u501f\u90fd\u6ca1\u80fd\u5982\u613f,\u6240\u4ee5\u8fd9\u6b21\u4e00\u770b\u5230\u5f53\u5f53\u6709,\u9a6c\u4e0a\u4e70\u4e86,\u7ea2\u8ff7\u4eec\u4e5f\u8981\u8bb0\u5f97\u5907\u8d27\u54e6!' , '\u5546\u54c1\u7684\u4e0d\u8db3\u6682\u65f6\u8fd8\u6ca1\u53d1\u73b0\uff0c\u4eac\u4e1c\u7684\u8ba2\u5355\u5904\u7406\u901f\u5ea6\u5b9e\u5728.......\u5468\u4e8c\u5c31\u6253\u5305\u5b8c\u6210\uff0c\u5468\u4e94\u624d\u53d1\u8d27...' ]} \u901a\u8fc7huggingface\u7684datasets\u5de5\u5177\uff0c\u52a0\u8f7d\u4ee3\u7801\u5982\u4e0b def dm_file2dataset (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train print ( ' \\n \u52a0\u8f7d\u8bad\u7ec3\u96c6' ) my_dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = 'train' ) print ( 'dataset_train--->' , my_dataset_train ) print ( my_dataset_train [ 0 : 3 ]) # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test print ( ' \\n \u52a0\u8f7d\u6d4b\u8bd5\u96c6' ) my_dataset_test = load_dataset ( 'csv' , data_files = './mydata1/test.csv' , split = 'train' ) print ( 'my_dataset_test--->' , my_dataset_test ) print ( my_dataset_test [ 0 : 3 ]) print ( ' \\n \u52a0\u8f7d\u9a8c\u8bc1\u96c6' ) # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train my_dataset_validation = load_dataset ( 'csv' , data_files = './mydata1/validation.csv' , split = \"train\" ) print ( 'my_dataset_validation--->' , my_dataset_validation ) print ( my_dataset_validation [ 0 : 3 ]) \u5bfc\u5165\u5de5\u5177\u5305\u548c\u8f85\u52a9\u5de5\u5177\u5b9e\u4f8b\u5316\u5bf9\u8c61 # \u5bfc\u5165\u5de5\u5177\u5305 import torch from datasets import load_dataset from transformers import BertTokenizer , BertModel from transformers import AdamW import time # \u52a0\u8f7d\u5b57\u5178\u548c\u5206\u8bcd\u5de5\u5177 \u5b9e\u4f8b\u5316\u5206\u8bcd\u5de5\u5177 my_tokenizer = BertTokenizer . from_pretrained ( 'bert-base-chinese' ) # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b \u5b9e\u4f8b\u5316\u9884\u8bad\u7ec3\u6a21\u578b my_model_pretrained = BertModel . from_pretrained ( 'bert-base-chinese' )","title":"2 \u6570\u636e\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#3","text":"\u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42\u3002 \u6570\u636e\u9884\u5904\u7406\u548c\u76f8\u5173\u6d4b\u8bd5\u51fd\u6570 # \u6570\u636e\u96c6\u5904\u7406\u81ea\u5b9a\u4e49\u51fd\u6570 def collate_fn1 ( data ): # data\u4f20\u8fc7\u6765\u7684\u6570\u636e\u662flist eg: \u6279\u6b21\u65708\uff0c8\u4e2a\u5b57\u5178 # [{'text':'xxxx','label':0} , {'text':'xxxx','label':1}, ...] sents = [ i [ 'text' ] for i in data ] labels = [ i [ 'label' ] for i in data ] # \u7f16\u7801text2id \u5bf9\u591a\u53e5\u8bdd\u8fdb\u884c\u7f16\u7801\u7528batch_encode_plus\u51fd\u6570 data = my_tokenizer . batch_encode_plus ( batch_text_or_text_pairs = sents , truncation = True , padding = 'max_length' , max_length = 500 , return_tensors = 'pt' , return_length = True ) # input_ids:\u7f16\u7801\u4e4b\u540e\u7684\u6570\u5b57 # attention_mask:\u662f\u8865\u96f6\u7684\u4f4d\u7f6e\u662f0,\u5176\u4ed6\u4f4d\u7f6e\u662f1 input_ids = data [ 'input_ids' ] attention_mask = data [ 'attention_mask' ] token_type_ids = data [ 'token_type_ids' ] labels = torch . LongTensor ( labels ) # \u8fd4\u56detext2id\u4fe1\u606f \u63a9\u7801\u4fe1\u606f \u53e5\u5b50\u5206\u6bb5\u4fe1\u606f \u6807\u7b7ey return input_ids , attention_mask , token_type_ids , labels # \u6d4b\u8bd5\u6570\u636e def dm01_test_dataset (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90 \u901a\u8fc7\u8bad\u7ec3\u6587\u4ef6 dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) print ( 'dataset_train--->' , dataset_train ) # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668 mydataloader mydataloader = torch . utils . data . DataLoader ( dataset_train , batch_size = 8 , collate_fn = collate_fn1 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , len ( mydataloader )) # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( len ( mydataloader )) print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) # \u6253\u5370\u53e5\u5b50text2id\u540e\u7684\u4fe1\u606f print ( 'input_ids' , input_ids ) # \u6253\u5370\u53e5\u5b50attention\u63a9\u7801\u4fe1\u606f print ( 'attention_mask' , attention_mask ) # \u6253\u5370\u53e5\u5b50\u5206\u6bb5\u4fe1\u606f print ( 'token_type_ids' , token_type_ids ) # \u6253\u5370\u76ee\u6807y\u4fe1\u606f print ( 'labels' , labels ) break \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c # \u663e\u793a\u8bad\u7ec3\u96c6\u5b57\u6bb5\u548c\u6837\u672c\u6570\u76ee dataset_train ---> Dataset ({ features : [ 'label' , 'text' ], num_rows : 9600 }) mydataloader ---> 1200 # \u663e\u793a\u5904\u7406\u540e\u9001\u7ed9\u6a21\u578b\u7684\u6570\u636e\u4fe1\u606f torch . Size ([ 8 , 500 ]) torch . Size ([ 8 , 500 ]) torch . Size ([ 8 , 500 ]) tensor ([ 1 , 1 , 0 , 0 , 1 , 0 , 0 , 0 ]) # \u53e5\u5b50text2id\u540e\u7684\u4fe1\u606f input_ids tensor ([[ 101 , 6848 , 2885 , ... , 0 , 0 , 0 ], [ 101 , 8115 , 119 , ... , 0 , 0 , 0 ], [ 101 , 2791 , 7313 , ... , 0 , 0 , 0 ], ... , [ 101 , 3322 , 1690 , ... , 0 , 0 , 0 ], [ 101 , 1457 , 1457 , ... , 0 , 0 , 0 ], [ 101 , 6821 , 3315 , ... , 0 , 0 , 0 ]]) # \u53e5\u5b50\u6ce8\u610f\u529b\u673a\u5236\u63a9\u7801\u4fe1\u606f attention_mask tensor ([[ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], ... , [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ], [ 1 , 1 , 1 , ... , 0 , 0 , 0 ]]) # \u53e5\u5b50\u5206\u6bb5\u4fe1\u606f token_type_ids tensor ([[ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], ... , [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ], [ 0 , 0 , 0 , ... , 0 , 0 , 0 ]]) # \u53e5\u5b50\u7684\u6807\u7b7e\u4fe1\u606f labels tensor ([ 1 , 1 , 0 , 0 , 1 , 0 , 0 , 0 ])","title":"3 \u6570\u636e\u9884\u5904\u7406"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#4","text":"\u81ea\u5b9a\u4e49\u5355\u5c42\u7684\u5168\u8fde\u63a5\u7f51\u7edc\u4f5c\u4e3a\u5fae\u8c03\u7f51\u7edc\u3002\u6839\u636e\u5b9e\u9645\u7ecf\u9a8c, \u81ea\u5b9a\u4e49\u7684\u5fae\u8c03\u7f51\u7edc\u53c2\u6570\u603b\u6570\u5e94\u5927\u4e8e0.5\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u5c0f\u4e8e10\u500d\u7684\u8bad\u7ec3\u6570\u636e\u91cf, \u8fd9\u6837\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u6536\u655b \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b # \u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b class MyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () # \u5b9a\u4e49\u5168\u8fde\u63a5\u5c42 self . fc = torch . nn . Linear ( 768 , 2 ) def forward ( self , input_ids , attention_mask , token_type_ids ): # \u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u8bad\u7ec3 \u53ea\u8fdb\u884c\u7279\u5f81\u62bd\u53d6 [8,500] ---> [8,768] with torch . no_grad (): out = my_model_pretrained ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3 \u6570\u636e\u7ecf\u8fc7\u5168\u8fde\u63a5\u5c42 [8,768] --> [8,2] out = self . fc ( out . last_hidden_state [:, 0 ]) # \u6570\u636e\u8fdb\u884csoftmax\u5f52\u4e00\u5316 \u5206\u7c7b\u6982\u7387\u503c out = out . softmax ( dim = 1 ) return out \u6a21\u578b\u6d4b\u8bd5 # \u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u8f93\u5165\u548c\u8f93\u51fa\u6d4b\u8bd5 def dm02_test_mymodel (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90 \u901a\u8fc7\u8bad\u7ec3\u6587\u4ef6 dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) # print('dataset_train--->', dataset_train) # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668 mydataloader mydataloader = torch . utils . data . DataLoader ( dataset_train , batch_size = 8 , collate_fn = collate_fn1 , shuffle = False , drop_last = True ) # print('mydataloader--->', len(mydataloader)) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b mymodel = MyModel () print ( 'mymodel--->' , mymodel ) # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( len ( mydataloader )) # print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels) # \u6570\u636e\u9001\u7ed9\u6a21\u578b y_out = mymodel ( input_ids , attention_mask , token_type_ids ) print ( 'y_out---->' , y_out . shape , y_out ) break \u8f93\u51fa\u6548\u679c # \u6a21\u578b\u4fe1\u606f\u6253\u5370 mymodel---> MyModel( (fc): Linear(in_features=768, out_features=2, bias=True) ) # \u6a21\u578b\u8fd0\u7b97\u540e\u5206\u7c7b\u7ed3\u679c\u5c55\u793a y_out----> torch.Size([8, 2]) tensor([[0.4062, 0.5938], [0.2788, 0.7212], [0.3671, 0.6329], [0.2496, 0.7504], [0.2995, 0.7005], [0.2566, 0.7434], [0.2537, 0.7463], [0.3832, 0.6168]], grad_fn=<SoftmaxBackward>)","title":"4 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#5","text":"# \u6a21\u578b\u8bad\u7ec3 def dm03_train_model (): # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model my_model = MyModel () # \u5b9e\u4f8b\u5316\u4f18\u5316\u5668my_optimizer my_optimizer = AdamW ( my_model . parameters (), lr = 5e-4 ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570my_criterion my_criterion = torch . nn . CrossEntropyLoss () # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train my_dataset_train = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) print ( 'dataset_train--->' , my_dataset_train ) # \u4e0d\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b \u53ea\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6570\u636e\u7279\u5f81 \u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 epochs = 3 # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b my_model . train () # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for eporch_idx in range ( epochs ): # \u6bcf\u6b21\u8f6e\u6b21\u5f00\u59cb\u8ba1\u7b97\u65f6\u95f4 starttime = ( int )( time . time ()) # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61my_dataloader my_dataloader = torch . utils . data . DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn1 , shuffle = True , drop_last = True ) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_dataloader , start = 1 ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,500] --> [8,2] my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8ba1\u7b97\u635f\u5931 my_loss = my_criterion ( my_out , labels ) # \u68af\u5ea6\u6e05\u96f6 my_optimizer . zero_grad () # \u53cd\u5411\u4f20\u64ad my_loss . backward () # \u68af\u5ea6\u66f4\u65b0 my_optimizer . step () # \u6bcf5\u6b21\u8fed\u4ee3 \u7b97\u4e00\u4e0b\u51c6\u786e\u7387 if i % 5 == 0 : out = my_out . argmax ( dim = 1 ) # [8,2] --> (8,) accuracy = ( out == labels ) . sum () . item () / len ( labels ) print ( '\u8f6e\u6b21: %d \u8fed\u4ee3\u6570: %d \u635f\u5931: %.6f \u51c6\u786e\u7387 %.3f \u65f6\u95f4 %d ' \\ % ( eporch_idx , i , my_loss . item (), accuracy , ( int )( time . time ()) - starttime )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_model . state_dict (), './my_model_ %d .bin' % ( eporch_idx + 1 )) \u6a21\u578b\u8bad\u7ec3\u6548\u679c\u8f93\u51fa \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 5 \u635f\u5931 : 0.735494 \u51c6\u786e\u73870 .250 \u65f6\u95f440 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 10 \u635f\u5931 : 0.614211 \u51c6\u786e\u73870 .875 \u65f6\u95f481 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 15 \u635f\u5931 : 0.635408 \u51c6\u786e\u73870 .750 \u65f6\u95f4119 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 20 \u635f\u5931 : 0.575522 \u51c6\u786e\u73871 .000 \u65f6\u95f4157 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 25 \u635f\u5931 : 0.661196 \u51c6\u786e\u73870 .625 \u65f6\u95f4196 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 30 \u635f\u5931 : 0.546462 \u51c6\u786e\u73870 .875 \u65f6\u95f4234 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 35 \u635f\u5931 : 0.609517 \u51c6\u786e\u73870 .875 \u65f6\u95f4272 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 40 \u635f\u5931 : 0.529246 \u51c6\u786e\u73871 .000 \u65f6\u95f4310 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 45 \u635f\u5931 : 0.474820 \u51c6\u786e\u73871 .000 \u65f6\u95f4348 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 50 \u635f\u5931 : 0.540127 \u51c6\u786e\u73870 .875 \u65f6\u95f4387 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 55 \u635f\u5931 : 0.575326 \u51c6\u786e\u73870 .625 \u65f6\u95f4426 # \u4ece\u4ee5\u4e0a\u7684\u8bad\u7ec3\u8f93\u51fa\u6548\u679c\u6765\u770b\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u5341\u5206\u5f3a\u5927\u7684\uff0c\u53ea\u9700\u8981\u77ed\u77ed\u7684\u51e0\u6b21\u8fed\u4ee3\uff0c\u5c31\u53ef\u4ee5\u8ba9\u51c6\u786e\u7387\u4e0a88%","title":"5 \u6a21\u578b\u8bad\u7ec3"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#6","text":"# \u6a21\u578b\u6d4b\u8bd5 def dm04_evaluate_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test print ( ' \\n \u52a0\u8f7d\u6d4b\u8bd5\u96c6' ) my_dataset_test = load_dataset ( 'csv' , data_files = './mydata1/test.csv' , split = 'train' ) print ( 'my_dataset_test--->' , my_dataset_test ) # print(my_dataset_test[0:3]) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model path = './my_model_3.bin' my_model = MyModel () my_model . load_state_dict ( torch . load ( path )) print ( 'my_model-->' , my_model ) # \u8bbe\u7f6e\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f my_model . eval () # \u8bbe\u7f6e\u8bc4\u4f30\u53c2\u6570 correct = 0 total = 0 # \u5b9e\u4f8b\u5316\u5316dataloader my_loader_test = torch . utils . data . DataLoader ( my_dataset_test , batch_size = 8 , collate_fn = collate_fn1 , shuffle = True , drop_last = True ) # \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_loader_test ): # \u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u62bd\u53d6 with torch . no_grad (): my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8d2a\u5fc3\u7b97\u6cd5\u6c42\u9884\u6d4b\u7ed3\u679c out = my_out . argmax ( dim = 1 ) # \u8ba1\u7b97\u51c6\u786e\u7387 correct += ( out == labels ) . sum () . item () total += len ( labels ) # \u6bcf5\u6b21\u8fed\u4ee3\u6253\u5370\u4e00\u6b21\u51c6\u786e\u7387 if i % 5 == 0 : print ( correct / total , end = \" \" ) print ( my_tokenizer . decode ( input_ids [ 0 ], skip_special_tokens = True ), end = \" \" ) print ( '\u9884\u6d4b\u503c \u771f\u5b9e\u503c:' , out [ 0 ] . item (), labels [ 0 ] . item ()) \u8f93\u51fa\u6548\u679c: 0.875 \u6211 \u6ca1 \u6709 \u6536 \u5230 \u8fd9 \u672c \u4e66 \uff0c \u6211 \u660e \u660e \u548c \u53e6 \u5916 \u4e00 \u672c \u4e00 \u8d77 \u4e70 \u7684 \uff0c \u670d \u52a1 \u4e5f \u4e0d \u597d \u3002 \u81f3 \u5c11 \u5e94 \u8be5 \u8ba9 \u6211 \u77e5 \u9053 \u8fd9 \u4e2a \u60c5 \u51b5 \u5728 \uff01 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 0 0 0.8125 \u4e5f \u8bb8 \u8fd9 \u4e0d \u7b97 \u4e00 \u4e2a \u5f88 \u597d \u7684 \u7406 \u7531, \u4f46 \u662f \u6211 \u4e4b \u6240 \u4ee5 \u559c \u6b22 \u8bfb \u4e66 \u800c \u4e0d \u662f \u770b \u7f51 \u4e0a \u7684 \u8d44 \u6599 \u4ec0 \u4e48 \u7684, \u5c31 \u662f \u559c \u6b22 \u95fb \u7740 \u4e66 \u9999. \u8fd9 \u672c \u4e66 \u53ef \u80fd \u662f \u5370 \u5237 \u7684 \u6cb9 \u58a8 \u4e0d \u597d \u8fd8 \u662f \u4ec0 \u4e48 \u539f \u56e0, \u611f \u89c9 \u81ed \u81ed \u7684 \u4e0d \u597d \u95fb. \u91cc \u9762 \u662f \u4e00 \u4e9b \u5173 \u4e8e \u4e2d \u5f0f \u82f1 \u8bed \u7684 \u5c0f \u8da3 \u95fb, \u6709 \u4e9b \u5c0f \u4e50 \u8da3, \u4f46 \u611f \u89c9 \u5bf9 \u4e8e \u6709 \u6d53 \u91cd \u4e2d \u5f0f \u601d \u7ef4 \u4e60 \u60ef \u8bf4 \u82f1 \u8bf4 \u7684 \u4eba \u6765 \u8bf4 \u624d \u6bd4 \u8f83 \u6709 \u70b9 \u7528 \u5904. \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 1 0 0.8409090909090909 1. \u6709 \u6025 \u4e8b \u51fa \u53bb \uff0c \u8981 \u4eec \u7ae5 \u53eb \u51fa \u79df \u8f66 \uff0c \u4ed6 \u4eec \u5c31 \u53eb \u9152 \u5e97 \u91cc \u7684 \u9ed1 \u8f66 \uff0c \u4ef7 \u683c \u662f \u666e \u901a \u51fa \u79df \u4ef7 \u7684 \u4e24 \u500d \u3002 \u4f60 \u63d0 \u51fa \u4e0d \u8981 \u9152 \u5e97 \u7684 \u9ed1 \u8f66 \u65f6 \uff0c \u4ed6 \u4eec \u5c31 \u544a \u8bc9 \u4f60 \u5916 \u9762 \u62e6 \u4e0d \u5230 \u51fa \u79df \u8f66 \uff0c \u6211 \u4eec \u81ea \u5df1 \u8d70 \u51fa \u53bb \u65f6 \uff0c \u5916 \u9762 \u51fa \u79df \u8f66 \u968f \u65f6 \u53ef \u4ee5 \u62e6 \u5230 \u3002 \u4f4f \u5e97 \u671f \u95f4 \u4e0d \u6b62 \u4e00 \u6b21 \u53d1 \u751f \u3002 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 0 0 0.828125 \u8fd9 \u672c \u4e66 \u4e2d \u7684 \u56fe \u7247 \u5f88 \u8ba9 \u4eba \u89e6 \u52a8 \uff0c \u6bd4 \u5982 \u4e3a \u52b3 \u62c9 \u7948 \u7977 \u7684 \u6821 \u53cb \uff0c \u60e0 \u7279 \u5c3c \u548c \u9a6c \u7279 \u7684 \u7b11 \u5bb9 \uff0c \u5f88 \u52a8 \u4eba \u3002 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 1 1 0.8511904761904762 \u8fd9 \u662f \u6211 \u4f4f \u8fc7 \u7684 \u6700 \u5dee \u7684 \u9152 \u5e97 \uff0c \u623f \u95f4 \u6c14 \u5473 \u96be \u95fb \uff0c \u521a \u6253 \u4e86 \u706d \u868a \u836f \u6c34 \uff0c \u6362 \u4e86 \u4e09 \u4e2a \u623f \u95f4 \u8fd8 \u662f \u5982 \u6b64 \uff0c \u670d \u52a1 \u5458 \u8bf4 \uff1a \u4f4f \u4e45 \u4e86 \u5c31 \u4e60 \u60ef \u4e86 \uff0c \u6bcf \u4e2a \u5bbe \u9986 \u90fd \u6709 \u81ea \u5df1 \u7684 \u5473 \u9053 \u3002 \u8003 \uff01 \u6211 \u53c8 \u4e0d \u662f \u6765 \u4f53 \u9a8c \u751f \u6d3b \u7684 \u3002 \u5468 \u56f4 \u73af \u5883 \u590d \u6742 \uff0c \u810f \u4e71 \u5dee \u3002 \u9884\u6d4b\u503c \u771f\u5b9e\u503c: 0 0","title":"6 \u6a21\u578b\u8bc4\u4f30"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#3-","text":"","title":"3 \u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u586b\u7a7a"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#1_2","text":"# \u8f93\u5165\u4e00\u53e5\u8bdd\uff0cMASK\u4e00\u4e2a\u5b57\uff0c\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u586b\u7a7a [ CLS ] \u9009 \u62e9 \u73e0 \u6c5f \u82b1 \u56ed \u7684 \u539f \u56e0 \u5c31 \u662f \u65b9 \u4fbf \uff0c \u6709 [ MASK ] \u52a8 \u6276 \u68af \u76f4 \u63a5 \u5230 \u8fbe \u6d77 \u8fb9 \uff0c \u5468 \u56f4 \u9910 \u9986 [ SEP ] # \u672c\u53e5MASK\u7684\u5b57\u4e3a\u201c\u7535\u201d","title":"1 \u4efb\u52a1\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#2_1","text":"\u6570\u636e\u6587\u4ef6\u6709\u4e09\u4e2atrain.csv\uff0ctest.csv\uff0cvalidation.csv\uff0c\u6570\u636e\u6837\u5f0f\u90fd\u662f\u4e00\u6837\u7684\u3002 label , text 1 , \u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf \uff0c \u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9 \uff0c \u5468\u56f4\u9910\u9986 \u3001 \u98df\u5eca \u3001 \u5546\u573a \u3001 \u8d85\u5e02 \u3001 \u644a\u4f4d\u4e00\u5e94\u4ff1\u5168 \u3002 \u9152\u5e97\u88c5\u4fee\u4e00\u822c \uff0c \u4f46\u8fd8\u7b97\u6574\u6d01 \u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876 \uff0c \u56e0\u6b64\u5f88\u5c0f \uff0c \u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22 \u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684 \uff0c \u8fd8\u7b97\u4e30\u5bcc \u3002 \u670d\u52a1\u5417 \uff0c \u4e00\u822c 1 , 15.4 \u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d \uff0c \u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86 \uff0c \u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8 \uff0c \u8f93\u6570\u5b57\u7279\u65b9\u4fbf \uff0c \u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2 \uff0c \u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519 0 , \u623f\u95f4\u592a\u5c0f \u3002 \u5176\u4ed6\u7684\u90fd\u4e00\u822c \u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002 0 , \"1.\u63a5\u7535\u6e90\u6ca1\u6709\u51e0\u5206\u949f,\u7535\u6e90\u9002\u914d\u5668\u70ed\u7684\u4e0d\u884c. 2.\u6444\u50cf\u5934\u7528\u4e0d\u8d77\u6765. 3.\u673a\u76d6\u7684\u94a2\u7434\u6f06\uff0c\u624b\u4e0d\u80fd\u6478\uff0c\u4e00\u6478\u4e00\u4e2a\u5370. 4.\u786c\u76d8\u5206\u533a\u4e0d\u597d\u529e.\" 1 , \"\u4eca\u5929\u624d\u77e5\u9053\u8fd9\u4e66\u8fd8\u6709\u7b2c6\u5377,\u771f\u6709\u70b9\u90c1\u95f7:\u4e3a\u4ec0\u4e48\u540c\u4e00\u5957\u4e66\u6709\u4e24\u79cd\u7248\u672c\u5462?\u5f53\u5f53\u7f51\u662f\u4e0d\u662f\u8be5\u8ddf\u51fa\u7248\u793e\u5546\u91cf\u5546\u91cf,\u5355\u72ec\u51fa\u4e2a\u7b2c6\u5377,\u8ba9\u6211\u4eec\u7684\u5b69\u5b50\u4e0d\u4f1a\u6709\u6240\u9057\u61be\u3002\" \u5bfc\u5165\u5de5\u5177\u5305\u548c\u8f85\u52a9\u5de5\u5177\u5b9e\u4f8b\u5316\u5bf9\u8c61 import torch from torch.utils.data import DataLoader from datasets import load_dataset from transformers import BertTokenizer , BertModel from transformers import AdamW import time # \u52a0\u8f7d\u5b57\u5178\u548c\u5206\u8bcd\u5de5\u5177 my_tokenizer = BertTokenizer . from_pretrained ( 'bert-base-chinese' ) # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b my_model_pretrained = BertModel . from_pretrained ( 'bert-base-chinese' )","title":"2 \u6570\u636e\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#3_1","text":"\u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42\u3002 \u6570\u636e\u9884\u5904\u7406\u548c\u76f8\u5173\u6d4b\u8bd5\u51fd\u6570 # \u6570\u636e\u96c6\u5904\u7406\u81ea\u5b9a\u4e49\u51fd\u6570 def collate_fn2 ( data ): sents = [ i [ 'text' ] for i in data ] # \u6587\u672c\u6570\u503c\u5316 data = my_tokenizer . batch_encode_plus ( batch_text_or_text_pairs = sents , truncation = True , padding = 'max_length' , max_length = 32 , return_tensors = 'pt' , return_length = True ) # input_ids \u7f16\u7801\u4e4b\u540e\u7684\u6570\u5b57 # attention_mask \u662f\u8865\u96f6\u7684\u4f4d\u7f6e\u662f0,\u5176\u4ed6\u4f4d\u7f6e\u662f1 input_ids = data [ 'input_ids' ] attention_mask = data [ 'attention_mask' ] token_type_ids = data [ 'token_type_ids' ] # \u628a\u7b2c16\u4e2a\u8bcd\u56fa\u5b9a\u66ff\u6362\u4e3amask labels = input_ids [:, 16 ] . reshape ( - 1 ) . clone () # \u53d6\u51fa\u6570\u636e8\u53e5\u8bdd \u5728\u7b2c16\u4e2a\u4f4d\u7f6eclone\u51fa\u6765 \u505a\u6807\u7b7e input_ids [:, 16 ] = my_tokenizer . get_vocab ()[ my_tokenizer . mask_token ] labels = torch . LongTensor ( labels ) # tmpa = input_ids[:, 16] # print('tmpa--->', tmpa, tmpa.shape) # torch.Size([8] # print('labels-->', labels.shape, labels) # torch.Size([8] return input_ids , attention_mask , token_type_ids , labels # \u6570\u636e\u6e90 \u6570\u636e\u8fed\u4ee3\u5668 \u6d4b\u8bd5 def dm01_test_dataset (): # \u751f\u6210\u6570\u636e\u6e90dataset\u5bf9\u8c61 dataset_train_tmp = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) # print('dataset_train_tmp--->', dataset_train_tmp) # \u6309\u7167\u6761\u4ef6\u8fc7\u6ee4\u6570\u636e\u6e90\u5bf9\u8c61 my_dataset_train = dataset_train_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) # print('my_dataset_train--->', my_dataset_train) # print('my_dataset_train[0:3]-->', my_dataset_train[0:3]) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) # \u4e0d\u8bad\u7ec3,\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) print ( ' \\n \u7b2c1\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 0 ])) print ( my_tokenizer . decode ( labels [ 0 ])) print ( ' \\n \u7b2c2\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 1 ])) print ( my_tokenizer . decode ( labels [ 1 ])) break \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c mydataloader ---> < torch . utils . data . dataloader . DataLoader object at 0x7fbe28611cd0 > torch . Size ([ 8 , 32 ]) torch . Size ([ 8 , 32 ]) torch . Size ([ 8 , 32 ]) tensor ([ 3160 , 1905 , 8152 , 7415 , 7231 , 1331 , 2360 , 5831 ]) \u7b2c1\u53e5mask\u7684\u4fe1\u606f [ CLS ] \u6587 \u7ae0 \u603b \u4f53 \u4e0d \u9519 \uff0c \u4e0d \u8fc7 \u592a \u591a \u7684 \u5386 \u53f2 \u8d44 [ MASK ] \u91cd \u590d \uff0c \u611f \u89c9 \u6ca1 \u6709 \u6b65 \u6b65 \u597d \u770b \u4e86 \uff0c \u4f5c [ SEP ] \u6599 \u7b2c2\u53e5mask\u7684\u4fe1\u606f [ CLS ] \u7b2c \u4e00 \u6563 \u70ed \u4e00 \u822c \u3002 \u3002 \u3002 \u7528 \u4e86 \u4e00 \u4f1a \u6258 \u8155 [ MASK ] \u5c31 \u53d1 \u70eb \u4e86 ? \u7b2c \u4e8c \u6db2 \u6676 \u5c4f \u548c \u4e3b \u673a \u90e8 [ SEP ] \u5904 \u8fc1\u79fb\u5b66\u4e60 \u4e2d\u6587\u586b\u7a7a End","title":"3 \u6570\u636e\u9884\u5904\u7406"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#4_1","text":"\u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b # \u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b class MyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () # \u5b9a\u4e49\u5168\u8fde\u63a5\u5c42 self . decoder = torch . nn . Linear ( 768 , my_tokenizer . vocab_size , bias = False ) # \u8bbe\u7f6e\u5168\u8fde\u63a5\u5c42\u504f\u7f6e\u4e3a\u96f6 self . decoder . bias = torch . nn . Parameter ( torch . zeros ( my_tokenizer . vocab_size )) def forward ( self , input_ids , attention_mask , token_type_ids ): # \u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u8fdb\u884c\u8bad\u7ec3 with torch . no_grad (): out = my_model_pretrained ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3 \u5f62\u72b6[8,768] ---> [8, 21128] out = self . decoder ( out . last_hidden_state [:, 16 ]) # \u8fd4\u56de return out \u6a21\u578b\u6d4b\u8bd5 # \u6a21\u578b\u8f93\u5165\u548c\u8f93\u51fa\u6d4b\u8bd5 def dm02_test_mymodel (): # \u751f\u6210\u6570\u636e\u6e90dataset\u5bf9\u8c61 dataset_train_tmp = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) # print('dataset_train_tmp--->', dataset_train_tmp) # \u6309\u7167\u6761\u4ef6\u8fc7\u6ee4\u6570\u636e\u6e90\u5bf9\u8c61 my_dataset_train = dataset_train_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) # print('my_dataset_train--->', my_dataset_train) # print('my_dataset_train[0:3]-->', my_dataset_train[0:3]) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) # \u4e0d\u8bad\u7ec3,\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b mymodel = MyModel () # \u8c03\u6574\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61\u6570\u636e\u8fd4\u56de\u683c\u5f0f for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( mydataloader ): print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) print ( ' \\n \u7b2c1\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 0 ])) print ( my_tokenizer . decode ( labels [ 0 ])) print ( ' \\n \u7b2c2\u53e5mask\u7684\u4fe1\u606f' ) print ( my_tokenizer . decode ( input_ids [ 1 ])) print ( my_tokenizer . decode ( labels [ 1 ])) # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,768] ---> [8,21128] \u586b\u7a7a\u5c31\u662f\u5206\u7c7b 21128\u4e2a\u5355\u8bcd\u4e2d\u627e\u4e00\u4e2a\u5355\u8bcd myout = mymodel ( input_ids , attention_mask , token_type_ids ) print ( 'myout--->' , myout . shape , myout ) break \u8f93\u51fa\u6548\u679c mydataloader---> <torch.utils.data.dataloader.DataLoader object at 0x7fd7c0765c10> torch.Size([8, 32]) torch.Size([8, 32]) torch.Size([8, 32]) tensor([ 702, 6381, 857, 8024, 2218, 1961, 3175, 2141]) \u7b2c1\u53e5mask\u7684\u4fe1\u606f [CLS] \u5f88 \u91cd, \u80cc \u51fa \u53bb \u5f88 \u7d2f. \u597d \u8c61 \u5c4f \u5e55 \u4e0a \u6709 [MASK] \u4eae \u70b9, \u4e0d \u8fc7 \u5f88 \u5c0f ( \u5c4f \u672c \u6765 \u5c31 \u5f88 \u5c0f [SEP] \u4e2a \u7b2c2\u53e5mask\u7684\u4fe1\u606f [CLS] \u6211 \u4e8e 11 \u6708 22 \u65e5 \u8ba2 \u8d2d \u4e86 \u300a \u675c \u62c9 \u62c9 \u5347 \u804c [MASK] \u300b \u5e76 \u5df2 \u901a \u8fc7 \u94f6 \u884c \u4ed8 \u6b3e \uff0c \u4e3a \u4ec0 \u4e48 \u8ba2 [SEP] \u8bb0 myout---> torch.Size([8, 21128]) tensor([[-0.3201, 0.3877, 0.1041, ..., 0.2262, 0.5397, 0.5053], [ 0.0626, 0.1335, 0.7057, ..., -0.6277, -0.2287, -0.0532], [ 0.3807, 0.2024, 0.0514, ..., -0.0113, 0.3084, 0.4678], ..., [ 0.3452, 0.1774, 0.0127, ..., -0.3960, 0.2417, -0.0260], [-0.4155, 0.2038, 0.2512, ..., -0.4112, -0.1052, 0.3574], [ 0.3464, 0.3439, 0.6628, ..., -0.1706, 0.1020, 0.4141]], grad_fn=<AddmmBackward>)","title":"4 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#5_1","text":"# \u6a21\u578b\u8bad\u7ec3 - \u586b\u7a7a def dm03_train_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_train dataset_train_tmp = load_dataset ( 'csv' , data_files = './mydata1/train.csv' , split = \"train\" ) my_dataset_train = dataset_train_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) print ( 'my_dataset_train--->' , my_dataset_train ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model my_model = MyModel () # \u5b9e\u4f8b\u5316\u4f18\u5316\u5668my_optimizer my_optimizer = AdamW ( my_model . parameters (), lr = 5e-4 ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570my_criterion my_criterion = torch . nn . CrossEntropyLoss () # \u4e0d\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b \u53ea\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6570\u636e\u7279\u5f81 \u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 epochs = 3 # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b my_model . train () # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for eporch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61my_dataloader my_dataloader = torch . utils . data . DataLoader ( my_dataset_train , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) starttime = ( int )( time . time ()) # \u5185\u5c42for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_dataloader , start = 1 ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,32] --> [8,21128] my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8ba1\u7b97\u635f\u5931 my_loss = my_criterion ( my_out , labels ) # \u68af\u5ea6\u6e05\u96f6 my_optimizer . zero_grad () # \u53cd\u5411\u4f20\u64ad my_loss . backward () # \u68af\u5ea6\u66f4\u65b0 my_optimizer . step () # \u6bcf5\u6b21\u8fed\u4ee3 \u7b97\u4e00\u4e0b\u51c6\u786e\u7387 if i % 20 == 0 : out = my_out . argmax ( dim = 1 ) # [8,21128] --> (8,) accuracy = ( out == labels ) . sum () . item () / len ( labels ) print ( '\u8f6e\u6b21: %d \u8fed\u4ee3\u6570: %d \u635f\u5931: %.6f \u51c6\u786e\u7387 %.3f \u65f6\u95f4 %d ' \\ % ( eporch_idx , i , my_loss . item (), accuracy , ( int )( time . time ()) - starttime )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_model . state_dict (), './my_model_mask_ %d .bin' % ( eporch_idx + 1 )) \u6a21\u578b\u8bad\u7ec3\u6548\u679c\u8f93\u51fa \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 680 \u635f\u5931 : 0.324525 \u51c6\u786e\u73870 .875 \u65f6\u95f4370 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 700 \u635f\u5931 : 0.776103 \u51c6\u786e\u73870 .750 \u65f6\u95f4381 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 720 \u635f\u5931 : 0.681674 \u51c6\u786e\u73870 .875 \u65f6\u95f4392 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 740 \u635f\u5931 : 0.654384 \u51c6\u786e\u73870 .750 \u65f6\u95f4402 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 760 \u635f\u5931 : 0.612616 \u51c6\u786e\u73870 .875 \u65f6\u95f4413 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 780 \u635f\u5931 : 0.918874 \u51c6\u786e\u73870 .625 \u65f6\u95f4424 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 800 \u635f\u5931 : 0.640087 \u51c6\u786e\u73870 .750 \u65f6\u95f4435 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 820 \u635f\u5931 : 0.410612 \u51c6\u786e\u73871 .000 \u65f6\u95f4446 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 840 \u635f\u5931 : 0.395016 \u51c6\u786e\u73871 .000 \u65f6\u95f4457 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 860 \u635f\u5931 : 0.313001 \u51c6\u786e\u73870 .875 \u65f6\u95f4468 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 880 \u635f\u5931 : 1.534165 \u51c6\u786e\u73870 .750 \u65f6\u95f4479 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 900 \u635f\u5931 : 0.384014 \u51c6\u786e\u73870 .875 \u65f6\u95f4490 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 920 \u635f\u5931 : 0.386283 \u51c6\u786e\u73871 .000 \u65f6\u95f4501 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 940 \u635f\u5931 : 1.168535 \u51c6\u786e\u73870 .750 \u65f6\u95f4512 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 960 \u635f\u5931 : 0.568617 \u51c6\u786e\u73870 .875 \u65f6\u95f4522 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 980 \u635f\u5931 : 1.178597 \u51c6\u786e\u73870 .750 \u65f6\u95f4533 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1000 \u635f\u5931 : 0.534274 \u51c6\u786e\u73870 .875 \u65f6\u95f4544 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1020 \u635f\u5931 : 0.371301 \u51c6\u786e\u73871 .000 \u65f6\u95f4555 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1040 \u635f\u5931 : 0.106059 \u51c6\u786e\u73871 .000 \u65f6\u95f4566 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1060 \u635f\u5931 : 1.153722 \u51c6\u786e\u73870 .750 \u65f6\u95f4577 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1080 \u635f\u5931 : 0.352150 \u51c6\u786e\u73871 .000 \u65f6\u95f4588 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1100 \u635f\u5931 : 1.242567 \u51c6\u786e\u73870 .625 \u65f6\u95f4598 \u8f6e\u6b21 : 2 \u8fed\u4ee3\u6570 : 1120 \u635f\u5931 : 0.548795 \u51c6\u786e\u73870 .875 \u65f6\u95f4609 # \u53ea\u9700\u89813\u4e2a\u8f6e\u6b21\uff0c\u5c31\u53ef\u4ee5\u586b\u7a7a\u51c6\u786e\u7387\u8fbe\u523088%\u4ee5\u4e0a","title":"5 \u6a21\u578b\u8bad\u7ec3"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#6_1","text":"# \u6a21\u578b\u6d4b\u8bd5\uff1a\u586b\u7a7a def dm04_evaluate_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test print ( ' \\n \u52a0\u8f7d\u6d4b\u8bd5\u96c6' ) my_dataset_tmp = load_dataset ( 'csv' , data_files = './mydata1/test.csv' , split = 'train' ) my_dataset_test = my_dataset_tmp . filter ( lambda x : len ( x [ 'text' ]) > 32 ) print ( 'my_dataset_test--->' , my_dataset_test ) # print(my_dataset_test[0:3]) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model path = './my_model_mask_3.bin' my_model = MyModel () my_model . load_state_dict ( torch . load ( path )) print ( 'my_model-->' , my_model ) # \u8bbe\u7f6e\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f my_model . eval () # \u8bbe\u7f6e\u8bc4\u4f30\u53c2\u6570 correct = 0 total = 0 # \u5b9e\u4f8b\u5316\u5316dataloader my_loader_test = torch . utils . data . DataLoader ( my_dataset_test , batch_size = 8 , collate_fn = collate_fn2 , shuffle = True , drop_last = True ) # \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_loader_test ): with torch . no_grad (): my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) out = my_out . argmax ( dim = 1 ) correct += ( out == labels ) . sum () . item () total += len ( labels ) if i % 25 == 0 : print ( i + 1 , my_tokenizer . decode ( input_ids [ 0 ])) print ( '\u9884\u6d4b\u503c:' , my_tokenizer . decode ( out [ 0 ]), ' \\t \u771f\u5b9e\u503c:' , my_tokenizer . decode ( labels [ 0 ])) print ( correct / total ) \u8f93\u51fa\u6548\u679c: 26 [CLS] \u5927 \u5802 \u662f \u55a7 \u95f9 \u7684 \uff0c \u623f \u95f4 \u5730 \u6bef \u662f \u810f \u7684 \uff0c [MASK] \u5ba4 \u6dcb \u6d74 \u9f99 \u5934 \u662f \u574f \u7684 \uff0c \u6bdb \u5dfe \u662f \u9ec4 \u989c [SEP] \u9884\u6d4b\u503c: \u5bdd \u771f\u5b9e\u503c: \u6d74 0.62 51 [CLS] \u5916 \u89c2 \u6f02 \u4eae \uff0c 13 \u5bf8 \u9ec4 \u91d1 \u6bd4 \u4f8b \uff0c \u60ac \u6d6e \u952e [MASK] \u8bbe \u8ba1 \uff0c \u566a \u97f3 \u5f88 \u4f4e \uff0c \u548c \u6211 \u516c \u53f8 \u53d1 \u7684 [SEP] \u9884\u6d4b\u503c: \u76d8 \u771f\u5b9e\u503c: \u76d8 0.6625 76 [CLS] \u67d0 \u4e9b \u9152 \u5e97 \u4eba \u5458 \u5bf9 \u5f85 \u987e \u5ba2 \u4e0d \u8bda \u6073 \u3002 \u8bf4 [MASK] \u4f60 \u6362 \u623f \u95f4 \uff0c \u9a97 \u4f60 \u8bf4 \u662f \u4ef7 \u94b1 \u8d35 \u7684 \u623f [SEP] \u9884\u6d4b\u503c: \u8ba9 \u771f\u5b9e\u503c: \u7ed9 0.68 101 [CLS] \u770b \u8fd9 \u672c \u4e66 \u6709 \u79cd \u611f \u89c9 \u5c31 \u662f \u4f5c \u8005 \u5728 \u82e6 \u53e3 [MASK] \u5fc3 \u7684 \u544a \u6212 \u8bfb \u8005 \u5982 \u4f55 \u5982 \u4f55 \uff0c \u4e3a \u4e86 \u8bc1 [SEP] \u9884\u6d4b\u503c: \u5a46 \u771f\u5b9e\u503c: \u5a46 0.6775 126 [CLS] \u623f \u95f4 \u5f88 \u8212 \u9002 \u4e5f \u5f88 \u5e72 \u51c0 \uff0c \u5468 \u56f4 \u867d \u7136 \u4e0d [MASK] \u95f9 \u4f46 \u5f88 \u5b89 \u9759 \uff0c \u79bb \u8679 \u6865 \u673a \u573a \u5f88 \u9759 \uff0c [SEP] \u9884\u6d4b\u503c: \u5435 \u771f\u5b9e\u503c: \u70ed 0.681","title":"6 \u6a21\u578b\u8bc4\u4f30"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#4-","text":"","title":"4 \u8fc1\u79fb\u5b66\u4e60-\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#1_3","text":"\u4e0b\u4e00\u53e5\u8bdd\u4efb\u52a1NSP ( Next Sentence Prediction ) \u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1 \u3002 \u8f93\u51652\u53e5\u8bdd \uff0c \u5224\u65ad\u7b2c\u4e8c\u53e5\u662f\u5426\u4e3a\u7b2c\u4e00\u53e5\u7684\u4e0b\u534a\u53e5 \u3002","title":"1 \u4efb\u52a1\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#2_2","text":"\u6570\u636e\u6587\u4ef6\u6709\u4e09\u4e2atrain.csv\uff0ctest.csv\uff0cvalidation.csv\uff0c\u6570\u636e\u6837\u5f0f\u90fd\u662f\u4e00\u6837\u7684\u3002 label , text 1 , \u9009\u62e9\u73e0\u6c5f\u82b1\u56ed\u7684\u539f\u56e0\u5c31\u662f\u65b9\u4fbf \uff0c \u6709\u7535\u52a8\u6276\u68af\u76f4\u63a5\u5230\u8fbe\u6d77\u8fb9 \uff0c \u5468\u56f4\u9910\u9986 \u3001 \u98df\u5eca \u3001 \u5546\u573a \u3001 \u8d85\u5e02 \u3001 \u644a\u4f4d\u4e00\u5e94\u4ff1\u5168 \u3002 \u9152\u5e97\u88c5\u4fee\u4e00\u822c \uff0c \u4f46\u8fd8\u7b97\u6574\u6d01 \u3002 \u6cf3\u6c60\u5728\u5927\u5802\u7684\u5c4b\u9876 \uff0c \u56e0\u6b64\u5f88\u5c0f \uff0c \u4e0d\u8fc7\u5973\u513f\u5012\u662f\u559c\u6b22 \u3002 \u5305\u7684\u65e9\u9910\u662f\u897f\u5f0f\u7684 \uff0c \u8fd8\u7b97\u4e30\u5bcc \u3002 \u670d\u52a1\u5417 \uff0c \u4e00\u822c 1 , 15.4 \u5bf8\u7b14\u8bb0\u672c\u7684\u952e\u76d8\u786e\u5b9e\u723d \uff0c \u57fa\u672c\u8ddf\u53f0\u5f0f\u673a\u5dee\u4e0d\u591a\u4e86 \uff0c \u86ee\u559c\u6b22\u6570\u5b57\u5c0f\u952e\u76d8 \uff0c \u8f93\u6570\u5b57\u7279\u65b9\u4fbf \uff0c \u6837\u5b50\u4e5f\u5f88\u7f8e\u89c2 \uff0c \u505a\u5de5\u4e5f\u76f8\u5f53\u4e0d\u9519 0 , \u623f\u95f4\u592a\u5c0f \u3002 \u5176\u4ed6\u7684\u90fd\u4e00\u822c \u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002\u3002 0 , \"1.\u63a5\u7535\u6e90\u6ca1\u6709\u51e0\u5206\u949f,\u7535\u6e90\u9002\u914d\u5668\u70ed\u7684\u4e0d\u884c. 2.\u6444\u50cf\u5934\u7528\u4e0d\u8d77\u6765. 3.\u673a\u76d6\u7684\u94a2\u7434\u6f06\uff0c\u624b\u4e0d\u80fd\u6478\uff0c\u4e00\u6478\u4e00\u4e2a\u5370. 4.\u786c\u76d8\u5206\u533a\u4e0d\u597d\u529e.\" 1 , \"\u4eca\u5929\u624d\u77e5\u9053\u8fd9\u4e66\u8fd8\u6709\u7b2c6\u5377,\u771f\u6709\u70b9\u90c1\u95f7:\u4e3a\u4ec0\u4e48\u540c\u4e00\u5957\u4e66\u6709\u4e24\u79cd\u7248\u672c\u5462?\u5f53\u5f53\u7f51\u662f\u4e0d\u662f\u8be5\u8ddf\u51fa\u7248\u793e\u5546\u91cf\u5546\u91cf,\u5355\u72ec\u51fa\u4e2a\u7b2c6\u5377,\u8ba9\u6211\u4eec\u7684\u5b69\u5b50\u4e0d\u4f1a\u6709\u6240\u9057\u61be\u3002\" \u5bfc\u5165\u5de5\u5177\u5305\u548c\u8f85\u52a9\u5de5\u5177\u5b9e\u4f8b\u5316\u5bf9\u8c61 import torch from torch.utils.data import DataLoader from datasets import load_dataset from transformers import BertTokenizer , BertModel from transformers import AdamW import time # \u52a0\u8f7d\u5b57\u5178\u548c\u5206\u8bcd\u5de5\u5177 my_tokenizer = BertTokenizer . from_pretrained ( 'bert-base-chinese' ) # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b my_model_pretrained = BertModel . from_pretrained ( 'bert-base-chinese' )","title":"2 \u6570\u636e\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#3_2","text":"\u5bf9\u6301\u4e45\u5316\u6587\u4ef6\u4e2d\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u6ee1\u8db3\u6a21\u578b\u8bad\u7ec3\u8981\u6c42\u3002 \u6570\u636e\u9884\u5904\u7406\u548c\u76f8\u5173\u6d4b\u8bd5\u51fd\u6570 # \u5b9a\u4e49\u6570\u636e\u6e90\u7c7b class MyDataSet ( Dataset ): def __init__ ( self , data_csv_files ): # \u751f\u6210\u6570\u636e\u6e90dataset\u5bf9\u8c61 my_dataset_temp = load_dataset ( 'csv' , data_files = data_csv_files , split = \"train\" ) # print('my_dataset_temp--->', my_dataset_temp) # \u6309\u7167\u6761\u4ef6\u8fc7\u6ee4\u6570\u636e\u6e90\u5bf9\u8c61 self . my_dataset = my_dataset_temp . filter ( lambda x : len ( x [ 'text' ]) > 44 ) # print('self.my_dataset--->', self.my_dataset) # print('self.my_dataset[0:3]-->', self.my_dataset[0:3]) def __len__ ( self ): self . len = len ( self . my_dataset ) return self . len # 7472 def __getitem__ ( self , index ): # 1\u662f\u4e0b\u4e00\u53e5\u8bdd 0\u4e0d\u662f\u4e0b\u4e00\u53e5 label = 1 text = self . my_dataset [ index ][ 'text' ] sentence1 = text [ 0 : 22 ] sentence2 = text [ 22 : 44 ] # \u4ea7\u751f\u8d1f\u6837\u672c: \u968f\u673a\u4ea7\u751f0\u548c1 \u4e00\u822c\u6982\u7387\u9009\u4e2d0, \u66ff\u6362\u4e3a\u65e0\u5173\u7684\u4e00\u53e5\u8bdd if random . randint ( 0 , 1 ) == 0 : j = random . randint ( 0 , self . len - 1 ) sentence2 = self . my_dataset [ j ][ 'text' ][ 22 : 44 ] label = 0 # \u8fd4\u56de\u4e24\u53e5\u8bdd \u548c\u4e24\u53e5\u8bdd\u4e4b\u95f4\u7684\u5173\u7cfb return sentence1 , sentence2 , label # \u6570\u636e\u96c6\u5904\u7406\u81ea\u5b9a\u4e49\u51fd\u6570 def collate_fn3 ( data ): sents = [ i [: 2 ] for i in data ] labels = [ i [ 2 ] for i in data ] # \u6587\u672c\u6570\u503c\u5316 data = my_tokenizer . batch_encode_plus ( batch_text_or_text_pairs = sents , truncation = True , padding = 'max_length' , max_length = 50 , # 44+cls+sep+sep+other = 44+3=47 return_tensors = 'pt' , return_length = True ) # input_ids \u7f16\u7801\u4e4b\u540e\u7684\u6570\u5b57 # attention_mask \u662f\u8865\u96f6\u7684\u4f4d\u7f6e\u662f0,\u5176\u4ed6\u4f4d\u7f6e\u662f1 input_ids = data [ 'input_ids' ] attention_mask = data [ 'attention_mask' ] token_type_ids = data [ 'token_type_ids' ] # \u6ce8\u610flabels\u4e0d\u8981\u5fd8\u8bb0\u9700\u8981\u8f6c\u6210tensor 1\u7ef4\u6570\u7ec4 labels = torch . LongTensor ( labels ) return input_ids , attention_mask , token_type_ids , labels \u6570\u636e\u6e90 \u6570\u636e\u8fed\u4ee3\u5668\u6d4b\u8bd5 # \u5b9a\u4e49\u6570\u636e\u6e90 DataSet def dm01_test_dataset (): data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) print ( 'mydataset-->' , mydataset , len ( mydataset )) # print(mydataset[3]) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , Tshuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) for ( input_ids , attention_mask , token_type_ids , labels ) in mydataloader : print ( my_tokenizer . decode ( input_ids [ 0 ])) # \u6253\u5370\u6bcf\u4e2a\u6279\u6b21\u7684\u7b2c1\u53e5\u8bdd print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) break \u7a0b\u5e8f\u8fd0\u884c\u6548\u679c mydataset --> < __main__ . MyDataSet object at 0x7fb4a8471ad0 > 7472 mydataloader ---> < torch . utils . data . dataloader . DataLoader object at 0x7fb44835d8d0 > # \u4e0b\u97622\u53e5\u8bdd \u4e0d\u662f\u4e0a\u4e0b\u6587\u5173\u7cfb [ CLS ] \u9009 \u62e9 \u73e0 \u6c5f \u82b1 \u56ed \u7684 \u539f \u56e0 \u5c31 \u662f \u65b9 \u4fbf \uff0c \u6709 \u7535 \u52a8 \u6276 \u68af \u76f4 \u63a5 \u5230 [ SEP ] \u8fbe \u6d77 \u8fb9 \uff0c \u5468 \u56f4 \u9910 \u9986 \u3001 \u98df \u5eca \u3001 \u5546 \u573a \u3001 \u8d85 \u5e02 \u3001 \u644a \u4f4d \u4e00 \u5e94 [ SEP ] [ PAD ] [ PAD ] [ PAD ] torch . Size ([ 8 , 50 ]) torch . Size ([ 8 , 50 ]) torch . Size ([ 8 , 50 ]) tensor ([ 1 , 1 , 1 , 0 , 0 , 0 , 1 , 0 ]) # \u4e0b\u97622\u53e5\u8bdd \u662f\u4e0a\u7ebf\u6587\u5173\u7cfb [ CLS ] \u9009 \u62e9 \u73e0 \u6c5f \u82b1 \u56ed \u7684 \u539f \u56e0 \u5c31 \u662f \u65b9 \u4fbf \uff0c \u6709 \u7535 \u52a8 \u6276 \u68af \u76f4 \u63a5 \u5230 [ SEP ] \u5fd9 \u5fd9 \u788c \u788c \u9547 \u548c \u9752 \u86d9 \u5f17 \u6d1b \u683c \u7684 \u6210 \u957f \u6545 \u4e8b \uff0c \u90a3 \u51e0 \u672c \u4e66 \u6211 [ SEP ] [ PAD ] [ PAD ] [ PAD ]","title":"3 \u6570\u636e\u9884\u5904\u7406"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#4_2","text":"\u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b # \u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bNSP class MyModel ( torch . nn . Module ): def __init__ ( self ): super () . __init__ () # \u5b9a\u4e49\u5168\u8fde\u63a5\u5c42 self . fc = torch . nn . Linear ( 768 , 2 ) def forward ( self , input_ids , attention_mask , token_type_ids ): # \u9884\u8bad\u7ec3\u6a21\u578b\u4e0d\u8bad\u7ec3 with torch . no_grad (): out = my_model_pretrained ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3 \u6570\u636e\u7ecf\u8fc7\u5168\u8fde\u63a5\u5c42 out = self . fc ( out . last_hidden_state [:, 0 ]) # \u6570\u636e\u8fdb\u884csoftmax\u5f52\u4e00\u5316 \u5206\u7c7b\u6982\u7387\u503c out = out . softmax ( dim = 1 ) return out \u6a21\u578b\u6d4b\u8bd5 # NSP\u6a21\u578b\u8f93\u5165\u548c\u8f93\u51fa\u6d4b\u8bd5 def dm02_test_mymodel (): data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) print ( 'mydataset-->' , mydataset , len ( mydataset )) # \u901a\u8fc7dataloader\u8fdb\u884c\u8fed\u4ee3 mydataloader = DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , shuffle = True , drop_last = True ) print ( 'mydataloader--->' , mydataloader ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b mymodel = MyModel () # \u7ed9\u6a21\u578b\u5582\u6570\u636e for ( input_ids , attention_mask , token_type_ids , labels ) in mydataloader : print ( my_tokenizer . decode ( input_ids [ 0 ])) print ( input_ids . shape , attention_mask . shape , token_type_ids . shape , labels ) # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,768] ---> [8,2] nsp\u4efb\u52a1\u662f\u4e8c\u5206\u7c7b myout = mymodel ( input_ids , attention_mask , token_type_ids ) print ( 'myout--->' , myout . shape , myout ) break \u8f93\u51fa\u6548\u679c mydataset--> <__main__.MyDataSet object at 0x7fe4b86b8610> 7472 mydataloader---> <torch.utils.data.dataloader.DataLoader object at 0x7fe4d8b4ad50> # \u8fd92\u53e5\u8bdd \u7b2c2\u53e5\u8bdd\u662f\u7b2c1\u53e5\u8bdd\u7684\u4e0b\u6587 [CLS] \u6211 \u5bb6 \u5c0f \u5b9d \u540c \u5b66 \u7279 \u522b \u559c \u7231 \u5361 \u6885 \u62c9 \u7cfb \u5217 \u6545 \u4e8b, \u6bcf \u5929 \u665a \u4e0a [SEP] \u7761 \u89c9 \u524d \u975e \u8bb2 \u4e0d \u53ef, \u90a3 \u4e9b \u53ef \u7231 \u7684 \u6545 \u4e8b \u4f1a \u966a \u7740 \u4ed6 \u8fdb \u5165 \u68a6 [SEP] [PAD] [PAD] [PAD] torch.Size([8, 50]) torch.Size([8, 50]) torch.Size([8, 50]) tensor([1, 0, 1, 1, 1, 1, 0, 1]) myout---> torch.Size([8, 2]) tensor([[0.8149, 0.1851], [0.4211, 0.5789], [0.7687, 0.2313], [0.7724, 0.2276], [0.7527, 0.2473], [0.7036, 0.2964], [0.7887, 0.2113], [0.7259, 0.2741]], grad_fn=<SoftmaxBackward>) # \u8fd92\u53e5\u8bdd \u7b2c2\u53e5\u8bdd\u4e0d\u662f\u7b2c1\u53e5\u8bdd\u7684\u4e0b\u6587 [CLS] \u6628 \u5929 \u6211 \u628a \u8fd9 \u5957 \u4e66 \u770b \u5b8c \u4e86 \uff0c \u7ed3 \u5c3e \u6211 \u4e0d \u662f \u5f88 \u559c \u6b22 \u6709 \u70b9 \u592a [SEP] \u4e00 \u90e8 \u5206 \u6ca1 \u610f \u601d, \u800c \u4e14 \u4e0d \u503c \u90a3 \u4e48 \u591a \u94b1, \u4e0a \u5f53 \u4e86, \u73b0 \u5728 [SEP] [PAD] [PAD] [PAD]","title":"4 \u81ea\u5b9a\u4e49\u4e0b\u6e38\u4efb\u52a1\u7f51\u7edc\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#5_2","text":"# \u6a21\u578b\u8bad\u7ec3NSP def dm03_train_model (): data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) # print('mydataset-->', mydataset, len(mydataset)) # print(mydataset[3]) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model my_model = MyModel () # \u5b9e\u4f8b\u5316\u4f18\u5316\u5668my_optimizer my_optimizer = AdamW ( my_model . parameters (), lr = 5e-4 ) # \u5b9e\u4f8b\u5316\u635f\u5931\u51fd\u6570my_criterion my_criterion = torch . nn . CrossEntropyLoss () # \u4e0d\u8bad\u7ec3\u9884\u8bad\u7ec3\u6a21\u578b \u53ea\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u6570\u636e\u7279\u5f81 \u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 for param in my_model_pretrained . parameters (): param . requires_grad_ ( False ) # \u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570 epochs = 3 # \u8bbe\u7f6e\u6a21\u578b\u4e3a\u8bad\u7ec3\u6a21\u578b my_model . train () # \u5916\u5c42for\u5faa\u73af \u63a7\u5236\u8f6e\u6570 for eporch_idx in range ( epochs ): # \u5b9e\u4f8b\u5316\u6570\u636e\u8fed\u4ee3\u5668\u5bf9\u8c61my_dataloader my_dataloader = torch . utils . data . DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , shuffle = True , drop_last = True ) starttime = ( int )( time . time ()) # \u5185\u5b58for\u5faa\u73af \u63a7\u5236\u8fed\u4ee3\u6b21\u6570 for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_dataloader ): # \u7ed9\u6a21\u578b\u5582\u6570\u636e [8,50] --> [8,2] my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) # \u8ba1\u7b97\u635f\u5931 my_loss = my_criterion ( my_out , labels ) # \u68af\u5ea6\u6e05\u96f6 my_optimizer . zero_grad () # \u53cd\u5411\u4f20\u64ad my_loss . backward () # \u68af\u5ea6\u66f4\u65b0 my_optimizer . step () # \u6bcf5\u6b21\u8fed\u4ee3 \u7b97\u4e00\u4e0b\u51c6\u786e\u7387 if i % 20 == 0 : out = my_out . argmax ( dim = 1 ) # [8,2] --> (8,) accuracy = ( out == labels ) . sum () . item () / len ( labels ) print ( '\u8f6e\u6b21: %d \u8fed\u4ee3\u6570: %d \u635f\u5931: %.6f \u51c6\u786e\u7387 %.3f \u65f6\u95f4 %d ' \\ % ( eporch_idx , i , my_loss . item (), accuracy , ( int )( time . time ()) - starttime )) # \u6bcf\u4e2a\u8f6e\u6b21\u4fdd\u5b58\u6a21\u578b torch . save ( my_model . state_dict (), './my_model_nsp_ %d .bin' % ( eporch_idx + 1 )) \u6a21\u578b\u8bad\u7ec3\u6548\u679c\u8f93\u51fa \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 20 \u635f\u5931 : 0.535745 \u51c6\u786e\u73870 .750 \u65f6\u95f413 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 40 \u635f\u5931 : 0.389325 \u51c6\u786e\u73871 .000 \u65f6\u95f426 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 60 \u635f\u5931 : 0.361616 \u51c6\u786e\u73871 .000 \u65f6\u95f439 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 80 \u635f\u5931 : 0.417564 \u51c6\u786e\u73870 .875 \u65f6\u95f452 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 100 \u635f\u5931 : 0.375896 \u51c6\u786e\u73871 .000 \u65f6\u95f465 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 120 \u635f\u5931 : 0.402824 \u51c6\u786e\u73870 .875 \u65f6\u95f478 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 140 \u635f\u5931 : 0.650159 \u51c6\u786e\u73870 .625 \u65f6\u95f491 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 160 \u635f\u5931 : 0.541370 \u51c6\u786e\u73870 .625 \u65f6\u95f4104 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 180 \u635f\u5931 : 0.613911 \u51c6\u786e\u73870 .625 \u65f6\u95f4117 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 200 \u635f\u5931 : 0.449493 \u51c6\u786e\u73870 .875 \u65f6\u95f4130 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 220 \u635f\u5931 : 0.545564 \u51c6\u786e\u73870 .750 \u65f6\u95f4143 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 240 \u635f\u5931 : 0.314993 \u51c6\u786e\u73871 .000 \u65f6\u95f4156 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 260 \u635f\u5931 : 0.409358 \u51c6\u786e\u73871 .000 \u65f6\u95f4169 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 280 \u635f\u5931 : 0.513622 \u51c6\u786e\u73870 .750 \u65f6\u95f4182 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 300 \u635f\u5931 : 0.360398 \u51c6\u786e\u73871 .000 \u65f6\u95f4197 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 320 \u635f\u5931 : 0.430326 \u51c6\u786e\u73870 .875 \u65f6\u95f4210 \u8f6e\u6b21 : 0 \u8fed\u4ee3\u6570 : 340 \u635f\u5931 : 0.379781 \u51c6\u786e\u73871 .000 \u65f6\u95f4224","title":"5 \u6a21\u578b\u8bad\u7ec3"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#6_2","text":"# \u6a21\u578b\u6d4b\u8bd5 def dm04_evaluate_model (): # \u5b9e\u4f8b\u5316\u6570\u636e\u6e90\u5bf9\u8c61my_dataset_test data_files = './mydata1/train.csv' mydataset = MyDataSet ( data_files ) # \u5b9e\u4f8b\u5316\u4e0b\u6e38\u4efb\u52a1\u6a21\u578bmy_model path = './my_model_nsp_3.bin' my_model = MyModel () my_model . load_state_dict ( torch . load ( path )) print ( 'my_model-->' , my_model ) # \u8bbe\u7f6e\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u4e3a\u8bc4\u4f30\u6a21\u5f0f my_model . eval () # \u8bbe\u7f6e\u8bc4\u4f30\u53c2\u6570 correct = 0 total = 0 # \u5b9e\u4f8b\u5316\u5316dataloader my_loader_test = torch . utils . data . DataLoader ( mydataset , batch_size = 8 , collate_fn = collate_fn3 , shuffle = True , drop_last = True ) # \u7ed9\u6a21\u578b\u9001\u6570\u636e \u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c for i , ( input_ids , attention_mask , token_type_ids , labels ) in enumerate ( my_loader_test ): with torch . no_grad (): my_out = my_model ( input_ids = input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids ) out = my_out . argmax ( dim = 1 ) correct += ( out == labels ) . sum () . item () total += len ( labels ) if i % 20 == 0 : print ( correct / total ) \u8f93\u51fa\u6548\u679c: # \u6bcf20\u6b21\u8fed\u4ee3 \u6253\u5370\u4e00\u4e0b\u51c6\u786e\u7387 0.875 0.9047619047619048 0.8871951219512195 0.8954918032786885 0.8981481481481481 0.8960396039603961 0.8915289256198347 0.8874113475177305 0.8843167701863354 0.8867403314917127 0.8868159203980099 0.8829185520361991 0.8858921161825726 0.8874521072796935 0.8883451957295374 0.8899501661129569","title":"6 \u6a21\u578b\u8bc4\u4f30"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#5-","text":"","title":"5 \u8fc1\u79fb\u5b66\u4e60-\u5fae\u8c03\u811a\u672c\u4e2d\u6587\u5206\u7c7b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#1_4","text":"\u4f7f\u7528\u6587\u672c\u4e8c\u5206\u7c7b\u7684\u4efb\u52a1\u7c7b\u578bSST-2\u7684\u5fae\u8c03\u811a\u672c\u5fae\u8c03\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b, \u540e\u63a5\u5e26\u6709\u5206\u7c7b\u8f93\u51fa\u5934\u7684\u9884\u5b9a\u4e49\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c. \u76ee\u6807\u662f\u5224\u65ad\u53e5\u5b50\u7684\u60c5\u611f\u503e\u5411 \u51c6\u5907\u4e2d\u6587\u9152\u5e97\u8bc4\u8bba\u7684\u60c5\u611f\u5206\u6790\u8bed\u6599, \u8bed\u6599\u6837\u5f0f\u4e0eSST-2\u6570\u636e\u96c6\u76f8\u540c, \u6807\u7b7e0\u4ee3\u8868\u5dee\u8bc4, \u6807\u7b7e1\u597d\u8bc4 \u8bed\u6599\u5b58\u653e\u5728\u4e0eglue_data/\u540c\u7ea7\u76ee\u5f55cn_data/\u4e0b, \u5176\u4e2d\u7684SST-2\u76ee\u5f55\u5305\u542btrain.csv\u548cdev.csv train.csv sentence label \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d,\u9910\u5385\u4e0d\u5206\u5438\u70df\u533a.\u623f\u95f4\u4e0d\u5206\u6709\u65e0\u70df\u623f. 0 \u53bb\u7684\u65f6\u5019 ,\u9152\u5e97\u5927\u5385\u548c\u9910\u5385\u5728\u88c5\u4fee,\u611f\u89c9\u5927\u5385\u6709\u70b9\u6324.\u7531\u4e8e\u9910\u5385\u88c5\u4fee\u672c\u6765\u8be5\u4eab\u53d7\u7684\u65e9\u996d,\u4e5f\u6ca1\u6709\u4eab\u53d7(\u4ed6\u4eec\u662f8\u70b9\u5f00\u59cb\u6bcf\u4e2a\u623f\u95f4\u9001,\u4f46\u662f\u6211\u65f6\u95f4\u6765\u4e0d\u53ca\u4e86)\u4e0d\u8fc7\u524d\u53f0\u670d\u52a1\u5458\u6001\u5ea6\u597d! 1 \u6709\u5f88\u957f\u65f6\u95f4\u6ca1\u6709\u5728\u897f\u85cf\u5927\u53a6\u4f4f\u4e86\uff0c\u4ee5\u524d\u53bb\u5317\u4eac\u5728\u8fd9\u91cc\u4f4f\u7684\u8f83\u591a\u3002\u8fd9\u6b21\u4f4f\u8fdb\u6765\u53d1\u73b0\u6362\u4e86\u6db2\u6676\u7535\u89c6\uff0c\u4f46\u7f51\u7edc\u4e0d\u662f\u5f88\u597d\uff0c\u4ed6\u4eec\u81ea\u5df1\u8bf4\u662f\u6536\u8d39\u7684\u539f\u56e0\u9020\u6210\u7684\u3002\u5176\u5b83\u8fd8\u597d\u3002 1 \u975e\u5e38\u597d\u7684\u5730\u7406\u4f4d\u7f6e\uff0c\u4f4f\u7684\u662f\u8c6a\u534e\u6d77\u666f\u623f\uff0c\u6253\u5f00\u7a97\u6237\u5c31\u53ef\u4ee5\u770b\u89c1\u6808\u6865\u548c\u6d77\u666f\u3002\u8bb0\u5f97\u5f88\u65e9\u4ee5\u524d\u4e5f\u4f4f\u8fc7\uff0c\u73b0\u5728\u91cd\u65b0\u88c5\u4fee\u4e86\u3002\u603b\u7684\u6765\u8bf4\u6bd4\u8f83\u6ee1\u610f\uff0c\u4ee5\u540e\u8fd8\u4f1a\u4f4f 1 \u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u623f\u95f4\u5c0f\u4e86\u4e00\u70b9\uff0c\u4f46\u662f\u5e72\u51c0\u6574\u6d01\uff0c\u5f88\u6709\u9999\u6e2f\u7684\u7279\u8272\uff0c\u6027\u4ef7\u6bd4\u8f83\u9ad8\uff0c\u63a8\u8350\u4e00\u4e0b\u54e6 1 \u9152\u5e97\u7684\u88c5\u4fee\u6bd4\u8f83\u9648\u65e7\uff0c\u623f\u95f4\u7684\u9694\u97f3\uff0c\u4e3b\u8981\u662f\u536b\u751f\u95f4\u7684\u9694\u97f3\u975e\u5e38\u5dee\uff0c\u53ea\u80fd\u7b97\u662f\u4e00\u822c\u7684 0 \u9152\u5e97\u6709\u70b9\u65e7\uff0c\u623f\u95f4\u6bd4\u8f83\u5c0f\uff0c\u4f46\u9152\u5e97\u7684\u4f4d\u5b50\u4e0d\u9519\uff0c\u5c31\u5728\u6d77\u8fb9\uff0c\u53ef\u4ee5\u76f4\u63a5\u53bb\u6e38\u6cf3\u30028\u697c\u7684\u6d77\u666f\u6253\u5f00\u7a97\u6237\u5c31\u662f\u6d77\u3002\u5982\u679c\u60f3\u4f4f\u5728\u70ed\u95f9\u7684\u5730\u5e26\uff0c\u8fd9\u91cc\u4e0d\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u9009\u62e9\uff0c\u4e0d\u8fc7\u5a01\u6d77\u57ce\u5e02\u771f\u7684\u6bd4\u8f83\u5c0f\uff0c\u6253\u8f66\u8fd8\u662f\u76f8\u5f53\u4fbf\u5b9c\u7684\u3002\u665a\u4e0a\u9152\u5e97\u95e8\u53e3\u51fa\u79df\u8f66\u6bd4\u8f83\u5c11\u3002 1 \u4f4d\u7f6e\u5f88\u597d\uff0c\u8d70\u8def\u5230\u6587\u5e99\u3001\u6e05\u51c9\u5bfa5\u5206\u949f\u90fd\u7528\u4e0d\u4e86\uff0c\u5468\u8fb9\u516c\u4ea4\u8f66\u5f88\u591a\u5f88\u65b9\u4fbf\uff0c\u5c31\u662f\u51fa\u79df\u8f66\u4e0d\u592a\u7231\u53bb\uff08\u8001\u57ce\u533a\u8def\u7a84\u7231\u5835\u8f66\uff09\uff0c\u56e0\u4e3a\u662f\u8001\u5bbe\u9986\u6240\u4ee5\u8bbe\u65bd\u8981\u9648\u65e7\u4e9b\uff0c 1 \u9152\u5e97\u8bbe\u5907\u4e00\u822c\uff0c\u5957\u623f\u91cc\u5367\u5ba4\u7684\u4e0d\u80fd\u4e0a\u7f51\uff0c\u8981\u5230\u5ba2\u5385\u53bb\u3002 0 dev.csv sentence label \u623f\u95f4\u91cc\u6709\u7535\u8111\uff0c\u867d\u7136\u623f\u95f4\u7684\u6761\u4ef6\u7565\u663e\u7b80\u964b\uff0c\u4f46\u73af\u5883\u3001\u670d\u52a1\u8fd8\u6709\u996d\u83dc\u90fd\u8fd8\u662f\u5f88\u4e0d\u9519\u7684\u3002\u5982\u679c\u4e0b\u6b21\u53bb\u65e0\u9521\uff0c\u6211\u8fd8\u662f\u4f1a\u9009\u62e9\u8fd9\u91cc\u7684\u3002 1 \u6211\u4eec\u662f5\u67081\u65e5\u901a\u8fc7\u643a\u7a0b\u7f51\u5165\u4f4f\u7684\uff0c\u6761\u4ef6\u662f\u592a\u5dee\u4e86\uff0c\u6839\u672c\u8fbe\u4e0d\u5230\u56db\u661f\u7ea7\u7684\u6807\u51c6\uff0c\u6240\u6709\u7684\u4e1c\u897f\u90fd\u5f88\u9648\u65e7\uff0c\u536b\u751f\u95f4\u6c34\u9f99\u5934\u7528\u5b8c\u7adf\u5173\u4e0d\u4e0a\uff0c\u6d74\u7f38\u7684\u6f06\u9762\u90fd\u6389\u4e86\uff0c\u4f30\u8ba1\u662f\u5341\u5e74\u524d\u7684\u56db\u661f\u7ea7\u5427\uff0c\u603b\u4e4b\u4e0b\u6b21\u662f\u4e0d\u4f1a\u5165\u4f4f\u4e86\u3002 0 \u79bb\u706b\u8f66\u7ad9\u5f88\u8fd1\u5f88\u65b9\u4fbf\u3002\u4f4f\u5728\u4e1c\u697c\u6807\u95f4\uff0c\u76f8\u6bd4\u8f83\u5728\u4e5d\u6c5f\u4f4f\u7684\u53e6\u4e00\u5bb6\u9152\u5e97\uff0c\u623f\u95f4\u6bd4\u8f83\u5927\u3002\u536b\u751f\u95f4\u8bbe\u65bd\u7565\u65e7\u3002\u670d\u52a1\u8fd8\u597d\u300210\u5143\u4e2d\u5f0f\u65e9\u9910\u4e5f\u4e0d\u9519\uff0c\u5f88\u4e30\u5bcc\uff0c\u5c45\u7136\u8fd8\u6709\u9752\u83dc\u8089\u7247\u6c64\u3002 1 \u5750\u843d\u5728\u9999\u6e2f\u7684\u8001\u57ce\u533a\uff0c\u53ef\u4ee5\u4f53\u9a8c\u9999\u6e2f\u5c45\u6c11\u751f\u6d3b\uff0c\u95e8\u53e3\u4ea4\u901a\u5f88\u65b9\u4fbf\uff0c\u5982\u679c\u65f6\u95f4\u4e0d\u7d27\uff0c\u5750\u53ee\u5f53\u8f66\u5f88\u597d\u5440\uff01\u5468\u56f4\u6709\u5f88\u591a\u5c0f\u9910\u9986\uff0c\u65e9\u9910\u5c31\u5728\u4e2d\u8fdc\u540e\u9762\u7684\u5357\u5317\u56bc\u5403\u7684\uff0c\u4e1c\u897f\u5f88\u4e0d\u9519\u3002\u6211\u4eec\u5b9a\u7684\u5927\u5e8a\u623f\uff0c\u633a\u5b89\u9759\u7684\uff0c\u603b\u4f53\u6765\u8bf4\u4e0d\u9519\u3002\u524d\u53f0\u7ed3\u8d26\u6ca1\u6709\u94f6\u8054\uff01 1 \u9152\u5e97\u524d\u53f0\u670d\u52a1\u5dee\uff0c\u5bf9\u5f85\u5ba2\u4eba\u4e0d\u70ed\u60c5\u3002\u53f7\u79f0\u643a\u7a0b\u6ca1\u6709\u9884\u5b9a\u3002\u611f\u89c9\u662f\u5ba2\u4eba\u5728\u6c42\u4ed6\u4eec\uff0c\u6211\u4eec\u4e00\u5b9a\u5f97\u4f4f\u3002\u8fd9\u6837\u7684\u5bbe\u9986\u4e0b\u6b21\u4e0d\u4f1a\u5165\u4f4f\uff01 0 \u4ef7\u683c\u786e\u5b9e\u6bd4\u8f83\u9ad8\uff0c\u800c\u4e14\u8fd8\u6ca1\u6709\u65e9\u9910\u63d0\u4f9b\u3002 1 \u662f\u4e00\u5bb6\u5f88\u5b9e\u60e0\u7684\u9152\u5e97\uff0c\u4ea4\u901a\u65b9\u4fbf\uff0c\u623f\u95f4\u4e5f\u5bbd\u655e\uff0c\u665a\u4e0a\u6ca1\u6709\u7535\u8bdd\u9a9a\u6270\uff0c\u4f4f\u4e86\u4e24\u6b21\uff0c\u6709\u4e00\u6b21\u4f4f\uff15\uff10\uff11\u623f\u95f4\uff0c\u6d17\u6fa1\u95f4\u6392\u6c34\u4e0d\u7545\u901a\uff0c\u4e5f\u8bb8\u662f\u4e2a\u522b\u95ee\u9898\uff0e\u670d\u52a1\u8d28\u91cf\u5f88\u597d\uff0c\u521a\u5165\u4f4f\u65f6\u6ca1\u6709\u8c03\u597d\u5bbd\u5e26\uff0c\u670d\u52a1\u5458\u5f88\u5feb\u5c31\u5e2e\u5fd9\u89e3\u51b3\u4e86\uff0e 1 \u4f4d\u7f6e\u975e\u5e38\u597d\uff0c\u5c31\u5728\u897f\u8857\u7684\u8857\u53e3\uff0c\u4f46\u662f\u5374\u95f9\u4e2d\u53d6\u9759\uff0c\u73af\u5883\u5f88\u6e05\u65b0\u4f18\u96c5\u3002 1 \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. 1 \u8bed\u6599\u5728\u670d\u52a1\u5668\u4e0a\u5b58\u653e\u8def\u5f84\u5c55\u793a","title":"1 \u6570\u636e\u4ecb\u7ecd"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#2_3","text":"\u5728run_glue.py\u540c\u7ea7\u76ee\u5f55\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 # \u5b9a\u4e49DATA_DIR: \u5fae\u8c03\u6570\u636e\u6240\u5728\u8def\u5f84, \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528glue_data\u4e2d\u7684\u6570\u636e\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e # export DATA_DIR=\"/root/data/glue_data\" # \u76ee\u524d\u7248\u672c\u6682\u4e0d\u9700\u8981 # \u5b9a\u4e49SAVE_DIR: \u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84, \u6211\u4eec\u5c06\u6a21\u578b\u4fdd\u5b58\u5728\u5f53\u524d\u76ee\u5f55\u7684bert_finetuning_test\u6587\u4ef6\u4e2d export SAVE_DIR = \"./bert_finetuning_sst2_test/\" # \u4f7f\u7528python\u8fd0\u884c\u5fae\u8c03\u811a\u672c # --model_name_or_path: \u9009\u62e9\u5177\u4f53\u7684\u6a21\u578b\u6216\u8005\u53d8\u4f53, \u4e2d\u6587\u8bed\u6599\u4e0a\u5fae\u8c03, \u9009\u62e9bert-base-uncased # --task_name: \u5b83\u5c06\u4ee3\u8868\u5bf9\u5e94\u7684\u4efb\u52a1\u7c7b\u578b, \u5982MRPC\u4ee3\u8868\u53e5\u5b50\u5bf9\u4e8c\u5206\u7c7b\u4efb\u52a1 # --do_train: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u8bad\u7ec3 # --do_eval: \u4f7f\u7528\u5fae\u8c03\u811a\u672c\u8fdb\u884c\u9a8c\u8bc1 # --max_seq_length: \u8f93\u5165\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u8d85\u8fc7\u5219\u622a\u65ad, \u4e0d\u8db3\u5219\u8865\u9f50 # --learning_rate: \u5b66\u4e60\u7387 # --num_train_epochs: \u8bad\u7ec3\u8f6e\u6570 # --output_dir $SAVE_DIR: \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u4fdd\u5b58\u8def\u5f84 # --overwrite_output_dir: \u518d\u6b21\u8bad\u7ec3\u65f6\u5c06\u6e05\u7a7a\u4e4b\u524d\u7684\u4fdd\u5b58\u8def\u5f84\u5185\u5bb9\u91cd\u65b0\u5199\u5165 # \u6ce81 \u865a\u62df\u673a\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8017\u65f6\u8f83\u957f\uff0c\u5efa\u8bae\u5728\u6709GPU\u7684\u4e3b\u673a\u4e0a\u6267\u884c # \u6ce82 \u8be5\u547d\u4ee4\u5df2\u5728\u865a\u62df\u673a\u6267\u884c\uff0c\u518d\u6b21\u6267\u884c\u4f1a\u8986\u76d6\u7f13\u5b58\u7684\u6a21\u578b python run_glue_forcnsst2.py \\ --task_name sst2 \\ --model_name_or_path bert-base-chinese \\ --do_train \\ --do_eval \\ --train_file \"/Users/xxxxxx/transformers/examples/pytorch/text-classification/cn_data/SST-2/train.csv\" \\ --validation_file \"/Users/xxxxxx/transformers/examples/pytorch/text-classification/cn_data/SST-2/dev.csv\" \\ --max_seq_length 128 \\ --learning_rate 2e-5 \\ --num_train_epochs 1 .0 \\ --output_dir \"bert-base-uncased-finetuning\" \\ --overwrite_output_dir # \u6ce83 \u672c\u547d\u4ee4\u5df2\u5f62\u6210mycnsst2.sh shell\u6587\u4ef6\uff0c\u4f7f\u7528\u547d\u4ee4\u76f4\u63a5\u6267\u884c\u5373\u53ef\u3002\u6bd4\u5982\uff1a sh mycnnsst2.sh \u68c0\u9a8c\u6548\u679c # \u627e\u5230mycnnsst2.sh\u7684\u6587\u4ef6\u8def\u5f84\uff0c\u6bd4\u5982\uff1a/Users/xxxxxx/transformers/examples/pytorch/text-classification (nlp382) xxxxxx@bogon text-classification % sh mycnsst2.sh # \u6700\u7ec8\u6253\u5370\u6a21\u578b\u7684\u9a8c\u8bc1\u7ed3\u679c, \u51c6\u786e\u7387\u9ad8\u8fbe0.88. [INFO|trainer.py:2463] 2022-05-15 11:50:30,323 >> ***** Running Evaluation ***** [INFO|trainer.py:2465] 2022-05-15 11:50:30,323 >> Num examples = 1000 [INFO|trainer.py:2468] 2022-05-15 11:50:30,324 >> Batch size = 8 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [04:06<00:00, 1.99s/it]05/15/2022 11:54:39 - INFO - datasets.metric - Removing /Users/xxxxxx/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [04:06<00:00, 1.97s/it] ***** eval metrics ***** epoch = 1.0 eval_accuracy = 0.885 eval_loss = 0.3328 eval_runtime = 0:04:08.99 eval_samples = 1000 eval_samples_per_second = 4.016 eval_steps_per_second = 0.502","title":"2 \u8fd0\u884c\u4ee3\u7801"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#3_3","text":"-rw-r--r-- 1 1390 5 15 11:54 README.md -rw-r--r-- 1 389 5 15 11:54 all_results.json -rw-r--r-- 1 216 5 15 11:54 eval_results.json -rw-r--r-- 1 590 5 15 11:50 trainer_state.json -rw-r--r-- 1 193 5 15 11:50 train_results.json -rw-r--r-- 1 3119 5 15 11:50 training_args.bin -rw-r--r-- 1 439390 5 15 11:50 tokenizer.json -rw-r--r-- 1 109540 5 15 11:50 vocab.txt -rw-r--r-- 1 112 5 15 11:50 special_tokens_map.json -rw-r--r-- 1 322 5 15 11:50 tokenizer_config.json -rw-r--r-- 1 409150611 5 15 11:50 pytorch_model.bin -rw-r--r-- 1 1005 5 15 11:50 config.json","title":"3 \u67e5\u770b\u6a21\u578b\u6587\u4ef6\u5185\u5bb9"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#4_3","text":"import torch from transformers import AutoTokenizer , AutoModelForSequenceClassification , AutoModel def dm01_use_model (): # 0 \u627e\u5230\u81ea\u5df1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84 mymodelname = '/Users/xxxx/transformers/examples/pytorch/text-classification/bert-base-uncased-finetuning' print ( mymodelname ) # 1 \u672c\u5730\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684tokenizer tokenizer = AutoTokenizer . from_pretrained ( mymodelname ) # 2 \u672c\u5730\u52a0\u8f7d \u9884\u8bad\u7ec3\u6a21\u578b \u5e26\u5206\u7c7b\u6a21\u578b\u5934 model = AutoModelForSequenceClassification . from_pretrained ( mymodelname ) # model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2) # 3 \u9ed8\u8ba4\u60c5\u51b5\u4e0b \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e0d\u5e26\u5934 # model2 = AutoModel.from_pretrained(mymodelname) text = \"\u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d\" index = tokenizer . encode ( text ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( 'result.logits--->' , result . logits ) predicted_label = torch . argmax ( result . logits ) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) text1 = \"\u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519.\" index = tokenizer . encode ( text1 ) tokens_tensor = torch . tensor ([ index ]) # \u4f7f\u7528\u8bc4\u4f30\u6a21\u5f0f with torch . no_grad (): # \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u83b7\u5f97\u7ed3\u679c result = model ( tokens_tensor ) print ( 'result.logits--->' , result . logits ) predicted_label = torch . argmax ( result [ 0 ]) . item () print ( '\u9884\u6d4b\u6807\u7b7e\u4e3a>' , predicted_label ) \u8f93\u51fa\u6548\u679c: \u8f93\u5165\u6587\u672c\u4e3a: \u65e9\u9910\u4e0d\u597d,\u670d\u52a1\u4e0d\u5230\u4f4d,\u665a\u9910\u65e0\u897f\u9910,\u65e9\u9910\u665a\u9910\u76f8\u540c,\u623f\u95f4\u6761\u4ef6\u4e0d\u597d \u9884\u6d4b\u6807\u7b7e\u4e3a: 0 \u8f93\u5165\u6587\u672c\u4e3a: \u623f\u95f4\u5e94\u8be5\u8d85\u51fa30\u5e73\u7c73,\u662fHK\u540c\u7ea7\u9152\u5e97\u4e2d\u5c11\u6709\u7684\u5927;\u91cd\u88c5\u4e4b\u540e,\u8bbe\u5907\u4e5f\u4e0d\u9519. \u9884\u6d4b\u6807\u7b7e\u4e3a: 1","title":"4 \u4f7f\u7528\u672c\u5730\u5fae\u8c03\u6a21\u578b"},{"location":"05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html#6_3","text":"\u5b66\u4e60\u4e86\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u4e2d\u6587\u5206\u7c7b\u4efb\u52a1\u5f00\u53d1 \u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6e90\u5c01\u88c5\u3001\u6570\u636e\u8fed\u4ee3\u5668\u7684\u4f7f\u7528 \u642d\u5efa\u4e2d\u6587\u5206\u7c7b\u4efb\u52a1\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3001\u6a21\u578b\u9884\u6d4b \u5b66\u4e60\u4e86\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u4e2d\u6587\u586b\u7a7a\u4efb\u52a1\u5f00\u53d1 \u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6e90\u5c01\u88c5\u3001\u6570\u636e\u8fed\u4ee3\u5668\u7684\u4f7f\u7528 \u642d\u5efa\u4e2d\u6587\u586b\u7a7a\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3001\u6a21\u578b\u9884\u6d4b \u5b66\u4e60\u4e86\u8fc1\u79fb\u5b66\u4e60\u65b9\u5f0f\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1\u5f00\u53d1 \u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6e90\u5c01\u88c5\u3001\u6570\u636e\u8fed\u4ee3\u5668\u7684\u4f7f\u7528 \u642d\u5efa\u4e2d\u6587\u53e5\u5b50\u5173\u7cfb\u4efb\u52a1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3001\u6a21\u578b\u9884\u6d4b","title":"6 \u5c0f\u7ed3"},{"location":"06_mkdocs_bert_pretrained_model/index.html","text":"","title":"Index"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662fBERT \u638c\u63e1BERT\u7684\u67b6\u6784 \u638c\u63e1BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f 1 BERT\u7b80\u4ecb \u00b6 BERT\u662f2018\u5e7410\u6708\u7531Google AI\u7814\u7a76\u9662\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. BERT\u7684\u5168\u79f0\u662fBidirectional Encoder Representation from Transformers. BERT\u5728\u673a\u5668\u9605\u8bfb\u7406\u89e3\u9876\u7ea7\u6c34\u5e73\u6d4b\u8bd5SQuAD1.1\u4e2d\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u6210\u7ee9: \u5168\u90e8\u4e24\u4e2a\u8861\u91cf\u6307\u6807\u4e0a\u5168\u9762\u8d85\u8d8a\u4eba\u7c7b, \u5e76\u4e14\u572811\u79cd\u4e0d\u540cNLP\u6d4b\u8bd5\u4e2d\u521b\u51faSOTA\u8868\u73b0. \u5305\u62ec\u5c06GLUE\u57fa\u51c6\u63a8\u9ad8\u81f380.4% (\u7edd\u5bf9\u6539\u8fdb7.6%), MultiNLI\u51c6\u786e\u5ea6\u8fbe\u523086.7% (\u7edd\u5bf9\u6539\u8fdb5.6%). \u6210\u4e3aNLP\u53d1\u5c55\u53f2\u4e0a\u7684\u91cc\u7a0b\u7891\u5f0f\u7684\u6a21\u578b\u6210\u5c31. 2 BERT\u7684\u67b6\u6784 \u00b6 \u603b\u4f53\u67b6\u6784: \u5982\u4e0b\u56fe\u6240\u793a, \u6700\u5de6\u8fb9\u7684\u5c31\u662fBERT\u7684\u67b6\u6784\u56fe, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230BERT\u91c7\u7528\u4e86Transformer Encoder block\u8fdb\u884c\u8fde\u63a5, \u56e0\u4e3a\u662f\u4e00\u4e2a\u5178\u578b\u7684\u53cc\u5411\u7f16\u7801\u6a21\u578b. \u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aBERT\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684Transformer\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u9884\u5fae\u8c03\u6a21\u5757. 2.1 Embedding\u6a21\u5757 \u00b6 BERT\u4e2d\u7684\u8be5\u6a21\u5757\u662f\u7531\u4e09\u79cdEmbedding\u5171\u540c\u7ec4\u6210\u800c\u6210, \u5982\u4e0b\u56fe Token Embeddings \u662f\u8bcd\u5d4c\u5165\u5f20\u91cf, \u7b2c\u4e00\u4e2a\u5355\u8bcd\u662fCLS\u6807\u5fd7, \u53ef\u4ee5\u7528\u4e8e\u4e4b\u540e\u7684\u5206\u7c7b\u4efb\u52a1. Segment Embeddings \u662f\u53e5\u5b50\u5206\u6bb5\u5d4c\u5165\u5f20\u91cf, \u662f\u4e3a\u4e86\u670d\u52a1\u540e\u7eed\u7684\u4e24\u4e2a\u53e5\u5b50\u4e3a\u8f93\u5165\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. Position Embeddings \u662f\u4f4d\u7f6e\u7f16\u7801\u5f20\u91cf, \u6b64\u5904\u6ce8\u610f\u548c\u4f20\u7edf\u7684Transformer\u4e0d\u540c, \u4e0d\u662f\u4e09\u89d2\u51fd\u6570\u8ba1\u7b97\u7684\u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801, \u800c\u662f\u901a\u8fc7\u5b66\u4e60\u5f97\u51fa\u6765\u7684. \u6574\u4e2aEmbedding\u6a21\u5757\u7684\u8f93\u51fa\u5f20\u91cf\u5c31\u662f\u8fd93\u4e2a\u5f20\u91cf\u7684\u76f4\u63a5\u52a0\u548c\u7ed3\u679c. 2.2 \u53cc\u5411Transformer\u6a21\u5757 \u00b6 BERT\u4e2d\u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206, \u5b8c\u5168\u820d\u5f03\u4e86Decoder\u90e8\u5206. \u800c\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\u4e5f\u96c6\u4e2d\u4f53\u73b0\u5728\u8bad\u7ec3Transformer\u6a21\u5757\u4e2d. 2.3 \u9884\u5fae\u8c03\u6a21\u5757 \u00b6 \u7ecf\u8fc7\u4e2d\u95f4\u5c42Transformer\u7684\u5904\u7406\u540e, BERT\u7684\u6700\u540e\u4e00\u5c42\u6839\u636e\u4efb\u52a1\u7684\u4e0d\u540c\u9700\u6c42\u800c\u505a\u4e0d\u540c\u7684\u8c03\u6574\u5373\u53ef. \u6bd4\u5982\u5bf9\u4e8esequence-level\u7684\u5206\u7c7b\u4efb\u52a1, BERT\u76f4\u63a5\u53d6\u7b2c\u4e00\u4e2a[CLS] token \u7684final hidden state, \u518d\u52a0\u4e00\u5c42\u5168\u8fde\u63a5\u5c42\u540e\u8fdb\u884csoftmax\u6765\u9884\u6d4b\u6700\u7ec8\u7684\u6807\u7b7e. \u5bf9\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1, \u5fae\u8c03\u90fd\u96c6\u4e2d\u5728\u9884\u5fae\u8c03\u6a21\u5757, \u51e0\u79cd\u91cd\u8981\u7684NLP\u5fae\u8c03\u4efb\u52a1\u67b6\u6784\u56fe\u5c55\u793a\u5982\u4e0b \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u53d1\u73b0, \u5728\u9762\u5bf9\u7279\u5b9a\u4efb\u52a1\u65f6, \u53ea\u9700\u8981\u5bf9\u9884\u5fae\u8c03\u5c42\u8fdb\u884c\u5fae\u8c03, \u5c31\u53ef\u4ee5\u5229\u7528Transformer\u5f3a\u5927\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u6a21\u62df\u5f88\u591a\u4e0b\u6e38\u4efb\u52a1, \u5e76\u5f97\u5230SOTA\u7684\u7ed3\u679c. (\u53e5\u5b50\u5bf9\u5173\u7cfb\u5224\u65ad, \u5355\u6587\u672c\u4e3b\u9898\u5206\u7c7b, \u95ee\u7b54\u4efb\u52a1(QA), \u5355\u53e5\u8d34\u6807\u7b7e(NER)) \u82e5\u5e72\u53ef\u9009\u7684\u8d85\u53c2\u6570\u5efa\u8bae\u5982\u4e0b: Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Epochs: 3, 4 3 BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u00b6 BERT\u5305\u542b\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1: \u4efb\u52a1\u4e00: Masked LM (\u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3) \u4efb\u52a1\u4e8c: Next Sentence Prediction (\u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1) 3.1 \u4efb\u52a1\u4e00: Masked LM \u00b6 \u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3 \u5173\u4e8e\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3, \u90fd\u662f\u91c7\u7528left-to-right, \u6216\u8005left-to-right + right-to-left\u7ed3\u5408\u7684\u65b9\u5f0f, \u4f46\u8fd9\u79cd\u5355\u5411\u65b9\u5f0f\u6216\u8005\u62fc\u63a5\u7684\u65b9\u5f0f\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u4e3a\u6b64BERT\u63d0\u51fa\u4e00\u4e2a\u6df1\u5ea6\u53cc\u5411\u8868\u8fbe\u6a21\u578b(deep bidirectional representation). \u5373\u91c7\u7528MASK\u4efb\u52a1\u6765\u8bad\u7ec3\u6a21\u578b. 1: \u5728\u539f\u59cb\u8bad\u7ec3\u6587\u672c\u4e2d, \u968f\u673a\u7684\u62bd\u53d615%\u7684token\u4f5c\u4e3a\u53c2\u4e0eMASK\u4efb\u52a1\u7684\u5bf9\u8c61. 2: \u5728\u8fd9\u4e9b\u88ab\u9009\u4e2d\u7684token\u4e2d, \u6570\u636e\u751f\u6210\u5668\u5e76\u4e0d\u662f\u628a\u5b83\u4eec\u5168\u90e8\u53d8\u6210[MASK], \u800c\u662f\u6709\u4e0b\u52173\u79cd\u60c5\u51b5. 2.1: \u572880%\u7684\u6982\u7387\u4e0b, \u7528[MASK]\u6807\u8bb0\u66ff\u6362\u8be5token, \u6bd4\u5982my dog is hairy -> my dog is [MASK] 2.2: \u572810%\u7684\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362token, \u6bd4\u5982my dog is hairy -> my dog is apple 2.3: \u572810%\u7684\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8, \u6bd4\u5982my dog is hairy -> my dog is hairy 3: \u6a21\u578b\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d, \u5e76\u4e0d\u77e5\u9053\u5b83\u5c06\u8981\u9884\u6d4b\u54ea\u4e9b\u5355\u8bcd? \u54ea\u4e9b\u5355\u8bcd\u662f\u539f\u59cb\u7684\u6837\u5b50? \u54ea\u4e9b\u5355\u8bcd\u88ab\u906e\u63a9\u6210\u4e86[MASK]? \u54ea\u4e9b\u5355\u8bcd\u88ab\u66ff\u6362\u6210\u4e86\u5176\u4ed6\u5355\u8bcd? \u6b63\u662f\u5728\u8fd9\u6837\u4e00\u79cd\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b, \u53cd\u5012\u903c\u7740\u6a21\u578b\u5feb\u901f\u5b66\u4e60\u8be5token\u7684\u5206\u5e03\u5f0f\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u5c3d\u6700\u5927\u52aa\u529b\u5b66\u4e60\u539f\u59cb\u8bed\u8a00\u8bf4\u8bdd\u7684\u6837\u5b50. \u540c\u65f6\u56e0\u4e3a\u539f\u59cb\u6587\u672c\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86MASK\u64cd\u4f5c, \u5e76\u4e0d\u4f1a\u7834\u574f\u539f\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u548c\u8bed\u8a00\u89c4\u5219. 3.2 \u4efb\u52a1\u4e8c: Next Sentence Prediction \u00b6 \u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1 \u5728NLP\u4e2d\u6709\u4e00\u7c7b\u91cd\u8981\u7684\u95ee\u9898\u6bd4\u5982QA(Quention-Answer), NLI(Natural Language Inference), \u9700\u8981\u6a21\u578b\u80fd\u591f\u5f88\u597d\u7684\u7406\u89e3\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u9700\u8981\u5728\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\u5f15\u5165\u5bf9\u5e94\u7684\u4efb\u52a1. \u5728BERT\u4e2d\u5f15\u5165\u7684\u5c31\u662fNext Sentence Prediction\u4efb\u52a1. \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u6765\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. 1: \u6240\u6709\u53c2\u4e0e\u4efb\u52a1\u8bad\u7ec3\u7684\u8bed\u53e5\u90fd\u88ab\u9009\u4e2d\u4f5c\u4e3a\u53e5\u5b50A. 1.1: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) 1.2: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) 2: \u5728\u4efb\u52a1\u4e8c\u4e2d, BERT\u6a21\u578b\u53ef\u4ee5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f9797%-98%\u7684\u51c6\u786e\u7387. 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fBERT. BERT\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer Encoder\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. BERT\u572811\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u521b\u51faSOAT\u8868\u73b0. \u5b66\u4e60\u4e86BERT\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757, \u5305\u62ecToken Embeddings, Segment Embeddings, Position Embeddings. \u4e2d\u95f4\u5c42\u7684Transformer\u6a21\u5757, \u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206. \u6700\u4e0a\u5c42\u7684\u9884\u5fae\u8c03\u6a21\u5757, \u5177\u4f53\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u7c7b\u578b\u6765\u505a\u76f8\u5e94\u7684\u5904\u7406. \u5b66\u4e60\u4e86BERT\u7684\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1. MLM\u4efb\u52a1(Masked Language Model), \u5728\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d615%\u7684token\u53c2\u4e0e\u4efb\u52a1. \u572880%\u6982\u7387\u4e0b, \u7528[MASK]\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8. NSP\u4efb\u52a1(Next Sentence Prediction), \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd.(\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) \u672c\u8282\u5e38\u89c1\u95ee\u7b54 \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f","title":"1 BERT\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fBERT \u638c\u63e1BERT\u7684\u67b6\u6784 \u638c\u63e1BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-bert","text":"BERT\u662f2018\u5e7410\u6708\u7531Google AI\u7814\u7a76\u9662\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. BERT\u7684\u5168\u79f0\u662fBidirectional Encoder Representation from Transformers. BERT\u5728\u673a\u5668\u9605\u8bfb\u7406\u89e3\u9876\u7ea7\u6c34\u5e73\u6d4b\u8bd5SQuAD1.1\u4e2d\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u6210\u7ee9: \u5168\u90e8\u4e24\u4e2a\u8861\u91cf\u6307\u6807\u4e0a\u5168\u9762\u8d85\u8d8a\u4eba\u7c7b, \u5e76\u4e14\u572811\u79cd\u4e0d\u540cNLP\u6d4b\u8bd5\u4e2d\u521b\u51faSOTA\u8868\u73b0. \u5305\u62ec\u5c06GLUE\u57fa\u51c6\u63a8\u9ad8\u81f380.4% (\u7edd\u5bf9\u6539\u8fdb7.6%), MultiNLI\u51c6\u786e\u5ea6\u8fbe\u523086.7% (\u7edd\u5bf9\u6539\u8fdb5.6%). \u6210\u4e3aNLP\u53d1\u5c55\u53f2\u4e0a\u7684\u91cc\u7a0b\u7891\u5f0f\u7684\u6a21\u578b\u6210\u5c31.","title":"1 BERT\u7b80\u4ecb"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-bert","text":"\u603b\u4f53\u67b6\u6784: \u5982\u4e0b\u56fe\u6240\u793a, \u6700\u5de6\u8fb9\u7684\u5c31\u662fBERT\u7684\u67b6\u6784\u56fe, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230BERT\u91c7\u7528\u4e86Transformer Encoder block\u8fdb\u884c\u8fde\u63a5, \u56e0\u4e3a\u662f\u4e00\u4e2a\u5178\u578b\u7684\u53cc\u5411\u7f16\u7801\u6a21\u578b. \u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aBERT\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684Transformer\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u9884\u5fae\u8c03\u6a21\u5757.","title":"2 BERT\u7684\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#21-embedding","text":"BERT\u4e2d\u7684\u8be5\u6a21\u5757\u662f\u7531\u4e09\u79cdEmbedding\u5171\u540c\u7ec4\u6210\u800c\u6210, \u5982\u4e0b\u56fe Token Embeddings \u662f\u8bcd\u5d4c\u5165\u5f20\u91cf, \u7b2c\u4e00\u4e2a\u5355\u8bcd\u662fCLS\u6807\u5fd7, \u53ef\u4ee5\u7528\u4e8e\u4e4b\u540e\u7684\u5206\u7c7b\u4efb\u52a1. Segment Embeddings \u662f\u53e5\u5b50\u5206\u6bb5\u5d4c\u5165\u5f20\u91cf, \u662f\u4e3a\u4e86\u670d\u52a1\u540e\u7eed\u7684\u4e24\u4e2a\u53e5\u5b50\u4e3a\u8f93\u5165\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. Position Embeddings \u662f\u4f4d\u7f6e\u7f16\u7801\u5f20\u91cf, \u6b64\u5904\u6ce8\u610f\u548c\u4f20\u7edf\u7684Transformer\u4e0d\u540c, \u4e0d\u662f\u4e09\u89d2\u51fd\u6570\u8ba1\u7b97\u7684\u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801, \u800c\u662f\u901a\u8fc7\u5b66\u4e60\u5f97\u51fa\u6765\u7684. \u6574\u4e2aEmbedding\u6a21\u5757\u7684\u8f93\u51fa\u5f20\u91cf\u5c31\u662f\u8fd93\u4e2a\u5f20\u91cf\u7684\u76f4\u63a5\u52a0\u548c\u7ed3\u679c.","title":"2.1 Embedding\u6a21\u5757"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#22-transformer","text":"BERT\u4e2d\u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206, \u5b8c\u5168\u820d\u5f03\u4e86Decoder\u90e8\u5206. \u800c\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\u4e5f\u96c6\u4e2d\u4f53\u73b0\u5728\u8bad\u7ec3Transformer\u6a21\u5757\u4e2d.","title":"2.2 \u53cc\u5411Transformer\u6a21\u5757"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#23","text":"\u7ecf\u8fc7\u4e2d\u95f4\u5c42Transformer\u7684\u5904\u7406\u540e, BERT\u7684\u6700\u540e\u4e00\u5c42\u6839\u636e\u4efb\u52a1\u7684\u4e0d\u540c\u9700\u6c42\u800c\u505a\u4e0d\u540c\u7684\u8c03\u6574\u5373\u53ef. \u6bd4\u5982\u5bf9\u4e8esequence-level\u7684\u5206\u7c7b\u4efb\u52a1, BERT\u76f4\u63a5\u53d6\u7b2c\u4e00\u4e2a[CLS] token \u7684final hidden state, \u518d\u52a0\u4e00\u5c42\u5168\u8fde\u63a5\u5c42\u540e\u8fdb\u884csoftmax\u6765\u9884\u6d4b\u6700\u7ec8\u7684\u6807\u7b7e. \u5bf9\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1, \u5fae\u8c03\u90fd\u96c6\u4e2d\u5728\u9884\u5fae\u8c03\u6a21\u5757, \u51e0\u79cd\u91cd\u8981\u7684NLP\u5fae\u8c03\u4efb\u52a1\u67b6\u6784\u56fe\u5c55\u793a\u5982\u4e0b \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u53d1\u73b0, \u5728\u9762\u5bf9\u7279\u5b9a\u4efb\u52a1\u65f6, \u53ea\u9700\u8981\u5bf9\u9884\u5fae\u8c03\u5c42\u8fdb\u884c\u5fae\u8c03, \u5c31\u53ef\u4ee5\u5229\u7528Transformer\u5f3a\u5927\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u6a21\u62df\u5f88\u591a\u4e0b\u6e38\u4efb\u52a1, \u5e76\u5f97\u5230SOTA\u7684\u7ed3\u679c. (\u53e5\u5b50\u5bf9\u5173\u7cfb\u5224\u65ad, \u5355\u6587\u672c\u4e3b\u9898\u5206\u7c7b, \u95ee\u7b54\u4efb\u52a1(QA), \u5355\u53e5\u8d34\u6807\u7b7e(NER)) \u82e5\u5e72\u53ef\u9009\u7684\u8d85\u53c2\u6570\u5efa\u8bae\u5982\u4e0b: Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Epochs: 3, 4","title":"2.3 \u9884\u5fae\u8c03\u6a21\u5757"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-bert","text":"BERT\u5305\u542b\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1: \u4efb\u52a1\u4e00: Masked LM (\u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3) \u4efb\u52a1\u4e8c: Next Sentence Prediction (\u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1)","title":"3 BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#31-masked-lm","text":"\u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3 \u5173\u4e8e\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3, \u90fd\u662f\u91c7\u7528left-to-right, \u6216\u8005left-to-right + right-to-left\u7ed3\u5408\u7684\u65b9\u5f0f, \u4f46\u8fd9\u79cd\u5355\u5411\u65b9\u5f0f\u6216\u8005\u62fc\u63a5\u7684\u65b9\u5f0f\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u4e3a\u6b64BERT\u63d0\u51fa\u4e00\u4e2a\u6df1\u5ea6\u53cc\u5411\u8868\u8fbe\u6a21\u578b(deep bidirectional representation). \u5373\u91c7\u7528MASK\u4efb\u52a1\u6765\u8bad\u7ec3\u6a21\u578b. 1: \u5728\u539f\u59cb\u8bad\u7ec3\u6587\u672c\u4e2d, \u968f\u673a\u7684\u62bd\u53d615%\u7684token\u4f5c\u4e3a\u53c2\u4e0eMASK\u4efb\u52a1\u7684\u5bf9\u8c61. 2: \u5728\u8fd9\u4e9b\u88ab\u9009\u4e2d\u7684token\u4e2d, \u6570\u636e\u751f\u6210\u5668\u5e76\u4e0d\u662f\u628a\u5b83\u4eec\u5168\u90e8\u53d8\u6210[MASK], \u800c\u662f\u6709\u4e0b\u52173\u79cd\u60c5\u51b5. 2.1: \u572880%\u7684\u6982\u7387\u4e0b, \u7528[MASK]\u6807\u8bb0\u66ff\u6362\u8be5token, \u6bd4\u5982my dog is hairy -> my dog is [MASK] 2.2: \u572810%\u7684\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362token, \u6bd4\u5982my dog is hairy -> my dog is apple 2.3: \u572810%\u7684\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8, \u6bd4\u5982my dog is hairy -> my dog is hairy 3: \u6a21\u578b\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d, \u5e76\u4e0d\u77e5\u9053\u5b83\u5c06\u8981\u9884\u6d4b\u54ea\u4e9b\u5355\u8bcd? \u54ea\u4e9b\u5355\u8bcd\u662f\u539f\u59cb\u7684\u6837\u5b50? \u54ea\u4e9b\u5355\u8bcd\u88ab\u906e\u63a9\u6210\u4e86[MASK]? \u54ea\u4e9b\u5355\u8bcd\u88ab\u66ff\u6362\u6210\u4e86\u5176\u4ed6\u5355\u8bcd? \u6b63\u662f\u5728\u8fd9\u6837\u4e00\u79cd\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b, \u53cd\u5012\u903c\u7740\u6a21\u578b\u5feb\u901f\u5b66\u4e60\u8be5token\u7684\u5206\u5e03\u5f0f\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u5c3d\u6700\u5927\u52aa\u529b\u5b66\u4e60\u539f\u59cb\u8bed\u8a00\u8bf4\u8bdd\u7684\u6837\u5b50. \u540c\u65f6\u56e0\u4e3a\u539f\u59cb\u6587\u672c\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86MASK\u64cd\u4f5c, \u5e76\u4e0d\u4f1a\u7834\u574f\u539f\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u548c\u8bed\u8a00\u89c4\u5219.","title":"3.1 \u4efb\u52a1\u4e00: Masked LM"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#32-next-sentence-prediction","text":"\u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1 \u5728NLP\u4e2d\u6709\u4e00\u7c7b\u91cd\u8981\u7684\u95ee\u9898\u6bd4\u5982QA(Quention-Answer), NLI(Natural Language Inference), \u9700\u8981\u6a21\u578b\u80fd\u591f\u5f88\u597d\u7684\u7406\u89e3\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u9700\u8981\u5728\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\u5f15\u5165\u5bf9\u5e94\u7684\u4efb\u52a1. \u5728BERT\u4e2d\u5f15\u5165\u7684\u5c31\u662fNext Sentence Prediction\u4efb\u52a1. \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u6765\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. 1: \u6240\u6709\u53c2\u4e0e\u4efb\u52a1\u8bad\u7ec3\u7684\u8bed\u53e5\u90fd\u88ab\u9009\u4e2d\u4f5c\u4e3a\u53e5\u5b50A. 1.1: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) 1.2: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) 2: \u5728\u4efb\u52a1\u4e8c\u4e2d, BERT\u6a21\u578b\u53ef\u4ee5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f9797%-98%\u7684\u51c6\u786e\u7387.","title":"3.2 \u4efb\u52a1\u4e8c: Next Sentence Prediction"},{"location":"06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fBERT. BERT\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer Encoder\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. BERT\u572811\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u521b\u51faSOAT\u8868\u73b0. \u5b66\u4e60\u4e86BERT\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757, \u5305\u62ecToken Embeddings, Segment Embeddings, Position Embeddings. \u4e2d\u95f4\u5c42\u7684Transformer\u6a21\u5757, \u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206. \u6700\u4e0a\u5c42\u7684\u9884\u5fae\u8c03\u6a21\u5757, \u5177\u4f53\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u7c7b\u578b\u6765\u505a\u76f8\u5e94\u7684\u5904\u7406. \u5b66\u4e60\u4e86BERT\u7684\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1. MLM\u4efb\u52a1(Masked Language Model), \u5728\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d615%\u7684token\u53c2\u4e0e\u4efb\u52a1. \u572880%\u6982\u7387\u4e0b, \u7528[MASK]\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8. NSP\u4efb\u52a1(Next Sentence Prediction), \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd.(\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) \u672c\u8282\u5e38\u89c1\u95ee\u7b54 \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f","title":"4 \u5c0f\u7ed3"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u7406\u89e3BERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u539f\u56e0. \u7406\u89e3BERT\u6a21\u578b\u7684\u7f3a\u70b9\u548c\u539f\u56e0. \u7406\u89e3\u5728MLM\u4efb\u52a1\u4e2d\u91c7\u752880%, 10%, 10%\u7b56\u7565\u7684\u539f\u56e0. \u638c\u63e1\u5229\u7528BERT\u5904\u7406\u957f\u6587\u672c\u7684\u4efb\u52a1\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u601d\u8003\u9898\uff1aBERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u7f3a\u70b9? \u601d\u8003\u9898\uff1aBERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565? \u601d\u8003\u9898\uff1a\u957f\u6587\u672c\u9884\u6d4b\u4efb\u52a1\u5982\u679c\u60f3\u7528BERT\u6765\u5b9e\u73b0, \u8981\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c? 1 BERT\u6a21\u578b\u4f18\u7f3a\u70b9 \u00b6 1.1 BERT\u7684\u4f18\u70b9 \u00b6 \u901a\u8fc7\u9884\u8bad\u7ec3, \u52a0\u4e0aFine-tunning, \u572811\u9879NLP\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c. BERT\u7684\u6839\u57fa\u6e90\u4e8eTransformer, \u76f8\u6bd4\u4f20\u7edfRNN\u66f4\u52a0\u9ad8\u6548, \u53ef\u4ee5\u5e76\u884c\u5316\u5904\u7406\u540c\u65f6\u80fd\u6355\u6349\u957f\u8ddd\u79bb\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757, \u4e0d\u4ec5\u4ec5\u83b7\u5f97\u4e86\u771f\u6b63\u610f\u4e49\u4e0a\u7684bidirectional context, \u800c\u4e14\u4e3a\u540e\u7eed\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u4e86\u8db3\u591f\u7684\u8c03\u6574\u7a7a\u95f4. 1.2 BERT\u7684\u7f3a\u70b9 \u00b6 BERT\u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u592a\u591a, \u4e0d\u5229\u4e8e\u8d44\u6e90\u7d27\u5f20\u7684\u5e94\u7528\u573a\u666f, \u4e5f\u4e0d\u5229\u4e8e\u4e0a\u7ebf\u7684\u5b9e\u65f6\u5904\u7406. BERT\u76ee\u524d\u7ed9\u51fa\u7684\u4e2d\u6587\u6a21\u578b\u4e2d, \u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u5f88\u591a\u9700\u8981\u8bcd\u5411\u91cf\u7684\u5e94\u7528\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528. \u540c\u65f6\u8be5\u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u5f88\u591a\u751f\u50fb\u8bcd, \u53ea\u80fd\u4ee5UNK\u4ee3\u66ff. BERT\u4e2d\u7b2c\u4e00\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1MLM\u4e2d, [MASK]\u6807\u8bb0\u53ea\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u800c\u5728\u9884\u6d4b\u9636\u6bb5\u4e0d\u4f1a\u51fa\u73b0, \u8fd9\u5c31\u9020\u6210\u4e86\u4e00\u5b9a\u7684\u4fe1\u606f\u504f\u5dee, \u56e0\u6b64\u8bad\u7ec3\u65f6\u4e0d\u80fd\u8fc7\u591a\u7684\u4f7f\u7528[MASK], \u5426\u5219\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u8868\u73b0. \u6309\u7167BERT\u7684MLM\u4efb\u52a1\u4e2d\u7684\u7ea6\u5b9a, \u6bcf\u4e2abatch\u6570\u636e\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u88ab\u6a21\u578b\u5b66\u4e60\u548c\u9884\u6d4b, \u6240\u4ee5BERT\u6536\u655b\u7684\u901f\u5ea6\u6bd4left-to-right\u6a21\u578b\u8981\u6162\u5f88\u591a(left-to-right\u6a21\u578b\u4e2d\u6bcf\u4e00\u4e2atoken\u90fd\u4f1a\u53c2\u4e0e\u8bad\u7ec3). 2 BERT\u7684MLM\u4efb\u52a1 \u00b6 2.1 BERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565? \u00b6 \u9996\u5148, \u5982\u679c\u6240\u6709\u53c2\u4e0e\u8bad\u7ec3\u7684token\u88ab100%\u7684[MASK], \u90a3\u4e48\u5728fine-tunning\u7684\u65f6\u5019\u6240\u6709\u5355\u8bcd\u90fd\u662f\u5df2\u77e5\u7684, \u4e0d\u5b58\u5728[MASK], \u90a3\u4e48\u6a21\u578b\u5c31\u53ea\u80fd\u6839\u636e\u5176\u4ed6token\u7684\u4fe1\u606f\u548c\u8bed\u5e8f\u7ed3\u6784\u6765\u9884\u6d4b\u5f53\u524d\u8bcd, \u800c\u65e0\u6cd5\u5229\u7528\u5230\u8fd9\u4e2a\u8bcd\u672c\u8eab\u7684\u4fe1\u606f, \u56e0\u4e3a\u5b83\u4eec\u4ece\u672a\u51fa\u73b0\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u7b49\u4e8e\u6a21\u578b\u4ece\u672a\u63a5\u89e6\u5230\u5b83\u4eec\u7684\u4fe1\u606f, \u7b49\u4e8e\u6574\u4e2a\u8bed\u4e49\u7a7a\u95f4\u635f\u5931\u4e86\u90e8\u5206\u4fe1\u606f. \u91c7\u752880%\u7684\u6982\u7387\u4e0b\u5e94\u7528[MASK], \u65e2\u53ef\u4ee5\u8ba9\u6a21\u578b\u53bb\u5b66\u7740\u9884\u6d4b\u8fd9\u4e9b\u5355\u8bcd, \u53c8\u4ee520%\u7684\u6982\u7387\u4fdd\u7559\u4e86\u8bed\u4e49\u4fe1\u606f\u5c55\u793a\u7ed9\u6a21\u578b. \u4fdd\u7559\u4e0b\u6765\u7684\u4fe1\u606f\u5982\u679c\u5168\u90e8\u4f7f\u7528\u539f\u59cbtoken, \u90a3\u4e48\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u5077\u61d2, \u76f4\u63a5\u7167\u6284\u5f53\u524dtoken\u4fe1\u606f. \u91c7\u752810%\u6982\u7387\u4e0brandom token\u6765\u968f\u673a\u66ff\u6362\u5f53\u524dtoken, \u4f1a\u8ba9\u6a21\u578b\u4e0d\u80fd\u53bb\u6b7b\u8bb0\u786c\u80cc\u5f53\u524d\u7684token, \u800c\u53bb\u5c3d\u529b\u5b66\u4e60\u5355\u8bcd\u5468\u8fb9\u7684\u8bed\u4e49\u8868\u8fbe\u548c\u8fdc\u8ddd\u79bb\u7684\u4fe1\u606f\u4f9d\u8d56, \u5c1d\u8bd5\u5efa\u6a21\u5b8c\u6574\u7684\u8bed\u8a00\u4fe1\u606f. \u6700\u540e\u518d\u4ee510%\u7684\u6982\u7387\u4fdd\u7559\u539f\u59cb\u7684token, \u610f\u4e49\u5c31\u662f\u4fdd\u7559\u8bed\u8a00\u672c\u6765\u7684\u9762\u8c8c, \u8ba9\u4fe1\u606f\u4e0d\u81f3\u4e8e\u5b8c\u5168\u88ab\u906e\u63a9, \u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\"\u770b\u6e05\"\u771f\u5b9e\u7684\u8bed\u8a00\u9762\u8c8c. 3 BERT\u5904\u7406\u957f\u6587\u672c\u7684\u65b9\u6cd5 \u00b6 \u9996\u9009\u8981\u660e\u786e\u4e00\u70b9, BERT\u9884\u8bad\u7ec3\u6a21\u578b\u6240\u63a5\u6536\u7684\u6700\u5927sequence\u957f\u5ea6\u662f512. \u90a3\u4e48\u5bf9\u4e8e\u957f\u6587\u672c(\u6587\u672c\u957f\u5ea6\u8d85\u8fc7512\u7684\u53e5\u5b50), \u5c31\u9700\u8981\u7279\u6b8a\u7684\u65b9\u5f0f\u6765\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u6838\u5fc3\u5c31\u662f\u5982\u4f55\u8fdb\u884c\u622a\u65ad. head-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5934\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u524d510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). tail-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5c3e\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u6700\u540e510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). head+only\u65b9\u5f0f: \u9009\u62e9\u524d128\u4e2atoken\u548c\u6700\u540e382\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5728800\u4ee5\u5185), \u6216\u8005\u524d256\u4e2atoken\u548c\u6700\u540e254\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5927\u4e8e800). 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86BERT\u6a21\u578b\u76843\u4e2a\u4f18\u70b9: \u572811\u4e2aNLP\u4efb\u52a1\u4e0a\u53d6\u5f97SOAT\u6210\u7ee9. \u5229\u7528\u4e86Transformer\u7684\u5e76\u884c\u5316\u80fd\u529b\u4ee5\u53ca\u957f\u8bed\u53e5\u6355\u6349\u8bed\u4e49\u4f9d\u8d56\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u5b9e\u73b0\u4e86\u53cc\u5411Transformer\u5e76\u4e3a\u540e\u7eed\u7684\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u8db3\u591f\u7684\u7a7a\u95f4. \u5b66\u4e60\u4e86BERT\u6a21\u578b\u76844\u4e2a\u7f3a\u70b9: BERT\u6a21\u578b\u592a\u5927, \u592a\u6162. BERT\u6a21\u578b\u4e2d\u7684\u4e2d\u6587\u6a21\u578b\u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u65e0\u6cd5\u5229\u7528\u8bcd\u5411\u91cf, \u65e0\u6cd5\u8bc6\u522b\u751f\u50fb\u8bcd. BERT\u6a21\u578b\u4e2d\u7684MLM\u4efb\u52a1, [MASK]\u6807\u8bb0\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u9884\u6d4b\u9636\u6bb5\u4e0d\u51fa\u73b0, \u8fd9\u79cd\u504f\u5dee\u4f1a\u5bf9\u6a21\u578b\u6709\u4e00\u5b9a\u5f71\u54cd. BERT\u6a21\u578b\u7684MLM\u4efb\u52a1, \u6bcf\u4e2abatch\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u9020\u6210\u5927\u91cf\u6587\u672c\u6570\u636e\u7684\"\u65e0\u7528\", \u6536\u655b\u901f\u5ea6\u6162, \u9700\u8981\u7684\u7b97\u529b\u548c\u7b97\u65f6\u90fd\u5927\u5927\u63d0\u9ad8. \u5b66\u4e60\u4e86\u957f\u6587\u672c\u5904\u7406\u5982\u679c\u8981\u5229\u7528BERT\u7684\u8bdd, \u9700\u8981\u8fdb\u884c\u622a\u65ad\u5904\u7406. \u7b2c\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u524d\u9762510\u4e2atoken. \u7b2c\u4e8c\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u540e\u9762510\u4e2atoken. \u7b2c\u4e09\u79cd\u65b9\u5f0f\u5c31\u662f\u524d\u540e\u5206\u522b\u4fdd\u7559\u4e00\u90e8\u5206token, \u603b\u6570\u662f510. BERT\u4e2dMLM\u4efb\u52a1\u4e2d\u7684[MASK]\u662f\u4ee5\u4e00\u79cd\u663e\u793a\u7684\u65b9\u5f0f\u544a\u8bc9\u6a21\u578b\"\u8fd9\u4e2a\u8bcd\u6211\u4e0d\u544a\u8bc9\u4f60, \u4f60\u81ea\u5df1\u4ece\u4e0a\u4e0b\u6587\u91cc\u731c\", \u975e\u5e38\u7c7b\u4f3c\u4e8e\u540c\u5b66\u4eec\u5728\u505a\u5b8c\u5f62\u586b\u7a7a. \u5982\u679c[MASK]\u4ee5\u5916\u7684\u90e8\u5206\u5168\u90e8\u90fd\u7528\u539f\u59cbtoken, \u6a21\u578b\u4f1a\u5b66\u4e60\u5230\"\u5982\u679c\u5f53\u524d\u8bcd\u662f[MASK], \u5c31\u6839\u636e\u5176\u4ed6\u8bcd\u7684\u4fe1\u606f\u63a8\u65ad\u8fd9\u4e2a\u8bcd; \u5982\u679c\u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u6b63\u5e38\u7684\u5355\u8bcd, \u5c31\u76f4\u63a5\u7167\u6284\". \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6240\u6709\u5355\u8bcd\u90fd\u662f\u6b63\u5e38\u5355\u8bcd\u4e86, \u6a21\u578b\u5c31\u4f1a\u7167\u6284\u6240\u6709\u5355\u8bcd, \u4e0d\u518d\u63d0\u53d6\u5355\u8bcd\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4e86. BERT\u4e2dMLM\u4efb\u52a1\u4ee510%\u7684\u6982\u7387\u586b\u5165random token, \u5c31\u662f\u8ba9\u6a21\u578b\u65f6\u523b\u5904\u4e8e\"\u7d27\u5f20\u60c5\u7eea\"\u4e2d, \u8ba9\u6a21\u578b\u641e\u4e0d\u6e05\u695a\u5f53\u524d\u770b\u5230\u7684token\u662f\u771f\u5b9e\u7684\u5355\u8bcd\u8fd8\u662f\u88ab\u968f\u673a\u66ff\u6362\u6389\u7684\u5355\u8bcd, \u8fd9\u6837\u6a21\u578b\u5728\u4efb\u610f\u7684token\u4f4d\u7f6e\u5c31\u53ea\u80fd\u628a\u5f53\u524dtoken\u7684\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7ed3\u5408\u8d77\u6765\u505a\u7efc\u5408\u7684\u5224\u65ad\u548c\u5efa\u6a21. \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6a21\u578b\u4e5f\u4f1a\u540c\u65f6\u63d0\u53d6\u8fd9\u4e24\u65b9\u9762\u7684\u4fe1\u606f, \u56e0\u4e3a\u6a21\u578b\"\u5fc3\u7406\u5f88\u7d27\u5f20\", \u5b83\u4e0d\u77e5\u9053\u5f53\u524d\u770b\u5230\u7684\u8fd9\u4e2atoken, \u6240\u8c13\u7684\"\u6b63\u5e38\u5355\u8bcd\"\u5230\u5e95\u6709\u6ca1\u6709\"\u63d0\u524d\u88ab\u52a8\u8fc7\u624b\u811a\".","title":"2 BERT\u6a21\u578b\u7279\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#_1","text":"\u7406\u89e3BERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u539f\u56e0. \u7406\u89e3BERT\u6a21\u578b\u7684\u7f3a\u70b9\u548c\u539f\u56e0. \u7406\u89e3\u5728MLM\u4efb\u52a1\u4e2d\u91c7\u752880%, 10%, 10%\u7b56\u7565\u7684\u539f\u56e0. \u638c\u63e1\u5229\u7528BERT\u5904\u7406\u957f\u6587\u672c\u7684\u4efb\u52a1\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u601d\u8003\u9898\uff1aBERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u7f3a\u70b9? \u601d\u8003\u9898\uff1aBERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565? \u601d\u8003\u9898\uff1a\u957f\u6587\u672c\u9884\u6d4b\u4efb\u52a1\u5982\u679c\u60f3\u7528BERT\u6765\u5b9e\u73b0, \u8981\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#1-bert","text":"","title":"1 BERT\u6a21\u578b\u4f18\u7f3a\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#11-bert","text":"\u901a\u8fc7\u9884\u8bad\u7ec3, \u52a0\u4e0aFine-tunning, \u572811\u9879NLP\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c. BERT\u7684\u6839\u57fa\u6e90\u4e8eTransformer, \u76f8\u6bd4\u4f20\u7edfRNN\u66f4\u52a0\u9ad8\u6548, \u53ef\u4ee5\u5e76\u884c\u5316\u5904\u7406\u540c\u65f6\u80fd\u6355\u6349\u957f\u8ddd\u79bb\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757, \u4e0d\u4ec5\u4ec5\u83b7\u5f97\u4e86\u771f\u6b63\u610f\u4e49\u4e0a\u7684bidirectional context, \u800c\u4e14\u4e3a\u540e\u7eed\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u4e86\u8db3\u591f\u7684\u8c03\u6574\u7a7a\u95f4.","title":"1.1 BERT\u7684\u4f18\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#12-bert","text":"BERT\u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u592a\u591a, \u4e0d\u5229\u4e8e\u8d44\u6e90\u7d27\u5f20\u7684\u5e94\u7528\u573a\u666f, \u4e5f\u4e0d\u5229\u4e8e\u4e0a\u7ebf\u7684\u5b9e\u65f6\u5904\u7406. BERT\u76ee\u524d\u7ed9\u51fa\u7684\u4e2d\u6587\u6a21\u578b\u4e2d, \u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u5f88\u591a\u9700\u8981\u8bcd\u5411\u91cf\u7684\u5e94\u7528\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528. \u540c\u65f6\u8be5\u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u5f88\u591a\u751f\u50fb\u8bcd, \u53ea\u80fd\u4ee5UNK\u4ee3\u66ff. BERT\u4e2d\u7b2c\u4e00\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1MLM\u4e2d, [MASK]\u6807\u8bb0\u53ea\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u800c\u5728\u9884\u6d4b\u9636\u6bb5\u4e0d\u4f1a\u51fa\u73b0, \u8fd9\u5c31\u9020\u6210\u4e86\u4e00\u5b9a\u7684\u4fe1\u606f\u504f\u5dee, \u56e0\u6b64\u8bad\u7ec3\u65f6\u4e0d\u80fd\u8fc7\u591a\u7684\u4f7f\u7528[MASK], \u5426\u5219\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u8868\u73b0. \u6309\u7167BERT\u7684MLM\u4efb\u52a1\u4e2d\u7684\u7ea6\u5b9a, \u6bcf\u4e2abatch\u6570\u636e\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u88ab\u6a21\u578b\u5b66\u4e60\u548c\u9884\u6d4b, \u6240\u4ee5BERT\u6536\u655b\u7684\u901f\u5ea6\u6bd4left-to-right\u6a21\u578b\u8981\u6162\u5f88\u591a(left-to-right\u6a21\u578b\u4e2d\u6bcf\u4e00\u4e2atoken\u90fd\u4f1a\u53c2\u4e0e\u8bad\u7ec3).","title":"1.2 BERT\u7684\u7f3a\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#2-bertmlm","text":"","title":"2 BERT\u7684MLM\u4efb\u52a1"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#21-bertmlm80-10-10","text":"\u9996\u5148, \u5982\u679c\u6240\u6709\u53c2\u4e0e\u8bad\u7ec3\u7684token\u88ab100%\u7684[MASK], \u90a3\u4e48\u5728fine-tunning\u7684\u65f6\u5019\u6240\u6709\u5355\u8bcd\u90fd\u662f\u5df2\u77e5\u7684, \u4e0d\u5b58\u5728[MASK], \u90a3\u4e48\u6a21\u578b\u5c31\u53ea\u80fd\u6839\u636e\u5176\u4ed6token\u7684\u4fe1\u606f\u548c\u8bed\u5e8f\u7ed3\u6784\u6765\u9884\u6d4b\u5f53\u524d\u8bcd, \u800c\u65e0\u6cd5\u5229\u7528\u5230\u8fd9\u4e2a\u8bcd\u672c\u8eab\u7684\u4fe1\u606f, \u56e0\u4e3a\u5b83\u4eec\u4ece\u672a\u51fa\u73b0\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u7b49\u4e8e\u6a21\u578b\u4ece\u672a\u63a5\u89e6\u5230\u5b83\u4eec\u7684\u4fe1\u606f, \u7b49\u4e8e\u6574\u4e2a\u8bed\u4e49\u7a7a\u95f4\u635f\u5931\u4e86\u90e8\u5206\u4fe1\u606f. \u91c7\u752880%\u7684\u6982\u7387\u4e0b\u5e94\u7528[MASK], \u65e2\u53ef\u4ee5\u8ba9\u6a21\u578b\u53bb\u5b66\u7740\u9884\u6d4b\u8fd9\u4e9b\u5355\u8bcd, \u53c8\u4ee520%\u7684\u6982\u7387\u4fdd\u7559\u4e86\u8bed\u4e49\u4fe1\u606f\u5c55\u793a\u7ed9\u6a21\u578b. \u4fdd\u7559\u4e0b\u6765\u7684\u4fe1\u606f\u5982\u679c\u5168\u90e8\u4f7f\u7528\u539f\u59cbtoken, \u90a3\u4e48\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u5077\u61d2, \u76f4\u63a5\u7167\u6284\u5f53\u524dtoken\u4fe1\u606f. \u91c7\u752810%\u6982\u7387\u4e0brandom token\u6765\u968f\u673a\u66ff\u6362\u5f53\u524dtoken, \u4f1a\u8ba9\u6a21\u578b\u4e0d\u80fd\u53bb\u6b7b\u8bb0\u786c\u80cc\u5f53\u524d\u7684token, \u800c\u53bb\u5c3d\u529b\u5b66\u4e60\u5355\u8bcd\u5468\u8fb9\u7684\u8bed\u4e49\u8868\u8fbe\u548c\u8fdc\u8ddd\u79bb\u7684\u4fe1\u606f\u4f9d\u8d56, \u5c1d\u8bd5\u5efa\u6a21\u5b8c\u6574\u7684\u8bed\u8a00\u4fe1\u606f. \u6700\u540e\u518d\u4ee510%\u7684\u6982\u7387\u4fdd\u7559\u539f\u59cb\u7684token, \u610f\u4e49\u5c31\u662f\u4fdd\u7559\u8bed\u8a00\u672c\u6765\u7684\u9762\u8c8c, \u8ba9\u4fe1\u606f\u4e0d\u81f3\u4e8e\u5b8c\u5168\u88ab\u906e\u63a9, \u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\"\u770b\u6e05\"\u771f\u5b9e\u7684\u8bed\u8a00\u9762\u8c8c.","title":"2.1 BERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565?"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#3-bert","text":"\u9996\u9009\u8981\u660e\u786e\u4e00\u70b9, BERT\u9884\u8bad\u7ec3\u6a21\u578b\u6240\u63a5\u6536\u7684\u6700\u5927sequence\u957f\u5ea6\u662f512. \u90a3\u4e48\u5bf9\u4e8e\u957f\u6587\u672c(\u6587\u672c\u957f\u5ea6\u8d85\u8fc7512\u7684\u53e5\u5b50), \u5c31\u9700\u8981\u7279\u6b8a\u7684\u65b9\u5f0f\u6765\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u6838\u5fc3\u5c31\u662f\u5982\u4f55\u8fdb\u884c\u622a\u65ad. head-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5934\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u524d510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). tail-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5c3e\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u6700\u540e510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). head+only\u65b9\u5f0f: \u9009\u62e9\u524d128\u4e2atoken\u548c\u6700\u540e382\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5728800\u4ee5\u5185), \u6216\u8005\u524d256\u4e2atoken\u548c\u6700\u540e254\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5927\u4e8e800).","title":"3 BERT\u5904\u7406\u957f\u6587\u672c\u7684\u65b9\u6cd5"},{"location":"06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#4","text":"\u5b66\u4e60\u4e86BERT\u6a21\u578b\u76843\u4e2a\u4f18\u70b9: \u572811\u4e2aNLP\u4efb\u52a1\u4e0a\u53d6\u5f97SOAT\u6210\u7ee9. \u5229\u7528\u4e86Transformer\u7684\u5e76\u884c\u5316\u80fd\u529b\u4ee5\u53ca\u957f\u8bed\u53e5\u6355\u6349\u8bed\u4e49\u4f9d\u8d56\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u5b9e\u73b0\u4e86\u53cc\u5411Transformer\u5e76\u4e3a\u540e\u7eed\u7684\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u8db3\u591f\u7684\u7a7a\u95f4. \u5b66\u4e60\u4e86BERT\u6a21\u578b\u76844\u4e2a\u7f3a\u70b9: BERT\u6a21\u578b\u592a\u5927, \u592a\u6162. BERT\u6a21\u578b\u4e2d\u7684\u4e2d\u6587\u6a21\u578b\u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u65e0\u6cd5\u5229\u7528\u8bcd\u5411\u91cf, \u65e0\u6cd5\u8bc6\u522b\u751f\u50fb\u8bcd. BERT\u6a21\u578b\u4e2d\u7684MLM\u4efb\u52a1, [MASK]\u6807\u8bb0\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u9884\u6d4b\u9636\u6bb5\u4e0d\u51fa\u73b0, \u8fd9\u79cd\u504f\u5dee\u4f1a\u5bf9\u6a21\u578b\u6709\u4e00\u5b9a\u5f71\u54cd. BERT\u6a21\u578b\u7684MLM\u4efb\u52a1, \u6bcf\u4e2abatch\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u9020\u6210\u5927\u91cf\u6587\u672c\u6570\u636e\u7684\"\u65e0\u7528\", \u6536\u655b\u901f\u5ea6\u6162, \u9700\u8981\u7684\u7b97\u529b\u548c\u7b97\u65f6\u90fd\u5927\u5927\u63d0\u9ad8. \u5b66\u4e60\u4e86\u957f\u6587\u672c\u5904\u7406\u5982\u679c\u8981\u5229\u7528BERT\u7684\u8bdd, \u9700\u8981\u8fdb\u884c\u622a\u65ad\u5904\u7406. \u7b2c\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u524d\u9762510\u4e2atoken. \u7b2c\u4e8c\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u540e\u9762510\u4e2atoken. \u7b2c\u4e09\u79cd\u65b9\u5f0f\u5c31\u662f\u524d\u540e\u5206\u522b\u4fdd\u7559\u4e00\u90e8\u5206token, \u603b\u6570\u662f510. BERT\u4e2dMLM\u4efb\u52a1\u4e2d\u7684[MASK]\u662f\u4ee5\u4e00\u79cd\u663e\u793a\u7684\u65b9\u5f0f\u544a\u8bc9\u6a21\u578b\"\u8fd9\u4e2a\u8bcd\u6211\u4e0d\u544a\u8bc9\u4f60, \u4f60\u81ea\u5df1\u4ece\u4e0a\u4e0b\u6587\u91cc\u731c\", \u975e\u5e38\u7c7b\u4f3c\u4e8e\u540c\u5b66\u4eec\u5728\u505a\u5b8c\u5f62\u586b\u7a7a. \u5982\u679c[MASK]\u4ee5\u5916\u7684\u90e8\u5206\u5168\u90e8\u90fd\u7528\u539f\u59cbtoken, \u6a21\u578b\u4f1a\u5b66\u4e60\u5230\"\u5982\u679c\u5f53\u524d\u8bcd\u662f[MASK], \u5c31\u6839\u636e\u5176\u4ed6\u8bcd\u7684\u4fe1\u606f\u63a8\u65ad\u8fd9\u4e2a\u8bcd; \u5982\u679c\u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u6b63\u5e38\u7684\u5355\u8bcd, \u5c31\u76f4\u63a5\u7167\u6284\". \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6240\u6709\u5355\u8bcd\u90fd\u662f\u6b63\u5e38\u5355\u8bcd\u4e86, \u6a21\u578b\u5c31\u4f1a\u7167\u6284\u6240\u6709\u5355\u8bcd, \u4e0d\u518d\u63d0\u53d6\u5355\u8bcd\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4e86. BERT\u4e2dMLM\u4efb\u52a1\u4ee510%\u7684\u6982\u7387\u586b\u5165random token, \u5c31\u662f\u8ba9\u6a21\u578b\u65f6\u523b\u5904\u4e8e\"\u7d27\u5f20\u60c5\u7eea\"\u4e2d, \u8ba9\u6a21\u578b\u641e\u4e0d\u6e05\u695a\u5f53\u524d\u770b\u5230\u7684token\u662f\u771f\u5b9e\u7684\u5355\u8bcd\u8fd8\u662f\u88ab\u968f\u673a\u66ff\u6362\u6389\u7684\u5355\u8bcd, \u8fd9\u6837\u6a21\u578b\u5728\u4efb\u610f\u7684token\u4f4d\u7f6e\u5c31\u53ea\u80fd\u628a\u5f53\u524dtoken\u7684\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7ed3\u5408\u8d77\u6765\u505a\u7efc\u5408\u7684\u5224\u65ad\u548c\u5efa\u6a21. \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6a21\u578b\u4e5f\u4f1a\u540c\u65f6\u63d0\u53d6\u8fd9\u4e24\u65b9\u9762\u7684\u4fe1\u606f, \u56e0\u4e3a\u6a21\u578b\"\u5fc3\u7406\u5f88\u7d27\u5f20\", \u5b83\u4e0d\u77e5\u9053\u5f53\u524d\u770b\u5230\u7684\u8fd9\u4e2atoken, \u6240\u8c13\u7684\"\u6b63\u5e38\u5355\u8bcd\"\u5230\u5e95\u6709\u6ca1\u6709\"\u63d0\u524d\u88ab\u52a8\u8fc7\u624b\u811a\".","title":"4 \u5c0f\u7ed3"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"BERT\u7cfb\u5217\u6a21\u578b \u00b6 \u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4e0d\u540c\u7c7b\u578b\u7684BERT\u7cfb\u5217\u6a21\u578b. \u638c\u63e1BERT\u7cfb\u5217\u6a21\u578b\u4e4b\u95f4\u7684\u533a\u522b\u548c\u8054\u7cfb. 1 AlBERT\u6a21\u578b \u00b6 \u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3AlBERT\u6a21\u578b\u7684\u67b6\u6784. \u638c\u63e1AlBERT\u6a21\u578b\u7684\u4f18\u5316\u70b9. 1.1 AlBERT\u6a21\u578b\u7684\u67b6\u6784 \u00b6 AlBERT\u6a21\u578b\u53d1\u5e03\u4e8eICLR 2020\u4f1a\u8bae, \u662f\u57fa\u4e8eBERT\u6a21\u578b\u7684\u91cd\u8981\u6539\u8fdb\u7248\u672c. \u662f\u8c37\u6b4c\u7814\u7a76\u9662\u548c\u829d\u52a0\u54e5\u5927\u5b66\u5171\u540c\u53d1\u5e03\u7684\u7814\u7a76\u6210\u679c. \u8bba\u6587\u5168\u79f0<< A Lite BERT For Self-Supervised Learning Of Language Representations >>. \u4ece\u6a21\u578b\u67b6\u6784\u4e0a\u770b, AlBERT\u548cBERT\u57fa\u672c\u4e00\u81f4, \u6838\u5fc3\u6a21\u5757\u90fd\u662f\u57fa\u4e8eTransformer\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b. \u5728\u672c\u7bc7\u8bba\u6587\u4e2d, \u9996\u5148\u5bf9\u6bd4\u4e86\u8fc7\u53bb\u51e0\u5e74\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e3b\u6d41\u64cd\u4f5c\u601d\u8def. \u7b2c\u4e00: \u5927\u89c4\u6a21\u7684\u8bed\u6599. \u7b2c\u4e8c: \u66f4\u6df1\u7684\u7f51\u7edc, \u66f4\u591a\u7684\u53c2\u6570. \u7b2c\u4e09: \u591a\u4efb\u52a1\u8bad\u7ec3. 1.2 AlBERT\u6a21\u578b\u7684\u4f18\u5316\u70b9 \u00b6 \u76f8\u6bd4\u8f83\u4e8eBERT\u6a21\u578b, AlBERT\u7684\u51fa\u53d1\u70b9\u5373\u662f\u5e0c\u671b\u964d\u4f4e\u9884\u8bad\u7ec3\u7684\u96be\u5ea6, \u540c\u65f6\u63d0\u5347\u6a21\u578b\u5173\u952e\u80fd\u529b. \u4e3b\u8981\u5f15\u5165\u4e865\u5927\u4f18\u5316. \u7b2c\u4e00: \u8bcd\u5d4c\u5165\u53c2\u6570\u7684\u56e0\u5f0f\u5206\u89e3. \u7b2c\u4e8c: \u9690\u85cf\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u5171\u4eab. \u7b2c\u4e09: \u53bb\u6389NSP, \u589e\u52a0SOP\u9884\u8bad\u7ec3\u4efb\u52a1. \u7b2c\u56db: \u53bb\u6389dropout\u64cd\u4f5c. \u7b2c\u4e94: MLM\u4efb\u52a1\u7684\u4f18\u5316. \u7b2c\u4e00: \u8bcd\u5d4c\u5165\u53c2\u6570\u7684\u56e0\u5f0f\u5206\u89e3. AlBERT\u7684\u4f5c\u8005\u8ba4\u4e3a, \u8bcd\u5411\u91cf\u53ea\u8bb0\u5f55\u4e86\u5c11\u91cf\u7684\u8bcd\u6c47\u672c\u8eab\u7684\u4fe1\u606f, \u66f4\u591a\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u53e5\u6cd5\u4fe1\u606f\u5305\u542b\u5728\u9690\u85cf\u5c42\u4e2d. \u56e0\u6b64\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u4e0d\u4e00\u5b9a\u975e\u8981\u548c\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\u4e00\u81f4. \u5177\u4f53\u505a\u6cd5\u5c31\u662f\u901a\u8fc7\u56e0\u5f0f\u5206\u89e3\u6765\u964d\u4f4e\u5d4c\u5165\u77e9\u9635\u7684\u53c2\u6570: BERT: embedding_dim * vocab_size = hidden_size * vocab_size, \u5176\u4e2dembedding_dim=768, vocab_size\u5927\u7ea6\u4e3a30000\u5de6\u53f3\u7684\u7ea7\u522b, \u5927\u7ea6\u7b49\u4e8e30000 * 768 = 23040000(2300\u4e07). AlBERT: vocab_size * project + project * hidden_size, \u5176\u4e2dproject\u662f\u56e0\u5f0f\u5206\u89e3\u7684\u4e2d\u95f4\u6620\u5c04\u5c42\u7ef4\u5ea6, \u4e00\u822c\u53d6128, \u53c2\u6570\u603b\u91cf\u5927\u7ea6\u7b49\u4e8e30000 * 128 + 128 * 768 = 482304(48\u4e07). \u7b2c\u4e8c: \u9690\u85cf\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u5171\u4eab. \u5728BERT\u6a21\u578b\u4e2d, \u65e0\u8bba\u662f12\u5c42\u7684base, \u8fd8\u662f24\u5c42\u7684large\u6a21\u578b, \u5176\u4e2d\u6bcf\u4e00\u4e2aEncoder Block\u90fd\u62e5\u6709\u72ec\u7acb\u7684\u53c2\u6570\u6a21\u5757, \u5305\u542b\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42, \u524d\u9988\u5168\u8fde\u63a5\u5c42. \u975e\u5e38\u91cd\u8981\u7684\u4e00\u70b9\u662f, \u8fd9\u4e9b\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u90fd\u662f\u72ec\u7acb\u7684, \u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\u90fd\u4e0d\u4e00\u6837\u4e86! \u90a3\u4e48\u4e3a\u4e86\u51cf\u5c11\u6a21\u578b\u7684\u53c2\u6570\u91cf, \u4e00\u4e2a\u5f88\u76f4\u89c2\u7684\u505a\u6cd5\u4fbf\u662f\u8ba9\u8fd9\u4e9b\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u5171\u4eab, \u672c\u8d28\u4e0a\u53ea\u6709\u4e00\u5957Encoder Block\u7684\u53c2\u6570! \u5728AlBERT\u6a21\u578b\u4e2d, \u6240\u6709\u7684\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42, \u5168\u8fde\u63a5\u5c42\u7684\u53c2\u6570\u90fd\u662f\u5206\u522b\u5171\u4eab\u7684, \u901a\u8fc7\u8fd9\u6837\u7684\u65b9\u5f0f, AlBERT\u5c5e\u4e8eBlock\u7684\u53c2\u6570\u91cf\u5728BERT\u7684\u57fa\u7840\u4e0a, \u5206\u522b\u4e0b\u964d\u5230\u539f\u6765\u76841/12, 1/24. \u7b2c\u4e09: \u53bb\u6389NSP, \u589e\u52a0SOP\u9884\u8bad\u7ec3\u4efb\u52a1. BERT\u6a21\u578b\u7684\u6210\u529f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u4e24\u70b9, \u4e00\u4e2a\u662f\u57fa\u7840\u67b6\u6784\u91c7\u7528Transformer, \u53e6\u4e00\u4e2a\u5c31\u662f\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1, MLM\u548cNSP. \u4f46\u662fBERT\u63d0\u51fa\u540e\u4e0d\u4e45, \u4fbf\u6709\u7814\u7a76\u4eba\u5458\u5bf9NSP\u4efb\u52a1\u63d0\u51fa\u8d28\u7591, \u6211\u4eec\u4e5f\u53ef\u4ee5\u53cd\u601d\u4e00\u4e0bNSP\u4efb\u52a1\u6709\u4ec0\u4e48\u95ee\u9898? \u5728AlBERT\u6a21\u578b\u4e2d, \u76f4\u63a5\u820d\u5f03\u6389\u4e86NSP\u4efb\u52a1, \u65b0\u63d0\u51fa\u4e86SOP\u4efb\u52a1(Sentence Order Prediction), \u5373\u4e24\u53e5\u8bdd\u7684\u987a\u5e8f\u9884\u6d4b, \u6587\u672c\u4e2d\u6b63\u5e38\u8bed\u5e8f\u7684\u5148\u540e\u4e24\u53e5\u8bdd[A, B]\u4f5c\u4e3a\u6b63\u6837\u672c, \u5219[B, A]\u4f5c\u4e3a\u8d1f\u6837\u672c. \u589e\u52a0\u4e86SOP\u9884\u8bad\u7ec3\u4efb\u52a1\u540e, \u4f7f\u5f97AlBERT\u62e5\u6709\u4e86\u66f4\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u8bed\u5e8f\u5173\u7cfb\u7684\u9884\u6d4b\u80fd\u529b. \u7b2c\u56db: \u53bb\u6389dropout\u64cd\u4f5c. \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230, \u5728AlBERT\u8bad\u7ec3\u8fbe\u5230100\u4e07\u4e2abatch_size\u65f6, \u6a21\u578b\u4f9d\u7136\u6ca1\u6709\u8fc7\u62df\u5408, \u4f5c\u8005\u57fa\u4e8e\u8fd9\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u76f4\u63a5\u53bb\u6389\u4e86Dropout\u64cd\u4f5c, \u7adf\u7136\u610f\u5916\u7684\u53d1\u73b0AlBERT\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u679c\u6709\u4e86\u8fdb\u4e00\u6b65\u7684\u63d0\u5347. \u8fd9\u662fNLP\u9886\u57df\u7b2c\u4e00\u6b21\u53d1\u73b0dropout\u5bf9\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u4f1a\u9020\u6210\u8d1f\u9762\u5f71\u54cd, \u4e5f\u4f7f\u5f97AlBERT v2.0\u7248\u672c\u6210\u4e3a\u7b2c\u4e00\u4e2a\u4e0d\u4f7f\u7528dropout\u64cd\u4f5c\u800c\u83b7\u5f97\u4f18\u5f02\u8868\u73b0\u7684\u4e3b\u6d41\u9884\u8bad\u7ec3\u6a21\u578b \u7b2c\u4e94: MLM\u4efb\u52a1\u7684\u4f18\u5316. segments-pair\u7684\u4f18\u5316: BERT\u4e3a\u4e86\u52a0\u901f\u8bad\u7ec3, \u524d90%\u7684steps\u4f7f\u7528\u4e86\u957f\u5ea6\u4e3a128\u4e2atoken\u7684\u77ed\u53e5\u5b50, \u540e10%\u7684steps\u624d\u4f7f\u7528\u957f\u5ea6\u4e3a512\u4e2atoken\u7684\u957f\u53e5\u5b50. AlBERT\u572890%\u7684steps\u4e2d\u4f7f\u7528\u4e86\u957f\u5ea6\u4e3a512\u4e2atoken\u7684\u957f\u53e5\u5b50, \u66f4\u957f\u7684\u53e5\u5b50\u53ef\u4ee5\u63d0\u4f9b\u66f4\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f, \u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u80fd\u529b. Masked-Ngram-LM\u7684\u4f18\u5316: BERT\u7684MLM\u76ee\u6807\u662f\u968f\u673amask\u638915%\u7684token\u6765\u8fdb\u884c\u9884\u6d4b, \u5176\u4e2d\u7684token\u65e9\u5df2\u5206\u597d, \u4e00\u4e2a\u4e2a\u7b97. AlBERT\u9884\u6d4b\u7684\u662fNgram\u7247\u6bb5, \u6bcf\u4e2a\u7247\u6bb5\u957f\u5ea6\u4e3an (n=1,2,3), \u6bcf\u4e2aNgram\u7247\u6bb5\u7684\u6982\u7387\u6309\u7167\u516c\u5f0f\u5206\u522b\u8ba1\u7b97\u5373\u53ef. \u6bd4\u59821-gram, 2-gram, 3-gram\u7684\u6982\u7387\u5206\u522b\u4e3a6/11, 3/11, 2/11. AlBERT\u7cfb\u5217\u4e2d\u5305\u542b\u4e00\u4e2aalbert-tiny\u6a21\u578b, \u9690\u85cf\u5c42\u4ec5\u67094\u5c42, \u53c2\u6570\u91cf1.8M, \u975e\u5e38\u8f7b\u5de7. \u76f8\u6bd4\u8f83BERT, \u5176\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea610\u500d, \u4f46\u7cbe\u5ea6\u57fa\u672c\u4fdd\u7559, \u8bed\u4e49\u76f8\u4f3c\u5ea6\u6570\u636e\u96c6LCQMC\u6d4b\u8bd5\u96c6\u8fbe\u523085.4%, \u76f8\u6bd4\u4e8ebert-base\u4ec5\u4e0b\u964d1.5%, \u975e\u5e38\u4f18\u79c0. 2 RoBERTa\u6a21\u578b \u00b6 \u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1RoBERTa\u6a21\u578b\u7684\u67b6\u6784. \u7406\u89e3RoBERTa\u6a21\u578b\u7684\u4f18\u5316\u70b9. 2.1 RoBERTa\u6a21\u578b\u7684\u67b6\u6784 \u00b6 \u539f\u59cb\u8bba\u6587<< RoBERTa: A Robustly Optimized BERT Pretraining Approach >>, \u7531FaceBook\u548c\u534e\u76db\u987f\u5927\u5b66\u8054\u5408\u4e8e2019\u5e74\u63d0\u51fa\u7684\u6a21\u578b. \u4ece\u6a21\u578b\u67b6\u6784\u4e0a\u770b, RoBERTa\u548cBERT\u5b8c\u5168\u4e00\u81f4, \u6838\u5fc3\u6a21\u5757\u90fd\u662f\u57fa\u4e8eTransformer\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b. \u6539\u8fdb\u70b9\u4e3b\u8981\u96c6\u4e2d\u5728\u4e00\u4e9b\u8bad\u7ec3\u7ec6\u8282\u4e0a. \u7b2c1\u70b9: More data \u7b2c2\u70b9: Larger batch size \u7b2c3\u70b9: Training longer \u7b2c4\u70b9: No NSP \u7b2c5\u70b9: Dynamic masking \u7b2c6\u70b9: Byte level BPE 2.2 RoBERTa\u6a21\u578b\u7684\u4f18\u5316\u70b9 \u00b6 \u9488\u5bf9\u4e8e\u4e0a\u9762\u63d0\u5230\u76847\u70b9\u7ec6\u8282, \u4e00\u4e00\u5c55\u5f00\u8bf4\u660e: \u7b2c1\u70b9: More data (\u66f4\u5927\u7684\u6570\u636e\u91cf) \u539f\u59cbBERT\u7684\u8bad\u7ec3\u8bed\u6599\u91c7\u7528\u4e8616GB\u7684\u6587\u672c\u6570\u636e. RoBERTa\u91c7\u7528\u4e86160GB\u7684\u6587\u672c\u6570\u636e. 1: Books Corpus + English Wikipedia (16GB): BERT\u539f\u6587\u4f7f\u7528\u7684\u4e4b\u6570\u636e. 2: CC-News (76GB): \u81eaCommonCrawl News\u6570\u636e\u4e2d\u7b5b\u9009\u540e\u5f97\u5230\u6570\u636e, \u7ea6\u542b6300\u4e07\u7bc7\u65b0\u95fb, 2016\u5e749\u6708-2019\u5e742\u6708. 3: OpenWebText (38GB): \u8be5\u6570\u636e\u662f\u501f\u9274GPT2, \u4eceReddit\u8bba\u575b\u4e2d\u83b7\u53d6, \u53d6\u70b9\u8d5e\u6570\u5927\u4e8e3\u7684\u5185\u5bb9. 4: Storie (31GB): \u540c\u6837\u4eceCommonCrawl\u83b7\u53d6, \u5c5e\u4e8e\u6545\u4e8b\u7c7b\u6570\u636e, \u800c\u975e\u65b0\u95fb\u7c7b. \u7b2c2\u70b9: Larger batch size (\u66f4\u5927\u7684batch size) BERT\u91c7\u7528\u7684batch size\u7b49\u4e8e256. RoBERTa\u7684\u8bad\u7ec3\u5728\u591a\u79cd\u6a21\u5f0f\u4e0b\u91c7\u7528\u4e86\u66f4\u5927\u7684batch size, \u4ece256\u4e00\u76f4\u5230\u6700\u5927\u76848000. \u7b2c3\u70b9: Training longer (\u66f4\u591a\u7684\u8bad\u7ec3\u6b65\u6570) RoBERTa\u7684\u8bad\u7ec3\u91c7\u7528\u4e86\u66f4\u591a\u7684\u8bad\u7ec3\u6b65\u6570, \u8ba9\u6a21\u578b\u5145\u5206\u5b66\u4e60\u6570\u636e\u4e2d\u7684\u7279\u5f81. \u7b2c4\u70b9: No NSP (\u53bb\u6389NSP\u4efb\u52a1) \u4ece2019\u5e74\u5f00\u59cb, \u5df2\u7ecf\u6709\u8d8a\u6765\u8d8a\u591a\u7684\u8bc1\u636e\u8868\u660eNSP\u4efb\u52a1\u5bf9\u4e8e\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u4e00\u4e2a\u8d1f\u9762\u4f5c\u7528, \u56e0\u6b64\u5728RoBERTa\u4e2d\u76f4\u63a5\u53d6\u6d88\u6389NSP\u4efb\u52a1. \u8bba\u6587\u4f5c\u8005\u8fdb\u884c\u4e86\u591a\u7ec4\u5bf9\u7167\u8bd5\u9a8c: 1: Segment + NSP (\u5373BERT\u6a21\u5f0f). \u8f93\u5165\u5305\u542b\u4e24\u90e8\u5206, \u6bcf\u4e2a\u90e8\u5206\u662f\u6765\u81ea\u540c\u4e00\u6587\u6863\u6216\u8005\u4e0d\u540c\u6587\u6863\u7684segment(segment\u662f\u8fde\u7eed\u7684\u591a\u4e2a\u53e5\u5b50), \u8fd9\u4e24\u4e2asegment\u7684token\u603b\u6570\u5c11\u4e8e512, \u9884\u8bad\u7ec3\u5305\u542bMLM\u4efb\u52a1\u548cNSP\u4efb\u52a1. 2: Sentence pair + NSP (\u4f7f\u7528\u4e24\u4e2a\u8fde\u7eed\u7684\u53e5\u5b50 + NSP, \u5e76\u91c7\u7528\u66f4\u5927\u7684batch size). \u8f93\u5165\u4e5f\u662f\u5305\u542b\u4e24\u90e8\u5206, \u6bcf\u4e2a\u90e8\u5206\u662f\u6765\u81ea\u540c\u4e00\u4e2a\u6587\u6863\u6216\u8005\u4e0d\u540c\u6587\u6863\u7684\u5355\u4e2a\u53e5\u5b50, \u8fd9\u4e24\u4e2a\u53e5\u5b50\u7684token \u603b\u6570\u5c11\u4e8e512. \u7531\u4e8e\u8fd9\u4e9b\u8f93\u5165\u660e\u663e\u5c11\u4e8e512\u4e2atokens, \u56e0\u6b64\u589e\u52a0batch size\u7684\u5927\u5c0f, \u4ee5\u4f7ftokens\u603b\u6570\u4fdd\u6301\u4e0eSEGMENT-PAIR + NSP\u76f8\u4f3c, \u9884\u8bad\u7ec3\u5305\u542bMLM\u4efb\u52a1\u548cNSP\u4efb\u52a1. 3: Full-sentences (\u5982\u679c\u8f93\u5165\u7684\u6700\u5927\u957f\u5ea6\u4e3a512, \u90a3\u4e48\u5c3d\u91cf\u9009\u62e9512\u957f\u5ea6\u7684\u8fde\u7eed\u53e5\u5b50; \u5982\u679c\u8de8\u8d8adocument, \u5c31\u5728\u4e2d\u95f4\u52a0\u4e0a\u4e00\u4e2a\u7279\u6b8a\u5206\u9694\u7b26, \u6bd4\u5982[SEP]; \u8be5\u8bd5\u9a8c\u6ca1\u6709NSP). \u8f93\u5165\u53ea\u6709\u4e00\u90e8\u5206(\u800c\u4e0d\u662f\u4e24\u90e8\u5206), \u6765\u81ea\u540c\u4e00\u4e2a\u6587\u6863\u6216\u8005\u4e0d\u540c\u6587\u6863\u7684\u8fde\u7eed\u591a\u4e2a\u53e5\u5b50, token\u603b\u6570\u4e0d\u8d85\u8fc7512. \u8f93\u5165\u53ef\u80fd\u8de8\u8d8a\u6587\u6863\u8fb9\u754c, \u5982\u679c\u8de8\u6587\u6863, \u5219\u5728\u4e0a\u4e00\u4e2a\u6587\u6863\u672b\u5c3e\u6dfb\u52a0\u6587\u6863\u8fb9\u754ctoken, \u9884\u8bad\u7ec3\u4e0d\u5305\u542bNSP\u4efb\u52a1. 4: Document-sentences (\u548c\u60c5\u51b53\u4e00\u6837, \u4f46\u662f\u6b65\u8de8\u8d8adocument; \u8be5\u5b9e\u9a8c\u6ca1\u6709NSP). \u8f93\u5165\u53ea\u6709\u4e00\u90e8\u5206(\u800c\u4e0d\u662f\u4e24\u90e8\u5206), \u8f93\u5165\u7684\u6784\u9020\u7c7b\u4f3c\u4e8eFull-sentences, \u53ea\u662f\u4e0d\u9700\u8981\u8de8\u8d8a\u6587\u6863\u8fb9\u754c, \u5176\u8f93\u5165\u6765\u81ea\u540c\u4e00\u4e2a\u6587\u6863\u7684\u8fde\u7eed\u53e5\u5b50, token\u603b\u6570\u4e0d\u8d85\u8fc7512. \u5728\u6587\u6863\u672b\u5c3e\u9644\u8fd1\u91c7\u6837\u7684\u8f93\u5165\u53ef\u4ee5\u77ed\u4e8e512\u4e2atokens, \u56e0\u6b64\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u52a8\u6001\u589e\u52a0batch size\u5927\u5c0f\u4ee5\u8fbe\u5230\u4e0eFull-sentecens\u76f8\u540c\u7684tokens\u603b\u6570, \u9884\u8bad\u7ec3\u4e0d\u5305\u542bNSP\u4efb\u52a1. \u603b\u7684\u6765\u8bf4, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e1 < 2 < 3 < 4. \u771f\u5b9e\u53e5\u5b50\u8fc7\u77ed\u7684\u8bdd, \u4e0d\u5982\u62fc\u63a5\u6210\u53e5\u5b50\u6bb5. \u6ca1\u6709NSP\u4efb\u52a1\u66f4\u4f18. \u4e0d\u8de8\u8d8adocument\u66f4\u4f18. \u7b2c5\u70b9: Dynamic masking (\u91c7\u7528\u52a8\u6001masking\u7b56\u7565) \u539f\u59cb\u9759\u6001mask: \u5373BERT\u7248\u672c\u7684mask\u7b56\u7565, \u51c6\u5907\u8bad\u7ec3\u6570\u636e\u65f6, \u6bcf\u4e2a\u6837\u672c\u53ea\u4f1a\u8fdb\u884c\u4e00\u6b21\u968f\u673amask(\u56e0\u6b64\u6bcf\u4e2aepoch\u90fd\u662f\u91cd\u590d\u7684), \u540e\u7eed\u7684\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u90fd\u91c7\u7528\u76f8\u540c\u7684mask\u65b9\u5f0f, \u8fd9\u662f\u539f\u59cb\u9759\u6001mask. \u52a8\u6001mask: \u5e76\u6ca1\u6709\u5728\u9884\u5904\u7406\u7684\u65f6\u5019\u6267\u884cmask, \u800c\u662f\u5728\u6bcf\u6b21\u5411\u6a21\u578b\u63d0\u4f9b\u8f93\u5165\u65f6\u52a8\u6001\u751f\u6210mask, \u6240\u4ee5\u5230\u5e95\u54ea\u4e9btokens\u88abmask\u6389\u4e86\u662f\u65f6\u523b\u53d8\u5316\u7684, \u65e0\u6cd5\u63d0\u524d\u9884\u77e5\u7684. \u7b2c6\u70b9: Byte level BPE (\u91c7\u7528\u5b57\u8282\u7ea7\u522b\u7684Encoding) \u57fa\u4e8echar-level: \u539f\u59cbBERT\u7684\u65b9\u5f0f, \u5728\u4e2d\u6587\u573a\u666f\u4e0b\u5c31\u662f\u5904\u7406\u4e00\u4e2a\u4e2a\u7684\u6c49\u5b57. \u57fa\u4e8ebytes-level: \u4e0echar-level\u7684\u533a\u522b\u5728\u4e8e\u7f16\u7801\u7684\u7c92\u5ea6\u662fbytes, \u800c\u4e0d\u662funicode\u5b57\u7b26\u4f5c\u4e3asub-word\u7684\u57fa\u672c\u5355\u4f4d. \u5f53\u91c7\u7528bytes-level\u7684BPE\u4e4b\u540e, \u8bcd\u8868\u5927\u5c0f\u4ece3\u4e07(\u539f\u59cbBERT\u7684char-level)\u589e\u52a0\u52305\u4e07. \u8fd9\u5206\u522b\u4e3aBERT-base\u548cBERT-large\u589e\u52a0\u4e861500\u4e07\u548c2000\u4e07\u989d\u5916\u7684\u53c2\u6570. \u4e4b\u524d\u6709\u7814\u7a76\u8868\u660e, \u8fd9\u6837\u7684\u505a\u6cd5\u5728\u6709\u4e9b\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4f1a\u5bfc\u81f4\u8f7b\u5fae\u7684\u6027\u80fd\u4e0b\u964d. \u4f46\u8bba\u6587\u4f5c\u8005\u76f8\u4fe1: \u8fd9\u79cd\u7edf\u4e00\u7f16\u7801\u7684\u4f18\u52bf\u4f1a\u8d85\u8fc7\u6027\u80fd\u7684\u8f7b\u5fae\u4e0b\u964d. 3 MacBert\u6a21\u578b \u00b6 3.1 \u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1MacBert\u6a21\u578b\u7684\u67b6\u6784. \u638c\u63e1MacBert\u6a21\u578b\u7684\u4f18\u5316\u70b9. 3.2 MacBert\u6a21\u578b\u7684\u67b6\u6784 \u00b6 MacBert\u6a21\u578b\u7531\u54c8\u5de5\u5927NLP\u5b9e\u9a8c\u5ba4\u4e8e2020\u5e7411\u6708\u63d0\u51fa, 2021\u5e745\u6708\u53d1\u5e03\u5e94\u7528, \u662f\u9488\u5bf9\u4e8eBERT\u6a21\u578b\u505a\u4e86\u4f18\u5316\u6539\u826f\u540e\u7684\u9884\u8bad\u7ec3\u6a21\u578b. << Revisiting Pre-trained Models for Chinese Natural Language Processing >>, \u901a\u8fc7\u539f\u59cb\u8bba\u6587\u9898\u76ee\u4e5f\u53ef\u4ee5\u77e5\u9053, MacBert\u662f\u9488\u5bf9\u4e8e\u4e2d\u6587\u573a\u666f\u4e0b\u7684BERT\u4f18\u5316. MacBert\u6a21\u578b\u7684\u67b6\u6784\u548cBERT\u5927\u90e8\u5206\u4fdd\u6301\u4e00\u81f4, \u6700\u5927\u7684\u53d8\u5316\u6709\u4e24\u70b9: \u7b2c\u4e00\u70b9: \u5bf9\u4e8eMLM\u9884\u8bad\u7ec3\u4efb\u52a1, \u91c7\u7528\u4e86\u4e0d\u540c\u7684MASK\u7b56\u7565. \u7b2c\u4e8c\u70b9: \u5220\u9664\u4e86NSP\u4efb\u52a1, \u66ff\u6362\u6210SOP\u4efb\u52a1. 3.3 MacBert\u6a21\u578b\u7684\u4f18\u5316\u70b9 \u00b6 \u7b2c\u4e00\u70b9: \u5bf9\u4e8eMLM\u9884\u8bad\u7ec3\u4efb\u52a1, \u91c7\u7528\u4e86\u4e0d\u540c\u7684MASK\u7b56\u7565. 1: \u4f7f\u7528\u4e86\u5168\u8bcdmasked\u4ee5\u53can-gram masked\u7b56\u7565\u6765\u9009\u62e9tokens\u5982\u4f55\u88ab\u906e\u63a9, \u4ece\u5355\u4e2a\u5b57\u7b26\u52304\u4e2a\u5b57\u7b26\u7684\u906e\u63a9\u6bd4\u4f8b\u5206\u522b\u4e3a40%, 30%, 20%, 10% 2: \u539f\u59cbBERT\u6a21\u578b\u4e2d\u7684[MASK]\u51fa\u73b0\u5728\u8bad\u7ec3\u9636\u6bb5, \u4f46\u6ca1\u6709\u51fa\u73b0\u5728\u5fae\u8c03\u9636\u6bb5, \u8fd9\u4f1a\u9020\u6210exposure bias\u7684\u95ee\u9898. \u56e0\u6b64\u5728MacBert\u4e2d\u63d0\u51fa\u4f7f\u7528\u7c7b\u4f3c\u7684\u5355\u8bcd\u6765\u8fdb\u884cmasked. \u5177\u4f53\u6765\u8bf4, \u4f7f\u7528\u57fa\u4e8eWord2Vec\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5305\u8bad\u7ec3\u8bcd\u5411\u91cf, \u540e\u7eed\u5229\u7528\u8fd9\u91cc\u9762\u627e\u8fd1\u4e49\u8bcd\u7684\u529f\u80fd\u6765\u8f85\u52a9mask, \u6bd4\u5982\u4ee530%\u7684\u6982\u7387\u9009\u62e9\u4e86\u4e00\u4e2a3-gram\u7684\u5355\u8bcd\u8fdb\u884cmasked, \u5219\u5c06\u5728Word2Vec\u4e2d\u5bfb\u627e3-gram\u7684\u8fd1\u4e49\u8bcd\u6765\u66ff\u6362, \u5728\u6781\u5c11\u6570\u60c5\u51b5\u4e0b, \u5f53\u6ca1\u6709\u7b26\u5408\u6761\u4ef6\u7684\u76f8\u4f3c\u5355\u8bcd\u65f6, \u7b56\u7565\u4f1a\u8fdb\u884c\u964d\u7ea7, \u76f4\u63a5\u4f7f\u7528\u968f\u673a\u5355\u8bcd\u8fdb\u884c\u66ff\u6362. 3: \u4f7f\u752815%\u7684\u767e\u5206\u6bd4\u5bf9\u8f93\u5165\u5355\u8bcd\u8fdb\u884cMASK, \u5176\u4e2d80%\u7684\u6982\u7387\u4e0b\u6267\u884c\u7b56\u75652(\u5373\u66ff\u6362\u4e3a\u76f8\u4f3c\u5355\u8bcd), 10%\u7684\u6982\u7387\u4e0b\u66ff\u6362\u4e3a\u968f\u673a\u5355\u8bcd, 10%\u7684\u6982\u7387\u4e0b\u4fdd\u7559\u539f\u59cb\u5355\u8bcd\u4e0d\u53d8. \u7b2c\u4e8c\u70b9: \u5220\u9664\u4e86NSP\u4efb\u52a1, \u66ff\u6362\u6210SOP\u4efb\u52a1. \u7b2c\u4e8c\u70b9\u4f18\u5316\u662f\u76f4\u63a5\u501f\u9274\u4e86AlBERT\u6a21\u578b\u4e2d\u63d0\u51fa\u7684SOP\u4efb\u52a1. \u5728NLP\u8457\u540d\u7684\u96be\u4efb\u52a1\u9605\u8bfb\u7406\u89e3\u4e2d, MacBert\u5c55\u73b0\u51fa\u975e\u5e38\u4f18\u79c0\u7684\u8868\u73b0. 4 SpanBERT\u6a21\u578b \u00b6 \u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1SpanBERT\u6a21\u578b\u7684\u67b6\u6784. \u638c\u63e1SpanBERT\u6a21\u578b\u7684\u4f18\u5316\u70b9. 4.1 SpanBERT\u6a21\u578b\u7684\u67b6\u6784 \u00b6 \u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\u67093\u70b9: 1: \u63d0\u51fa\u4e86\u66f4\u597d\u7684Span Mask\u65b9\u6848, \u518d\u6b21\u5c55\u793a\u4e86\u968f\u673a\u906e\u63a9\u8fde\u7eed\u4e00\u6bb5tokens\u6bd4\u968f\u673a\u906e\u63a9\u5355\u4e2atoken\u8981\u597d. 2: \u901a\u8fc7\u52a0\u5165\u4e86Span Boundary Objective(SBO)\u8bad\u7ec3\u4efb\u52a1, \u589e\u5f3a\u4e86BERT\u7684\u6027\u80fd, \u7279\u522b\u5728\u4e00\u4e9b\u548cSpan\u9002\u914d\u7684\u4efb\u52a1, \u5982\u62bd\u53d6\u5f0f\u95ee\u7b54. 3: \u7528\u5b9e\u9a8c\u6570\u636e\u83b7\u5f97\u4e86\u548cXLNet\u4e00\u81f4\u7684\u7ed3\u679c, \u53d1\u73b0\u53bb\u9664\u6389NSP\u4efb\u52a1, \u76f4\u63a5\u7528\u8fde\u7eed\u4e00\u957f\u53e5\u8bad\u7ec3\u6548\u679c\u66f4\u597d. SpanBERT\u7684\u67b6\u6784\u56fe\u5982\u4e0b: \u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u6e05\u6670\u7684\u5c55\u793a\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u70b9: Span Masking Span Boundary Objective 4.2 Span Masking \u00b6 \u5173\u4e8e\u521b\u65b0\u7684MASK\u673a\u5236, \u4e00\u822c\u6765\u8bf4\u90fd\u662f\u76f8\u5bf9\u4e8e\u539f\u59cbBERT\u7684\u57fa\u51c6\u8fdb\u884c\u6539\u8fdb. \u5bf9\u4e8eBERT, \u8bad\u7ec3\u65f6\u4f1a\u968f\u673a\u9009\u53d6\u6574\u53e5\u4e2d\u7684\u6700\u5c0f\u8f93\u5165\u5355\u5143token\u6765\u8fdb\u884c\u906e\u63a9, \u4e2d\u6587\u573a\u666f\u4e0b\u672c\u8d28\u4e0a\u5c31\u662f\u8fdb\u884c\u5b57\u7ea7\u522b\u7684MASK. \u4f46\u662f\u8fd9\u79cd\u65b9\u5f0f\u4f1a\u8ba9\u672c\u6765\u5e94\u8be5\u6709\u5f3a\u76f8\u5173\u7684\u4e00\u4e9b\u8fde\u5728\u4e00\u8d77\u7684\u5b57\u8bcd, \u5728\u8bad\u7ec3\u65f6\u88ab\u5272\u88c2\u5f00\u4e86. \u90a3\u4e48\u9996\u5148\u60f3\u5230\u7684\u505a\u6cd5: \u65e2\u7136\u80fd\u906e\u63a9\u5b57, \u90a3\u4e48\u80fd\u4e0d\u80fd\u76f4\u63a5\u906e\u63a9\u6574\u4e2a\u8bcd\u5462? \u8fd9\u5c31\u662fBERT-WWM\u6a21\u578b\u7684\u601d\u60f3. \u539f\u59cb\u8f93\u5165: \u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684\u6982\u7387. \u539f\u59cbBERT: \u4f7f\u7528\u8bed\u8a00[MASK]\u578b\u6765[MASK]\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684[MASK]\u7387. BERT-WWM: \u4f7f\u7528\u8bed\u8a00[MASK][MASK]\u6765[MASK][MASK]\u4e0b\u4e00\u4e2a\u8bcd\u7684[MASK][MASK]. \u5f15\u7533: \u767e\u5ea6\u8457\u540d\u7684ERNIE\u6a21\u578b\u4e2d, \u76f4\u63a5\u5f15\u5165\u547d\u540d\u5b9e\u4f53(Named Entity)\u7684\u5916\u90e8\u77e5\u8bc6, \u8fdb\u884c\u6574\u4e2a\u5b9e\u4f53\u7684\u906e\u63a9, \u8fdb\u884c\u8bad\u7ec3. \u7efc\u5408\u4e0a\u9762\u6240\u8bf4, \u4f1a\u66f4\u81ea\u7136\u7684\u60f3\u5230, \u65e2\u7136\u6574\u8bcd\u7684MASK, \u90a3\u4e48\u5982\u679c\u62e5\u6709\u8bcd\u7684\u8fb9\u754c\u4fe1\u606f\u4f1a\u4e0d\u4f1a\u8ba9\u6a21\u578b\u7684\u80fd\u529b\u66f4\u4e0a\u4e00\u5c42\u697c\u5462? SpanBERT\u7ed9\u51fa\u7684\u662f\u80af\u5b9a\u7684\u56de\u7b54!!! \u8bba\u6587\u4e2d\u5173\u4e8espan\u7684\u9009\u62e9, \u8d70\u4e86\u8fd9\u6837\u4e00\u4e2a\u6d41\u7a0b: \u7b2c\u4e00\u6b65: \u6839\u636e\u51e0\u4f55\u5206\u5e03, \u5148\u968f\u673a\u9009\u62e9\u4e00\u4e2a**span\u957f\u5ea6**. \u7b2c\u4e8c\u6b65: \u518d\u6839\u636e\u5747\u5300\u5206\u5e03\u968f\u673a\u9009\u62e9\u8fd9\u4e00\u6bb5\u7684**\u8d77\u59cb\u4f4d\u7f6e**. \u7b2c\u4e09\u6b65: \u6700\u540e\u6839\u636e\u524d\u4e24\u6b65\u7684start\u548clength\u76f4\u63a5\u8fdb\u884cMASK. \u7ed3\u8bba: \u8bba\u6587\u4e2d\u8be6\u7ec6\u8bba\u8bc1\u4e86\u6309\u7167\u4e0a\u8ff0\u7b97\u6cd5\u8fdb\u884cMASK, \u968f\u673a\u88ab\u906e\u63a9\u7684\u6587\u672c\u5e73\u5747\u957f\u5ea6\u7b49\u4e8e3.8 4.3 Span Boundary Objective(SBO) \u00b6 SBO\u4efb\u52a1\u662f\u672c\u7bc7\u8bba\u6587\u6700\u6838\u5fc3\u7684\u521b\u65b0\u70b9, \u5e0c\u671b\u901a\u8fc7\u589e\u52a0\u8fd9\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1, \u53ef\u4ee5\u8ba9\u88ab\u906e\u63a9\u7684Span Boundary\u7684\u8bcd\u5411\u91cf, \u80fd\u591f\u5b66\u4e60\u5230Span\u5185\u90e8\u7684\u4fe1\u606f. \u5177\u4f53\u7684\u505a\u6cd5: \u5728\u8bad\u7ec3\u65f6\u53d6Span\u524d\u540e\u8fb9\u754c\u7684\u4e24\u4e2a\u8bcd, \u9700\u8981\u6ce8\u610f\u8fd9\u4e24\u4e2a\u8bcd\u4e0d\u5728Span\u5185, \u7136\u540e\u7528\u8fd9\u4e24\u4e2a\u8bcd\u5411\u91cf\u52a0\u4e0aSpan\u4e2d\u88abMASK\u6389\u7684\u8bcd\u7684\u4f4d\u7f6e\u5411\u91cf, \u6765\u9884\u6d4b\u539f\u8bcd. \u66f4\u8be6\u7ec6\u7684\u64cd\u4f5c\u5982\u4e0b, \u5373\u5c06\u8bcd\u5411\u91cf\u548c\u4f4d\u7f6e\u5411\u91cf\u8fdb\u884c\u62fc\u63a5, \u7ecf\u8fc7GeLU\u6fc0\u6d3b\u548cLayerNorm\u5904\u7406, \u8fde\u7eed\u7ecf\u8fc7\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42, \u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u5f20\u91cf: \u6700\u540e\u9884\u6d4bSpan\u4e2d\u539f\u8bcd\u7684\u65f6\u5019\u4f1a\u5f97\u5230\u4e00\u4e2a\u635f\u5931, \u8fd9\u5c31\u662fSBO\u4efb\u52a1\u7684\u635f\u5931; \u518d\u5c06\u5176\u548cBERT\u81ea\u8eab\u7684MLM\u4efb\u52a1\u7684\u635f\u5931\u8fdb\u884c\u52a0\u548c, \u5171\u540c\u4f5c\u4e3aSpanBERT\u7684\u76ee\u6807\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3: 4.4 NSP\u4efb\u52a1\u53cd\u601d \u00b6 \u4e3a\u4ec0\u4e48\u9009\u62e9Single Sentence\u800c\u4e0d\u662fBERT\u7684Two Sentence? 1: \u8bad\u7ec3\u6587\u672c\u7684\u957f\u5ea6\u66f4\u5927, \u53ef\u4ee5\u5b66\u4f1a\u957f\u7a0b\u4f9d\u8d56. 2: \u5bf9\u4e8eNSP\u7684\u8d1f\u6837\u672c, \u57fa\u4e8e\u53e6\u4e00\u4e2a\u4e3b\u9898\u6587\u6863\u7684\u53e5\u5b50\u6765\u9884\u6d4b\u5355\u8bcd, \u4f1a\u7ed9MLM\u4efb\u52a1\u5f15\u5165\u5f88\u5927\u7684\u566a\u58f0. 3: AlBERT\u6a21\u578b\u5df2\u7ecf\u7ed9\u51fa\u4e86\u8bba\u8bc1, \u56e0\u4e3aNSP\u4efb\u52a1\u592a\u7b80\u5355\u4e86.","title":"3 BERT\u7cfb\u5217\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#bert","text":"","title":"BERT\u7cfb\u5217\u6a21\u578b"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3\u4e0d\u540c\u7c7b\u578b\u7684BERT\u7cfb\u5217\u6a21\u578b. \u638c\u63e1BERT\u7cfb\u5217\u6a21\u578b\u4e4b\u95f4\u7684\u533a\u522b\u548c\u8054\u7cfb.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-albert","text":"","title":"1 AlBERT\u6a21\u578b"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_2","text":"\u4e86\u89e3AlBERT\u6a21\u578b\u7684\u67b6\u6784. \u638c\u63e1AlBERT\u6a21\u578b\u7684\u4f18\u5316\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#11-albert","text":"AlBERT\u6a21\u578b\u53d1\u5e03\u4e8eICLR 2020\u4f1a\u8bae, \u662f\u57fa\u4e8eBERT\u6a21\u578b\u7684\u91cd\u8981\u6539\u8fdb\u7248\u672c. \u662f\u8c37\u6b4c\u7814\u7a76\u9662\u548c\u829d\u52a0\u54e5\u5927\u5b66\u5171\u540c\u53d1\u5e03\u7684\u7814\u7a76\u6210\u679c. \u8bba\u6587\u5168\u79f0<< A Lite BERT For Self-Supervised Learning Of Language Representations >>. \u4ece\u6a21\u578b\u67b6\u6784\u4e0a\u770b, AlBERT\u548cBERT\u57fa\u672c\u4e00\u81f4, \u6838\u5fc3\u6a21\u5757\u90fd\u662f\u57fa\u4e8eTransformer\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b. \u5728\u672c\u7bc7\u8bba\u6587\u4e2d, \u9996\u5148\u5bf9\u6bd4\u4e86\u8fc7\u53bb\u51e0\u5e74\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e3b\u6d41\u64cd\u4f5c\u601d\u8def. \u7b2c\u4e00: \u5927\u89c4\u6a21\u7684\u8bed\u6599. \u7b2c\u4e8c: \u66f4\u6df1\u7684\u7f51\u7edc, \u66f4\u591a\u7684\u53c2\u6570. \u7b2c\u4e09: \u591a\u4efb\u52a1\u8bad\u7ec3.","title":"1.1 AlBERT\u6a21\u578b\u7684\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#12-albert","text":"\u76f8\u6bd4\u8f83\u4e8eBERT\u6a21\u578b, AlBERT\u7684\u51fa\u53d1\u70b9\u5373\u662f\u5e0c\u671b\u964d\u4f4e\u9884\u8bad\u7ec3\u7684\u96be\u5ea6, \u540c\u65f6\u63d0\u5347\u6a21\u578b\u5173\u952e\u80fd\u529b. \u4e3b\u8981\u5f15\u5165\u4e865\u5927\u4f18\u5316. \u7b2c\u4e00: \u8bcd\u5d4c\u5165\u53c2\u6570\u7684\u56e0\u5f0f\u5206\u89e3. \u7b2c\u4e8c: \u9690\u85cf\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u5171\u4eab. \u7b2c\u4e09: \u53bb\u6389NSP, \u589e\u52a0SOP\u9884\u8bad\u7ec3\u4efb\u52a1. \u7b2c\u56db: \u53bb\u6389dropout\u64cd\u4f5c. \u7b2c\u4e94: MLM\u4efb\u52a1\u7684\u4f18\u5316. \u7b2c\u4e00: \u8bcd\u5d4c\u5165\u53c2\u6570\u7684\u56e0\u5f0f\u5206\u89e3. AlBERT\u7684\u4f5c\u8005\u8ba4\u4e3a, \u8bcd\u5411\u91cf\u53ea\u8bb0\u5f55\u4e86\u5c11\u91cf\u7684\u8bcd\u6c47\u672c\u8eab\u7684\u4fe1\u606f, \u66f4\u591a\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u53e5\u6cd5\u4fe1\u606f\u5305\u542b\u5728\u9690\u85cf\u5c42\u4e2d. \u56e0\u6b64\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u4e0d\u4e00\u5b9a\u975e\u8981\u548c\u9690\u85cf\u5c42\u7684\u7ef4\u5ea6\u4e00\u81f4. \u5177\u4f53\u505a\u6cd5\u5c31\u662f\u901a\u8fc7\u56e0\u5f0f\u5206\u89e3\u6765\u964d\u4f4e\u5d4c\u5165\u77e9\u9635\u7684\u53c2\u6570: BERT: embedding_dim * vocab_size = hidden_size * vocab_size, \u5176\u4e2dembedding_dim=768, vocab_size\u5927\u7ea6\u4e3a30000\u5de6\u53f3\u7684\u7ea7\u522b, \u5927\u7ea6\u7b49\u4e8e30000 * 768 = 23040000(2300\u4e07). AlBERT: vocab_size * project + project * hidden_size, \u5176\u4e2dproject\u662f\u56e0\u5f0f\u5206\u89e3\u7684\u4e2d\u95f4\u6620\u5c04\u5c42\u7ef4\u5ea6, \u4e00\u822c\u53d6128, \u53c2\u6570\u603b\u91cf\u5927\u7ea6\u7b49\u4e8e30000 * 128 + 128 * 768 = 482304(48\u4e07). \u7b2c\u4e8c: \u9690\u85cf\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u5171\u4eab. \u5728BERT\u6a21\u578b\u4e2d, \u65e0\u8bba\u662f12\u5c42\u7684base, \u8fd8\u662f24\u5c42\u7684large\u6a21\u578b, \u5176\u4e2d\u6bcf\u4e00\u4e2aEncoder Block\u90fd\u62e5\u6709\u72ec\u7acb\u7684\u53c2\u6570\u6a21\u5757, \u5305\u542b\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42, \u524d\u9988\u5168\u8fde\u63a5\u5c42. \u975e\u5e38\u91cd\u8981\u7684\u4e00\u70b9\u662f, \u8fd9\u4e9b\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u90fd\u662f\u72ec\u7acb\u7684, \u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\u90fd\u4e0d\u4e00\u6837\u4e86! \u90a3\u4e48\u4e3a\u4e86\u51cf\u5c11\u6a21\u578b\u7684\u53c2\u6570\u91cf, \u4e00\u4e2a\u5f88\u76f4\u89c2\u7684\u505a\u6cd5\u4fbf\u662f\u8ba9\u8fd9\u4e9b\u5c42\u4e4b\u95f4\u7684\u53c2\u6570\u5171\u4eab, \u672c\u8d28\u4e0a\u53ea\u6709\u4e00\u5957Encoder Block\u7684\u53c2\u6570! \u5728AlBERT\u6a21\u578b\u4e2d, \u6240\u6709\u7684\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42, \u5168\u8fde\u63a5\u5c42\u7684\u53c2\u6570\u90fd\u662f\u5206\u522b\u5171\u4eab\u7684, \u901a\u8fc7\u8fd9\u6837\u7684\u65b9\u5f0f, AlBERT\u5c5e\u4e8eBlock\u7684\u53c2\u6570\u91cf\u5728BERT\u7684\u57fa\u7840\u4e0a, \u5206\u522b\u4e0b\u964d\u5230\u539f\u6765\u76841/12, 1/24. \u7b2c\u4e09: \u53bb\u6389NSP, \u589e\u52a0SOP\u9884\u8bad\u7ec3\u4efb\u52a1. BERT\u6a21\u578b\u7684\u6210\u529f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u4e24\u70b9, \u4e00\u4e2a\u662f\u57fa\u7840\u67b6\u6784\u91c7\u7528Transformer, \u53e6\u4e00\u4e2a\u5c31\u662f\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1, MLM\u548cNSP. \u4f46\u662fBERT\u63d0\u51fa\u540e\u4e0d\u4e45, \u4fbf\u6709\u7814\u7a76\u4eba\u5458\u5bf9NSP\u4efb\u52a1\u63d0\u51fa\u8d28\u7591, \u6211\u4eec\u4e5f\u53ef\u4ee5\u53cd\u601d\u4e00\u4e0bNSP\u4efb\u52a1\u6709\u4ec0\u4e48\u95ee\u9898? \u5728AlBERT\u6a21\u578b\u4e2d, \u76f4\u63a5\u820d\u5f03\u6389\u4e86NSP\u4efb\u52a1, \u65b0\u63d0\u51fa\u4e86SOP\u4efb\u52a1(Sentence Order Prediction), \u5373\u4e24\u53e5\u8bdd\u7684\u987a\u5e8f\u9884\u6d4b, \u6587\u672c\u4e2d\u6b63\u5e38\u8bed\u5e8f\u7684\u5148\u540e\u4e24\u53e5\u8bdd[A, B]\u4f5c\u4e3a\u6b63\u6837\u672c, \u5219[B, A]\u4f5c\u4e3a\u8d1f\u6837\u672c. \u589e\u52a0\u4e86SOP\u9884\u8bad\u7ec3\u4efb\u52a1\u540e, \u4f7f\u5f97AlBERT\u62e5\u6709\u4e86\u66f4\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u8bed\u5e8f\u5173\u7cfb\u7684\u9884\u6d4b\u80fd\u529b. \u7b2c\u56db: \u53bb\u6389dropout\u64cd\u4f5c. \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230, \u5728AlBERT\u8bad\u7ec3\u8fbe\u5230100\u4e07\u4e2abatch_size\u65f6, \u6a21\u578b\u4f9d\u7136\u6ca1\u6709\u8fc7\u62df\u5408, \u4f5c\u8005\u57fa\u4e8e\u8fd9\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u76f4\u63a5\u53bb\u6389\u4e86Dropout\u64cd\u4f5c, \u7adf\u7136\u610f\u5916\u7684\u53d1\u73b0AlBERT\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u679c\u6709\u4e86\u8fdb\u4e00\u6b65\u7684\u63d0\u5347. \u8fd9\u662fNLP\u9886\u57df\u7b2c\u4e00\u6b21\u53d1\u73b0dropout\u5bf9\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u4f1a\u9020\u6210\u8d1f\u9762\u5f71\u54cd, \u4e5f\u4f7f\u5f97AlBERT v2.0\u7248\u672c\u6210\u4e3a\u7b2c\u4e00\u4e2a\u4e0d\u4f7f\u7528dropout\u64cd\u4f5c\u800c\u83b7\u5f97\u4f18\u5f02\u8868\u73b0\u7684\u4e3b\u6d41\u9884\u8bad\u7ec3\u6a21\u578b \u7b2c\u4e94: MLM\u4efb\u52a1\u7684\u4f18\u5316. segments-pair\u7684\u4f18\u5316: BERT\u4e3a\u4e86\u52a0\u901f\u8bad\u7ec3, \u524d90%\u7684steps\u4f7f\u7528\u4e86\u957f\u5ea6\u4e3a128\u4e2atoken\u7684\u77ed\u53e5\u5b50, \u540e10%\u7684steps\u624d\u4f7f\u7528\u957f\u5ea6\u4e3a512\u4e2atoken\u7684\u957f\u53e5\u5b50. AlBERT\u572890%\u7684steps\u4e2d\u4f7f\u7528\u4e86\u957f\u5ea6\u4e3a512\u4e2atoken\u7684\u957f\u53e5\u5b50, \u66f4\u957f\u7684\u53e5\u5b50\u53ef\u4ee5\u63d0\u4f9b\u66f4\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f, \u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u80fd\u529b. Masked-Ngram-LM\u7684\u4f18\u5316: BERT\u7684MLM\u76ee\u6807\u662f\u968f\u673amask\u638915%\u7684token\u6765\u8fdb\u884c\u9884\u6d4b, \u5176\u4e2d\u7684token\u65e9\u5df2\u5206\u597d, \u4e00\u4e2a\u4e2a\u7b97. AlBERT\u9884\u6d4b\u7684\u662fNgram\u7247\u6bb5, \u6bcf\u4e2a\u7247\u6bb5\u957f\u5ea6\u4e3an (n=1,2,3), \u6bcf\u4e2aNgram\u7247\u6bb5\u7684\u6982\u7387\u6309\u7167\u516c\u5f0f\u5206\u522b\u8ba1\u7b97\u5373\u53ef. \u6bd4\u59821-gram, 2-gram, 3-gram\u7684\u6982\u7387\u5206\u522b\u4e3a6/11, 3/11, 2/11. AlBERT\u7cfb\u5217\u4e2d\u5305\u542b\u4e00\u4e2aalbert-tiny\u6a21\u578b, \u9690\u85cf\u5c42\u4ec5\u67094\u5c42, \u53c2\u6570\u91cf1.8M, \u975e\u5e38\u8f7b\u5de7. \u76f8\u6bd4\u8f83BERT, \u5176\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea610\u500d, \u4f46\u7cbe\u5ea6\u57fa\u672c\u4fdd\u7559, \u8bed\u4e49\u76f8\u4f3c\u5ea6\u6570\u636e\u96c6LCQMC\u6d4b\u8bd5\u96c6\u8fbe\u523085.4%, \u76f8\u6bd4\u4e8ebert-base\u4ec5\u4e0b\u964d1.5%, \u975e\u5e38\u4f18\u79c0.","title":"1.2 AlBERT\u6a21\u578b\u7684\u4f18\u5316\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-roberta","text":"","title":"2 RoBERTa\u6a21\u578b"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_3","text":"\u638c\u63e1RoBERTa\u6a21\u578b\u7684\u67b6\u6784. \u7406\u89e3RoBERTa\u6a21\u578b\u7684\u4f18\u5316\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#21-roberta","text":"\u539f\u59cb\u8bba\u6587<< RoBERTa: A Robustly Optimized BERT Pretraining Approach >>, \u7531FaceBook\u548c\u534e\u76db\u987f\u5927\u5b66\u8054\u5408\u4e8e2019\u5e74\u63d0\u51fa\u7684\u6a21\u578b. \u4ece\u6a21\u578b\u67b6\u6784\u4e0a\u770b, RoBERTa\u548cBERT\u5b8c\u5168\u4e00\u81f4, \u6838\u5fc3\u6a21\u5757\u90fd\u662f\u57fa\u4e8eTransformer\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b. \u6539\u8fdb\u70b9\u4e3b\u8981\u96c6\u4e2d\u5728\u4e00\u4e9b\u8bad\u7ec3\u7ec6\u8282\u4e0a. \u7b2c1\u70b9: More data \u7b2c2\u70b9: Larger batch size \u7b2c3\u70b9: Training longer \u7b2c4\u70b9: No NSP \u7b2c5\u70b9: Dynamic masking \u7b2c6\u70b9: Byte level BPE","title":"2.1 RoBERTa\u6a21\u578b\u7684\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#22-roberta","text":"\u9488\u5bf9\u4e8e\u4e0a\u9762\u63d0\u5230\u76847\u70b9\u7ec6\u8282, \u4e00\u4e00\u5c55\u5f00\u8bf4\u660e: \u7b2c1\u70b9: More data (\u66f4\u5927\u7684\u6570\u636e\u91cf) \u539f\u59cbBERT\u7684\u8bad\u7ec3\u8bed\u6599\u91c7\u7528\u4e8616GB\u7684\u6587\u672c\u6570\u636e. RoBERTa\u91c7\u7528\u4e86160GB\u7684\u6587\u672c\u6570\u636e. 1: Books Corpus + English Wikipedia (16GB): BERT\u539f\u6587\u4f7f\u7528\u7684\u4e4b\u6570\u636e. 2: CC-News (76GB): \u81eaCommonCrawl News\u6570\u636e\u4e2d\u7b5b\u9009\u540e\u5f97\u5230\u6570\u636e, \u7ea6\u542b6300\u4e07\u7bc7\u65b0\u95fb, 2016\u5e749\u6708-2019\u5e742\u6708. 3: OpenWebText (38GB): \u8be5\u6570\u636e\u662f\u501f\u9274GPT2, \u4eceReddit\u8bba\u575b\u4e2d\u83b7\u53d6, \u53d6\u70b9\u8d5e\u6570\u5927\u4e8e3\u7684\u5185\u5bb9. 4: Storie (31GB): \u540c\u6837\u4eceCommonCrawl\u83b7\u53d6, \u5c5e\u4e8e\u6545\u4e8b\u7c7b\u6570\u636e, \u800c\u975e\u65b0\u95fb\u7c7b. \u7b2c2\u70b9: Larger batch size (\u66f4\u5927\u7684batch size) BERT\u91c7\u7528\u7684batch size\u7b49\u4e8e256. RoBERTa\u7684\u8bad\u7ec3\u5728\u591a\u79cd\u6a21\u5f0f\u4e0b\u91c7\u7528\u4e86\u66f4\u5927\u7684batch size, \u4ece256\u4e00\u76f4\u5230\u6700\u5927\u76848000. \u7b2c3\u70b9: Training longer (\u66f4\u591a\u7684\u8bad\u7ec3\u6b65\u6570) RoBERTa\u7684\u8bad\u7ec3\u91c7\u7528\u4e86\u66f4\u591a\u7684\u8bad\u7ec3\u6b65\u6570, \u8ba9\u6a21\u578b\u5145\u5206\u5b66\u4e60\u6570\u636e\u4e2d\u7684\u7279\u5f81. \u7b2c4\u70b9: No NSP (\u53bb\u6389NSP\u4efb\u52a1) \u4ece2019\u5e74\u5f00\u59cb, \u5df2\u7ecf\u6709\u8d8a\u6765\u8d8a\u591a\u7684\u8bc1\u636e\u8868\u660eNSP\u4efb\u52a1\u5bf9\u4e8e\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u4e00\u4e2a\u8d1f\u9762\u4f5c\u7528, \u56e0\u6b64\u5728RoBERTa\u4e2d\u76f4\u63a5\u53d6\u6d88\u6389NSP\u4efb\u52a1. \u8bba\u6587\u4f5c\u8005\u8fdb\u884c\u4e86\u591a\u7ec4\u5bf9\u7167\u8bd5\u9a8c: 1: Segment + NSP (\u5373BERT\u6a21\u5f0f). \u8f93\u5165\u5305\u542b\u4e24\u90e8\u5206, \u6bcf\u4e2a\u90e8\u5206\u662f\u6765\u81ea\u540c\u4e00\u6587\u6863\u6216\u8005\u4e0d\u540c\u6587\u6863\u7684segment(segment\u662f\u8fde\u7eed\u7684\u591a\u4e2a\u53e5\u5b50), \u8fd9\u4e24\u4e2asegment\u7684token\u603b\u6570\u5c11\u4e8e512, \u9884\u8bad\u7ec3\u5305\u542bMLM\u4efb\u52a1\u548cNSP\u4efb\u52a1. 2: Sentence pair + NSP (\u4f7f\u7528\u4e24\u4e2a\u8fde\u7eed\u7684\u53e5\u5b50 + NSP, \u5e76\u91c7\u7528\u66f4\u5927\u7684batch size). \u8f93\u5165\u4e5f\u662f\u5305\u542b\u4e24\u90e8\u5206, \u6bcf\u4e2a\u90e8\u5206\u662f\u6765\u81ea\u540c\u4e00\u4e2a\u6587\u6863\u6216\u8005\u4e0d\u540c\u6587\u6863\u7684\u5355\u4e2a\u53e5\u5b50, \u8fd9\u4e24\u4e2a\u53e5\u5b50\u7684token \u603b\u6570\u5c11\u4e8e512. \u7531\u4e8e\u8fd9\u4e9b\u8f93\u5165\u660e\u663e\u5c11\u4e8e512\u4e2atokens, \u56e0\u6b64\u589e\u52a0batch size\u7684\u5927\u5c0f, \u4ee5\u4f7ftokens\u603b\u6570\u4fdd\u6301\u4e0eSEGMENT-PAIR + NSP\u76f8\u4f3c, \u9884\u8bad\u7ec3\u5305\u542bMLM\u4efb\u52a1\u548cNSP\u4efb\u52a1. 3: Full-sentences (\u5982\u679c\u8f93\u5165\u7684\u6700\u5927\u957f\u5ea6\u4e3a512, \u90a3\u4e48\u5c3d\u91cf\u9009\u62e9512\u957f\u5ea6\u7684\u8fde\u7eed\u53e5\u5b50; \u5982\u679c\u8de8\u8d8adocument, \u5c31\u5728\u4e2d\u95f4\u52a0\u4e0a\u4e00\u4e2a\u7279\u6b8a\u5206\u9694\u7b26, \u6bd4\u5982[SEP]; \u8be5\u8bd5\u9a8c\u6ca1\u6709NSP). \u8f93\u5165\u53ea\u6709\u4e00\u90e8\u5206(\u800c\u4e0d\u662f\u4e24\u90e8\u5206), \u6765\u81ea\u540c\u4e00\u4e2a\u6587\u6863\u6216\u8005\u4e0d\u540c\u6587\u6863\u7684\u8fde\u7eed\u591a\u4e2a\u53e5\u5b50, token\u603b\u6570\u4e0d\u8d85\u8fc7512. \u8f93\u5165\u53ef\u80fd\u8de8\u8d8a\u6587\u6863\u8fb9\u754c, \u5982\u679c\u8de8\u6587\u6863, \u5219\u5728\u4e0a\u4e00\u4e2a\u6587\u6863\u672b\u5c3e\u6dfb\u52a0\u6587\u6863\u8fb9\u754ctoken, \u9884\u8bad\u7ec3\u4e0d\u5305\u542bNSP\u4efb\u52a1. 4: Document-sentences (\u548c\u60c5\u51b53\u4e00\u6837, \u4f46\u662f\u6b65\u8de8\u8d8adocument; \u8be5\u5b9e\u9a8c\u6ca1\u6709NSP). \u8f93\u5165\u53ea\u6709\u4e00\u90e8\u5206(\u800c\u4e0d\u662f\u4e24\u90e8\u5206), \u8f93\u5165\u7684\u6784\u9020\u7c7b\u4f3c\u4e8eFull-sentences, \u53ea\u662f\u4e0d\u9700\u8981\u8de8\u8d8a\u6587\u6863\u8fb9\u754c, \u5176\u8f93\u5165\u6765\u81ea\u540c\u4e00\u4e2a\u6587\u6863\u7684\u8fde\u7eed\u53e5\u5b50, token\u603b\u6570\u4e0d\u8d85\u8fc7512. \u5728\u6587\u6863\u672b\u5c3e\u9644\u8fd1\u91c7\u6837\u7684\u8f93\u5165\u53ef\u4ee5\u77ed\u4e8e512\u4e2atokens, \u56e0\u6b64\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u52a8\u6001\u589e\u52a0batch size\u5927\u5c0f\u4ee5\u8fbe\u5230\u4e0eFull-sentecens\u76f8\u540c\u7684tokens\u603b\u6570, \u9884\u8bad\u7ec3\u4e0d\u5305\u542bNSP\u4efb\u52a1. \u603b\u7684\u6765\u8bf4, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e1 < 2 < 3 < 4. \u771f\u5b9e\u53e5\u5b50\u8fc7\u77ed\u7684\u8bdd, \u4e0d\u5982\u62fc\u63a5\u6210\u53e5\u5b50\u6bb5. \u6ca1\u6709NSP\u4efb\u52a1\u66f4\u4f18. \u4e0d\u8de8\u8d8adocument\u66f4\u4f18. \u7b2c5\u70b9: Dynamic masking (\u91c7\u7528\u52a8\u6001masking\u7b56\u7565) \u539f\u59cb\u9759\u6001mask: \u5373BERT\u7248\u672c\u7684mask\u7b56\u7565, \u51c6\u5907\u8bad\u7ec3\u6570\u636e\u65f6, \u6bcf\u4e2a\u6837\u672c\u53ea\u4f1a\u8fdb\u884c\u4e00\u6b21\u968f\u673amask(\u56e0\u6b64\u6bcf\u4e2aepoch\u90fd\u662f\u91cd\u590d\u7684), \u540e\u7eed\u7684\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u90fd\u91c7\u7528\u76f8\u540c\u7684mask\u65b9\u5f0f, \u8fd9\u662f\u539f\u59cb\u9759\u6001mask. \u52a8\u6001mask: \u5e76\u6ca1\u6709\u5728\u9884\u5904\u7406\u7684\u65f6\u5019\u6267\u884cmask, \u800c\u662f\u5728\u6bcf\u6b21\u5411\u6a21\u578b\u63d0\u4f9b\u8f93\u5165\u65f6\u52a8\u6001\u751f\u6210mask, \u6240\u4ee5\u5230\u5e95\u54ea\u4e9btokens\u88abmask\u6389\u4e86\u662f\u65f6\u523b\u53d8\u5316\u7684, \u65e0\u6cd5\u63d0\u524d\u9884\u77e5\u7684. \u7b2c6\u70b9: Byte level BPE (\u91c7\u7528\u5b57\u8282\u7ea7\u522b\u7684Encoding) \u57fa\u4e8echar-level: \u539f\u59cbBERT\u7684\u65b9\u5f0f, \u5728\u4e2d\u6587\u573a\u666f\u4e0b\u5c31\u662f\u5904\u7406\u4e00\u4e2a\u4e2a\u7684\u6c49\u5b57. \u57fa\u4e8ebytes-level: \u4e0echar-level\u7684\u533a\u522b\u5728\u4e8e\u7f16\u7801\u7684\u7c92\u5ea6\u662fbytes, \u800c\u4e0d\u662funicode\u5b57\u7b26\u4f5c\u4e3asub-word\u7684\u57fa\u672c\u5355\u4f4d. \u5f53\u91c7\u7528bytes-level\u7684BPE\u4e4b\u540e, \u8bcd\u8868\u5927\u5c0f\u4ece3\u4e07(\u539f\u59cbBERT\u7684char-level)\u589e\u52a0\u52305\u4e07. \u8fd9\u5206\u522b\u4e3aBERT-base\u548cBERT-large\u589e\u52a0\u4e861500\u4e07\u548c2000\u4e07\u989d\u5916\u7684\u53c2\u6570. \u4e4b\u524d\u6709\u7814\u7a76\u8868\u660e, \u8fd9\u6837\u7684\u505a\u6cd5\u5728\u6709\u4e9b\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4f1a\u5bfc\u81f4\u8f7b\u5fae\u7684\u6027\u80fd\u4e0b\u964d. \u4f46\u8bba\u6587\u4f5c\u8005\u76f8\u4fe1: \u8fd9\u79cd\u7edf\u4e00\u7f16\u7801\u7684\u4f18\u52bf\u4f1a\u8d85\u8fc7\u6027\u80fd\u7684\u8f7b\u5fae\u4e0b\u964d.","title":"2.2 RoBERTa\u6a21\u578b\u7684\u4f18\u5316\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-macbert","text":"","title":"3 MacBert\u6a21\u578b"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#31","text":"\u638c\u63e1MacBert\u6a21\u578b\u7684\u67b6\u6784. \u638c\u63e1MacBert\u6a21\u578b\u7684\u4f18\u5316\u70b9.","title":"3.1 \u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#32-macbert","text":"MacBert\u6a21\u578b\u7531\u54c8\u5de5\u5927NLP\u5b9e\u9a8c\u5ba4\u4e8e2020\u5e7411\u6708\u63d0\u51fa, 2021\u5e745\u6708\u53d1\u5e03\u5e94\u7528, \u662f\u9488\u5bf9\u4e8eBERT\u6a21\u578b\u505a\u4e86\u4f18\u5316\u6539\u826f\u540e\u7684\u9884\u8bad\u7ec3\u6a21\u578b. << Revisiting Pre-trained Models for Chinese Natural Language Processing >>, \u901a\u8fc7\u539f\u59cb\u8bba\u6587\u9898\u76ee\u4e5f\u53ef\u4ee5\u77e5\u9053, MacBert\u662f\u9488\u5bf9\u4e8e\u4e2d\u6587\u573a\u666f\u4e0b\u7684BERT\u4f18\u5316. MacBert\u6a21\u578b\u7684\u67b6\u6784\u548cBERT\u5927\u90e8\u5206\u4fdd\u6301\u4e00\u81f4, \u6700\u5927\u7684\u53d8\u5316\u6709\u4e24\u70b9: \u7b2c\u4e00\u70b9: \u5bf9\u4e8eMLM\u9884\u8bad\u7ec3\u4efb\u52a1, \u91c7\u7528\u4e86\u4e0d\u540c\u7684MASK\u7b56\u7565. \u7b2c\u4e8c\u70b9: \u5220\u9664\u4e86NSP\u4efb\u52a1, \u66ff\u6362\u6210SOP\u4efb\u52a1.","title":"3.2 MacBert\u6a21\u578b\u7684\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#33-macbert","text":"\u7b2c\u4e00\u70b9: \u5bf9\u4e8eMLM\u9884\u8bad\u7ec3\u4efb\u52a1, \u91c7\u7528\u4e86\u4e0d\u540c\u7684MASK\u7b56\u7565. 1: \u4f7f\u7528\u4e86\u5168\u8bcdmasked\u4ee5\u53can-gram masked\u7b56\u7565\u6765\u9009\u62e9tokens\u5982\u4f55\u88ab\u906e\u63a9, \u4ece\u5355\u4e2a\u5b57\u7b26\u52304\u4e2a\u5b57\u7b26\u7684\u906e\u63a9\u6bd4\u4f8b\u5206\u522b\u4e3a40%, 30%, 20%, 10% 2: \u539f\u59cbBERT\u6a21\u578b\u4e2d\u7684[MASK]\u51fa\u73b0\u5728\u8bad\u7ec3\u9636\u6bb5, \u4f46\u6ca1\u6709\u51fa\u73b0\u5728\u5fae\u8c03\u9636\u6bb5, \u8fd9\u4f1a\u9020\u6210exposure bias\u7684\u95ee\u9898. \u56e0\u6b64\u5728MacBert\u4e2d\u63d0\u51fa\u4f7f\u7528\u7c7b\u4f3c\u7684\u5355\u8bcd\u6765\u8fdb\u884cmasked. \u5177\u4f53\u6765\u8bf4, \u4f7f\u7528\u57fa\u4e8eWord2Vec\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5305\u8bad\u7ec3\u8bcd\u5411\u91cf, \u540e\u7eed\u5229\u7528\u8fd9\u91cc\u9762\u627e\u8fd1\u4e49\u8bcd\u7684\u529f\u80fd\u6765\u8f85\u52a9mask, \u6bd4\u5982\u4ee530%\u7684\u6982\u7387\u9009\u62e9\u4e86\u4e00\u4e2a3-gram\u7684\u5355\u8bcd\u8fdb\u884cmasked, \u5219\u5c06\u5728Word2Vec\u4e2d\u5bfb\u627e3-gram\u7684\u8fd1\u4e49\u8bcd\u6765\u66ff\u6362, \u5728\u6781\u5c11\u6570\u60c5\u51b5\u4e0b, \u5f53\u6ca1\u6709\u7b26\u5408\u6761\u4ef6\u7684\u76f8\u4f3c\u5355\u8bcd\u65f6, \u7b56\u7565\u4f1a\u8fdb\u884c\u964d\u7ea7, \u76f4\u63a5\u4f7f\u7528\u968f\u673a\u5355\u8bcd\u8fdb\u884c\u66ff\u6362. 3: \u4f7f\u752815%\u7684\u767e\u5206\u6bd4\u5bf9\u8f93\u5165\u5355\u8bcd\u8fdb\u884cMASK, \u5176\u4e2d80%\u7684\u6982\u7387\u4e0b\u6267\u884c\u7b56\u75652(\u5373\u66ff\u6362\u4e3a\u76f8\u4f3c\u5355\u8bcd), 10%\u7684\u6982\u7387\u4e0b\u66ff\u6362\u4e3a\u968f\u673a\u5355\u8bcd, 10%\u7684\u6982\u7387\u4e0b\u4fdd\u7559\u539f\u59cb\u5355\u8bcd\u4e0d\u53d8. \u7b2c\u4e8c\u70b9: \u5220\u9664\u4e86NSP\u4efb\u52a1, \u66ff\u6362\u6210SOP\u4efb\u52a1. \u7b2c\u4e8c\u70b9\u4f18\u5316\u662f\u76f4\u63a5\u501f\u9274\u4e86AlBERT\u6a21\u578b\u4e2d\u63d0\u51fa\u7684SOP\u4efb\u52a1. \u5728NLP\u8457\u540d\u7684\u96be\u4efb\u52a1\u9605\u8bfb\u7406\u89e3\u4e2d, MacBert\u5c55\u73b0\u51fa\u975e\u5e38\u4f18\u79c0\u7684\u8868\u73b0.","title":"3.3 MacBert\u6a21\u578b\u7684\u4f18\u5316\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4-spanbert","text":"","title":"4 SpanBERT\u6a21\u578b"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_4","text":"\u638c\u63e1SpanBERT\u6a21\u578b\u7684\u67b6\u6784. \u638c\u63e1SpanBERT\u6a21\u578b\u7684\u4f18\u5316\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#41-spanbert","text":"\u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\u67093\u70b9: 1: \u63d0\u51fa\u4e86\u66f4\u597d\u7684Span Mask\u65b9\u6848, \u518d\u6b21\u5c55\u793a\u4e86\u968f\u673a\u906e\u63a9\u8fde\u7eed\u4e00\u6bb5tokens\u6bd4\u968f\u673a\u906e\u63a9\u5355\u4e2atoken\u8981\u597d. 2: \u901a\u8fc7\u52a0\u5165\u4e86Span Boundary Objective(SBO)\u8bad\u7ec3\u4efb\u52a1, \u589e\u5f3a\u4e86BERT\u7684\u6027\u80fd, \u7279\u522b\u5728\u4e00\u4e9b\u548cSpan\u9002\u914d\u7684\u4efb\u52a1, \u5982\u62bd\u53d6\u5f0f\u95ee\u7b54. 3: \u7528\u5b9e\u9a8c\u6570\u636e\u83b7\u5f97\u4e86\u548cXLNet\u4e00\u81f4\u7684\u7ed3\u679c, \u53d1\u73b0\u53bb\u9664\u6389NSP\u4efb\u52a1, \u76f4\u63a5\u7528\u8fde\u7eed\u4e00\u957f\u53e5\u8bad\u7ec3\u6548\u679c\u66f4\u597d. SpanBERT\u7684\u67b6\u6784\u56fe\u5982\u4e0b: \u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u6e05\u6670\u7684\u5c55\u793a\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u70b9: Span Masking Span Boundary Objective","title":"4.1 SpanBERT\u6a21\u578b\u7684\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#42-span-masking","text":"\u5173\u4e8e\u521b\u65b0\u7684MASK\u673a\u5236, \u4e00\u822c\u6765\u8bf4\u90fd\u662f\u76f8\u5bf9\u4e8e\u539f\u59cbBERT\u7684\u57fa\u51c6\u8fdb\u884c\u6539\u8fdb. \u5bf9\u4e8eBERT, \u8bad\u7ec3\u65f6\u4f1a\u968f\u673a\u9009\u53d6\u6574\u53e5\u4e2d\u7684\u6700\u5c0f\u8f93\u5165\u5355\u5143token\u6765\u8fdb\u884c\u906e\u63a9, \u4e2d\u6587\u573a\u666f\u4e0b\u672c\u8d28\u4e0a\u5c31\u662f\u8fdb\u884c\u5b57\u7ea7\u522b\u7684MASK. \u4f46\u662f\u8fd9\u79cd\u65b9\u5f0f\u4f1a\u8ba9\u672c\u6765\u5e94\u8be5\u6709\u5f3a\u76f8\u5173\u7684\u4e00\u4e9b\u8fde\u5728\u4e00\u8d77\u7684\u5b57\u8bcd, \u5728\u8bad\u7ec3\u65f6\u88ab\u5272\u88c2\u5f00\u4e86. \u90a3\u4e48\u9996\u5148\u60f3\u5230\u7684\u505a\u6cd5: \u65e2\u7136\u80fd\u906e\u63a9\u5b57, \u90a3\u4e48\u80fd\u4e0d\u80fd\u76f4\u63a5\u906e\u63a9\u6574\u4e2a\u8bcd\u5462? \u8fd9\u5c31\u662fBERT-WWM\u6a21\u578b\u7684\u601d\u60f3. \u539f\u59cb\u8f93\u5165: \u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684\u6982\u7387. \u539f\u59cbBERT: \u4f7f\u7528\u8bed\u8a00[MASK]\u578b\u6765[MASK]\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684[MASK]\u7387. BERT-WWM: \u4f7f\u7528\u8bed\u8a00[MASK][MASK]\u6765[MASK][MASK]\u4e0b\u4e00\u4e2a\u8bcd\u7684[MASK][MASK]. \u5f15\u7533: \u767e\u5ea6\u8457\u540d\u7684ERNIE\u6a21\u578b\u4e2d, \u76f4\u63a5\u5f15\u5165\u547d\u540d\u5b9e\u4f53(Named Entity)\u7684\u5916\u90e8\u77e5\u8bc6, \u8fdb\u884c\u6574\u4e2a\u5b9e\u4f53\u7684\u906e\u63a9, \u8fdb\u884c\u8bad\u7ec3. \u7efc\u5408\u4e0a\u9762\u6240\u8bf4, \u4f1a\u66f4\u81ea\u7136\u7684\u60f3\u5230, \u65e2\u7136\u6574\u8bcd\u7684MASK, \u90a3\u4e48\u5982\u679c\u62e5\u6709\u8bcd\u7684\u8fb9\u754c\u4fe1\u606f\u4f1a\u4e0d\u4f1a\u8ba9\u6a21\u578b\u7684\u80fd\u529b\u66f4\u4e0a\u4e00\u5c42\u697c\u5462? SpanBERT\u7ed9\u51fa\u7684\u662f\u80af\u5b9a\u7684\u56de\u7b54!!! \u8bba\u6587\u4e2d\u5173\u4e8espan\u7684\u9009\u62e9, \u8d70\u4e86\u8fd9\u6837\u4e00\u4e2a\u6d41\u7a0b: \u7b2c\u4e00\u6b65: \u6839\u636e\u51e0\u4f55\u5206\u5e03, \u5148\u968f\u673a\u9009\u62e9\u4e00\u4e2a**span\u957f\u5ea6**. \u7b2c\u4e8c\u6b65: \u518d\u6839\u636e\u5747\u5300\u5206\u5e03\u968f\u673a\u9009\u62e9\u8fd9\u4e00\u6bb5\u7684**\u8d77\u59cb\u4f4d\u7f6e**. \u7b2c\u4e09\u6b65: \u6700\u540e\u6839\u636e\u524d\u4e24\u6b65\u7684start\u548clength\u76f4\u63a5\u8fdb\u884cMASK. \u7ed3\u8bba: \u8bba\u6587\u4e2d\u8be6\u7ec6\u8bba\u8bc1\u4e86\u6309\u7167\u4e0a\u8ff0\u7b97\u6cd5\u8fdb\u884cMASK, \u968f\u673a\u88ab\u906e\u63a9\u7684\u6587\u672c\u5e73\u5747\u957f\u5ea6\u7b49\u4e8e3.8","title":"4.2 Span Masking"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#43-span-boundary-objectivesbo","text":"SBO\u4efb\u52a1\u662f\u672c\u7bc7\u8bba\u6587\u6700\u6838\u5fc3\u7684\u521b\u65b0\u70b9, \u5e0c\u671b\u901a\u8fc7\u589e\u52a0\u8fd9\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1, \u53ef\u4ee5\u8ba9\u88ab\u906e\u63a9\u7684Span Boundary\u7684\u8bcd\u5411\u91cf, \u80fd\u591f\u5b66\u4e60\u5230Span\u5185\u90e8\u7684\u4fe1\u606f. \u5177\u4f53\u7684\u505a\u6cd5: \u5728\u8bad\u7ec3\u65f6\u53d6Span\u524d\u540e\u8fb9\u754c\u7684\u4e24\u4e2a\u8bcd, \u9700\u8981\u6ce8\u610f\u8fd9\u4e24\u4e2a\u8bcd\u4e0d\u5728Span\u5185, \u7136\u540e\u7528\u8fd9\u4e24\u4e2a\u8bcd\u5411\u91cf\u52a0\u4e0aSpan\u4e2d\u88abMASK\u6389\u7684\u8bcd\u7684\u4f4d\u7f6e\u5411\u91cf, \u6765\u9884\u6d4b\u539f\u8bcd. \u66f4\u8be6\u7ec6\u7684\u64cd\u4f5c\u5982\u4e0b, \u5373\u5c06\u8bcd\u5411\u91cf\u548c\u4f4d\u7f6e\u5411\u91cf\u8fdb\u884c\u62fc\u63a5, \u7ecf\u8fc7GeLU\u6fc0\u6d3b\u548cLayerNorm\u5904\u7406, \u8fde\u7eed\u7ecf\u8fc7\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42, \u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u5f20\u91cf: \u6700\u540e\u9884\u6d4bSpan\u4e2d\u539f\u8bcd\u7684\u65f6\u5019\u4f1a\u5f97\u5230\u4e00\u4e2a\u635f\u5931, \u8fd9\u5c31\u662fSBO\u4efb\u52a1\u7684\u635f\u5931; \u518d\u5c06\u5176\u548cBERT\u81ea\u8eab\u7684MLM\u4efb\u52a1\u7684\u635f\u5931\u8fdb\u884c\u52a0\u548c, \u5171\u540c\u4f5c\u4e3aSpanBERT\u7684\u76ee\u6807\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3:","title":"4.3 Span Boundary Objective(SBO)"},{"location":"06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#44-nsp","text":"\u4e3a\u4ec0\u4e48\u9009\u62e9Single Sentence\u800c\u4e0d\u662fBERT\u7684Two Sentence? 1: \u8bad\u7ec3\u6587\u672c\u7684\u957f\u5ea6\u66f4\u5927, \u53ef\u4ee5\u5b66\u4f1a\u957f\u7a0b\u4f9d\u8d56. 2: \u5bf9\u4e8eNSP\u7684\u8d1f\u6837\u672c, \u57fa\u4e8e\u53e6\u4e00\u4e2a\u4e3b\u9898\u6587\u6863\u7684\u53e5\u5b50\u6765\u9884\u6d4b\u5355\u8bcd, \u4f1a\u7ed9MLM\u4efb\u52a1\u5f15\u5165\u5f88\u5927\u7684\u566a\u58f0. 3: AlBERT\u6a21\u578b\u5df2\u7ecf\u7ed9\u51fa\u4e86\u8bba\u8bc1, \u56e0\u4e3aNSP\u4efb\u52a1\u592a\u7b80\u5355\u4e86.","title":"4.4 NSP\u4efb\u52a1\u53cd\u601d"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662fELMo. \u638c\u63e1ELMo\u7684\u67b6\u6784. \u638c\u63e1ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u4e86\u89e3ELMo\u7684\u6548\u679c\u548c\u6210\u7ee9. \u4e86\u89e3ELMo\u7684\u4f18\u7f3a\u70b9. 1 ELMo\u7b80\u4ecb \u00b6 ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. ELMo\u7684\u5168\u79f0\u662fEmbeddings from Language Models. ELMo\u6a21\u578b\u7684\u63d0\u51fa\u6e90\u4e8e\u8bba\u6587 << Deep Contextualized Word Representations >> . ELMo\u6a21\u578b\u63d0\u51fa\u7684\u52a8\u673a\u6e90\u4e8e\u7814\u7a76\u4eba\u5458\u8ba4\u4e3a\u4e00\u4e2a\u597d\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5e94\u8be5\u80fd\u591f\u5305\u542b\u4e30\u5bcc\u7684\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f, \u5e76\u4e14\u80fd\u591f\u5bf9\u591a\u4e49\u8bcd\u8fdb\u884c\u5efa\u6a21. \u800c\u4f20\u7edf\u7684\u8bcd\u5411\u91cf(2013\u5e74\u7684word2vec, 2014\u5e74\u7684GloVe)\u90fd\u662f\u4e0a\u4e0b\u6587\u65e0\u5173\u7684, \u4e5f\u5c31\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf. \u6700\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\"apple\"\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b, \u5e94\u8be5\u53ef\u4ee5\u8868\u793a\u6c34\u679c\u6216\u516c\u53f8, \u4f46\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf\u663e\u7136\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9. \u56e0\u6b64\u7814\u7a76\u56e2\u961f\u5229\u7528\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e00\u4e2a\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u9884\u8bad\u7ec3\u6a21\u578b, \u6210\u4e3aELMo, \u5e76\u57286\u4e2aNLP\u4efb\u52a1\u4e0a\u83b7\u5f97\u63d0\u5347. 2 ELMo\u7684\u67b6\u6784 \u00b6 2.1 \u603b\u4f53\u67b6\u6784 \u00b6 \u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aELMo\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684\u4e24\u90e8\u5206\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757. 2.2 Embedding\u6a21\u5757 \u00b6 ELMo\u6700\u5e95\u5c42\u7684\u8bcd\u5d4c\u5165\u91c7\u7528CNN\u5bf9\u5b57\u7b26\u7ea7\u8fdb\u884c\u7f16\u7801, \u672c\u8d28\u5c31\u662f\u83b7\u5f97\u4e00\u4e2a\u9759\u6001\u7684\u8bcd\u5d4c\u5165\u5411\u91cf\u4f5c\u4e3a\u7f51\u7edc\u7684\u5e95\u5c42\u8f93\u5165. 2.3 \u4e24\u90e8\u5206\u7684\u53cc\u5c42LSTM\u6a21\u5757 \u00b6 \u8fd9\u662f\u6574\u4e2aELMo\u4e2d\u6700\u91cd\u8981\u7684\u90e8\u5206, \u67b6\u6784\u4e2d\u5206\u6210\u5de6\u4fa7\u7684\u524d\u5411LSTM\u7f51\u7edc, \u548c\u53f3\u4fa7\u7684\u53cd\u5411LSTM\u7f51\u7edc. ELMo\u7684\u505a\u6cd5\u662f\u6211\u4eec\u53ea\u9884\u8bad\u7ec3\u4e00\u4e2aLanguage Model, \u800cword embedding\u662f\u901a\u8fc7\u8f93\u5165\u7684\u53e5\u5b50\u5b9e\u65f6\u7ed9\u51fa\u7684, \u8fd9\u6837\u5355\u8bcd\u7684\u5d4c\u5165\u5411\u91cf\u5c31\u5305\u542b\u4e86\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f, \u4e5f\u5c31\u5f7b\u5e95\u6539\u53d8\u4e86Word2Vec\u548cGloVe\u7684\u9759\u6001\u8bcd\u5411\u91cf\u7684\u505a\u6cd5. ELMo\u7684\u8fd9\u4e00\u6a21\u5757\u5206\u4e3a\u5de6\u53f3\u4e24\u90e8\u5206, \u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u53cc\u5411LM, \u5bf9\u4e8e\u5de6\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t1, t2, ..., tN), Language Model\u901a\u8fc7\u524d\u9762k-1\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u524d\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) \u540c\u7406, \u5bf9\u4e8e\u67b6\u6784\u4e2d\u7684\u53f3\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t(k+1), t(k+2), ..., t(N)), Language Model\u901a\u8fc7\u540e\u9762N-k\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u540e\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) ELMo\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u76ee\u6807\u51fd\u6570\u5c31\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u516c\u5f0f: \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s)) \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s)) 2.4 \u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757 \u00b6 \u56e0\u4e3aELMo\u662f\u4e2a\u8bed\u8a00\u6a21\u578b, \u5bf9\u4e8e\u6bcf\u4e2atoken, \u901a\u8fc7\u4e00\u4e2aL\u5c42\u7684\u53cc\u5411LSTM\u7f51\u7edc\u53ef\u4ee5\u8ba1\u7b97\u51fa2L+1\u4e2a\u8868\u793a\u5411\u91cf\u5982\u4e0b: R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} \u4ece\u4e0a\u9762\u7684\u516c\u5f0f\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230, \u67093\u4e2a\u4e0d\u540c\u7684\u7ec4\u6210\u90e8\u5206, \u7b2c\u4e00\u4e2a\u5c31\u662f\u5bf9token\u76f4\u63a5\u8fdb\u884cCNN\u7f16\u7801\u7684\u7ed3\u679c, \u4e5f\u662fELMo\u6700\u5e95\u5c42\u6a21\u5757\u7684\u8f93\u51fa; \u7b2c\u4e8c\u4e2a\u5c31\u662f\u524d\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7b2c\u4e09\u4e2a\u5c31\u662f\u540e\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7efc\u5408\u4e09\u90e8\u5206\u7684\u8f93\u51fa\u52a0\u5728\u4e00\u8d77, \u5c31\u662f2L+1\u4e2a\u8f93\u51fa\u5411\u91cf. \u901a\u8fc7\u6574\u4e2a\u7f51\u7edc, \u6bcf\u4e00\u4e2atoken\u5f97\u5230\u4e862L+1\u4e2a\u8868\u793a\u5411\u91cf, \u4f46\u662f\u6211\u4eec\u5e0c\u671b\u6bcf\u4e00\u4e2atoken\u80fd\u5bf9\u5e94\u4e00\u4e2a\u5411\u91cf. \u6700\u7b80\u5355\u7684\u505a\u6cd5\u5c31\u662f\u53d6\u6700\u4e0a\u5c42\u7684\u8f93\u51fa\u7ed3\u679c\u4f5c\u4e3atoken\u7684\u8868\u793a\u5411\u91cf, \u66f4\u901a\u7528\u7684\u505a\u6cd5\u662f\u52a0\u5165\u82e5\u5e72\u53c2\u6570\u6765\u878d\u5408\u6240\u6709\u5c42\u7684\u4fe1\u606f, \u5982\u4e0b\u6240\u793a: ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} \u4e0a\u5f0f\u7684\u610f\u601d\u662f\u5bf9\u4e8e2L+1\u4e2a\u5411\u91cf, \u6bcf\u4e00\u4e2a\u524d\u9762\u90fd\u52a0\u4e0a\u4e00\u4e2a\u6743\u91cd\u7a00\u758f, \u7136\u540e\u76f4\u63a5\u878d\u5408\u6210\u4e00\u4e2a\u5411\u91cf, \u6700\u540e\u518d\u4e58\u4e00\u4e2a\u7cfb\u6570\u4f5c\u4e3a\u6700\u7ec8\u8be5token\u7684\u8bcd\u5411\u91cf. \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u6700\u524d\u9762\u7684\u90a3\u4e2a\u7cfb\u6570, \u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u53d6\u4e0d\u540c\u7684\u503c\u6548\u679c\u4f1a\u6709\u8f83\u5927\u7684\u5dee\u5f02, \u9700\u8981\u6ce8\u610f\u5728SQuAD\u4e2d\u8bbe\u7f6e\u4e3a0.01\u53d6\u5f97\u7684\u6548\u679c\u8981\u597d\u4e8e\u8bbe\u7f6e\u4e3a1. \u539f\u59cb\u8bba\u6587\u4e2d\u5728\u8fdb\u884c\u5e95\u5c42token\u7f16\u7801\u65f6, \u7528CNN\u5f62\u6210\u4e86\u4e00\u4e2a512\u7ef4\u7684\u5217\u5411\u91cf, \u4e5f\u5c31\u662f\u521d\u59cb\u5d4c\u5165\u7ef4\u5ea6\u7b49\u4e8e512. \u4e2d\u95f4\u5c42\u4f7f\u7528\u4e86\u53cc\u5c42\u7684LSTM\u5206\u522b\u8fdb\u884c\u524d\u5411\u7f16\u7801\u548c\u540e\u5411\u7f16\u7801, \u6bcf\u5c42\u7684\u5355\u4e2aLSTM\u8f93\u5165\u7ef4\u5ea6\u662f512, \u8f93\u51fa\u7ef4\u5ea6\u4e5f\u662f512, \u4fdd\u6301\u4e00\u81f4. \u56e0\u4e3a\u662f\u53cc\u5411\u7f16\u7801\u5e76\u4e14\u5206\u5de6\u53f3\u4e24\u90e8\u5206, \u6240\u4ee5\u6bcf\u5c42\u7684\u8f93\u51fa\u7ef4\u5ea6\u662f512*2=1024, \u6700\u540e\u8fdb\u884c\u6743\u91cd\u878d\u5408\u540e\u7684\u5411\u91cf\u7ef4\u5ea6\u5c31\u662f1024. 3 ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u00b6 3.1 ELMo\u7684\u672c\u8d28\u601d\u60f3 \u00b6 \u9996\u5148\u7528\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5b66\u597d\u4e00\u4e2a\u5355\u8bcd\u7684word embedding, \u6b64\u65f6\u662f\u65e0\u6cd5\u533a\u5206\u591a\u4e49\u8bcd\u7684, \u4f46\u6ca1\u5173\u7cfb. \u5f53\u5b9e\u9645\u4f7f\u7528word embedding\u7684\u65f6\u5019, \u8be5\u5355\u8bcd\u5df2\u7ecf\u5177\u5907\u4e86\u7279\u5b9a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f, \u8fd9\u4e2a\u65f6\u5019\u53ef\u4ee5\u6839\u636e\u4e0a\u4e0b\u6587\u5355\u8bcd\u7684\u8bed\u4e49\u53bb\u8c03\u6574\u5355\u8bcd\u7684word embedding\u8868\u793a, \u8fd9\u6837\u7ecf\u8fc7\u8c03\u6574\u540e\u5f97\u5230\u7684word embedding\u5411\u91cf\u5c31\u53ef\u4ee5\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u5728\u5f53\u524d\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u5b9e\u542b\u4e49\u4e86, \u4e5f\u5c31\u81ea\u7136\u7684\u89e3\u51b3\u4e86\u591a\u4e49\u8bcd\u95ee\u9898. \u7ed3\u8bba\u5c31\u662fELMo\u6a21\u578b\u662f\u4e2a\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b. 3.2 ELMo\u7684\u9884\u8bad\u7ec3\u91c7\u7528\u4e86\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b \u00b6 \u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u505a\u4e0b\u6e38\u4efb\u52a1\u65f6, \u4ece\u9884\u8bad\u7ec3\u7f51\u7edc\u4e2d\u63d0\u53d6\u5bf9\u5e94\u5355\u8bcd\u7684\u7f51\u7edc\u5404\u5c42\u7684word embedding\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u5230\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u7b2c\u4e00\u9636\u6bb5: \u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3. \u518d\u6b21\u56de\u5230ELMo\u7684\u603b\u4f53\u67b6\u6784\u56fe, \u7f51\u7edc\u7ed3\u6784\u91c7\u7528\u4e86\u53cc\u5c42\u53cc\u5411LSTM. \u76ee\u524d\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u4efb\u52a1\u76ee\u6807\u662f\u6839\u636e\u5355\u8bcdWi\u7684\u4e0a\u4e0b\u6587\u53bb\u6b63\u786e\u9884\u6d4b\u5355\u8bcdWi, Wi\u4e4b\u524d\u7684\u5355\u8bcd\u5e8f\u5217context-before\u79f0\u4e3a\u4e0a\u6587, Wi\u4e4b\u540e\u7684\u5355\u8bcd\u5e8f\u5217context-after\u79f0\u4e3a\u4e0b\u6587. \u67b6\u6784\u56fe\u4e0a\u5de6\u4fa7\u7684\u524d\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u6b63\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u5de6\u5411\u53f3\u987a\u5e8f\u7684\u9664\u4e86\u9884\u6d4b\u5355\u8bcdWi\u4e4b\u5916\u7684\u4e0a\u6587context-before; \u53f3\u4fa7\u7684\u53cd\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u53cd\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u53f3\u5411\u5de6\u7684\u9006\u5e8f\u7684\u4e0b\u6587context-after; \u6bcf\u4e2a\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u90fd\u662fL=2, \u5373\u53cc\u5c42LSTM\u53e0\u52a0. \u4f7f\u7528\u4e0a\u8ff0\u7684\u7f51\u7edc\u7ed3\u6784\u5229\u7528\u5927\u91cf\u8bed\u6599\u505a\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u5c31\u80fd\u9884\u8bad\u7ec3\u597d\u8fd9\u4e2a\u7f51\u7edc. \u5f53\u8f93\u5165\u4e00\u4e2a\u65b0\u53e5\u5b50S_new\u65f6, \u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u90fd\u80fd\u5f97\u5230\u5bf9\u5e94\u76843\u4e2aembedding\u5411\u91cf: 1-\u6700\u5e95\u5c42\u7684\u5355\u8bcd\u7684word embedding. 2-\u4e2d\u95f4\u7b2c\u4e00\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u53e5\u6cd5\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. 3-\u4e2d\u95f4\u7b2c\u4e8c\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u8bed\u4e49\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. ELMo\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u4ec5\u4ec5\u5b66\u4f1a\u4e86\u5355\u8bcd\u7684word embedding, \u8fd8\u5b66\u4e60\u4e86\u4e00\u4e2a\u53cc\u5c42\u53cc\u5411\u7684LSTM\u7f51\u7edc, \u8fd9\u4e24\u8005\u540e\u7eed\u90fd\u4f1a\u7528\u5230, \u662f\u6574\u4e2aELMo\u9884\u8bad\u7ec3\u7684\u4e24\u5927\u4ea7\u51fa\u7ed3\u679c. \u7b2c\u4e8c\u9636\u6bb5: \u4e0b\u6e38\u4efb\u52a1\u7684\u8c03\u6574. \u6bd4\u5982\u6211\u4eec\u7684\u4e0b\u6e38\u4efb\u52a1\u662fQA\u95ee\u9898. \u5bf9\u4e8e\u95ee\u53e5X, \u53ef\u4ee5\u5148\u5c06\u53e5\u5b50X\u4f5c\u4e3a\u9884\u8bad\u7ec3\u597d\u7684ELMo\u7f51\u7edc\u7684\u8f93\u5165, \u8fd9\u6837X\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5728ELMo\u4e2d\u90fd\u80fd\u83b7\u5f973\u4e2a\u5bf9\u5e94\u7684embedding\u5411\u91cf. \u4e4b\u540e\u8d4b\u7ed9\u8fd93\u4e2a\u5411\u91cf\u5404\u81ea\u4e00\u4e2a\u6743\u91cda, \u8fd9\u4e2a\u6743\u91cda\u65e2\u53ef\u4ee5\u662f\u5b66\u4e60\u5f97\u6765\u7684\u4e5f\u53ef\u4ee5\u662f\u6700\u7b80\u5355\u7684\u5e73\u5747\u5206\u5e03\u8d4b\u503c, \u7136\u540e\u628a3\u4e2a\u5411\u91cf\u52a0\u6743\u6c42\u548c, \u6574\u4e2a\u6210\u4e00\u4e2a\u8bcd\u5411\u91cf. \u6700\u540e\u5c06\u6574\u5408\u540e\u7684\u8bcd\u5411\u91cf\u4f5c\u4e3aX\u5728\u81ea\u5df1\u4efb\u52a1\u7684\u90a3\u4e2a\u7f51\u7edc\u7ed3\u6784\u4e2d\u5bf9\u5e94\u5355\u8bcd\u7684\u8f93\u5165, \u4ee5\u6b64\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u8fdb\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u5bf9\u4e8e\u56de\u7b54Y\u53ef\u4ee5\u540c\u6837\u5904\u7406. \u56e0\u4e3aELMo\u7ed9\u4e0b\u6e38\u63d0\u4f9b\u7684\u662f\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u5f62\u5f0f, \u6240\u4ee5\u8fd9\u4e00\u7c7b\u9884\u8bad\u7ec3\u65b9\u6cd5\u88ab\u79f0\u4e3a\"Feature-based Pre-Training\". 4 ELMo\u6a21\u578b\u7684\u6548\u679c \u00b6 ELMo\u5bf9\u4e8e\u591a\u4e49\u8bcd\u95ee\u9898\u7684\u89e3\u51b3\u7ed3\u679c: \u524d\u9762\u63d0\u5230\u9759\u6001\u7684word embedding\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898, \u90a3\u4e48ELMo\u5f15\u5165\u4e0a\u4e0b\u6587\u52a8\u6001\u8bed\u4e49\u8c03\u6574\u540e\u7684embedding word\u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u95ee\u9898\u5417? \u7b54\u6848\u6b63\u5982\u4e0a\u56fe\u6240\u793a, \u800c\u4e14\u6bd4\u6211\u4eec\u671f\u5f85\u7684\u89e3\u51b3\u6548\u679c\u8981\u66f4\u597d. \u4e0a\u56fe\u4e2d\u7684\u4f8b\u5b50, \u5bf9\u4e8eGloVe\u8bad\u7ec3\u51fa\u6765\u7684word embedding\u6765\u8bf4, \u591a\u4e49\u8bcd\u6bd4\u5982play, \u6839\u636e\u5b83\u7684embedding\u627e\u51fa\u6700\u63a5\u8fd1\u5176\u8bed\u4e49\u7684\u5355\u8bcd, \u53d1\u73b0\u7ed3\u679c\u96c6\u5408\u51e0\u4e4e\u5168\u90e8\u90fd\u5728\u4f53\u80b2\u9886\u57df, \u8fd9\u5f88\u660e\u663e\u662f\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u4e2d\u5305\u542bplay\u7684\u8bed\u53e5\u4e2d\u4f53\u80b2\u9886\u57df\u7684\u6570\u91cf\u660e\u663e\u5360\u591a\u6570\u5bfc\u81f4\u7684. \u518d\u6765\u770b\u4f7f\u7528ELMo\u540e\u7684\u6548\u679c, \u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u540e\u7684embedding word\u4e0d\u4ec5\u4ec5\u80fd\u627e\u51fa\u5bf9\u5e94\u4e8e\"play\":\"\u6f14\u51fa\"\u7684\u76f8\u540c\u8bed\u4e49\u7684\u53e5\u5b50, \u800c\u4e14\u8fd8\u53ef\u4ee5\u4fdd\u8bc1\u627e\u51fa\u7684\u53e5\u5b50\u4e2d\u7684play\u5bf9\u5e94\u7684\u8bcd\u6027\u4e5f\u662f\u76f8\u540c\u7684, \u8fd9\u771f\u7684\u662f\u8d85\u51fa\u671f\u5f85\u4e4b\u5916\u7684\u60ca\u559c! \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230ELMo\u7684\u8bd5\u9a8c\u6548\u679c, \u57286\u4e2aNLP\u4e3b\u6d41\u4efb\u52a1\u4e2d\u6027\u80fd\u90fd\u6709\u4e0d\u540c\u5e45\u5ea6\u7684\u63d0\u5347, \u6700\u9ad8\u7684\u63d0\u5347\u8fbe\u523025%, \u4efb\u52a1\u7684\u8986\u76d6\u8303\u56f4\u5f88\u5e7f, \u5305\u542b\u53e5\u5b50\u8bed\u4e49\u5173\u7cfb\u5224\u65ad, \u5206\u7c7b\u4efb\u52a1, \u9605\u8bfb\u7406\u89e3\u7b49\u7b49. 5 ELMo\u7684\u5f85\u6539\u8fdb\u70b9 \u00b6 ELMo\u5728\u4f20\u7edf\u9759\u6001word embedding\u65b9\u6cd5(Word2Vec, GloVe)\u7684\u57fa\u7840\u4e0a\u63d0\u5347\u4e86\u5f88\u591a, \u4f46\u662f\u4f9d\u7136\u5b58\u5728\u7f3a\u9677, \u6709\u5f88\u5927\u7684\u6539\u8fdb\u4f59\u5730. \u7b2c\u4e00\u70b9: \u4e00\u4e2a\u5f88\u660e\u663e\u7684\u7f3a\u70b9\u5728\u4e8e\u7279\u5f81\u63d0\u53d6\u5668\u7684\u9009\u62e9\u4e0a, ELMo\u4f7f\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM, \u800c\u4e0d\u662f\u73b0\u5728\u6a2a\u626b\u5343\u519b\u7684Transformer, \u5728\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0a\u80af\u5b9a\u662f\u8981\u5f31\u4e00\u4e9b\u7684. \u8bbe\u60f3\u5982\u679cELMo\u7684\u63d0\u5347\u63d0\u53d6\u5668\u9009\u7528Transformer, \u90a3\u4e48\u540e\u6765\u7684BERT\u7684\u53cd\u54cd\u5c06\u8fdc\u4e0d\u5982\u5f53\u65f6\u90a3\u4e48\u706b\u7206\u4e86. \u7b2c\u4e8c\u70b9: ELMo\u9009\u7528\u53cc\u5411\u62fc\u63a5\u7684\u65b9\u5f0f\u8fdb\u884c\u7279\u5f81\u878d\u5408, \u8fd9\u79cd\u65b9\u6cd5\u80af\u5b9a\u4e0d\u5982BERT\u4e00\u4f53\u5316\u7684\u53cc\u5411\u63d0\u53d6\u7279\u5f81\u597d. 6 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fELMo. ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. ELMo\u57286\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u6709\u5f88\u5927\u7684\u63d0\u5347\u8868\u73b0. \u5b66\u4e60\u4e86ELMo\u7684\u7ed3\u6784. ELMo\u67b6\u6784\u603b\u4f53\u4e0a\u91c7\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u7684\u53cc\u5411\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757. \u5b66\u4e60\u4e86ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. ELMo\u7684\u672c\u8d28\u601d\u60f3\u5c31\u662f\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u8fdb\u884c\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b. ELMo\u7684\u9884\u8bad\u7ec3\u662f\u4e00\u4e2a\u660e\u663e\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b. \u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3, \u5f97\u5230\u57fa\u7840\u9759\u6001\u8bcd\u5411\u91cf\u548c\u53cc\u5411\u53cc\u5c42LSTM\u7f51\u7edc. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u62e5\u6709\u4e0a\u4e0b\u6587\u7684\u73af\u5883\u4e2d, \u5c06\u4e0a\u4e0b\u6587\u8f93\u5165\u53cc\u5411\u53cc\u5c42LSTM\u4e2d, \u5f97\u5230\u52a8\u6001\u8c03\u6574\u540e\u7684word embedding, \u7b49\u4e8e\u5c06\u5355\u8bcd\u878d\u5408\u8fdb\u4e86\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u53ef\u4ee5\u66f4\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u7684\u771f\u5b9e\u542b\u4e49. \u5b66\u4e60\u4e86ELMo\u7684\u6548\u679c. \u7ecf\u8fc7\u4e0eGloVe\u9759\u6001\u8bcd\u5411\u91cf\u7684\u5bf9\u6bd4, \u660e\u663e\u53ef\u4ee5\u770b\u51faELMo\u7684\u8bcd\u5411\u91cf\u53ef\u4ee5\u66f4\u597d\u7684\u8868\u8fbe\u771f\u5b9e\u8bed\u4e49, \u66f4\u597d\u7684\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u5b66\u4e60\u4e86ELMo\u7684\u5f85\u6539\u8fdb\u70b9. ELMo\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6ca1\u6709\u9009\u7528\u66f4\u5f3a\u5927\u7684Transformer, \u5728\u63d0\u53d6\u7279\u5f81\u4e0a\u80af\u5b9a\u5f31\u4e8e\u73b0\u5728\u7684\u6700\u4f18\u7ed3\u679c.","title":"4 ELMo\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fELMo. \u638c\u63e1ELMo\u7684\u67b6\u6784. \u638c\u63e1ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u4e86\u89e3ELMo\u7684\u6548\u679c\u548c\u6210\u7ee9. \u4e86\u89e3ELMo\u7684\u4f18\u7f3a\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-elmo","text":"ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. ELMo\u7684\u5168\u79f0\u662fEmbeddings from Language Models. ELMo\u6a21\u578b\u7684\u63d0\u51fa\u6e90\u4e8e\u8bba\u6587 << Deep Contextualized Word Representations >> . ELMo\u6a21\u578b\u63d0\u51fa\u7684\u52a8\u673a\u6e90\u4e8e\u7814\u7a76\u4eba\u5458\u8ba4\u4e3a\u4e00\u4e2a\u597d\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5e94\u8be5\u80fd\u591f\u5305\u542b\u4e30\u5bcc\u7684\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f, \u5e76\u4e14\u80fd\u591f\u5bf9\u591a\u4e49\u8bcd\u8fdb\u884c\u5efa\u6a21. \u800c\u4f20\u7edf\u7684\u8bcd\u5411\u91cf(2013\u5e74\u7684word2vec, 2014\u5e74\u7684GloVe)\u90fd\u662f\u4e0a\u4e0b\u6587\u65e0\u5173\u7684, \u4e5f\u5c31\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf. \u6700\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\"apple\"\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b, \u5e94\u8be5\u53ef\u4ee5\u8868\u793a\u6c34\u679c\u6216\u516c\u53f8, \u4f46\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf\u663e\u7136\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9. \u56e0\u6b64\u7814\u7a76\u56e2\u961f\u5229\u7528\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e00\u4e2a\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u9884\u8bad\u7ec3\u6a21\u578b, \u6210\u4e3aELMo, \u5e76\u57286\u4e2aNLP\u4efb\u52a1\u4e0a\u83b7\u5f97\u63d0\u5347.","title":"1 ELMo\u7b80\u4ecb"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-elmo","text":"","title":"2 ELMo\u7684\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#21","text":"\u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aELMo\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684\u4e24\u90e8\u5206\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757.","title":"2.1 \u603b\u4f53\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#22-embedding","text":"ELMo\u6700\u5e95\u5c42\u7684\u8bcd\u5d4c\u5165\u91c7\u7528CNN\u5bf9\u5b57\u7b26\u7ea7\u8fdb\u884c\u7f16\u7801, \u672c\u8d28\u5c31\u662f\u83b7\u5f97\u4e00\u4e2a\u9759\u6001\u7684\u8bcd\u5d4c\u5165\u5411\u91cf\u4f5c\u4e3a\u7f51\u7edc\u7684\u5e95\u5c42\u8f93\u5165.","title":"2.2 Embedding\u6a21\u5757"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#23-lstm","text":"\u8fd9\u662f\u6574\u4e2aELMo\u4e2d\u6700\u91cd\u8981\u7684\u90e8\u5206, \u67b6\u6784\u4e2d\u5206\u6210\u5de6\u4fa7\u7684\u524d\u5411LSTM\u7f51\u7edc, \u548c\u53f3\u4fa7\u7684\u53cd\u5411LSTM\u7f51\u7edc. ELMo\u7684\u505a\u6cd5\u662f\u6211\u4eec\u53ea\u9884\u8bad\u7ec3\u4e00\u4e2aLanguage Model, \u800cword embedding\u662f\u901a\u8fc7\u8f93\u5165\u7684\u53e5\u5b50\u5b9e\u65f6\u7ed9\u51fa\u7684, \u8fd9\u6837\u5355\u8bcd\u7684\u5d4c\u5165\u5411\u91cf\u5c31\u5305\u542b\u4e86\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f, \u4e5f\u5c31\u5f7b\u5e95\u6539\u53d8\u4e86Word2Vec\u548cGloVe\u7684\u9759\u6001\u8bcd\u5411\u91cf\u7684\u505a\u6cd5. ELMo\u7684\u8fd9\u4e00\u6a21\u5757\u5206\u4e3a\u5de6\u53f3\u4e24\u90e8\u5206, \u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u53cc\u5411LM, \u5bf9\u4e8e\u5de6\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t1, t2, ..., tN), Language Model\u901a\u8fc7\u524d\u9762k-1\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u524d\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) \u540c\u7406, \u5bf9\u4e8e\u67b6\u6784\u4e2d\u7684\u53f3\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t(k+1), t(k+2), ..., t(N)), Language Model\u901a\u8fc7\u540e\u9762N-k\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u540e\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) ELMo\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u76ee\u6807\u51fd\u6570\u5c31\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u516c\u5f0f: \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s)) \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s))","title":"2.3 \u4e24\u90e8\u5206\u7684\u53cc\u5c42LSTM\u6a21\u5757"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#24","text":"\u56e0\u4e3aELMo\u662f\u4e2a\u8bed\u8a00\u6a21\u578b, \u5bf9\u4e8e\u6bcf\u4e2atoken, \u901a\u8fc7\u4e00\u4e2aL\u5c42\u7684\u53cc\u5411LSTM\u7f51\u7edc\u53ef\u4ee5\u8ba1\u7b97\u51fa2L+1\u4e2a\u8868\u793a\u5411\u91cf\u5982\u4e0b: R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} \u4ece\u4e0a\u9762\u7684\u516c\u5f0f\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230, \u67093\u4e2a\u4e0d\u540c\u7684\u7ec4\u6210\u90e8\u5206, \u7b2c\u4e00\u4e2a\u5c31\u662f\u5bf9token\u76f4\u63a5\u8fdb\u884cCNN\u7f16\u7801\u7684\u7ed3\u679c, \u4e5f\u662fELMo\u6700\u5e95\u5c42\u6a21\u5757\u7684\u8f93\u51fa; \u7b2c\u4e8c\u4e2a\u5c31\u662f\u524d\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7b2c\u4e09\u4e2a\u5c31\u662f\u540e\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7efc\u5408\u4e09\u90e8\u5206\u7684\u8f93\u51fa\u52a0\u5728\u4e00\u8d77, \u5c31\u662f2L+1\u4e2a\u8f93\u51fa\u5411\u91cf. \u901a\u8fc7\u6574\u4e2a\u7f51\u7edc, \u6bcf\u4e00\u4e2atoken\u5f97\u5230\u4e862L+1\u4e2a\u8868\u793a\u5411\u91cf, \u4f46\u662f\u6211\u4eec\u5e0c\u671b\u6bcf\u4e00\u4e2atoken\u80fd\u5bf9\u5e94\u4e00\u4e2a\u5411\u91cf. \u6700\u7b80\u5355\u7684\u505a\u6cd5\u5c31\u662f\u53d6\u6700\u4e0a\u5c42\u7684\u8f93\u51fa\u7ed3\u679c\u4f5c\u4e3atoken\u7684\u8868\u793a\u5411\u91cf, \u66f4\u901a\u7528\u7684\u505a\u6cd5\u662f\u52a0\u5165\u82e5\u5e72\u53c2\u6570\u6765\u878d\u5408\u6240\u6709\u5c42\u7684\u4fe1\u606f, \u5982\u4e0b\u6240\u793a: ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} \u4e0a\u5f0f\u7684\u610f\u601d\u662f\u5bf9\u4e8e2L+1\u4e2a\u5411\u91cf, \u6bcf\u4e00\u4e2a\u524d\u9762\u90fd\u52a0\u4e0a\u4e00\u4e2a\u6743\u91cd\u7a00\u758f, \u7136\u540e\u76f4\u63a5\u878d\u5408\u6210\u4e00\u4e2a\u5411\u91cf, \u6700\u540e\u518d\u4e58\u4e00\u4e2a\u7cfb\u6570\u4f5c\u4e3a\u6700\u7ec8\u8be5token\u7684\u8bcd\u5411\u91cf. \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u6700\u524d\u9762\u7684\u90a3\u4e2a\u7cfb\u6570, \u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u53d6\u4e0d\u540c\u7684\u503c\u6548\u679c\u4f1a\u6709\u8f83\u5927\u7684\u5dee\u5f02, \u9700\u8981\u6ce8\u610f\u5728SQuAD\u4e2d\u8bbe\u7f6e\u4e3a0.01\u53d6\u5f97\u7684\u6548\u679c\u8981\u597d\u4e8e\u8bbe\u7f6e\u4e3a1. \u539f\u59cb\u8bba\u6587\u4e2d\u5728\u8fdb\u884c\u5e95\u5c42token\u7f16\u7801\u65f6, \u7528CNN\u5f62\u6210\u4e86\u4e00\u4e2a512\u7ef4\u7684\u5217\u5411\u91cf, \u4e5f\u5c31\u662f\u521d\u59cb\u5d4c\u5165\u7ef4\u5ea6\u7b49\u4e8e512. \u4e2d\u95f4\u5c42\u4f7f\u7528\u4e86\u53cc\u5c42\u7684LSTM\u5206\u522b\u8fdb\u884c\u524d\u5411\u7f16\u7801\u548c\u540e\u5411\u7f16\u7801, \u6bcf\u5c42\u7684\u5355\u4e2aLSTM\u8f93\u5165\u7ef4\u5ea6\u662f512, \u8f93\u51fa\u7ef4\u5ea6\u4e5f\u662f512, \u4fdd\u6301\u4e00\u81f4. \u56e0\u4e3a\u662f\u53cc\u5411\u7f16\u7801\u5e76\u4e14\u5206\u5de6\u53f3\u4e24\u90e8\u5206, \u6240\u4ee5\u6bcf\u5c42\u7684\u8f93\u51fa\u7ef4\u5ea6\u662f512*2=1024, \u6700\u540e\u8fdb\u884c\u6743\u91cd\u878d\u5408\u540e\u7684\u5411\u91cf\u7ef4\u5ea6\u5c31\u662f1024.","title":"2.4 \u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-elmo","text":"","title":"3 ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#31-elmo","text":"\u9996\u5148\u7528\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5b66\u597d\u4e00\u4e2a\u5355\u8bcd\u7684word embedding, \u6b64\u65f6\u662f\u65e0\u6cd5\u533a\u5206\u591a\u4e49\u8bcd\u7684, \u4f46\u6ca1\u5173\u7cfb. \u5f53\u5b9e\u9645\u4f7f\u7528word embedding\u7684\u65f6\u5019, \u8be5\u5355\u8bcd\u5df2\u7ecf\u5177\u5907\u4e86\u7279\u5b9a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f, \u8fd9\u4e2a\u65f6\u5019\u53ef\u4ee5\u6839\u636e\u4e0a\u4e0b\u6587\u5355\u8bcd\u7684\u8bed\u4e49\u53bb\u8c03\u6574\u5355\u8bcd\u7684word embedding\u8868\u793a, \u8fd9\u6837\u7ecf\u8fc7\u8c03\u6574\u540e\u5f97\u5230\u7684word embedding\u5411\u91cf\u5c31\u53ef\u4ee5\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u5728\u5f53\u524d\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u5b9e\u542b\u4e49\u4e86, \u4e5f\u5c31\u81ea\u7136\u7684\u89e3\u51b3\u4e86\u591a\u4e49\u8bcd\u95ee\u9898. \u7ed3\u8bba\u5c31\u662fELMo\u6a21\u578b\u662f\u4e2a\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b.","title":"3.1 ELMo\u7684\u672c\u8d28\u601d\u60f3"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#32-elmo","text":"\u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u505a\u4e0b\u6e38\u4efb\u52a1\u65f6, \u4ece\u9884\u8bad\u7ec3\u7f51\u7edc\u4e2d\u63d0\u53d6\u5bf9\u5e94\u5355\u8bcd\u7684\u7f51\u7edc\u5404\u5c42\u7684word embedding\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u5230\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u7b2c\u4e00\u9636\u6bb5: \u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3. \u518d\u6b21\u56de\u5230ELMo\u7684\u603b\u4f53\u67b6\u6784\u56fe, \u7f51\u7edc\u7ed3\u6784\u91c7\u7528\u4e86\u53cc\u5c42\u53cc\u5411LSTM. \u76ee\u524d\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u4efb\u52a1\u76ee\u6807\u662f\u6839\u636e\u5355\u8bcdWi\u7684\u4e0a\u4e0b\u6587\u53bb\u6b63\u786e\u9884\u6d4b\u5355\u8bcdWi, Wi\u4e4b\u524d\u7684\u5355\u8bcd\u5e8f\u5217context-before\u79f0\u4e3a\u4e0a\u6587, Wi\u4e4b\u540e\u7684\u5355\u8bcd\u5e8f\u5217context-after\u79f0\u4e3a\u4e0b\u6587. \u67b6\u6784\u56fe\u4e0a\u5de6\u4fa7\u7684\u524d\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u6b63\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u5de6\u5411\u53f3\u987a\u5e8f\u7684\u9664\u4e86\u9884\u6d4b\u5355\u8bcdWi\u4e4b\u5916\u7684\u4e0a\u6587context-before; \u53f3\u4fa7\u7684\u53cd\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u53cd\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u53f3\u5411\u5de6\u7684\u9006\u5e8f\u7684\u4e0b\u6587context-after; \u6bcf\u4e2a\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u90fd\u662fL=2, \u5373\u53cc\u5c42LSTM\u53e0\u52a0. \u4f7f\u7528\u4e0a\u8ff0\u7684\u7f51\u7edc\u7ed3\u6784\u5229\u7528\u5927\u91cf\u8bed\u6599\u505a\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u5c31\u80fd\u9884\u8bad\u7ec3\u597d\u8fd9\u4e2a\u7f51\u7edc. \u5f53\u8f93\u5165\u4e00\u4e2a\u65b0\u53e5\u5b50S_new\u65f6, \u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u90fd\u80fd\u5f97\u5230\u5bf9\u5e94\u76843\u4e2aembedding\u5411\u91cf: 1-\u6700\u5e95\u5c42\u7684\u5355\u8bcd\u7684word embedding. 2-\u4e2d\u95f4\u7b2c\u4e00\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u53e5\u6cd5\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. 3-\u4e2d\u95f4\u7b2c\u4e8c\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u8bed\u4e49\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. ELMo\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u4ec5\u4ec5\u5b66\u4f1a\u4e86\u5355\u8bcd\u7684word embedding, \u8fd8\u5b66\u4e60\u4e86\u4e00\u4e2a\u53cc\u5c42\u53cc\u5411\u7684LSTM\u7f51\u7edc, \u8fd9\u4e24\u8005\u540e\u7eed\u90fd\u4f1a\u7528\u5230, \u662f\u6574\u4e2aELMo\u9884\u8bad\u7ec3\u7684\u4e24\u5927\u4ea7\u51fa\u7ed3\u679c. \u7b2c\u4e8c\u9636\u6bb5: \u4e0b\u6e38\u4efb\u52a1\u7684\u8c03\u6574. \u6bd4\u5982\u6211\u4eec\u7684\u4e0b\u6e38\u4efb\u52a1\u662fQA\u95ee\u9898. \u5bf9\u4e8e\u95ee\u53e5X, \u53ef\u4ee5\u5148\u5c06\u53e5\u5b50X\u4f5c\u4e3a\u9884\u8bad\u7ec3\u597d\u7684ELMo\u7f51\u7edc\u7684\u8f93\u5165, \u8fd9\u6837X\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5728ELMo\u4e2d\u90fd\u80fd\u83b7\u5f973\u4e2a\u5bf9\u5e94\u7684embedding\u5411\u91cf. \u4e4b\u540e\u8d4b\u7ed9\u8fd93\u4e2a\u5411\u91cf\u5404\u81ea\u4e00\u4e2a\u6743\u91cda, \u8fd9\u4e2a\u6743\u91cda\u65e2\u53ef\u4ee5\u662f\u5b66\u4e60\u5f97\u6765\u7684\u4e5f\u53ef\u4ee5\u662f\u6700\u7b80\u5355\u7684\u5e73\u5747\u5206\u5e03\u8d4b\u503c, \u7136\u540e\u628a3\u4e2a\u5411\u91cf\u52a0\u6743\u6c42\u548c, \u6574\u4e2a\u6210\u4e00\u4e2a\u8bcd\u5411\u91cf. \u6700\u540e\u5c06\u6574\u5408\u540e\u7684\u8bcd\u5411\u91cf\u4f5c\u4e3aX\u5728\u81ea\u5df1\u4efb\u52a1\u7684\u90a3\u4e2a\u7f51\u7edc\u7ed3\u6784\u4e2d\u5bf9\u5e94\u5355\u8bcd\u7684\u8f93\u5165, \u4ee5\u6b64\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u8fdb\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u5bf9\u4e8e\u56de\u7b54Y\u53ef\u4ee5\u540c\u6837\u5904\u7406. \u56e0\u4e3aELMo\u7ed9\u4e0b\u6e38\u63d0\u4f9b\u7684\u662f\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u5f62\u5f0f, \u6240\u4ee5\u8fd9\u4e00\u7c7b\u9884\u8bad\u7ec3\u65b9\u6cd5\u88ab\u79f0\u4e3a\"Feature-based Pre-Training\".","title":"3.2 ELMo\u7684\u9884\u8bad\u7ec3\u91c7\u7528\u4e86\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4-elmo","text":"ELMo\u5bf9\u4e8e\u591a\u4e49\u8bcd\u95ee\u9898\u7684\u89e3\u51b3\u7ed3\u679c: \u524d\u9762\u63d0\u5230\u9759\u6001\u7684word embedding\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898, \u90a3\u4e48ELMo\u5f15\u5165\u4e0a\u4e0b\u6587\u52a8\u6001\u8bed\u4e49\u8c03\u6574\u540e\u7684embedding word\u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u95ee\u9898\u5417? \u7b54\u6848\u6b63\u5982\u4e0a\u56fe\u6240\u793a, \u800c\u4e14\u6bd4\u6211\u4eec\u671f\u5f85\u7684\u89e3\u51b3\u6548\u679c\u8981\u66f4\u597d. \u4e0a\u56fe\u4e2d\u7684\u4f8b\u5b50, \u5bf9\u4e8eGloVe\u8bad\u7ec3\u51fa\u6765\u7684word embedding\u6765\u8bf4, \u591a\u4e49\u8bcd\u6bd4\u5982play, \u6839\u636e\u5b83\u7684embedding\u627e\u51fa\u6700\u63a5\u8fd1\u5176\u8bed\u4e49\u7684\u5355\u8bcd, \u53d1\u73b0\u7ed3\u679c\u96c6\u5408\u51e0\u4e4e\u5168\u90e8\u90fd\u5728\u4f53\u80b2\u9886\u57df, \u8fd9\u5f88\u660e\u663e\u662f\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u4e2d\u5305\u542bplay\u7684\u8bed\u53e5\u4e2d\u4f53\u80b2\u9886\u57df\u7684\u6570\u91cf\u660e\u663e\u5360\u591a\u6570\u5bfc\u81f4\u7684. \u518d\u6765\u770b\u4f7f\u7528ELMo\u540e\u7684\u6548\u679c, \u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u540e\u7684embedding word\u4e0d\u4ec5\u4ec5\u80fd\u627e\u51fa\u5bf9\u5e94\u4e8e\"play\":\"\u6f14\u51fa\"\u7684\u76f8\u540c\u8bed\u4e49\u7684\u53e5\u5b50, \u800c\u4e14\u8fd8\u53ef\u4ee5\u4fdd\u8bc1\u627e\u51fa\u7684\u53e5\u5b50\u4e2d\u7684play\u5bf9\u5e94\u7684\u8bcd\u6027\u4e5f\u662f\u76f8\u540c\u7684, \u8fd9\u771f\u7684\u662f\u8d85\u51fa\u671f\u5f85\u4e4b\u5916\u7684\u60ca\u559c! \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230ELMo\u7684\u8bd5\u9a8c\u6548\u679c, \u57286\u4e2aNLP\u4e3b\u6d41\u4efb\u52a1\u4e2d\u6027\u80fd\u90fd\u6709\u4e0d\u540c\u5e45\u5ea6\u7684\u63d0\u5347, \u6700\u9ad8\u7684\u63d0\u5347\u8fbe\u523025%, \u4efb\u52a1\u7684\u8986\u76d6\u8303\u56f4\u5f88\u5e7f, \u5305\u542b\u53e5\u5b50\u8bed\u4e49\u5173\u7cfb\u5224\u65ad, \u5206\u7c7b\u4efb\u52a1, \u9605\u8bfb\u7406\u89e3\u7b49\u7b49.","title":"4 ELMo\u6a21\u578b\u7684\u6548\u679c"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#5-elmo","text":"ELMo\u5728\u4f20\u7edf\u9759\u6001word embedding\u65b9\u6cd5(Word2Vec, GloVe)\u7684\u57fa\u7840\u4e0a\u63d0\u5347\u4e86\u5f88\u591a, \u4f46\u662f\u4f9d\u7136\u5b58\u5728\u7f3a\u9677, \u6709\u5f88\u5927\u7684\u6539\u8fdb\u4f59\u5730. \u7b2c\u4e00\u70b9: \u4e00\u4e2a\u5f88\u660e\u663e\u7684\u7f3a\u70b9\u5728\u4e8e\u7279\u5f81\u63d0\u53d6\u5668\u7684\u9009\u62e9\u4e0a, ELMo\u4f7f\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM, \u800c\u4e0d\u662f\u73b0\u5728\u6a2a\u626b\u5343\u519b\u7684Transformer, \u5728\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0a\u80af\u5b9a\u662f\u8981\u5f31\u4e00\u4e9b\u7684. \u8bbe\u60f3\u5982\u679cELMo\u7684\u63d0\u5347\u63d0\u53d6\u5668\u9009\u7528Transformer, \u90a3\u4e48\u540e\u6765\u7684BERT\u7684\u53cd\u54cd\u5c06\u8fdc\u4e0d\u5982\u5f53\u65f6\u90a3\u4e48\u706b\u7206\u4e86. \u7b2c\u4e8c\u70b9: ELMo\u9009\u7528\u53cc\u5411\u62fc\u63a5\u7684\u65b9\u5f0f\u8fdb\u884c\u7279\u5f81\u878d\u5408, \u8fd9\u79cd\u65b9\u6cd5\u80af\u5b9a\u4e0d\u5982BERT\u4e00\u4f53\u5316\u7684\u53cc\u5411\u63d0\u53d6\u7279\u5f81\u597d.","title":"5 ELMo\u7684\u5f85\u6539\u8fdb\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#6","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fELMo. ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. ELMo\u57286\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u6709\u5f88\u5927\u7684\u63d0\u5347\u8868\u73b0. \u5b66\u4e60\u4e86ELMo\u7684\u7ed3\u6784. ELMo\u67b6\u6784\u603b\u4f53\u4e0a\u91c7\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u7684\u53cc\u5411\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757. \u5b66\u4e60\u4e86ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. ELMo\u7684\u672c\u8d28\u601d\u60f3\u5c31\u662f\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u8fdb\u884c\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b. ELMo\u7684\u9884\u8bad\u7ec3\u662f\u4e00\u4e2a\u660e\u663e\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b. \u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3, \u5f97\u5230\u57fa\u7840\u9759\u6001\u8bcd\u5411\u91cf\u548c\u53cc\u5411\u53cc\u5c42LSTM\u7f51\u7edc. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u62e5\u6709\u4e0a\u4e0b\u6587\u7684\u73af\u5883\u4e2d, \u5c06\u4e0a\u4e0b\u6587\u8f93\u5165\u53cc\u5411\u53cc\u5c42LSTM\u4e2d, \u5f97\u5230\u52a8\u6001\u8c03\u6574\u540e\u7684word embedding, \u7b49\u4e8e\u5c06\u5355\u8bcd\u878d\u5408\u8fdb\u4e86\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u53ef\u4ee5\u66f4\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u7684\u771f\u5b9e\u542b\u4e49. \u5b66\u4e60\u4e86ELMo\u7684\u6548\u679c. \u7ecf\u8fc7\u4e0eGloVe\u9759\u6001\u8bcd\u5411\u91cf\u7684\u5bf9\u6bd4, \u660e\u663e\u53ef\u4ee5\u770b\u51faELMo\u7684\u8bcd\u5411\u91cf\u53ef\u4ee5\u66f4\u597d\u7684\u8868\u8fbe\u771f\u5b9e\u8bed\u4e49, \u66f4\u597d\u7684\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u5b66\u4e60\u4e86ELMo\u7684\u5f85\u6539\u8fdb\u70b9. ELMo\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6ca1\u6709\u9009\u7528\u66f4\u5f3a\u5927\u7684Transformer, \u5728\u63d0\u53d6\u7279\u5f81\u4e0a\u80af\u5b9a\u5f31\u4e8e\u73b0\u5728\u7684\u6700\u4f18\u7ed3\u679c.","title":"6 \u5c0f\u7ed3"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662fGPT. \u638c\u63e1GPT\u7684\u67b6\u6784. \u638c\u63e1GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. 1 GPT\u4ecb\u7ecd \u00b6 GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. OpenAI\u5728\u8bba\u6587 << Improving Language Understanding by Generative Pre-Training >> \u4e2d\u63d0\u51faGPT\u6a21\u578b. OpenAI\u540e\u7eed\u53c8\u5728\u8bba\u6587 << Language Models are Unsupervised Multitask Learners >> \u4e2d\u63d0\u51faGPT2\u6a21\u578b. GPT\u548cGPT2\u6a21\u578b\u7ed3\u6784\u5dee\u522b\u4e0d\u5927, \u4f46\u662fGPT2\u91c7\u7528\u4e86\u66f4\u5927\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3. OpenAI GPT\u6a21\u578b\u662f\u5728Google BERT\u6a21\u578b\u4e4b\u524d\u63d0\u51fa\u7684, \u4e0eBERT\u6700\u5927\u7684\u533a\u522b\u5728\u4e8eGPT\u91c7\u7528\u4e86\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3, \u5373\u4f7f\u7528\u5355\u8bcd\u7684\u4e0a\u6587\u6765\u9884\u6d4b\u5355\u8bcd, \u800cBERT\u662f\u91c7\u7528\u4e86\u53cc\u5411\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f\u5171\u540c\u6765\u9884\u6d4b\u5355\u8bcd. \u6b63\u662f\u56e0\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u4e0a\u7684\u533a\u522b, \u4f7f\u5f97GPT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1(NLG), \u800cBERT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1(NLU). 2 GPT\u7684\u67b6\u6784 \u00b6 \u770b\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6bd4\u67b6\u6784\u56fe, \u4e2d\u95f4\u7684\u5c31\u662fGPT: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230GPT\u91c7\u7528\u7684\u662f\u5355\u5411Transformer\u6a21\u578b, \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50[u1, u2, ..., un], GPT\u5728\u9884\u6d4b\u5355\u8bcdui\u7684\u65f6\u5019\u53ea\u4f1a\u5229\u7528[u1, u2, ..., u(i-1)]\u7684\u4fe1\u606f, \u800cBERT\u4f1a\u540c\u65f6\u5229\u7528\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f[u1, u2, ..., u(i-1), u(i+1), ..., un]. \u4f5c\u4e3a\u4e24\u5927\u6a21\u578b\u7684\u76f4\u63a5\u5bf9\u6bd4, BERT\u91c7\u7528\u4e86Transformer\u7684Encoder\u6a21\u5757, \u800cGPT\u91c7\u7528\u4e86Transformer\u7684Decoder\u6a21\u5757. \u5e76\u4e14GPT\u7684Decoder Block\u548c\u7ecf\u5178Transformer Decoder Block\u8fd8\u6709\u6240\u4e0d\u540c, \u5982\u4e0b\u56fe\u6240\u793a: \u5982\u4e0a\u56fe\u6240\u793a, \u7ecf\u5178\u7684Transformer Decoder Block\u5305\u542b3\u4e2a\u5b50\u5c42, \u5206\u522b\u662fMasked Multi-Head Attention\u5c42, encoder-decoder attention\u5c42, \u4ee5\u53caFeed Forward\u5c42. \u4f46\u662f\u5728GPT\u4e2d\u53d6\u6d88\u4e86\u7b2c\u4e8c\u4e2aencoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5c42, \u548cFeed Forward\u5c42. \u4f5c\u4e3a\u5355\u5411Transformer Decoder\u6a21\u578b, GPT\u5229\u7528\u53e5\u5b50\u5e8f\u5217\u4fe1\u606f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u65f6\u5019, \u8981\u4f7f\u7528Masked Multi-Head Attention\u5bf9\u5355\u8bcd\u7684\u4e0b\u6587\u8fdb\u884c\u906e\u63a9(look ahead mask), \u6765\u9632\u6b62\u672a\u6765\u4fe1\u606f\u7684\u63d0\u524d\u6cc4\u9732. \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50\u5305\u542b4\u4e2a\u5355\u8bcd[A, B, C, D], GPT\u9700\u8981\u7528[A]\u9884\u6d4bB, \u7528[A, B]\u9884\u6d4bC, \u7528[A, B, C]\u9884\u6d4bD. \u5f88\u663e\u7136\u7684\u5c31\u662f\u5f53\u8981\u9884\u6d4bB\u65f6, \u9700\u8981\u5c06[B, C, D]\u906e\u63a9\u8d77\u6765. \u5177\u4f53\u7684\u906e\u63a9\u64cd\u4f5c\u662f\u5728slef-attention\u8fdb\u884csoftmax\u4e4b\u524d\u8fdb\u884c\u7684, \u4e00\u822c\u7684\u5b9e\u73b0\u662f\u5c06MASK\u7684\u4f4d\u7f6e\u7528\u4e00\u4e2a\u65e0\u7a77\u5c0f\u7684\u6570\u503c-inf\u6765\u66ff\u6362, \u66ff\u6362\u540e\u6267\u884csoftmax\u8ba1\u7b97\u5f97\u5230\u65b0\u7684\u7ed3\u679c\u77e9\u9635. \u8fd9\u6837-inf\u7684\u4f4d\u7f6e\u5c31\u53d8\u6210\u4e860. \u5982\u4e0a\u56fe\u6240\u793a, \u6700\u540e\u7684\u77e9\u9635\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u505a\u5230\u5f53\u5229\u7528A\u9884\u6d4bB\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A\u7684\u4fe1\u606f; \u5f53\u5229\u7528[A, B]\u9884\u6d4bC\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A, B\u7684\u4fe1\u606f. \u6ce8\u610f: \u5bf9\u6bd4\u4e8e\u7ecf\u5178\u7684Transformer\u67b6\u6784, \u89e3\u7801\u5668\u6a21\u5757\u91c7\u7528\u4e866\u4e2aDecoder Block; GPT\u7684\u67b6\u6784\u4e2d\u91c7\u7528\u4e8612\u4e2aDecoder Block. 3 GPT\u8bad\u7ec3\u8fc7\u7a0b \u00b6 GPT\u7684\u8bad\u7ec3\u4e5f\u662f\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b: \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning. 3.1 \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b \u00b6 \u7ed9\u5b9a\u53e5\u5b50U = [u1, u2, ..., un], GPT\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u4f3c\u7136\u51fd\u6570: $$ L_1(U)=\\sum_i\\log P(u_i|u_{i-k},\\cdots,u_{i-1};\\Theta) $$ \u6709\u4e0a\u8ff0\u516c\u5f0f\u53ef\u77e5, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5047\u8bbe\u8f93\u5165\u5f20\u91cf\u7528h0\u8868\u793a, \u5219\u8ba1\u7b97\u516c\u5f0f\u5982\u4e0b: $$ h_0 = UW_e + W_p $$ \u5176\u4e2dWp\u662f\u5355\u8bcd\u7684\u4f4d\u7f6e\u7f16\u7801, We\u662f\u5355\u8bcd\u672c\u8eab\u7684word embedding. Wp\u7684\u5f62\u72b6\u662f[max_seq_len, embedding_dim], We\u7684\u5f62\u72b6\u662f[vocab_size, embedding_dim]. \u5f97\u5230\u8f93\u5165\u5f20\u91cfh0\u540e, \u8981\u5c06h0\u4f20\u5165GPT\u7684Decoder Block\u4e2d, \u4f9d\u6b21\u5f97\u5230ht: $$ h_t = transformer\\_block(h_{l-1})\\;\\;\\;\\;l\\in[1,t] $$ \u6700\u540e\u901a\u8fc7\u5f97\u5230\u7684ht\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd: $$ P(u)=softmax(h_tW_e^T) $$ 3.2 \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning \u00b6 GPT\u7ecf\u8fc7\u9884\u8bad\u7ec3\u540e, \u4f1a\u9488\u5bf9\u5177\u4f53\u7684\u4e0b\u6e38\u4efb\u52a1\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03. \u5fae\u8c03\u91c7\u7528\u7684\u662f\u6709\u76d1\u7763\u5b66\u4e60, \u8bad\u7ec3\u6837\u672c\u5305\u62ec\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u548clabel y. GPT\u5fae\u8c03\u7684\u76ee\u6807\u4efb\u52a1\u662f\u6839\u636e\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u9884\u6d4b\u6807\u7b7ey. P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) \u5176\u4e2d W_y W_y \u8868\u793a\u9884\u6d4b\u8f93\u51fa\u7684\u77e9\u9635\u53c2\u6570, \u5fae\u8c03\u4efb\u52a1\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u51fd\u6570: L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) \u7efc\u5408\u4e24\u4e2a\u9636\u6bb5\u7684\u76ee\u6807\u4efb\u52a1\u51fd\u6570, \u53ef\u77e5GPT\u7684\u6700\u7ec8\u4f18\u5316\u51fd\u6570\u4e3a: L_3 = L_2 + \\lambda L_1 L_3 = L_2 + \\lambda L_1 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fGPT. GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u672c\u8d28\u4e0a\u6765\u8bf4, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b. \u5b66\u4e60\u4e86GPT\u7684\u67b6\u6784. GPT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684\u89e3\u7801\u5668\u6a21\u5757. GPT\u5728\u4f7f\u7528\u89e3\u7801\u5668\u6a21\u5757\u65f6\u505a\u4e86\u4e00\u5b9a\u7684\u6539\u9020, \u5c06\u4f20\u7edf\u76843\u5c42Decoder Block\u53d8\u6210\u4e862\u5c42Block, \u5220\u9664\u4e86encoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5b50\u5c42\u548cFeed Forward\u5b50\u5c42. GPT\u7684\u89e3\u7801\u5668\u603b\u5171\u662f\u753112\u4e2a\u6539\u9020\u540e\u7684Decoder Block\u7ec4\u6210\u7684. \u5b66\u4e60\u4e86GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u53ea\u5229\u7528\u5355\u8bcd\u524d\u9762\u7684\u4fe1\u606f\u6765\u9884\u6d4b\u5f53\u524d\u5355\u8bcd. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning.","title":"5 GPT\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fGPT. \u638c\u63e1GPT\u7684\u67b6\u6784. \u638c\u63e1GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-gpt","text":"GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. OpenAI\u5728\u8bba\u6587 << Improving Language Understanding by Generative Pre-Training >> \u4e2d\u63d0\u51faGPT\u6a21\u578b. OpenAI\u540e\u7eed\u53c8\u5728\u8bba\u6587 << Language Models are Unsupervised Multitask Learners >> \u4e2d\u63d0\u51faGPT2\u6a21\u578b. GPT\u548cGPT2\u6a21\u578b\u7ed3\u6784\u5dee\u522b\u4e0d\u5927, \u4f46\u662fGPT2\u91c7\u7528\u4e86\u66f4\u5927\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3. OpenAI GPT\u6a21\u578b\u662f\u5728Google BERT\u6a21\u578b\u4e4b\u524d\u63d0\u51fa\u7684, \u4e0eBERT\u6700\u5927\u7684\u533a\u522b\u5728\u4e8eGPT\u91c7\u7528\u4e86\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3, \u5373\u4f7f\u7528\u5355\u8bcd\u7684\u4e0a\u6587\u6765\u9884\u6d4b\u5355\u8bcd, \u800cBERT\u662f\u91c7\u7528\u4e86\u53cc\u5411\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f\u5171\u540c\u6765\u9884\u6d4b\u5355\u8bcd. \u6b63\u662f\u56e0\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u4e0a\u7684\u533a\u522b, \u4f7f\u5f97GPT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1(NLG), \u800cBERT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1(NLU).","title":"1 GPT\u4ecb\u7ecd"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-gpt","text":"\u770b\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6bd4\u67b6\u6784\u56fe, \u4e2d\u95f4\u7684\u5c31\u662fGPT: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230GPT\u91c7\u7528\u7684\u662f\u5355\u5411Transformer\u6a21\u578b, \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50[u1, u2, ..., un], GPT\u5728\u9884\u6d4b\u5355\u8bcdui\u7684\u65f6\u5019\u53ea\u4f1a\u5229\u7528[u1, u2, ..., u(i-1)]\u7684\u4fe1\u606f, \u800cBERT\u4f1a\u540c\u65f6\u5229\u7528\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f[u1, u2, ..., u(i-1), u(i+1), ..., un]. \u4f5c\u4e3a\u4e24\u5927\u6a21\u578b\u7684\u76f4\u63a5\u5bf9\u6bd4, BERT\u91c7\u7528\u4e86Transformer\u7684Encoder\u6a21\u5757, \u800cGPT\u91c7\u7528\u4e86Transformer\u7684Decoder\u6a21\u5757. \u5e76\u4e14GPT\u7684Decoder Block\u548c\u7ecf\u5178Transformer Decoder Block\u8fd8\u6709\u6240\u4e0d\u540c, \u5982\u4e0b\u56fe\u6240\u793a: \u5982\u4e0a\u56fe\u6240\u793a, \u7ecf\u5178\u7684Transformer Decoder Block\u5305\u542b3\u4e2a\u5b50\u5c42, \u5206\u522b\u662fMasked Multi-Head Attention\u5c42, encoder-decoder attention\u5c42, \u4ee5\u53caFeed Forward\u5c42. \u4f46\u662f\u5728GPT\u4e2d\u53d6\u6d88\u4e86\u7b2c\u4e8c\u4e2aencoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5c42, \u548cFeed Forward\u5c42. \u4f5c\u4e3a\u5355\u5411Transformer Decoder\u6a21\u578b, GPT\u5229\u7528\u53e5\u5b50\u5e8f\u5217\u4fe1\u606f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u65f6\u5019, \u8981\u4f7f\u7528Masked Multi-Head Attention\u5bf9\u5355\u8bcd\u7684\u4e0b\u6587\u8fdb\u884c\u906e\u63a9(look ahead mask), \u6765\u9632\u6b62\u672a\u6765\u4fe1\u606f\u7684\u63d0\u524d\u6cc4\u9732. \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50\u5305\u542b4\u4e2a\u5355\u8bcd[A, B, C, D], GPT\u9700\u8981\u7528[A]\u9884\u6d4bB, \u7528[A, B]\u9884\u6d4bC, \u7528[A, B, C]\u9884\u6d4bD. \u5f88\u663e\u7136\u7684\u5c31\u662f\u5f53\u8981\u9884\u6d4bB\u65f6, \u9700\u8981\u5c06[B, C, D]\u906e\u63a9\u8d77\u6765. \u5177\u4f53\u7684\u906e\u63a9\u64cd\u4f5c\u662f\u5728slef-attention\u8fdb\u884csoftmax\u4e4b\u524d\u8fdb\u884c\u7684, \u4e00\u822c\u7684\u5b9e\u73b0\u662f\u5c06MASK\u7684\u4f4d\u7f6e\u7528\u4e00\u4e2a\u65e0\u7a77\u5c0f\u7684\u6570\u503c-inf\u6765\u66ff\u6362, \u66ff\u6362\u540e\u6267\u884csoftmax\u8ba1\u7b97\u5f97\u5230\u65b0\u7684\u7ed3\u679c\u77e9\u9635. \u8fd9\u6837-inf\u7684\u4f4d\u7f6e\u5c31\u53d8\u6210\u4e860. \u5982\u4e0a\u56fe\u6240\u793a, \u6700\u540e\u7684\u77e9\u9635\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u505a\u5230\u5f53\u5229\u7528A\u9884\u6d4bB\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A\u7684\u4fe1\u606f; \u5f53\u5229\u7528[A, B]\u9884\u6d4bC\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A, B\u7684\u4fe1\u606f. \u6ce8\u610f: \u5bf9\u6bd4\u4e8e\u7ecf\u5178\u7684Transformer\u67b6\u6784, \u89e3\u7801\u5668\u6a21\u5757\u91c7\u7528\u4e866\u4e2aDecoder Block; GPT\u7684\u67b6\u6784\u4e2d\u91c7\u7528\u4e8612\u4e2aDecoder Block.","title":"2 GPT\u7684\u67b6\u6784"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-gpt","text":"GPT\u7684\u8bad\u7ec3\u4e5f\u662f\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b: \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning.","title":"3 GPT\u8bad\u7ec3\u8fc7\u7a0b"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#31","text":"\u7ed9\u5b9a\u53e5\u5b50U = [u1, u2, ..., un], GPT\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u4f3c\u7136\u51fd\u6570: $$ L_1(U)=\\sum_i\\log P(u_i|u_{i-k},\\cdots,u_{i-1};\\Theta) $$ \u6709\u4e0a\u8ff0\u516c\u5f0f\u53ef\u77e5, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5047\u8bbe\u8f93\u5165\u5f20\u91cf\u7528h0\u8868\u793a, \u5219\u8ba1\u7b97\u516c\u5f0f\u5982\u4e0b: $$ h_0 = UW_e + W_p $$ \u5176\u4e2dWp\u662f\u5355\u8bcd\u7684\u4f4d\u7f6e\u7f16\u7801, We\u662f\u5355\u8bcd\u672c\u8eab\u7684word embedding. Wp\u7684\u5f62\u72b6\u662f[max_seq_len, embedding_dim], We\u7684\u5f62\u72b6\u662f[vocab_size, embedding_dim]. \u5f97\u5230\u8f93\u5165\u5f20\u91cfh0\u540e, \u8981\u5c06h0\u4f20\u5165GPT\u7684Decoder Block\u4e2d, \u4f9d\u6b21\u5f97\u5230ht: $$ h_t = transformer\\_block(h_{l-1})\\;\\;\\;\\;l\\in[1,t] $$ \u6700\u540e\u901a\u8fc7\u5f97\u5230\u7684ht\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd: $$ P(u)=softmax(h_tW_e^T) $$","title":"3.1 \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#32-fine-tunning","text":"GPT\u7ecf\u8fc7\u9884\u8bad\u7ec3\u540e, \u4f1a\u9488\u5bf9\u5177\u4f53\u7684\u4e0b\u6e38\u4efb\u52a1\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03. \u5fae\u8c03\u91c7\u7528\u7684\u662f\u6709\u76d1\u7763\u5b66\u4e60, \u8bad\u7ec3\u6837\u672c\u5305\u62ec\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u548clabel y. GPT\u5fae\u8c03\u7684\u76ee\u6807\u4efb\u52a1\u662f\u6839\u636e\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u9884\u6d4b\u6807\u7b7ey. P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) \u5176\u4e2d W_y W_y \u8868\u793a\u9884\u6d4b\u8f93\u51fa\u7684\u77e9\u9635\u53c2\u6570, \u5fae\u8c03\u4efb\u52a1\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u51fd\u6570: L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) \u7efc\u5408\u4e24\u4e2a\u9636\u6bb5\u7684\u76ee\u6807\u4efb\u52a1\u51fd\u6570, \u53ef\u77e5GPT\u7684\u6700\u7ec8\u4f18\u5316\u51fd\u6570\u4e3a: L_3 = L_2 + \\lambda L_1 L_3 = L_2 + \\lambda L_1","title":"3.2 \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning"},{"location":"06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fGPT. GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u672c\u8d28\u4e0a\u6765\u8bf4, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b. \u5b66\u4e60\u4e86GPT\u7684\u67b6\u6784. GPT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684\u89e3\u7801\u5668\u6a21\u5757. GPT\u5728\u4f7f\u7528\u89e3\u7801\u5668\u6a21\u5757\u65f6\u505a\u4e86\u4e00\u5b9a\u7684\u6539\u9020, \u5c06\u4f20\u7edf\u76843\u5c42Decoder Block\u53d8\u6210\u4e862\u5c42Block, \u5220\u9664\u4e86encoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5b50\u5c42\u548cFeed Forward\u5b50\u5c42. GPT\u7684\u89e3\u7801\u5668\u603b\u5171\u662f\u753112\u4e2a\u6539\u9020\u540e\u7684Decoder Block\u7ec4\u6210\u7684. \u5b66\u4e60\u4e86GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u53ea\u5229\u7528\u5355\u8bcd\u524d\u9762\u7684\u4fe1\u606f\u6765\u9884\u6d4b\u5f53\u524d\u5355\u8bcd. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning.","title":"4 \u5c0f\u7ed3"},{"location":"06_mkdocs_bert_pretrained_model/6%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u95f4\u7684\u4e0d\u540c\u70b9. \u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u6bd4\u8f83\u4e0b\u7684\u5404\u81ea\u4f18\u70b9\u548c\u7f3a\u70b9. 1 BERT, GPT, ELMo\u4e4b\u95f4\u7684\u4e0d\u540c\u70b9 \u00b6 \u5173\u4e8e\u7279\u5f81\u63d0\u53d6\u5668: ELMo\u91c7\u7528\u4e24\u90e8\u5206\u53cc\u5c42\u53cc\u5411LSTM\u8fdb\u884c\u7279\u5f81\u63d0\u53d6, \u7136\u540e\u518d\u8fdb\u884c\u7279\u5f81\u62fc\u63a5\u6765\u878d\u5408\u8bed\u4e49\u4fe1\u606f. GPT\u548cBERT\u91c7\u7528Transformer\u8fdb\u884c\u7279\u5f81\u63d0\u53d6. \u5f88\u591aNLP\u4efb\u52a1\u8868\u660eTransformer\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a\u4e8eLSTM, \u5bf9\u4e8eELMo\u800c\u8a00, \u91c7\u75281\u5c42\u9759\u6001token embedding + 2\u5c42LSTM, \u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u5355/\u53cc\u5411\u8bed\u8a00\u6a21\u578b: \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709GPT\u91c7\u7528\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u800cELMo\u548cBERT\u90fd\u91c7\u7528\u53cc\u5411\u8bed\u8a00\u6a21\u578b. ELMo\u867d\u7136\u88ab\u8ba4\u4e3a\u91c7\u7528\u4e86\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u7136\u540e\u8fdb\u884c\u7279\u5f81\u62fc\u63a5, \u8fd9\u79cd\u878d\u5408\u7279\u5f81\u7684\u80fd\u529b\u6bd4BERT\u4e00\u4f53\u5316\u7684\u878d\u5408\u7279\u5f81\u65b9\u5f0f\u5f31. \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709ELMo\u6ca1\u6709\u91c7\u7528Transformer. GPT\u548cBERT\u90fd\u6e90\u4e8eTransformer\u67b6\u6784, GPT\u7684\u5355\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86\u7ecf\u8fc7\u4fee\u6539\u540e\u7684Decoder\u6a21\u5757, Decoder\u91c7\u7528\u4e86look-ahead mask, \u53ea\u80fd\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u672a\u6765\u7684\u4fe1\u606f\u90fd\u88abmask\u6389\u4e86. \u800cBERT\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86Encoder\u6a21\u5757, Encoder\u53ea\u91c7\u7528\u4e86padding mask, \u53ef\u4ee5\u540c\u65f6\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u4ee5\u53cacontext after\u4e0b\u6587\u4fe1\u606f. 2 BERT, GPT, ELMo\u5404\u81ea\u7684\u4f18\u70b9\u548c\u7f3a\u70b9 \u00b6 ELMo: \u4f18\u70b9: \u4ece\u65e9\u671f\u7684Word2Vec\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u5927\u7f3a\u70b9\u51fa\u53d1, \u8fdb\u884c\u6539\u8fdb, \u8fd9\u4e00\u7f3a\u70b9\u5c31\u662f\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. ELMo\u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574word embedding, \u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u7f3a\u70b9: ELMo\u4f7f\u7528LSTM\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. ELMo\u4f7f\u7528\u5411\u91cf\u62fc\u63a5\u7684\u65b9\u5f0f\u878d\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. GPT: \u4f18\u70b9: GPT\u4f7f\u7528\u4e86Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u7f3a\u70b9: GPT\u53ea\u4f7f\u7528\u4e86\u5355\u5411Decoder, \u65e0\u6cd5\u878d\u5408\u672a\u6765\u7684\u4fe1\u606f. BERT: \u4f18\u70b9: BERT\u4f7f\u7528\u4e86\u53cc\u5411Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u6dfb\u52a0\u4e86\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1, MLM + NSP\u7684\u591a\u4efb\u52a1\u65b9\u5f0f\u8fdb\u884c\u6a21\u578b\u9884\u8bad\u7ec3. \u7f3a\u70b9: \u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u91cf\u592a\u591a, \u9700\u8981\u7684\u6570\u636e\u548c\u7b97\u529b\u8981\u6c42\u8fc7\u9ad8, \u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e94\u7528\u573a\u666f\u8981\u6c42\u9ad8. \u66f4\u9002\u5408\u7528\u4e8e\u8bed\u8a00\u5d4c\u5165\u8868\u8fbe, \u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u4efb\u52a1, \u4e0d\u9002\u5408\u7528\u4e8e\u751f\u6210\u5f0f\u7684\u4efb\u52a1. 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86BERT, GPT, ELMo\u4e4b\u95f4\u7684\u533a\u522b: * \u4e09\u8005\u6240\u9009\u53d6\u7684\u7279\u5f81\u63d0\u53d6\u5668\u4e0d\u540c. * BERT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757. * GPT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757. * ELMo\u91c7\u7528\u7684\u53cc\u5c42\u53cc\u5411LSTM\u6a21\u5757. \u4e09\u8005\u6240\u91c7\u7528\u7684\u8bed\u8a00\u6a21\u578b\u5355/\u53cc\u5411\u4e0d\u540c. BERT\u91c7\u7528\u7684\u662f\u6700\u5f7b\u5e95\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u53ef\u4ee5\u540c\u65f6\u5173\u6ce8context before\u548ccontext after. GPT\u91c7\u7528\u7684\u662f\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5373Transformer\u4e2d\u7684Decoder, \u7531\u4e8e\u91c7\u7528\u4e86mask\u673a\u5236, \u6240\u4ee5\u672a\u6765\u4fe1\u606fcontext after\u90fd\u4e0d\u53ef\u89c1. ELMo\u8868\u9762\u4e0a\u88ab\u8ba4\u4e3a\u662f\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411LSTM\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u5728\u8fdb\u884c\u7b80\u5355\u7684\u62fc\u63a5\u878d\u5408.","title":"6 BERT GPT ELMo\u6a21\u578b\u7684\u5bf9\u6bd4"},{"location":"06_mkdocs_bert_pretrained_model/6%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#_1","text":"\u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u95f4\u7684\u4e0d\u540c\u70b9. \u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u6bd4\u8f83\u4e0b\u7684\u5404\u81ea\u4f18\u70b9\u548c\u7f3a\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_bert_pretrained_model/6%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#1-bert-gpt-elmo","text":"\u5173\u4e8e\u7279\u5f81\u63d0\u53d6\u5668: ELMo\u91c7\u7528\u4e24\u90e8\u5206\u53cc\u5c42\u53cc\u5411LSTM\u8fdb\u884c\u7279\u5f81\u63d0\u53d6, \u7136\u540e\u518d\u8fdb\u884c\u7279\u5f81\u62fc\u63a5\u6765\u878d\u5408\u8bed\u4e49\u4fe1\u606f. GPT\u548cBERT\u91c7\u7528Transformer\u8fdb\u884c\u7279\u5f81\u63d0\u53d6. \u5f88\u591aNLP\u4efb\u52a1\u8868\u660eTransformer\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a\u4e8eLSTM, \u5bf9\u4e8eELMo\u800c\u8a00, \u91c7\u75281\u5c42\u9759\u6001token embedding + 2\u5c42LSTM, \u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u5355/\u53cc\u5411\u8bed\u8a00\u6a21\u578b: \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709GPT\u91c7\u7528\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u800cELMo\u548cBERT\u90fd\u91c7\u7528\u53cc\u5411\u8bed\u8a00\u6a21\u578b. ELMo\u867d\u7136\u88ab\u8ba4\u4e3a\u91c7\u7528\u4e86\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u7136\u540e\u8fdb\u884c\u7279\u5f81\u62fc\u63a5, \u8fd9\u79cd\u878d\u5408\u7279\u5f81\u7684\u80fd\u529b\u6bd4BERT\u4e00\u4f53\u5316\u7684\u878d\u5408\u7279\u5f81\u65b9\u5f0f\u5f31. \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709ELMo\u6ca1\u6709\u91c7\u7528Transformer. GPT\u548cBERT\u90fd\u6e90\u4e8eTransformer\u67b6\u6784, GPT\u7684\u5355\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86\u7ecf\u8fc7\u4fee\u6539\u540e\u7684Decoder\u6a21\u5757, Decoder\u91c7\u7528\u4e86look-ahead mask, \u53ea\u80fd\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u672a\u6765\u7684\u4fe1\u606f\u90fd\u88abmask\u6389\u4e86. \u800cBERT\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86Encoder\u6a21\u5757, Encoder\u53ea\u91c7\u7528\u4e86padding mask, \u53ef\u4ee5\u540c\u65f6\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u4ee5\u53cacontext after\u4e0b\u6587\u4fe1\u606f.","title":"1 BERT, GPT, ELMo\u4e4b\u95f4\u7684\u4e0d\u540c\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/6%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#2-bert-gpt-elmo","text":"ELMo: \u4f18\u70b9: \u4ece\u65e9\u671f\u7684Word2Vec\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u5927\u7f3a\u70b9\u51fa\u53d1, \u8fdb\u884c\u6539\u8fdb, \u8fd9\u4e00\u7f3a\u70b9\u5c31\u662f\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. ELMo\u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574word embedding, \u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u7f3a\u70b9: ELMo\u4f7f\u7528LSTM\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. ELMo\u4f7f\u7528\u5411\u91cf\u62fc\u63a5\u7684\u65b9\u5f0f\u878d\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. GPT: \u4f18\u70b9: GPT\u4f7f\u7528\u4e86Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u7f3a\u70b9: GPT\u53ea\u4f7f\u7528\u4e86\u5355\u5411Decoder, \u65e0\u6cd5\u878d\u5408\u672a\u6765\u7684\u4fe1\u606f. BERT: \u4f18\u70b9: BERT\u4f7f\u7528\u4e86\u53cc\u5411Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u6dfb\u52a0\u4e86\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1, MLM + NSP\u7684\u591a\u4efb\u52a1\u65b9\u5f0f\u8fdb\u884c\u6a21\u578b\u9884\u8bad\u7ec3. \u7f3a\u70b9: \u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u91cf\u592a\u591a, \u9700\u8981\u7684\u6570\u636e\u548c\u7b97\u529b\u8981\u6c42\u8fc7\u9ad8, \u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e94\u7528\u573a\u666f\u8981\u6c42\u9ad8. \u66f4\u9002\u5408\u7528\u4e8e\u8bed\u8a00\u5d4c\u5165\u8868\u8fbe, \u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u4efb\u52a1, \u4e0d\u9002\u5408\u7528\u4e8e\u751f\u6210\u5f0f\u7684\u4efb\u52a1.","title":"2 BERT, GPT, ELMo\u5404\u81ea\u7684\u4f18\u70b9\u548c\u7f3a\u70b9"},{"location":"06_mkdocs_bert_pretrained_model/6%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#3","text":"\u5b66\u4e60\u4e86BERT, GPT, ELMo\u4e4b\u95f4\u7684\u533a\u522b: * \u4e09\u8005\u6240\u9009\u53d6\u7684\u7279\u5f81\u63d0\u53d6\u5668\u4e0d\u540c. * BERT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757. * GPT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757. * ELMo\u91c7\u7528\u7684\u53cc\u5c42\u53cc\u5411LSTM\u6a21\u5757. \u4e09\u8005\u6240\u91c7\u7528\u7684\u8bed\u8a00\u6a21\u578b\u5355/\u53cc\u5411\u4e0d\u540c. BERT\u91c7\u7528\u7684\u662f\u6700\u5f7b\u5e95\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u53ef\u4ee5\u540c\u65f6\u5173\u6ce8context before\u548ccontext after. GPT\u91c7\u7528\u7684\u662f\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5373Transformer\u4e2d\u7684Decoder, \u7531\u4e8e\u91c7\u7528\u4e86mask\u673a\u5236, \u6240\u4ee5\u672a\u6765\u4fe1\u606fcontext after\u90fd\u4e0d\u53ef\u89c1. ELMo\u8868\u9762\u4e0a\u88ab\u8ba4\u4e3a\u662f\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411LSTM\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u5728\u8fdb\u884c\u7b80\u5355\u7684\u62fc\u63a5\u878d\u5408.","title":"3 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/index.html","text":"","title":"Index"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662fBERT \u638c\u63e1BERT\u7684\u67b6\u6784 \u638c\u63e1BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f 1 BERT\u7b80\u4ecb \u00b6 BERT\u662f2018\u5e7410\u6708\u7531Google AI\u7814\u7a76\u9662\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. BERT\u7684\u5168\u79f0\u662fBidirectional Encoder Representation from Transformers. BERT\u5728\u673a\u5668\u9605\u8bfb\u7406\u89e3\u9876\u7ea7\u6c34\u5e73\u6d4b\u8bd5SQuAD1.1\u4e2d\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u6210\u7ee9: \u5168\u90e8\u4e24\u4e2a\u8861\u91cf\u6307\u6807\u4e0a\u5168\u9762\u8d85\u8d8a\u4eba\u7c7b, \u5e76\u4e14\u572811\u79cd\u4e0d\u540cNLP\u6d4b\u8bd5\u4e2d\u521b\u51faSOTA\u8868\u73b0. \u5305\u62ec\u5c06GLUE\u57fa\u51c6\u63a8\u9ad8\u81f380.4% (\u7edd\u5bf9\u6539\u8fdb7.6%), MultiNLI\u51c6\u786e\u5ea6\u8fbe\u523086.7% (\u7edd\u5bf9\u6539\u8fdb5.6%). \u6210\u4e3aNLP\u53d1\u5c55\u53f2\u4e0a\u7684\u91cc\u7a0b\u7891\u5f0f\u7684\u6a21\u578b\u6210\u5c31. 2 BERT\u7684\u67b6\u6784 \u00b6 \u603b\u4f53\u67b6\u6784: \u5982\u4e0b\u56fe\u6240\u793a, \u6700\u5de6\u8fb9\u7684\u5c31\u662fBERT\u7684\u67b6\u6784\u56fe, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230BERT\u91c7\u7528\u4e86Transformer Encoder block\u8fdb\u884c\u8fde\u63a5, \u56e0\u4e3a\u662f\u4e00\u4e2a\u5178\u578b\u7684\u53cc\u5411\u7f16\u7801\u6a21\u578b. \u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aBERT\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684Transformer\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u9884\u5fae\u8c03\u6a21\u5757. 2.1 Embedding\u6a21\u5757 \u00b6 BERT\u4e2d\u7684\u8be5\u6a21\u5757\u662f\u7531\u4e09\u79cdEmbedding\u5171\u540c\u7ec4\u6210\u800c\u6210, \u5982\u4e0b\u56fe Token Embeddings \u662f\u8bcd\u5d4c\u5165\u5f20\u91cf, \u7b2c\u4e00\u4e2a\u5355\u8bcd\u662fCLS\u6807\u5fd7, \u53ef\u4ee5\u7528\u4e8e\u4e4b\u540e\u7684\u5206\u7c7b\u4efb\u52a1. Segment Embeddings \u662f\u53e5\u5b50\u5206\u6bb5\u5d4c\u5165\u5f20\u91cf, \u662f\u4e3a\u4e86\u670d\u52a1\u540e\u7eed\u7684\u4e24\u4e2a\u53e5\u5b50\u4e3a\u8f93\u5165\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. Position Embeddings \u662f\u4f4d\u7f6e\u7f16\u7801\u5f20\u91cf, \u6b64\u5904\u6ce8\u610f\u548c\u4f20\u7edf\u7684Transformer\u4e0d\u540c, \u4e0d\u662f\u4e09\u89d2\u51fd\u6570\u8ba1\u7b97\u7684\u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801, \u800c\u662f\u901a\u8fc7\u5b66\u4e60\u5f97\u51fa\u6765\u7684. \u6574\u4e2aEmbedding\u6a21\u5757\u7684\u8f93\u51fa\u5f20\u91cf\u5c31\u662f\u8fd93\u4e2a\u5f20\u91cf\u7684\u76f4\u63a5\u52a0\u548c\u7ed3\u679c. 2.2 \u53cc\u5411Transformer\u6a21\u5757 \u00b6 BERT\u4e2d\u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206, \u5b8c\u5168\u820d\u5f03\u4e86Decoder\u90e8\u5206. \u800c\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\u4e5f\u96c6\u4e2d\u4f53\u73b0\u5728\u8bad\u7ec3Transformer\u6a21\u5757\u4e2d. 2.3 \u9884\u5fae\u8c03\u6a21\u5757 \u00b6 \u7ecf\u8fc7\u4e2d\u95f4\u5c42Transformer\u7684\u5904\u7406\u540e, BERT\u7684\u6700\u540e\u4e00\u5c42\u6839\u636e\u4efb\u52a1\u7684\u4e0d\u540c\u9700\u6c42\u800c\u505a\u4e0d\u540c\u7684\u8c03\u6574\u5373\u53ef. \u6bd4\u5982\u5bf9\u4e8esequence-level\u7684\u5206\u7c7b\u4efb\u52a1, BERT\u76f4\u63a5\u53d6\u7b2c\u4e00\u4e2a[CLS] token \u7684final hidden state, \u518d\u52a0\u4e00\u5c42\u5168\u8fde\u63a5\u5c42\u540e\u8fdb\u884csoftmax\u6765\u9884\u6d4b\u6700\u7ec8\u7684\u6807\u7b7e. \u5bf9\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1, \u5fae\u8c03\u90fd\u96c6\u4e2d\u5728\u9884\u5fae\u8c03\u6a21\u5757, \u51e0\u79cd\u91cd\u8981\u7684NLP\u5fae\u8c03\u4efb\u52a1\u67b6\u6784\u56fe\u5c55\u793a\u5982\u4e0b \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u53d1\u73b0, \u5728\u9762\u5bf9\u7279\u5b9a\u4efb\u52a1\u65f6, \u53ea\u9700\u8981\u5bf9\u9884\u5fae\u8c03\u5c42\u8fdb\u884c\u5fae\u8c03, \u5c31\u53ef\u4ee5\u5229\u7528Transformer\u5f3a\u5927\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u6a21\u62df\u5f88\u591a\u4e0b\u6e38\u4efb\u52a1, \u5e76\u5f97\u5230SOTA\u7684\u7ed3\u679c. (\u53e5\u5b50\u5bf9\u5173\u7cfb\u5224\u65ad, \u5355\u6587\u672c\u4e3b\u9898\u5206\u7c7b, \u95ee\u7b54\u4efb\u52a1(QA), \u5355\u53e5\u8d34\u6807\u7b7e(NER)) \u82e5\u5e72\u53ef\u9009\u7684\u8d85\u53c2\u6570\u5efa\u8bae\u5982\u4e0b: Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Epochs: 3, 4 3 BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u00b6 BERT\u5305\u542b\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1: \u4efb\u52a1\u4e00: Masked LM (\u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3) \u4efb\u52a1\u4e8c: Next Sentence Prediction (\u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1) 3.1 \u4efb\u52a1\u4e00: Masked LM \u00b6 \u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3 \u5173\u4e8e\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3, \u90fd\u662f\u91c7\u7528left-to-right, \u6216\u8005left-to-right + right-to-left\u7ed3\u5408\u7684\u65b9\u5f0f, \u4f46\u8fd9\u79cd\u5355\u5411\u65b9\u5f0f\u6216\u8005\u62fc\u63a5\u7684\u65b9\u5f0f\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u4e3a\u6b64BERT\u63d0\u51fa\u4e00\u4e2a\u6df1\u5ea6\u53cc\u5411\u8868\u8fbe\u6a21\u578b(deep bidirectional representation). \u5373\u91c7\u7528MASK\u4efb\u52a1\u6765\u8bad\u7ec3\u6a21\u578b. 1: \u5728\u539f\u59cb\u8bad\u7ec3\u6587\u672c\u4e2d, \u968f\u673a\u7684\u62bd\u53d615%\u7684token\u4f5c\u4e3a\u53c2\u4e0eMASK\u4efb\u52a1\u7684\u5bf9\u8c61. 2: \u5728\u8fd9\u4e9b\u88ab\u9009\u4e2d\u7684token\u4e2d, \u6570\u636e\u751f\u6210\u5668\u5e76\u4e0d\u662f\u628a\u5b83\u4eec\u5168\u90e8\u53d8\u6210[MASK], \u800c\u662f\u6709\u4e0b\u52173\u79cd\u60c5\u51b5. 2.1: \u572880%\u7684\u6982\u7387\u4e0b, \u7528[MASK]\u6807\u8bb0\u66ff\u6362\u8be5token, \u6bd4\u5982my dog is hairy -> my dog is [MASK] 2.2: \u572810%\u7684\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362token, \u6bd4\u5982my dog is hairy -> my dog is apple 2.3: \u572810%\u7684\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8, \u6bd4\u5982my dog is hairy -> my dog is hairy 3: \u6a21\u578b\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d, \u5e76\u4e0d\u77e5\u9053\u5b83\u5c06\u8981\u9884\u6d4b\u54ea\u4e9b\u5355\u8bcd? \u54ea\u4e9b\u5355\u8bcd\u662f\u539f\u59cb\u7684\u6837\u5b50? \u54ea\u4e9b\u5355\u8bcd\u88ab\u906e\u63a9\u6210\u4e86[MASK]? \u54ea\u4e9b\u5355\u8bcd\u88ab\u66ff\u6362\u6210\u4e86\u5176\u4ed6\u5355\u8bcd? \u6b63\u662f\u5728\u8fd9\u6837\u4e00\u79cd\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b, \u53cd\u5012\u903c\u7740\u6a21\u578b\u5feb\u901f\u5b66\u4e60\u8be5token\u7684\u5206\u5e03\u5f0f\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u5c3d\u6700\u5927\u52aa\u529b\u5b66\u4e60\u539f\u59cb\u8bed\u8a00\u8bf4\u8bdd\u7684\u6837\u5b50. \u540c\u65f6\u56e0\u4e3a\u539f\u59cb\u6587\u672c\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86MASK\u64cd\u4f5c, \u5e76\u4e0d\u4f1a\u7834\u574f\u539f\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u548c\u8bed\u8a00\u89c4\u5219. 3.2 \u4efb\u52a1\u4e8c: Next Sentence Prediction \u00b6 \u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1 \u5728NLP\u4e2d\u6709\u4e00\u7c7b\u91cd\u8981\u7684\u95ee\u9898\u6bd4\u5982QA(Quention-Answer), NLI(Natural Language Inference), \u9700\u8981\u6a21\u578b\u80fd\u591f\u5f88\u597d\u7684\u7406\u89e3\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u9700\u8981\u5728\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\u5f15\u5165\u5bf9\u5e94\u7684\u4efb\u52a1. \u5728BERT\u4e2d\u5f15\u5165\u7684\u5c31\u662fNext Sentence Prediction\u4efb\u52a1. \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u6765\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. 1: \u6240\u6709\u53c2\u4e0e\u4efb\u52a1\u8bad\u7ec3\u7684\u8bed\u53e5\u90fd\u88ab\u9009\u4e2d\u4f5c\u4e3a\u53e5\u5b50A. 1.1: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) 1.2: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) 2: \u5728\u4efb\u52a1\u4e8c\u4e2d, BERT\u6a21\u578b\u53ef\u4ee5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f9797%-98%\u7684\u51c6\u786e\u7387. 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fBERT. BERT\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer Encoder\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. BERT\u572811\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u521b\u51faSOAT\u8868\u73b0. \u5b66\u4e60\u4e86BERT\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757, \u5305\u62ecToken Embeddings, Segment Embeddings, Position Embeddings. \u4e2d\u95f4\u5c42\u7684Transformer\u6a21\u5757, \u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206. \u6700\u4e0a\u5c42\u7684\u9884\u5fae\u8c03\u6a21\u5757, \u5177\u4f53\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u7c7b\u578b\u6765\u505a\u76f8\u5e94\u7684\u5904\u7406. \u5b66\u4e60\u4e86BERT\u7684\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1. MLM\u4efb\u52a1(Masked Language Model), \u5728\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d615%\u7684token\u53c2\u4e0e\u4efb\u52a1. \u572880%\u6982\u7387\u4e0b, \u7528[MASK]\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8. NSP\u4efb\u52a1(Next Sentence Prediction), \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd.(\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) \u672c\u8282\u5e38\u89c1\u95ee\u7b54 \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f","title":"1 BERT\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fBERT \u638c\u63e1BERT\u7684\u67b6\u6784 \u638c\u63e1BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u601d\u8003\u9898\uff1aBert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-bert","text":"BERT\u662f2018\u5e7410\u6708\u7531Google AI\u7814\u7a76\u9662\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. BERT\u7684\u5168\u79f0\u662fBidirectional Encoder Representation from Transformers. BERT\u5728\u673a\u5668\u9605\u8bfb\u7406\u89e3\u9876\u7ea7\u6c34\u5e73\u6d4b\u8bd5SQuAD1.1\u4e2d\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u6210\u7ee9: \u5168\u90e8\u4e24\u4e2a\u8861\u91cf\u6307\u6807\u4e0a\u5168\u9762\u8d85\u8d8a\u4eba\u7c7b, \u5e76\u4e14\u572811\u79cd\u4e0d\u540cNLP\u6d4b\u8bd5\u4e2d\u521b\u51faSOTA\u8868\u73b0. \u5305\u62ec\u5c06GLUE\u57fa\u51c6\u63a8\u9ad8\u81f380.4% (\u7edd\u5bf9\u6539\u8fdb7.6%), MultiNLI\u51c6\u786e\u5ea6\u8fbe\u523086.7% (\u7edd\u5bf9\u6539\u8fdb5.6%). \u6210\u4e3aNLP\u53d1\u5c55\u53f2\u4e0a\u7684\u91cc\u7a0b\u7891\u5f0f\u7684\u6a21\u578b\u6210\u5c31.","title":"1 BERT\u7b80\u4ecb"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-bert","text":"\u603b\u4f53\u67b6\u6784: \u5982\u4e0b\u56fe\u6240\u793a, \u6700\u5de6\u8fb9\u7684\u5c31\u662fBERT\u7684\u67b6\u6784\u56fe, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230BERT\u91c7\u7528\u4e86Transformer Encoder block\u8fdb\u884c\u8fde\u63a5, \u56e0\u4e3a\u662f\u4e00\u4e2a\u5178\u578b\u7684\u53cc\u5411\u7f16\u7801\u6a21\u578b. \u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aBERT\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684Transformer\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u9884\u5fae\u8c03\u6a21\u5757.","title":"2 BERT\u7684\u67b6\u6784"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#21-embedding","text":"BERT\u4e2d\u7684\u8be5\u6a21\u5757\u662f\u7531\u4e09\u79cdEmbedding\u5171\u540c\u7ec4\u6210\u800c\u6210, \u5982\u4e0b\u56fe Token Embeddings \u662f\u8bcd\u5d4c\u5165\u5f20\u91cf, \u7b2c\u4e00\u4e2a\u5355\u8bcd\u662fCLS\u6807\u5fd7, \u53ef\u4ee5\u7528\u4e8e\u4e4b\u540e\u7684\u5206\u7c7b\u4efb\u52a1. Segment Embeddings \u662f\u53e5\u5b50\u5206\u6bb5\u5d4c\u5165\u5f20\u91cf, \u662f\u4e3a\u4e86\u670d\u52a1\u540e\u7eed\u7684\u4e24\u4e2a\u53e5\u5b50\u4e3a\u8f93\u5165\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. Position Embeddings \u662f\u4f4d\u7f6e\u7f16\u7801\u5f20\u91cf, \u6b64\u5904\u6ce8\u610f\u548c\u4f20\u7edf\u7684Transformer\u4e0d\u540c, \u4e0d\u662f\u4e09\u89d2\u51fd\u6570\u8ba1\u7b97\u7684\u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801, \u800c\u662f\u901a\u8fc7\u5b66\u4e60\u5f97\u51fa\u6765\u7684. \u6574\u4e2aEmbedding\u6a21\u5757\u7684\u8f93\u51fa\u5f20\u91cf\u5c31\u662f\u8fd93\u4e2a\u5f20\u91cf\u7684\u76f4\u63a5\u52a0\u548c\u7ed3\u679c.","title":"2.1 Embedding\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#22-transformer","text":"BERT\u4e2d\u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206, \u5b8c\u5168\u820d\u5f03\u4e86Decoder\u90e8\u5206. \u800c\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\u4e5f\u96c6\u4e2d\u4f53\u73b0\u5728\u8bad\u7ec3Transformer\u6a21\u5757\u4e2d.","title":"2.2 \u53cc\u5411Transformer\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#23","text":"\u7ecf\u8fc7\u4e2d\u95f4\u5c42Transformer\u7684\u5904\u7406\u540e, BERT\u7684\u6700\u540e\u4e00\u5c42\u6839\u636e\u4efb\u52a1\u7684\u4e0d\u540c\u9700\u6c42\u800c\u505a\u4e0d\u540c\u7684\u8c03\u6574\u5373\u53ef. \u6bd4\u5982\u5bf9\u4e8esequence-level\u7684\u5206\u7c7b\u4efb\u52a1, BERT\u76f4\u63a5\u53d6\u7b2c\u4e00\u4e2a[CLS] token \u7684final hidden state, \u518d\u52a0\u4e00\u5c42\u5168\u8fde\u63a5\u5c42\u540e\u8fdb\u884csoftmax\u6765\u9884\u6d4b\u6700\u7ec8\u7684\u6807\u7b7e. \u5bf9\u4e8e\u4e0d\u540c\u7684\u4efb\u52a1, \u5fae\u8c03\u90fd\u96c6\u4e2d\u5728\u9884\u5fae\u8c03\u6a21\u5757, \u51e0\u79cd\u91cd\u8981\u7684NLP\u5fae\u8c03\u4efb\u52a1\u67b6\u6784\u56fe\u5c55\u793a\u5982\u4e0b \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u53d1\u73b0, \u5728\u9762\u5bf9\u7279\u5b9a\u4efb\u52a1\u65f6, \u53ea\u9700\u8981\u5bf9\u9884\u5fae\u8c03\u5c42\u8fdb\u884c\u5fae\u8c03, \u5c31\u53ef\u4ee5\u5229\u7528Transformer\u5f3a\u5927\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u6a21\u62df\u5f88\u591a\u4e0b\u6e38\u4efb\u52a1, \u5e76\u5f97\u5230SOTA\u7684\u7ed3\u679c. (\u53e5\u5b50\u5bf9\u5173\u7cfb\u5224\u65ad, \u5355\u6587\u672c\u4e3b\u9898\u5206\u7c7b, \u95ee\u7b54\u4efb\u52a1(QA), \u5355\u53e5\u8d34\u6807\u7b7e(NER)) \u82e5\u5e72\u53ef\u9009\u7684\u8d85\u53c2\u6570\u5efa\u8bae\u5982\u4e0b: Batch size: 16, 32 Learning rate (Adam): 5e-5, 3e-5, 2e-5 Epochs: 3, 4","title":"2.3 \u9884\u5fae\u8c03\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-bert","text":"BERT\u5305\u542b\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1: \u4efb\u52a1\u4e00: Masked LM (\u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3) \u4efb\u52a1\u4e8c: Next Sentence Prediction (\u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1)","title":"3 BERT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#31-masked-lm","text":"\u5e26mask\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3 \u5173\u4e8e\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3, \u90fd\u662f\u91c7\u7528left-to-right, \u6216\u8005left-to-right + right-to-left\u7ed3\u5408\u7684\u65b9\u5f0f, \u4f46\u8fd9\u79cd\u5355\u5411\u65b9\u5f0f\u6216\u8005\u62fc\u63a5\u7684\u65b9\u5f0f\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u4e3a\u6b64BERT\u63d0\u51fa\u4e00\u4e2a\u6df1\u5ea6\u53cc\u5411\u8868\u8fbe\u6a21\u578b(deep bidirectional representation). \u5373\u91c7\u7528MASK\u4efb\u52a1\u6765\u8bad\u7ec3\u6a21\u578b. 1: \u5728\u539f\u59cb\u8bad\u7ec3\u6587\u672c\u4e2d, \u968f\u673a\u7684\u62bd\u53d615%\u7684token\u4f5c\u4e3a\u53c2\u4e0eMASK\u4efb\u52a1\u7684\u5bf9\u8c61. 2: \u5728\u8fd9\u4e9b\u88ab\u9009\u4e2d\u7684token\u4e2d, \u6570\u636e\u751f\u6210\u5668\u5e76\u4e0d\u662f\u628a\u5b83\u4eec\u5168\u90e8\u53d8\u6210[MASK], \u800c\u662f\u6709\u4e0b\u52173\u79cd\u60c5\u51b5. 2.1: \u572880%\u7684\u6982\u7387\u4e0b, \u7528[MASK]\u6807\u8bb0\u66ff\u6362\u8be5token, \u6bd4\u5982my dog is hairy -> my dog is [MASK] 2.2: \u572810%\u7684\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362token, \u6bd4\u5982my dog is hairy -> my dog is apple 2.3: \u572810%\u7684\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8, \u6bd4\u5982my dog is hairy -> my dog is hairy 3: \u6a21\u578b\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d, \u5e76\u4e0d\u77e5\u9053\u5b83\u5c06\u8981\u9884\u6d4b\u54ea\u4e9b\u5355\u8bcd? \u54ea\u4e9b\u5355\u8bcd\u662f\u539f\u59cb\u7684\u6837\u5b50? \u54ea\u4e9b\u5355\u8bcd\u88ab\u906e\u63a9\u6210\u4e86[MASK]? \u54ea\u4e9b\u5355\u8bcd\u88ab\u66ff\u6362\u6210\u4e86\u5176\u4ed6\u5355\u8bcd? \u6b63\u662f\u5728\u8fd9\u6837\u4e00\u79cd\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b, \u53cd\u5012\u903c\u7740\u6a21\u578b\u5feb\u901f\u5b66\u4e60\u8be5token\u7684\u5206\u5e03\u5f0f\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u5c3d\u6700\u5927\u52aa\u529b\u5b66\u4e60\u539f\u59cb\u8bed\u8a00\u8bf4\u8bdd\u7684\u6837\u5b50. \u540c\u65f6\u56e0\u4e3a\u539f\u59cb\u6587\u672c\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86MASK\u64cd\u4f5c, \u5e76\u4e0d\u4f1a\u7834\u574f\u539f\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u548c\u8bed\u8a00\u89c4\u5219.","title":"3.1 \u4efb\u52a1\u4e00: Masked LM"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#32-next-sentence-prediction","text":"\u4e0b\u4e00\u53e5\u8bdd\u9884\u6d4b\u4efb\u52a1 \u5728NLP\u4e2d\u6709\u4e00\u7c7b\u91cd\u8981\u7684\u95ee\u9898\u6bd4\u5982QA(Quention-Answer), NLI(Natural Language Inference), \u9700\u8981\u6a21\u578b\u80fd\u591f\u5f88\u597d\u7684\u7406\u89e3\u4e24\u4e2a\u53e5\u5b50\u4e4b\u95f4\u7684\u5173\u7cfb, \u4ece\u800c\u9700\u8981\u5728\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\u5f15\u5165\u5bf9\u5e94\u7684\u4efb\u52a1. \u5728BERT\u4e2d\u5f15\u5165\u7684\u5c31\u662fNext Sentence Prediction\u4efb\u52a1. \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u6765\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. 1: \u6240\u6709\u53c2\u4e0e\u4efb\u52a1\u8bad\u7ec3\u7684\u8bed\u53e5\u90fd\u88ab\u9009\u4e2d\u4f5c\u4e3a\u53e5\u5b50A. 1.1: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) 1.2: \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) 2: \u5728\u4efb\u52a1\u4e8c\u4e2d, BERT\u6a21\u578b\u53ef\u4ee5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f9797%-98%\u7684\u51c6\u786e\u7387.","title":"3.2 \u4efb\u52a1\u4e8c: Next Sentence Prediction"},{"location":"06_mkdocs_pretrained_model_old/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fBERT. BERT\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer Encoder\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. BERT\u572811\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u521b\u51faSOAT\u8868\u73b0. \u5b66\u4e60\u4e86BERT\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757, \u5305\u62ecToken Embeddings, Segment Embeddings, Position Embeddings. \u4e2d\u95f4\u5c42\u7684Transformer\u6a21\u5757, \u53ea\u4f7f\u7528\u4e86\u7ecf\u5178Transformer\u67b6\u6784\u4e2d\u7684Encoder\u90e8\u5206. \u6700\u4e0a\u5c42\u7684\u9884\u5fae\u8c03\u6a21\u5757, \u5177\u4f53\u6839\u636e\u4e0d\u540c\u7684\u4efb\u52a1\u7c7b\u578b\u6765\u505a\u76f8\u5e94\u7684\u5904\u7406. \u5b66\u4e60\u4e86BERT\u7684\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1. MLM\u4efb\u52a1(Masked Language Model), \u5728\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d615%\u7684token\u53c2\u4e0e\u4efb\u52a1. \u572880%\u6982\u7387\u4e0b, \u7528[MASK]\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u7528\u4e00\u4e2a\u968f\u673a\u7684\u5355\u8bcd\u66ff\u6362\u8be5token. \u572810%\u6982\u7387\u4e0b, \u4fdd\u6301\u8be5token\u4e0d\u53d8. NSP\u4efb\u52a1(Next Sentence Prediction), \u91c7\u7528\u7684\u65b9\u5f0f\u662f\u8f93\u5165\u53e5\u5b50\u5bf9(A, B), \u6a21\u578b\u9884\u6d4b\u53e5\u5b50B\u662f\u4e0d\u662f\u53e5\u5b50A\u7684\u771f\u5b9e\u7684\u4e0b\u4e00\u53e5\u8bdd. \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u771f\u5b9e\u8ddf\u968fA\u7684\u4e0b\u4e00\u53e5\u8bdd.(\u6807\u8bb0\u4e3aIsNext, \u4ee3\u8868\u6b63\u6837\u672c) \u5176\u4e2d50%\u7684B\u662f\u539f\u59cb\u6587\u672c\u4e2d\u968f\u673a\u62bd\u53d6\u7684\u4e00\u53e5\u8bdd. (\u6807\u8bb0\u4e3aNotNext, \u4ee3\u8868\u8d1f\u6837\u672c) \u672c\u8282\u5e38\u89c1\u95ee\u7b54 \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u7684\u67b6\u6784\u4ee5\u53ca\u6bcf\u4e00\u90e8\u5206\u7684\u4f5c\u7528\uff1f \u8bf4\u4e00\u8bf4Bert\u6a21\u578b\u4e24\u5927\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u8c08\u4e00\u8c08\u4f60\u7684\u7406\u89e3\uff1f","title":"4 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/10%20GPT2%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1GPT2\u7684\u67b6\u6784 \u638c\u63e1GPT2\u7684\u8bad\u7ec3\u4efb\u52a1\u548c\u6a21\u578b\u7ec6\u8282 1 GPT2\u7684\u67b6\u6784 \u00b6 \u4ece\u6a21\u578b\u67b6\u6784\u4e0a\u770b, GPT2\u5e76\u6ca1\u6709\u7279\u522b\u65b0\u9896\u7684\u67b6\u6784, \u5b83\u548c\u53ea\u5e26\u6709\u89e3\u7801\u5668\u6a21\u5757\u7684Transformer\u5f88\u50cf. \u6240\u8c13\u8bed\u8a00\u6a21\u578b, \u4f5c\u7528\u5c31\u662f\u6839\u636e\u5df2\u6709\u53e5\u5b50\u7684\u4e00\u90e8\u5206, \u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u4f1a\u662f\u4ec0\u4e48. \u73b0\u5b9e\u5e94\u7528\u4e2d\u5927\u5bb6\u6700\u719f\u6089\u7684\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5e94\u7528, \u5c31\u662f\u667a\u80fd\u624b\u673a\u4e0a\u7684\u8f93\u5165\u6cd5, \u5b83\u53ef\u4ee5\u6839\u636e\u5f53\u524d\u8f93\u5165\u7684\u5185\u5bb9\u667a\u80fd\u63a8\u8350\u4e0b\u4e00\u4e2a\u8981\u6253\u7684\u5b57. GPT2\u4e5f\u662f\u4e00\u4e2a\u8bed\u8a00\u9884\u6d4b\u751f\u6210\u6a21\u578b, \u53ea\u4e0d\u8fc7\u6bd4\u624b\u673a\u4e0a\u5e94\u7528\u7684\u6a21\u578b\u8981\u5927\u5f88\u591a, \u4e5f\u66f4\u52a0\u590d\u6742. \u5e38\u89c1\u7684\u624b\u673a\u7aef\u5e94\u7528\u7684\u8f93\u5165\u6cd5\u6a21\u578b\u57fa\u672c\u5360\u752850MB\u7a7a\u95f4, \u800cOpenAI\u7684\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u4e8640GB\u7684\u8d85\u5927\u6570\u636e\u96c6\u6765\u8bad\u7ec3GPT2, \u8bad\u7ec3\u540e\u7684GPT2\u6a21\u578b\u6700\u5c0f\u7684\u7248\u672c\u4e5f\u8981\u5360\u7528\u8d85\u8fc7500MB\u7a7a\u95f4\u6765\u5b58\u50a8\u6240\u6709\u7684\u53c2\u6570, \u81f3\u4e8e\u6700\u5927\u7248\u672c\u7684GPT2\u5219\u9700\u8981\u8d85\u8fc76.5GB\u7684\u5b58\u50a8\u7a7a\u95f4. \u81ea\u4eceTransformer\u95ee\u4e16\u4ee5\u6765, \u5f88\u591a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4f5c\u90fd\u5728\u5c1d\u8bd5\u5c06\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\u5806\u53e0\u7684\u5c3d\u53ef\u80fd\u9ad8, \u90a3\u7c7b\u4f3c\u7684\u6a21\u578b\u53ef\u4ee5\u5806\u53e0\u5230\u591a\u6df1\u5462? \u4e8b\u5b9e\u4e0a, \u8fd9\u4e2a\u95ee\u9898\u7684\u7b54\u6848\u4e5f\u5c31\u662f\u533a\u522b\u4e0d\u540cGPT2\u7248\u672c\u7684\u4e3b\u8981\u56e0\u7d20\u4e4b\u4e00. \u6bd4\u5982\u6700\u5c0f\u7248\u672c\u7684GPT2\u5806\u53e0\u4e8612\u5c42, \u4e2d\u53f7\u768424\u5c42, \u5927\u53f7\u768436\u5c42, \u8d85\u5927\u53f7\u7684\u5806\u53e0\u4e86\u6574\u657448\u5c42! 2 GPT2\u6a21\u578b\u7684\u7ec6\u8282 \u00b6 \u4ee5\u673a\u5668\u4eba\u7b2c\u4e00\u6cd5\u5219\u4e3a\u4f8b, \u6765\u5177\u4f53\u770bGPT2\u7684\u5de5\u4f5c\u7ec6\u8282. * \u673a\u5668\u4eba\u7b2c\u4e00\u6cd5\u5219: \u673a\u5668\u4eba\u4e0d\u5f97\u4f24\u5bb3\u4eba\u7c7b, \u6216\u8005\u76ee\u7779\u4eba\u7c7b\u5c06\u906d\u53d7\u5371\u9669\u800c\u8896\u624b\u65c1\u89c2. 2.1 \u6a21\u578b\u8fc7\u7a0b \u00b6 \u9996\u5148\u660e\u786e\u4e00\u70b9: GPT2\u7684\u5de5\u4f5c\u6d41\u7a0b\u5f88\u50cf\u4f20\u7edf\u8bed\u8a00\u6a21\u578b, \u4e00\u6b21\u53ea\u8f93\u51fa\u4e00\u4e2a\u5355\u8bcd(token). GPT2\u4e4b\u6240\u4ee5\u5728\u751f\u6210\u5f0f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0, \u662f\u56e0\u4e3a\u5728\u6bcf\u4e2a\u65b0\u5355\u8bcd(token)\u4ea7\u751f\u540e, \u8be5\u5355\u8bcd\u5c31\u88ab\u6dfb\u52a0\u5728\u4e4b\u524d\u751f\u6210\u7684\u5355\u8bcd\u5e8f\u5217\u540e\u9762, \u6dfb\u52a0\u540e\u7684\u65b0\u5e8f\u5217\u53c8\u4f1a\u6210\u4e3a\u6a21\u578b\u4e0b\u4e00\u6b65\u7684\u65b0\u8f93\u5165. \u8fd9\u79cd\u673a\u5236\u5c31\u53eb\u505a\u81ea\u56de\u5f52(auto-regression), \u5982\u4e0b\u6240\u793a: \u5176\u6b21\u660e\u786e\u4e00\u70b9: GPT2\u6a21\u578b\u662f\u4e00\u4e2a\u53ea\u5305\u542b\u4e86Transformer Decoder\u6a21\u5757\u7684\u6a21\u578b. \u548cBERT\u6a21\u578b\u76f8\u6bd4, GPT2\u7684\u89e3\u7801\u5668\u5728self-attention\u5c42\u4e0a\u6709\u4e00\u4e2a\u5173\u952e\u7684\u5dee\u5f02: \u5b83\u5c06\u540e\u9762\u7684\u5355\u8bcd(token)\u906e\u63a9\u6389, \u800cBERT\u662f\u6309\u7167\u4e00\u5b9a\u89c4\u5219\u5c06\u5355\u8bcd\u66ff\u6362\u6210[MASK]. \u4e3e\u4e2a\u4f8b\u5b50, \u5982\u679c\u6211\u4eec\u91cd\u70b9\u5173\u6ce84\u53f7\u4f4d\u7f6e\u7684\u5355\u8bcd\u53ca\u5176\u524d\u5e8f\u8def\u5f84, \u6211\u4eec\u53ef\u4ee5\u8ba9\u6a21\u578b\u53ea\u5141\u8bb8\u6ce8\u610f\u5f53\u524d\u8ba1\u7b97\u7684\u5355\u8bcd\u548c\u5b83\u4e4b\u524d\u7684\u5355\u8bcd, \u5982\u4e0b\u56fe\u6240\u793a: \u6ce8\u610f: \u80fd\u591f\u6e05\u695a\u7684\u533a\u5206BERT\u4f7f\u7528\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757(self-attention)\u548cGPT2\u4f7f\u7528\u7684\u5e26\u63a9\u7801\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757(masked self-attention)\u5f88\u91cd\u8981! \u666e\u901a\u7684self-attention\u5141\u8bb8\u6a21\u578b\u7684\u4efb\u610f\u4e00\u4e2a\u4f4d\u7f6e\u770b\u5230\u5b83\u53f3\u4fa7\u7684\u4fe1\u606f(\u4e0b\u56fe\u5de6\u4fa7), \u800c\u5e26\u63a9\u7801\u7684self-attention\u5219\u4e0d\u5141\u8bb8\u8fd9\u4e48\u505a(\u4e0b\u56fe\u53f3\u4fa7). \u5728Transformer\u539f\u59cb\u8bba\u6587\u53d1\u8868\u540e, \u4e00\u7bc7\u540d\u4e3a<< Generating Wikipedia by Summarizing Long Sequences >>\u7684\u8bba\u6587\u63d0\u51fa\u7528\u53e6\u4e00\u79cdTransformer\u6a21\u5757\u7684\u6392\u5217\u65b9\u5f0f\u6765\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21-\u5b83\u76f4\u63a5\u6254\u6389\u4e86\u7f16\u7801\u5668, \u53ea\u4fdd\u7559\u89e3\u7801\u5668. \u8fd9\u4e2a\u65e9\u671f\u7684\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u75316\u4e2aDecoder Block\u5806\u53e0\u800c\u6210: \u4e0a\u56fe\u4e2d\u6240\u6709\u7684\u89e3\u7801\u5668\u6a21\u5757\u90fd\u662f\u4e00\u6837\u7684, \u56e0\u4e3a\u53ea\u5c55\u5f00\u4e86\u7b2c\u4e00\u4e2a\u89e3\u7801\u5668\u7684\u5185\u90e8\u7ed3\u6784. \u548cGPT\u4e00\u6837, \u53ea\u4fdd\u7559\u4e86\u5e26\u63a9\u7801\u7684self-attention\u5b50\u5c42, \u548cFeed Forward\u5b50\u5c42. \u8fd9\u4e9b\u89e3\u7801\u5668\u548c\u7ecf\u5178Transformer\u539f\u59cb\u8bba\u6587\u4e2d\u7684\u89e3\u7801\u5668\u6a21\u5757\u76f8\u6bd4, \u9664\u4e86\u5220\u9664\u4e86\u7b2c\u4e8c\u4e2aEncoder-Decoder Attention\u5b50\u5c42\u5916, \u5176\u4ed6\u6784\u9020\u90fd\u4e00\u6837. 2.2 GPT2\u5de5\u4f5c\u7ec6\u8282\u63a2\u7a76 \u00b6 GPT2\u53ef\u4ee5\u5904\u7406\u6700\u957f1024\u4e2a\u5355\u8bcd\u7684\u5e8f\u5217. \u6bcf\u4e2a\u5355\u8bcd\u90fd\u4f1a\u548c\u5b83\u7684\u524d\u5e8f\u8def\u5f84\u4e00\u8d77\"\u6d41\u7ecf\"\u6240\u6709\u7684\u89e3\u7801\u5668\u6a21\u5757. \u5bf9\u4e8e\u751f\u6210\u5f0f\u6a21\u578b\u6765\u8bf4, \u57fa\u672c\u5de5\u4f5c\u65b9\u5f0f\u90fd\u662f\u63d0\u4f9b\u4e00\u4e2a\u9884\u5148\u5b9a\u4e49\u597d\u7684\u8d77\u59cbtoken, \u6bd4\u5982\u8bb0\u505a\"s\". \u6b64\u65f6\u6a21\u578b\u7684\u8f93\u5165\u53ea\u6709\u4e00\u4e2a\u5355\u8bcd, \u6240\u4ee5\u53ea\u6709\u8fd9\u4e2a\u5355\u8bcd\u7684\u8def\u5f84\u662f\u6d3b\u8dc3\u7684. \u5355\u8bcd\u7ecf\u8fc7\u5c42\u5c42\u5904\u7406, \u6700\u7ec8\u5f97\u5230\u4e00\u4e2a\u8bcd\u5411\u91cf. \u8be5\u5411\u91cf\u53ef\u4ee5\u5bf9\u4e8e\u8bcd\u6c47\u8868\u7684\u6bcf\u4e2a\u5355\u8bcd\u8ba1\u7b97\u51fa\u4e00\u4e2a\u6982\u7387(GPT2\u7684\u8bcd\u6c47\u8868\u4e2d\u670950000\u4e2a\u5355\u8bcd). \u5728\u672c\u4f8b\u4e2d, \u6211\u4eec\u9009\u62e9\u6982\u7387\u6700\u9ad8\u7684\u5355\u8bcd[\"The\"]\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u5355\u8bcd. \u6ce8\u610f: \u8fd9\u79cd\u9009\u62e9\u6700\u9ad8\u6982\u7387\u8f93\u51fa\u7684\u7b56\u7565\u6709\u65f6\u4f1a\u51fa\u73b0\u95ee\u9898-\u5982\u679c\u6211\u4eec\u6301\u7eed\u70b9\u51fb\u8f93\u5165\u6cd5\u63a8\u8350\u5355\u8bcd\u7684\u7b2c\u4e00\u4e2a, \u5b83\u53ef\u80fd\u4f1a\u9677\u5165\u63a8\u8350\u540c\u4e00\u4e2a\u8bcd\u7684\u5faa\u73af\u4e2d, \u53ea\u6709\u4f60\u70b9\u51fb\u7b2c\u4e8c\u4e2a\u6216\u7b2c\u4e09\u4e2a\u63a8\u8350\u8bcd, \u624d\u80fd\u8df3\u51fa\u8fd9\u79cd\u5faa\u73af. \u540c\u7406, GPT2\u6709\u4e00\u4e2atop-k\u53c2\u6570, \u6a21\u578b\u4f1a\u4ece\u6982\u7387\u6700\u5927\u7684\u524dk\u4e2a\u5355\u8bcd\u4e2d\u62bd\u6837\u9009\u53d6\u4e0b\u4e00\u4e2a\u5355\u8bcd. \u63a5\u4e0b\u6765, \u6211\u4eec\u5c06\u8f93\u51fa\u7684\u5355\u8bcd[\"The\"]\u6dfb\u52a0\u5728\u8f93\u5165\u5e8f\u5217\u7684\u5c3e\u90e8, \u4ece\u800c\u6784\u5efa\u51fa\u65b0\u7684\u8f93\u5165\u5e8f\u5217[\"s\", \"The\"], \u8ba9\u6a21\u578b\u8fdb\u884c\u4e0b\u4e00\u6b65\u7684\u9884\u6d4b: \u6b64\u65f6\u7b2c\u4e8c\u4e2a\u5355\u8bcd\u7684\u8def\u5f84\u662f\u5f53\u524d\u552f\u4e00\u6d3b\u8dc3\u7684\u8def\u5f84\u4e86. GPT2\u7684\u6bcf\u4e00\u5c42\u90fd\u4fdd\u7559\u4e86\u5b83\u4eec\u5bf9\u7b2c\u4e00\u4e2a\u5355\u8bcd\u7684\u89e3\u91ca, \u5e76\u4e14\u5c06\u8fd0\u7528\u8fd9\u4e9b\u4fe1\u606f\u5904\u7406\u7b2c\u4e8c\u4e2a\u5355\u8bcd, GPT2\u4e0d\u4f1a\u6839\u636e\u7b2c\u4e8c\u4e2a\u5355\u8bcd\u91cd\u65b0\u6765\u89e3\u91ca\u7b2c\u4e00\u4e2a\u5355\u8bcd. \u5173\u4e8e\u8f93\u5165\u7f16\u7801: \u5f53\u6211\u4eec\u66f4\u52a0\u6df1\u5165\u7684\u4e86\u89e3\u6a21\u578b\u7684\u5185\u90e8\u7ec6\u8282\u65f6, \u6700\u5f00\u59cb\u5c31\u8981\u9762\u5bf9\u6a21\u578b\u7684\u8f93\u5165, \u548c\u5176\u4ed6\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u4e00\u6837, GPT2\u540c\u6837\u4ece\u5d4c\u5165\u77e9\u9635\u4e2d\u67e5\u627e\u5355\u8bcd\u5bf9\u5e94\u7684\u5d4c\u5165\u5411\u91cf, \u8be5\u77e9\u9635(embedding matrix)\u4e5f\u662f\u6574\u4e2a\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u7684\u4e00\u90e8\u5206. \u5982\u4e0a\u56fe\u6240\u793a, \u6bcf\u4e00\u884c\u90fd\u662f\u4e00\u4e2a\u8bcd\u5d4c\u5165\u5411\u91cf: \u4e00\u4e2a\u80fd\u591f\u8868\u5f81\u67d0\u4e2a\u5355\u8bcd, \u5e76\u6355\u83b7\u5176\u8bed\u4e49\u7684\u6570\u5b57\u5411\u91cf. \u5d4c\u5165\u7684\u7ef4\u5ea6\u5927\u5c0f\u548cGPT2\u6a21\u578b\u7684\u5927\u5c0f\u76f8\u5173, \u6700\u5c0f\u7684\u6a21\u578b\u91c7\u7528\u4e86768\u8fd9\u4e2a\u7ef4\u5ea6, \u6700\u5927\u7684\u91c7\u7528\u4e861600\u8fd9\u4e2a\u7ef4\u5ea6. \u6240\u4ee5\u5728\u6574\u4e2a\u6a21\u578b\u8fd0\u4f5c\u8d77\u6765\u7684\u6700\u5f00\u59cb, \u6211\u4eec\u9700\u8981\u5728\u5d4c\u5165\u77e9\u9635\u4e2d\u67e5\u627e\u8d77\u59cb\u5355\u8bcd\"s\"\u5bf9\u5e94\u7684\u5d4c\u5165\u5411\u91cf. \u4f46\u5728\u5c06\u5176\u8f93\u5165\u7ed9\u6a21\u578b\u4e4b\u524d, \u8fd8\u9700\u8981\u5f15\u5165\u4f4d\u7f6e\u7f16\u7801(positional encoding), 1024\u5206\u8f93\u5165\u5e8f\u5217\u4f4d\u7f6e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u5bf9\u5e94\u4e86\u4e00\u4e2a\u4f4d\u7f6e\u7f16\u7801, \u540c\u7406\u4e8e\u8bcd\u5d4c\u5165\u77e9\u9635, \u8fd9\u4e9b\u4f4d\u7f6e\u7f16\u7801\u7ec4\u6210\u7684\u77e9\u9635\u4e5f\u662f\u6574\u4e2a\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u7684\u4e00\u90e8\u5206. \u7ecf\u5386\u524d\u9762\u76841, 2\u4e24\u6b65, \u8f93\u5165\u5355\u8bcd\u5728\u8fdb\u5165\u6a21\u578b\u7b2c\u4e00\u4e2atransformer\u6a21\u5757\u524d\u7684\u6240\u6709\u5904\u7406\u6b65\u9aa4\u5c31\u7ed3\u675f\u4e86. \u7efc\u4e0a\u6240\u8ff0, GPT2\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u6743\u503c\u77e9\u9635: \u8bcd\u5d4c\u5165\u77e9\u9635\u548c\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635. \u800c\u8f93\u5165\u5230transformer\u6a21\u5757\u4e2d\u7684\u5f20\u91cf\u5c31\u662f\u8fd9\u4e24\u4e2a\u77e9\u9635\u5bf9\u5e94\u7684\u52a0\u548c\u7ed3\u679c. transformer\u6a21\u5757\u7684\u5806\u53e0: \u6700\u5e95\u5c42\u7684transformer\u6a21\u5757\u5904\u7406\u5355\u8bcd\u7684\u6b65\u9aa4: \u9996\u5148\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u5c42\u5904\u7406, \u63a5\u7740\u5c06\u5176\u4f20\u9012\u7ed9\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8fd9\u5176\u4e2d\u5305\u542b\u6b8b\u5dee\u8fde\u63a5\u548cLayer Norm\u7b49\u5b50\u5c42\u64cd\u4f5c. \u6700\u5e95\u5c42\u7684transformer\u6a21\u5757\u5904\u7406\u7ed3\u675f\u540e, \u4f1a\u5c06\u7ed3\u679c\u5f20\u91cf\u4f20\u9012\u7ed9\u7b2c\u4e8c\u5c42\u7684transformer\u6a21\u5757, \u7ee7\u7eed\u8fdb\u884c\u8ba1\u7b97. \u6bcf\u4e00\u4e2atransformer\u6a21\u5757\u7684\u5904\u7406\u65b9\u5f0f\u90fd\u662f\u4e00\u6837\u7684, \u4e0d\u65ad\u7684\u91cd\u590d\u76f8\u540c\u7684\u6a21\u5f0f, \u4f46\u662f\u6bcf\u4e2a\u6a21\u5757\u90fd\u4f1a\u7ef4\u62a4\u81ea\u5df1\u7684self-attention\u5c42\u548cFeed Forward\u5c42\u7684\u6743\u91cd\u503c. GPT2\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u56de\u987e \u81ea\u7136\u8bed\u8a00\u7684\u542b\u4e49\u662f\u6781\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684, \u6bd4\u5982\u4e0b\u9762\u6240\u5c55\u793a\u7684\"\u673a\u5668\u4eba\u7b2c\u4e8c\u6cd5\u5219\": \u673a\u5668\u4eba\u5fc5\u987b\u9075\u5b88\u4eba\u7c7b\u7ed9\u5b83\u7684\u547d\u4ee4, \u9664\u975e\u8be5\u547d\u4ee4\u8fdd\u80cc\u4e86\u7b2c\u4e00\u6cd5\u5219. \u5728\u4e0a\u8ff0\u8bed\u53e5\u4e2d, \u6709\u4e09\u5904\u5355\u8bcd\u5177\u6709\u6307\u4ee3\u542b\u4e49, \u9664\u975e\u6211\u4eec\u77e5\u9053\u8fd9\u4e9b\u8bcd\u6240\u7cbe\u786e\u6307\u4ee3\u7684\u4e0a\u4e0b\u6587, \u5426\u5219\u6839\u672c\u4e0d\u53ef\u80fd\u7406\u89e3\u8fd9\u53e5\u8bdd\u7684\u771f\u5b9e\u8bed\u4e49. \u5f53\u6a21\u578b\u5904\u7406\u8fd9\u53e5\u8bdd\u7684\u65f6\u5019, \u6a21\u578b\u5fc5\u987b\u77e5\u9053\u4ee5\u4e0b\u4e09\u70b9: [\u5b83]\u6307\u4ee3\u673a\u5668\u4eba. [\u547d\u4ee4]\u6307\u4ee3\u524d\u534a\u53e5\u8bdd\u4e2d\u4eba\u7c7b\u7ed9\u673a\u5668\u4eba\u4e0b\u8fbe\u7684\u547d\u4ee4, \u5373[\u4eba\u7c7b\u7ed9\u5b83\u7684\u547d\u4ee4]. [\u7b2c\u4e00\u6cd5\u5219]\u6307\u4ee3\u673a\u5668\u4eba\u7b2c\u4e00\u6cd5\u5219\u7684\u5b8c\u6574\u5185\u5bb9. \u8fd9\u5c31\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6240\u505a\u7684\u5de5\u4f5c, \u5b83\u5728\u5904\u7406\u6bcf\u4e2a\u5355\u8bcd\u4e4b\u524d, \u878d\u5165\u4e86\u6a21\u578b\u5bf9\u4e8e\u7528\u6765\u89e3\u91ca\u67d0\u4e2a\u5355\u8bcd\u7684\u4e0a\u4e0b\u6587\u7684\u76f8\u5173\u5355\u8bcd\u7684\u7406\u89e3. \u5177\u4f53\u7684\u505a\u6cd5\u662f: \u7ed9\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5355\u8bcd\u90fd\u8d4b\u4e88\u4e00\u4e2a\u76f8\u5173\u5ea6\u5f97\u5206, \u672c\u8d28\u4e0a\u5c31\u662f\u6ce8\u610f\u529b\u6743\u91cd. \u770b\u4e0b\u56fe, \u4e3e\u4e2a\u4f8b\u5b50, \u6700\u4e0a\u5c42\u7684transformer\u6a21\u5757\u5728\u5904\u7406\u5355\u8bcd\"it\"\u7684\u65f6\u5019\u4f1a\u5173\u6ce8\"a robot\", \u6240\u4ee5\"a\", \"robot\", \"it\", \u8fd9\u4e09\u4e2a\u5355\u8bcd\u4e0e\u5176\u5f97\u5206\u76f8\u4e58\u52a0\u6743\u6c42\u548c\u540e\u7684\u7279\u5f81\u5411\u91cf\u4f1a\u88ab\u9001\u5165\u4e4b\u540e\u7684Feed Forward\u5c42. \u81ea\u6ce8\u610f\u529b\u673a\u5236\u6cbf\u7740\u5e8f\u5217\u7684\u6bcf\u4e00\u4e2a\u5355\u8bcd\u7684\u8def\u5f84\u8fdb\u884c\u5904\u7406, \u4e3b\u8981\u75313\u4e2a\u5411\u91cf\u7ec4\u6210: Query(\u67e5\u8be2\u5411\u91cf), \u5f53\u524d\u5355\u8bcd\u7684\u67e5\u8be2\u5411\u91cf\u88ab\u7528\u6765\u548c\u5176\u5b83\u5355\u8bcd\u7684\u952e\u5411\u91cf\u76f8\u4e58, \u4ece\u800c\u5f97\u5230\u5176\u5b83\u8bcd\u76f8\u5bf9\u4e8e\u5f53\u524d\u8bcd\u7684\u6ce8\u610f\u529b\u5f97\u5206. Key(\u952e\u5411\u91cf), \u952e\u5411\u91cf\u5c31\u50cf\u662f\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u6807\u7b7e, \u5b83\u4f7f\u6211\u4eec\u641c\u7d22\u76f8\u5173\u5355\u8bcd\u65f6\u7528\u6765\u5339\u914d\u7684\u5bf9\u8c61. Value(\u503c\u5411\u91cf), \u503c\u5411\u91cf\u662f\u5355\u8bcd\u771f\u6b63\u7684\u8868\u5f81, \u5f53\u6211\u4eec\u7b97\u51fa\u6ce8\u610f\u529b\u5f97\u5206\u540e, \u4f7f\u7528\u503c\u5411\u91cf\u8fdb\u884c\u52a0\u6743\u6c42\u548c\u5f97\u5230\u80fd\u4ee3\u8868\u5f53\u524d\u4f4d\u7f6e\u4e0a\u4e0b\u6587\u7684\u5411\u91cf. \u5982\u4e0a\u56fe\u6240\u793a, \u4e00\u4e2a\u7b80\u5355\u7684\u6bd4\u55bb\u662f\u5728\u6863\u6848\u67dc\u4e2d\u627e\u6587\u4ef6. \u67e5\u8be2\u5411\u91cfQuery\u5c31\u50cf\u4e00\u5f20\u4fbf\u5229\u8d34, \u4e0a\u9762\u5199\u7740\u4f60\u6b63\u5728\u7814\u7a76\u7684\u8bfe\u9898. \u952e\u5411\u91cfKey\u50cf\u662f\u6863\u6848\u67dc\u4e2d\u6587\u4ef6\u5939\u4e0a\u8d34\u7684\u6807\u7b7e. \u5f53\u4f60\u627e\u5230\u548c\u4fbf\u5229\u8d34\u4e0a\u6240\u5199\u76f8\u5339\u914d\u7684\u6587\u4ef6\u5939\u65f6, \u62ff\u51fa\u5bf9\u5e94\u7684\u6587\u4ef6\u5939, \u6587\u4ef6\u5939\u91cc\u7684\u4e1c\u897f\u4fbf\u662f\u503c\u5411\u91cfValue. \u5c06\u5355\u8bcd\u7684\u67e5\u8be2\u5411\u91cfQuery\u5206\u522b\u4e58\u4ee5\u6bcf\u4e2a\u6587\u4ef6\u5939\u7684\u952e\u5411\u91cfKey\uff0c\u5f97\u5230\u5404\u4e2a\u6587\u4ef6\u5939\u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u5f97\u5206Score. \u6211\u4eec\u5c06\u6bcf\u4e2a\u6587\u4ef6\u5939\u7684\u503c\u5411\u91cfValue\u4e58\u4ee5\u5176\u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u5f97\u5206Score, \u7136\u540e\u6c42\u548c, \u5f97\u5230\u6700\u7ec8\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u8f93\u51fa, \u5982\u4e0b\u56fe\u6240\u793a: \u8fd9\u6837\u5c06\u503c\u5411\u91cf\u52a0\u6743\u6df7\u5408\u5f97\u5230\u7684\u7ed3\u679c\u4e5f\u662f\u4e00\u4e2a\u5411\u91cf, \u5b83\u5c06\u517650%\u7684\u6ce8\u610f\u529b\u653e\u5728\u4e86\u5355\u8bcd\"robot\"\u4e0a, 30%\u7684\u6ce8\u610f\u529b\u653e\u5728\u4e86\"a\"\u4e0a, \u8fd8\u670919%\u7684\u6ce8\u610f\u529b\u653e\u5728\u4e86\"it\"\u4e0a. \u6a21\u578b\u7684\u8f93\u51fa: \u5f53\u6700\u540e\u4e00\u4e2atransformer\u6a21\u5757\u4ea7\u751f\u8f93\u51fa\u4e4b\u540e, \u6a21\u578b\u4f1a\u5c06\u8f93\u51fa\u5f20\u91cf\u4e58\u4e0a\u8bcd\u5d4c\u5165\u77e9\u9635: \u6211\u4eec\u77e5\u9053, \u8bcd\u5d4c\u5165\u77e9\u9635\u7684\u6bcf\u4e00\u884c\u90fd\u5bf9\u5e94\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u4e2d\u4e00\u4e2a\u5355\u8bcd\u7684\u5d4c\u5165\u5411\u91cf. \u6240\u4ee5\u8fd9\u4e2a\u4e58\u6cd5\u64cd\u4f5c\u5f97\u5230\u7684\u7ed3\u679c\u5c31\u662f\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u5f97\u5206, \u5982\u4e0b\u56fe\u6240\u793a: \u4e00\u822c\u6765\u8bf4, \u6211\u4eec\u90fd\u91c7\u7528\u8d2a\u5fc3\u7b97\u6cd5, \u9009\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u5355\u8bcd\u4f5c\u4e3a\u8f93\u51fa\u7ed3\u679c(top_k = 1). \u4f46\u662f\u4e00\u4e2a\u66f4\u597d\u7684\u7b56\u7565\u662f\u5bf9\u4e8e\u8bcd\u6c47\u8868\u4e2d\u5f97\u5206\u8f83\u9ad8\u7684\u4e00\u90e8\u5206\u5355\u8bcd, \u5c06\u5b83\u4eec\u7684\u5f97\u5206\u4f5c\u4e3a\u6982\u7387\u4ece\u6574\u4e2a\u5355\u8bcd\u5217\u8868\u4e2d\u8fdb\u884c\u62bd\u6837(\u5f97\u5206\u8d8a\u9ad8\u7684\u5355\u8bcd\u8d8a\u5bb9\u6613\u88ab\u9009\u4e2d). \u901a\u5e38\u4f1a\u7528\u4e00\u4e2a\u6298\u4e2d\u7684\u65b9\u6cd5, \u5373\u9009\u53d6top_k = 40, \u8fd9\u6837\u6a21\u578b\u4f1a\u8003\u8651\u6ce8\u610f\u529b\u5f97\u5206\u6392\u540d\u524d40\u7684\u5355\u8bcd. \u5982\u4e0a\u56fe\u6240\u793a, \u6a21\u578b\u5c31\u5b8c\u6210\u4e86\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8fed\u4ee3, \u8f93\u51fa\u4e86\u4e00\u4e2a\u5355\u8bcd. \u63a5\u4e0b\u6765\u6a21\u578b\u4f1a\u4e0d\u65ad\u7684\u8fed\u4ee3, \u76f4\u81f3\u751f\u6210\u5b8c\u6574\u7684\u5e8f\u5217(\u5e8f\u5217\u957f\u5ea6\u8fbe\u52301024\u7684\u4e0a\u9650, \u6216\u8005\u5e8f\u5217\u7684\u67d0\u4e00\u4e2a\u65f6\u95f4\u6b65\u751f\u6210\u4e86\u7ed3\u675f\u7b26). 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86GPT2\u7684\u67b6\u6784: GPT2\u53ea\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757. GPT2\u662f\u5728GPT\u57fa\u7840\u4e0a\u53d1\u5c55\u5904\u7684\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. \u5b66\u4e60\u4e86GPT2\u7684\u5de5\u4f5c\u7ec6\u8282: GPT2\u53ef\u4ee5\u5904\u7406\u6700\u957f1024\u4e2a\u5355\u8bcd\u7684\u5e8f\u5217. \u6bcf\u4e2a\u5355\u8bcd\u90fd\u4f1a\u548c\u5b83\u7684\u524d\u5e8f\u8def\u5f84\u4e00\u8d77\"\u6d41\u7ecf\"\u6240\u6709\u7684\u89e3\u7801\u5668\u6a21\u5757. GPT2\u672c\u8d28\u4e0a\u4e5f\u662f\u81ea\u56de\u5f52\u6a21\u578b. \u8f93\u5165\u5f20\u91cf\u8981\u7ecf\u5386\u8bcd\u5d4c\u5165\u77e9\u9635\u548c\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u7684\u52a0\u548c\u540e, \u624d\u80fd\u8f93\u5165\u8fdbtransformer\u6a21\u5757\u4e2d. \u5b66\u4e60\u4e86GPT2\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ec6\u8282: \u9996\u5148, GPT2\u7684\u81ea\u6ce8\u610f\u529b\u662fMasked self-attention, \u53ea\u80fd\u770b\u89c1\u5de6\u4fa7\u7684\u5e8f\u5217, \u4e0d\u80fd\u770b\u89c1\u53f3\u4fa7\u7684\u4fe1\u606f. Query, Key, Value\u8fd9\u4e09\u4e2a\u5f20\u91cf\u4e4b\u95f4\u7684\u5f62\u8c61\u5316\u7684\u4f8b\u5b50, \u751f\u52a8\u7684\u8bf4\u660e\u4e86\u5404\u81ea\u7684\u4f5c\u7528\u548c\u8fd0\u7b97\u65b9\u5f0f. \u6700\u540e\u7684\u8f93\u51fa\u53ef\u4ee5\u91c7\u7528\u591a\u4e2a\u65b9\u6cd5, \u8d2a\u5fc3\u65b9\u6848, \u6982\u7387\u5206\u5e03\u65b9\u6848, \u6216\u8005top-k\u65b9\u6848\u7b49.","title":"10 GPT2\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_pretrained_model_old/10%20GPT2%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u638c\u63e1GPT2\u7684\u67b6\u6784 \u638c\u63e1GPT2\u7684\u8bad\u7ec3\u4efb\u52a1\u548c\u6a21\u578b\u7ec6\u8282","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/10%20GPT2%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-gpt2","text":"\u4ece\u6a21\u578b\u67b6\u6784\u4e0a\u770b, GPT2\u5e76\u6ca1\u6709\u7279\u522b\u65b0\u9896\u7684\u67b6\u6784, \u5b83\u548c\u53ea\u5e26\u6709\u89e3\u7801\u5668\u6a21\u5757\u7684Transformer\u5f88\u50cf. \u6240\u8c13\u8bed\u8a00\u6a21\u578b, \u4f5c\u7528\u5c31\u662f\u6839\u636e\u5df2\u6709\u53e5\u5b50\u7684\u4e00\u90e8\u5206, \u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u4f1a\u662f\u4ec0\u4e48. \u73b0\u5b9e\u5e94\u7528\u4e2d\u5927\u5bb6\u6700\u719f\u6089\u7684\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5e94\u7528, \u5c31\u662f\u667a\u80fd\u624b\u673a\u4e0a\u7684\u8f93\u5165\u6cd5, \u5b83\u53ef\u4ee5\u6839\u636e\u5f53\u524d\u8f93\u5165\u7684\u5185\u5bb9\u667a\u80fd\u63a8\u8350\u4e0b\u4e00\u4e2a\u8981\u6253\u7684\u5b57. GPT2\u4e5f\u662f\u4e00\u4e2a\u8bed\u8a00\u9884\u6d4b\u751f\u6210\u6a21\u578b, \u53ea\u4e0d\u8fc7\u6bd4\u624b\u673a\u4e0a\u5e94\u7528\u7684\u6a21\u578b\u8981\u5927\u5f88\u591a, \u4e5f\u66f4\u52a0\u590d\u6742. \u5e38\u89c1\u7684\u624b\u673a\u7aef\u5e94\u7528\u7684\u8f93\u5165\u6cd5\u6a21\u578b\u57fa\u672c\u5360\u752850MB\u7a7a\u95f4, \u800cOpenAI\u7684\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u4e8640GB\u7684\u8d85\u5927\u6570\u636e\u96c6\u6765\u8bad\u7ec3GPT2, \u8bad\u7ec3\u540e\u7684GPT2\u6a21\u578b\u6700\u5c0f\u7684\u7248\u672c\u4e5f\u8981\u5360\u7528\u8d85\u8fc7500MB\u7a7a\u95f4\u6765\u5b58\u50a8\u6240\u6709\u7684\u53c2\u6570, \u81f3\u4e8e\u6700\u5927\u7248\u672c\u7684GPT2\u5219\u9700\u8981\u8d85\u8fc76.5GB\u7684\u5b58\u50a8\u7a7a\u95f4. \u81ea\u4eceTransformer\u95ee\u4e16\u4ee5\u6765, \u5f88\u591a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4f5c\u90fd\u5728\u5c1d\u8bd5\u5c06\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\u5806\u53e0\u7684\u5c3d\u53ef\u80fd\u9ad8, \u90a3\u7c7b\u4f3c\u7684\u6a21\u578b\u53ef\u4ee5\u5806\u53e0\u5230\u591a\u6df1\u5462? \u4e8b\u5b9e\u4e0a, \u8fd9\u4e2a\u95ee\u9898\u7684\u7b54\u6848\u4e5f\u5c31\u662f\u533a\u522b\u4e0d\u540cGPT2\u7248\u672c\u7684\u4e3b\u8981\u56e0\u7d20\u4e4b\u4e00. \u6bd4\u5982\u6700\u5c0f\u7248\u672c\u7684GPT2\u5806\u53e0\u4e8612\u5c42, \u4e2d\u53f7\u768424\u5c42, \u5927\u53f7\u768436\u5c42, \u8d85\u5927\u53f7\u7684\u5806\u53e0\u4e86\u6574\u657448\u5c42!","title":"1 GPT2\u7684\u67b6\u6784"},{"location":"06_mkdocs_pretrained_model_old/10%20GPT2%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-gpt2","text":"\u4ee5\u673a\u5668\u4eba\u7b2c\u4e00\u6cd5\u5219\u4e3a\u4f8b, \u6765\u5177\u4f53\u770bGPT2\u7684\u5de5\u4f5c\u7ec6\u8282. * \u673a\u5668\u4eba\u7b2c\u4e00\u6cd5\u5219: \u673a\u5668\u4eba\u4e0d\u5f97\u4f24\u5bb3\u4eba\u7c7b, \u6216\u8005\u76ee\u7779\u4eba\u7c7b\u5c06\u906d\u53d7\u5371\u9669\u800c\u8896\u624b\u65c1\u89c2.","title":"2 GPT2\u6a21\u578b\u7684\u7ec6\u8282"},{"location":"06_mkdocs_pretrained_model_old/10%20GPT2%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#21","text":"\u9996\u5148\u660e\u786e\u4e00\u70b9: GPT2\u7684\u5de5\u4f5c\u6d41\u7a0b\u5f88\u50cf\u4f20\u7edf\u8bed\u8a00\u6a21\u578b, \u4e00\u6b21\u53ea\u8f93\u51fa\u4e00\u4e2a\u5355\u8bcd(token). GPT2\u4e4b\u6240\u4ee5\u5728\u751f\u6210\u5f0f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0, \u662f\u56e0\u4e3a\u5728\u6bcf\u4e2a\u65b0\u5355\u8bcd(token)\u4ea7\u751f\u540e, \u8be5\u5355\u8bcd\u5c31\u88ab\u6dfb\u52a0\u5728\u4e4b\u524d\u751f\u6210\u7684\u5355\u8bcd\u5e8f\u5217\u540e\u9762, \u6dfb\u52a0\u540e\u7684\u65b0\u5e8f\u5217\u53c8\u4f1a\u6210\u4e3a\u6a21\u578b\u4e0b\u4e00\u6b65\u7684\u65b0\u8f93\u5165. \u8fd9\u79cd\u673a\u5236\u5c31\u53eb\u505a\u81ea\u56de\u5f52(auto-regression), \u5982\u4e0b\u6240\u793a: \u5176\u6b21\u660e\u786e\u4e00\u70b9: GPT2\u6a21\u578b\u662f\u4e00\u4e2a\u53ea\u5305\u542b\u4e86Transformer Decoder\u6a21\u5757\u7684\u6a21\u578b. \u548cBERT\u6a21\u578b\u76f8\u6bd4, GPT2\u7684\u89e3\u7801\u5668\u5728self-attention\u5c42\u4e0a\u6709\u4e00\u4e2a\u5173\u952e\u7684\u5dee\u5f02: \u5b83\u5c06\u540e\u9762\u7684\u5355\u8bcd(token)\u906e\u63a9\u6389, \u800cBERT\u662f\u6309\u7167\u4e00\u5b9a\u89c4\u5219\u5c06\u5355\u8bcd\u66ff\u6362\u6210[MASK]. \u4e3e\u4e2a\u4f8b\u5b50, \u5982\u679c\u6211\u4eec\u91cd\u70b9\u5173\u6ce84\u53f7\u4f4d\u7f6e\u7684\u5355\u8bcd\u53ca\u5176\u524d\u5e8f\u8def\u5f84, \u6211\u4eec\u53ef\u4ee5\u8ba9\u6a21\u578b\u53ea\u5141\u8bb8\u6ce8\u610f\u5f53\u524d\u8ba1\u7b97\u7684\u5355\u8bcd\u548c\u5b83\u4e4b\u524d\u7684\u5355\u8bcd, \u5982\u4e0b\u56fe\u6240\u793a: \u6ce8\u610f: \u80fd\u591f\u6e05\u695a\u7684\u533a\u5206BERT\u4f7f\u7528\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757(self-attention)\u548cGPT2\u4f7f\u7528\u7684\u5e26\u63a9\u7801\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757(masked self-attention)\u5f88\u91cd\u8981! \u666e\u901a\u7684self-attention\u5141\u8bb8\u6a21\u578b\u7684\u4efb\u610f\u4e00\u4e2a\u4f4d\u7f6e\u770b\u5230\u5b83\u53f3\u4fa7\u7684\u4fe1\u606f(\u4e0b\u56fe\u5de6\u4fa7), \u800c\u5e26\u63a9\u7801\u7684self-attention\u5219\u4e0d\u5141\u8bb8\u8fd9\u4e48\u505a(\u4e0b\u56fe\u53f3\u4fa7). \u5728Transformer\u539f\u59cb\u8bba\u6587\u53d1\u8868\u540e, \u4e00\u7bc7\u540d\u4e3a<< Generating Wikipedia by Summarizing Long Sequences >>\u7684\u8bba\u6587\u63d0\u51fa\u7528\u53e6\u4e00\u79cdTransformer\u6a21\u5757\u7684\u6392\u5217\u65b9\u5f0f\u6765\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21-\u5b83\u76f4\u63a5\u6254\u6389\u4e86\u7f16\u7801\u5668, \u53ea\u4fdd\u7559\u89e3\u7801\u5668. \u8fd9\u4e2a\u65e9\u671f\u7684\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u75316\u4e2aDecoder Block\u5806\u53e0\u800c\u6210: \u4e0a\u56fe\u4e2d\u6240\u6709\u7684\u89e3\u7801\u5668\u6a21\u5757\u90fd\u662f\u4e00\u6837\u7684, \u56e0\u4e3a\u53ea\u5c55\u5f00\u4e86\u7b2c\u4e00\u4e2a\u89e3\u7801\u5668\u7684\u5185\u90e8\u7ed3\u6784. \u548cGPT\u4e00\u6837, \u53ea\u4fdd\u7559\u4e86\u5e26\u63a9\u7801\u7684self-attention\u5b50\u5c42, \u548cFeed Forward\u5b50\u5c42. \u8fd9\u4e9b\u89e3\u7801\u5668\u548c\u7ecf\u5178Transformer\u539f\u59cb\u8bba\u6587\u4e2d\u7684\u89e3\u7801\u5668\u6a21\u5757\u76f8\u6bd4, \u9664\u4e86\u5220\u9664\u4e86\u7b2c\u4e8c\u4e2aEncoder-Decoder Attention\u5b50\u5c42\u5916, \u5176\u4ed6\u6784\u9020\u90fd\u4e00\u6837.","title":"2.1 \u6a21\u578b\u8fc7\u7a0b"},{"location":"06_mkdocs_pretrained_model_old/10%20GPT2%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#22-gpt2","text":"GPT2\u53ef\u4ee5\u5904\u7406\u6700\u957f1024\u4e2a\u5355\u8bcd\u7684\u5e8f\u5217. \u6bcf\u4e2a\u5355\u8bcd\u90fd\u4f1a\u548c\u5b83\u7684\u524d\u5e8f\u8def\u5f84\u4e00\u8d77\"\u6d41\u7ecf\"\u6240\u6709\u7684\u89e3\u7801\u5668\u6a21\u5757. \u5bf9\u4e8e\u751f\u6210\u5f0f\u6a21\u578b\u6765\u8bf4, \u57fa\u672c\u5de5\u4f5c\u65b9\u5f0f\u90fd\u662f\u63d0\u4f9b\u4e00\u4e2a\u9884\u5148\u5b9a\u4e49\u597d\u7684\u8d77\u59cbtoken, \u6bd4\u5982\u8bb0\u505a\"s\". \u6b64\u65f6\u6a21\u578b\u7684\u8f93\u5165\u53ea\u6709\u4e00\u4e2a\u5355\u8bcd, \u6240\u4ee5\u53ea\u6709\u8fd9\u4e2a\u5355\u8bcd\u7684\u8def\u5f84\u662f\u6d3b\u8dc3\u7684. \u5355\u8bcd\u7ecf\u8fc7\u5c42\u5c42\u5904\u7406, \u6700\u7ec8\u5f97\u5230\u4e00\u4e2a\u8bcd\u5411\u91cf. \u8be5\u5411\u91cf\u53ef\u4ee5\u5bf9\u4e8e\u8bcd\u6c47\u8868\u7684\u6bcf\u4e2a\u5355\u8bcd\u8ba1\u7b97\u51fa\u4e00\u4e2a\u6982\u7387(GPT2\u7684\u8bcd\u6c47\u8868\u4e2d\u670950000\u4e2a\u5355\u8bcd). \u5728\u672c\u4f8b\u4e2d, \u6211\u4eec\u9009\u62e9\u6982\u7387\u6700\u9ad8\u7684\u5355\u8bcd[\"The\"]\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u5355\u8bcd. \u6ce8\u610f: \u8fd9\u79cd\u9009\u62e9\u6700\u9ad8\u6982\u7387\u8f93\u51fa\u7684\u7b56\u7565\u6709\u65f6\u4f1a\u51fa\u73b0\u95ee\u9898-\u5982\u679c\u6211\u4eec\u6301\u7eed\u70b9\u51fb\u8f93\u5165\u6cd5\u63a8\u8350\u5355\u8bcd\u7684\u7b2c\u4e00\u4e2a, \u5b83\u53ef\u80fd\u4f1a\u9677\u5165\u63a8\u8350\u540c\u4e00\u4e2a\u8bcd\u7684\u5faa\u73af\u4e2d, \u53ea\u6709\u4f60\u70b9\u51fb\u7b2c\u4e8c\u4e2a\u6216\u7b2c\u4e09\u4e2a\u63a8\u8350\u8bcd, \u624d\u80fd\u8df3\u51fa\u8fd9\u79cd\u5faa\u73af. \u540c\u7406, GPT2\u6709\u4e00\u4e2atop-k\u53c2\u6570, \u6a21\u578b\u4f1a\u4ece\u6982\u7387\u6700\u5927\u7684\u524dk\u4e2a\u5355\u8bcd\u4e2d\u62bd\u6837\u9009\u53d6\u4e0b\u4e00\u4e2a\u5355\u8bcd. \u63a5\u4e0b\u6765, \u6211\u4eec\u5c06\u8f93\u51fa\u7684\u5355\u8bcd[\"The\"]\u6dfb\u52a0\u5728\u8f93\u5165\u5e8f\u5217\u7684\u5c3e\u90e8, \u4ece\u800c\u6784\u5efa\u51fa\u65b0\u7684\u8f93\u5165\u5e8f\u5217[\"s\", \"The\"], \u8ba9\u6a21\u578b\u8fdb\u884c\u4e0b\u4e00\u6b65\u7684\u9884\u6d4b: \u6b64\u65f6\u7b2c\u4e8c\u4e2a\u5355\u8bcd\u7684\u8def\u5f84\u662f\u5f53\u524d\u552f\u4e00\u6d3b\u8dc3\u7684\u8def\u5f84\u4e86. GPT2\u7684\u6bcf\u4e00\u5c42\u90fd\u4fdd\u7559\u4e86\u5b83\u4eec\u5bf9\u7b2c\u4e00\u4e2a\u5355\u8bcd\u7684\u89e3\u91ca, \u5e76\u4e14\u5c06\u8fd0\u7528\u8fd9\u4e9b\u4fe1\u606f\u5904\u7406\u7b2c\u4e8c\u4e2a\u5355\u8bcd, GPT2\u4e0d\u4f1a\u6839\u636e\u7b2c\u4e8c\u4e2a\u5355\u8bcd\u91cd\u65b0\u6765\u89e3\u91ca\u7b2c\u4e00\u4e2a\u5355\u8bcd. \u5173\u4e8e\u8f93\u5165\u7f16\u7801: \u5f53\u6211\u4eec\u66f4\u52a0\u6df1\u5165\u7684\u4e86\u89e3\u6a21\u578b\u7684\u5185\u90e8\u7ec6\u8282\u65f6, \u6700\u5f00\u59cb\u5c31\u8981\u9762\u5bf9\u6a21\u578b\u7684\u8f93\u5165, \u548c\u5176\u4ed6\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u4e00\u6837, GPT2\u540c\u6837\u4ece\u5d4c\u5165\u77e9\u9635\u4e2d\u67e5\u627e\u5355\u8bcd\u5bf9\u5e94\u7684\u5d4c\u5165\u5411\u91cf, \u8be5\u77e9\u9635(embedding matrix)\u4e5f\u662f\u6574\u4e2a\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u7684\u4e00\u90e8\u5206. \u5982\u4e0a\u56fe\u6240\u793a, \u6bcf\u4e00\u884c\u90fd\u662f\u4e00\u4e2a\u8bcd\u5d4c\u5165\u5411\u91cf: \u4e00\u4e2a\u80fd\u591f\u8868\u5f81\u67d0\u4e2a\u5355\u8bcd, \u5e76\u6355\u83b7\u5176\u8bed\u4e49\u7684\u6570\u5b57\u5411\u91cf. \u5d4c\u5165\u7684\u7ef4\u5ea6\u5927\u5c0f\u548cGPT2\u6a21\u578b\u7684\u5927\u5c0f\u76f8\u5173, \u6700\u5c0f\u7684\u6a21\u578b\u91c7\u7528\u4e86768\u8fd9\u4e2a\u7ef4\u5ea6, \u6700\u5927\u7684\u91c7\u7528\u4e861600\u8fd9\u4e2a\u7ef4\u5ea6. \u6240\u4ee5\u5728\u6574\u4e2a\u6a21\u578b\u8fd0\u4f5c\u8d77\u6765\u7684\u6700\u5f00\u59cb, \u6211\u4eec\u9700\u8981\u5728\u5d4c\u5165\u77e9\u9635\u4e2d\u67e5\u627e\u8d77\u59cb\u5355\u8bcd\"s\"\u5bf9\u5e94\u7684\u5d4c\u5165\u5411\u91cf. \u4f46\u5728\u5c06\u5176\u8f93\u5165\u7ed9\u6a21\u578b\u4e4b\u524d, \u8fd8\u9700\u8981\u5f15\u5165\u4f4d\u7f6e\u7f16\u7801(positional encoding), 1024\u5206\u8f93\u5165\u5e8f\u5217\u4f4d\u7f6e\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u5bf9\u5e94\u4e86\u4e00\u4e2a\u4f4d\u7f6e\u7f16\u7801, \u540c\u7406\u4e8e\u8bcd\u5d4c\u5165\u77e9\u9635, \u8fd9\u4e9b\u4f4d\u7f6e\u7f16\u7801\u7ec4\u6210\u7684\u77e9\u9635\u4e5f\u662f\u6574\u4e2a\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u7684\u4e00\u90e8\u5206. \u7ecf\u5386\u524d\u9762\u76841, 2\u4e24\u6b65, \u8f93\u5165\u5355\u8bcd\u5728\u8fdb\u5165\u6a21\u578b\u7b2c\u4e00\u4e2atransformer\u6a21\u5757\u524d\u7684\u6240\u6709\u5904\u7406\u6b65\u9aa4\u5c31\u7ed3\u675f\u4e86. \u7efc\u4e0a\u6240\u8ff0, GPT2\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u6743\u503c\u77e9\u9635: \u8bcd\u5d4c\u5165\u77e9\u9635\u548c\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635. \u800c\u8f93\u5165\u5230transformer\u6a21\u5757\u4e2d\u7684\u5f20\u91cf\u5c31\u662f\u8fd9\u4e24\u4e2a\u77e9\u9635\u5bf9\u5e94\u7684\u52a0\u548c\u7ed3\u679c. transformer\u6a21\u5757\u7684\u5806\u53e0: \u6700\u5e95\u5c42\u7684transformer\u6a21\u5757\u5904\u7406\u5355\u8bcd\u7684\u6b65\u9aa4: \u9996\u5148\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u5c42\u5904\u7406, \u63a5\u7740\u5c06\u5176\u4f20\u9012\u7ed9\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8fd9\u5176\u4e2d\u5305\u542b\u6b8b\u5dee\u8fde\u63a5\u548cLayer Norm\u7b49\u5b50\u5c42\u64cd\u4f5c. \u6700\u5e95\u5c42\u7684transformer\u6a21\u5757\u5904\u7406\u7ed3\u675f\u540e, \u4f1a\u5c06\u7ed3\u679c\u5f20\u91cf\u4f20\u9012\u7ed9\u7b2c\u4e8c\u5c42\u7684transformer\u6a21\u5757, \u7ee7\u7eed\u8fdb\u884c\u8ba1\u7b97. \u6bcf\u4e00\u4e2atransformer\u6a21\u5757\u7684\u5904\u7406\u65b9\u5f0f\u90fd\u662f\u4e00\u6837\u7684, \u4e0d\u65ad\u7684\u91cd\u590d\u76f8\u540c\u7684\u6a21\u5f0f, \u4f46\u662f\u6bcf\u4e2a\u6a21\u5757\u90fd\u4f1a\u7ef4\u62a4\u81ea\u5df1\u7684self-attention\u5c42\u548cFeed Forward\u5c42\u7684\u6743\u91cd\u503c. GPT2\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u56de\u987e \u81ea\u7136\u8bed\u8a00\u7684\u542b\u4e49\u662f\u6781\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\u7684, \u6bd4\u5982\u4e0b\u9762\u6240\u5c55\u793a\u7684\"\u673a\u5668\u4eba\u7b2c\u4e8c\u6cd5\u5219\": \u673a\u5668\u4eba\u5fc5\u987b\u9075\u5b88\u4eba\u7c7b\u7ed9\u5b83\u7684\u547d\u4ee4, \u9664\u975e\u8be5\u547d\u4ee4\u8fdd\u80cc\u4e86\u7b2c\u4e00\u6cd5\u5219. \u5728\u4e0a\u8ff0\u8bed\u53e5\u4e2d, \u6709\u4e09\u5904\u5355\u8bcd\u5177\u6709\u6307\u4ee3\u542b\u4e49, \u9664\u975e\u6211\u4eec\u77e5\u9053\u8fd9\u4e9b\u8bcd\u6240\u7cbe\u786e\u6307\u4ee3\u7684\u4e0a\u4e0b\u6587, \u5426\u5219\u6839\u672c\u4e0d\u53ef\u80fd\u7406\u89e3\u8fd9\u53e5\u8bdd\u7684\u771f\u5b9e\u8bed\u4e49. \u5f53\u6a21\u578b\u5904\u7406\u8fd9\u53e5\u8bdd\u7684\u65f6\u5019, \u6a21\u578b\u5fc5\u987b\u77e5\u9053\u4ee5\u4e0b\u4e09\u70b9: [\u5b83]\u6307\u4ee3\u673a\u5668\u4eba. [\u547d\u4ee4]\u6307\u4ee3\u524d\u534a\u53e5\u8bdd\u4e2d\u4eba\u7c7b\u7ed9\u673a\u5668\u4eba\u4e0b\u8fbe\u7684\u547d\u4ee4, \u5373[\u4eba\u7c7b\u7ed9\u5b83\u7684\u547d\u4ee4]. [\u7b2c\u4e00\u6cd5\u5219]\u6307\u4ee3\u673a\u5668\u4eba\u7b2c\u4e00\u6cd5\u5219\u7684\u5b8c\u6574\u5185\u5bb9. \u8fd9\u5c31\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6240\u505a\u7684\u5de5\u4f5c, \u5b83\u5728\u5904\u7406\u6bcf\u4e2a\u5355\u8bcd\u4e4b\u524d, \u878d\u5165\u4e86\u6a21\u578b\u5bf9\u4e8e\u7528\u6765\u89e3\u91ca\u67d0\u4e2a\u5355\u8bcd\u7684\u4e0a\u4e0b\u6587\u7684\u76f8\u5173\u5355\u8bcd\u7684\u7406\u89e3. \u5177\u4f53\u7684\u505a\u6cd5\u662f: \u7ed9\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5355\u8bcd\u90fd\u8d4b\u4e88\u4e00\u4e2a\u76f8\u5173\u5ea6\u5f97\u5206, \u672c\u8d28\u4e0a\u5c31\u662f\u6ce8\u610f\u529b\u6743\u91cd. \u770b\u4e0b\u56fe, \u4e3e\u4e2a\u4f8b\u5b50, \u6700\u4e0a\u5c42\u7684transformer\u6a21\u5757\u5728\u5904\u7406\u5355\u8bcd\"it\"\u7684\u65f6\u5019\u4f1a\u5173\u6ce8\"a robot\", \u6240\u4ee5\"a\", \"robot\", \"it\", \u8fd9\u4e09\u4e2a\u5355\u8bcd\u4e0e\u5176\u5f97\u5206\u76f8\u4e58\u52a0\u6743\u6c42\u548c\u540e\u7684\u7279\u5f81\u5411\u91cf\u4f1a\u88ab\u9001\u5165\u4e4b\u540e\u7684Feed Forward\u5c42. \u81ea\u6ce8\u610f\u529b\u673a\u5236\u6cbf\u7740\u5e8f\u5217\u7684\u6bcf\u4e00\u4e2a\u5355\u8bcd\u7684\u8def\u5f84\u8fdb\u884c\u5904\u7406, \u4e3b\u8981\u75313\u4e2a\u5411\u91cf\u7ec4\u6210: Query(\u67e5\u8be2\u5411\u91cf), \u5f53\u524d\u5355\u8bcd\u7684\u67e5\u8be2\u5411\u91cf\u88ab\u7528\u6765\u548c\u5176\u5b83\u5355\u8bcd\u7684\u952e\u5411\u91cf\u76f8\u4e58, \u4ece\u800c\u5f97\u5230\u5176\u5b83\u8bcd\u76f8\u5bf9\u4e8e\u5f53\u524d\u8bcd\u7684\u6ce8\u610f\u529b\u5f97\u5206. Key(\u952e\u5411\u91cf), \u952e\u5411\u91cf\u5c31\u50cf\u662f\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u7684\u6807\u7b7e, \u5b83\u4f7f\u6211\u4eec\u641c\u7d22\u76f8\u5173\u5355\u8bcd\u65f6\u7528\u6765\u5339\u914d\u7684\u5bf9\u8c61. Value(\u503c\u5411\u91cf), \u503c\u5411\u91cf\u662f\u5355\u8bcd\u771f\u6b63\u7684\u8868\u5f81, \u5f53\u6211\u4eec\u7b97\u51fa\u6ce8\u610f\u529b\u5f97\u5206\u540e, \u4f7f\u7528\u503c\u5411\u91cf\u8fdb\u884c\u52a0\u6743\u6c42\u548c\u5f97\u5230\u80fd\u4ee3\u8868\u5f53\u524d\u4f4d\u7f6e\u4e0a\u4e0b\u6587\u7684\u5411\u91cf. \u5982\u4e0a\u56fe\u6240\u793a, \u4e00\u4e2a\u7b80\u5355\u7684\u6bd4\u55bb\u662f\u5728\u6863\u6848\u67dc\u4e2d\u627e\u6587\u4ef6. \u67e5\u8be2\u5411\u91cfQuery\u5c31\u50cf\u4e00\u5f20\u4fbf\u5229\u8d34, \u4e0a\u9762\u5199\u7740\u4f60\u6b63\u5728\u7814\u7a76\u7684\u8bfe\u9898. \u952e\u5411\u91cfKey\u50cf\u662f\u6863\u6848\u67dc\u4e2d\u6587\u4ef6\u5939\u4e0a\u8d34\u7684\u6807\u7b7e. \u5f53\u4f60\u627e\u5230\u548c\u4fbf\u5229\u8d34\u4e0a\u6240\u5199\u76f8\u5339\u914d\u7684\u6587\u4ef6\u5939\u65f6, \u62ff\u51fa\u5bf9\u5e94\u7684\u6587\u4ef6\u5939, \u6587\u4ef6\u5939\u91cc\u7684\u4e1c\u897f\u4fbf\u662f\u503c\u5411\u91cfValue. \u5c06\u5355\u8bcd\u7684\u67e5\u8be2\u5411\u91cfQuery\u5206\u522b\u4e58\u4ee5\u6bcf\u4e2a\u6587\u4ef6\u5939\u7684\u952e\u5411\u91cfKey\uff0c\u5f97\u5230\u5404\u4e2a\u6587\u4ef6\u5939\u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u5f97\u5206Score. \u6211\u4eec\u5c06\u6bcf\u4e2a\u6587\u4ef6\u5939\u7684\u503c\u5411\u91cfValue\u4e58\u4ee5\u5176\u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u5f97\u5206Score, \u7136\u540e\u6c42\u548c, \u5f97\u5230\u6700\u7ec8\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u8f93\u51fa, \u5982\u4e0b\u56fe\u6240\u793a: \u8fd9\u6837\u5c06\u503c\u5411\u91cf\u52a0\u6743\u6df7\u5408\u5f97\u5230\u7684\u7ed3\u679c\u4e5f\u662f\u4e00\u4e2a\u5411\u91cf, \u5b83\u5c06\u517650%\u7684\u6ce8\u610f\u529b\u653e\u5728\u4e86\u5355\u8bcd\"robot\"\u4e0a, 30%\u7684\u6ce8\u610f\u529b\u653e\u5728\u4e86\"a\"\u4e0a, \u8fd8\u670919%\u7684\u6ce8\u610f\u529b\u653e\u5728\u4e86\"it\"\u4e0a. \u6a21\u578b\u7684\u8f93\u51fa: \u5f53\u6700\u540e\u4e00\u4e2atransformer\u6a21\u5757\u4ea7\u751f\u8f93\u51fa\u4e4b\u540e, \u6a21\u578b\u4f1a\u5c06\u8f93\u51fa\u5f20\u91cf\u4e58\u4e0a\u8bcd\u5d4c\u5165\u77e9\u9635: \u6211\u4eec\u77e5\u9053, \u8bcd\u5d4c\u5165\u77e9\u9635\u7684\u6bcf\u4e00\u884c\u90fd\u5bf9\u5e94\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u4e2d\u4e00\u4e2a\u5355\u8bcd\u7684\u5d4c\u5165\u5411\u91cf. \u6240\u4ee5\u8fd9\u4e2a\u4e58\u6cd5\u64cd\u4f5c\u5f97\u5230\u7684\u7ed3\u679c\u5c31\u662f\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u5e94\u7684\u6ce8\u610f\u529b\u5f97\u5206, \u5982\u4e0b\u56fe\u6240\u793a: \u4e00\u822c\u6765\u8bf4, \u6211\u4eec\u90fd\u91c7\u7528\u8d2a\u5fc3\u7b97\u6cd5, \u9009\u53d6\u5f97\u5206\u6700\u9ad8\u7684\u5355\u8bcd\u4f5c\u4e3a\u8f93\u51fa\u7ed3\u679c(top_k = 1). \u4f46\u662f\u4e00\u4e2a\u66f4\u597d\u7684\u7b56\u7565\u662f\u5bf9\u4e8e\u8bcd\u6c47\u8868\u4e2d\u5f97\u5206\u8f83\u9ad8\u7684\u4e00\u90e8\u5206\u5355\u8bcd, \u5c06\u5b83\u4eec\u7684\u5f97\u5206\u4f5c\u4e3a\u6982\u7387\u4ece\u6574\u4e2a\u5355\u8bcd\u5217\u8868\u4e2d\u8fdb\u884c\u62bd\u6837(\u5f97\u5206\u8d8a\u9ad8\u7684\u5355\u8bcd\u8d8a\u5bb9\u6613\u88ab\u9009\u4e2d). \u901a\u5e38\u4f1a\u7528\u4e00\u4e2a\u6298\u4e2d\u7684\u65b9\u6cd5, \u5373\u9009\u53d6top_k = 40, \u8fd9\u6837\u6a21\u578b\u4f1a\u8003\u8651\u6ce8\u610f\u529b\u5f97\u5206\u6392\u540d\u524d40\u7684\u5355\u8bcd. \u5982\u4e0a\u56fe\u6240\u793a, \u6a21\u578b\u5c31\u5b8c\u6210\u4e86\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8fed\u4ee3, \u8f93\u51fa\u4e86\u4e00\u4e2a\u5355\u8bcd. \u63a5\u4e0b\u6765\u6a21\u578b\u4f1a\u4e0d\u65ad\u7684\u8fed\u4ee3, \u76f4\u81f3\u751f\u6210\u5b8c\u6574\u7684\u5e8f\u5217(\u5e8f\u5217\u957f\u5ea6\u8fbe\u52301024\u7684\u4e0a\u9650, \u6216\u8005\u5e8f\u5217\u7684\u67d0\u4e00\u4e2a\u65f6\u95f4\u6b65\u751f\u6210\u4e86\u7ed3\u675f\u7b26).","title":"2.2 GPT2\u5de5\u4f5c\u7ec6\u8282\u63a2\u7a76"},{"location":"06_mkdocs_pretrained_model_old/10%20GPT2%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3","text":"\u5b66\u4e60\u4e86GPT2\u7684\u67b6\u6784: GPT2\u53ea\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757. GPT2\u662f\u5728GPT\u57fa\u7840\u4e0a\u53d1\u5c55\u5904\u7684\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. \u5b66\u4e60\u4e86GPT2\u7684\u5de5\u4f5c\u7ec6\u8282: GPT2\u53ef\u4ee5\u5904\u7406\u6700\u957f1024\u4e2a\u5355\u8bcd\u7684\u5e8f\u5217. \u6bcf\u4e2a\u5355\u8bcd\u90fd\u4f1a\u548c\u5b83\u7684\u524d\u5e8f\u8def\u5f84\u4e00\u8d77\"\u6d41\u7ecf\"\u6240\u6709\u7684\u89e3\u7801\u5668\u6a21\u5757. GPT2\u672c\u8d28\u4e0a\u4e5f\u662f\u81ea\u56de\u5f52\u6a21\u578b. \u8f93\u5165\u5f20\u91cf\u8981\u7ecf\u5386\u8bcd\u5d4c\u5165\u77e9\u9635\u548c\u4f4d\u7f6e\u7f16\u7801\u77e9\u9635\u7684\u52a0\u548c\u540e, \u624d\u80fd\u8f93\u5165\u8fdbtransformer\u6a21\u5757\u4e2d. \u5b66\u4e60\u4e86GPT2\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ec6\u8282: \u9996\u5148, GPT2\u7684\u81ea\u6ce8\u610f\u529b\u662fMasked self-attention, \u53ea\u80fd\u770b\u89c1\u5de6\u4fa7\u7684\u5e8f\u5217, \u4e0d\u80fd\u770b\u89c1\u53f3\u4fa7\u7684\u4fe1\u606f. Query, Key, Value\u8fd9\u4e09\u4e2a\u5f20\u91cf\u4e4b\u95f4\u7684\u5f62\u8c61\u5316\u7684\u4f8b\u5b50, \u751f\u52a8\u7684\u8bf4\u660e\u4e86\u5404\u81ea\u7684\u4f5c\u7528\u548c\u8fd0\u7b97\u65b9\u5f0f. \u6700\u540e\u7684\u8f93\u51fa\u53ef\u4ee5\u91c7\u7528\u591a\u4e2a\u65b9\u6cd5, \u8d2a\u5fc3\u65b9\u6848, \u6982\u7387\u5206\u5e03\u65b9\u6848, \u6216\u8005top-k\u65b9\u6848\u7b49.","title":"3 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u8ba4\u8bc6GPT-3\u6a21\u578b \u4e86\u89e3GPT-3\u6a21\u578b\u7684\u76f8\u5173\u7ec6\u8282\u5185\u5bb9 \u4e86\u89e3Chat-GPT\u6a21\u578b\u4e0eGPT-3\u6a21\u578b\u7684\u533a\u522b\u548c\u8054\u7cfb \u638c\u63e1\u5bf9Chat_GPT\u6a21\u578bAPI\u63a5\u53e3\u7684\u4f7f\u7528 1 GPT-3\u7684\u4ecb\u7ecd \u00b6 GPT-3 (Generative Pre-training Transformer 3) \u662f\u7531OpenAI\u5f00\u53d1\u7684\u4e00\u79cd\u5927\u578b\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u578b\uff0c\u5177\u6709\u975e\u5e38\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u80fd\u529b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c\u3002GPT-3\u80fd\u591f\u6267\u884c\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u5982\u7ffb\u8bd1\u3001\u95ee\u7b54\u3001\u6458\u8981\u751f\u6210\u3001\u6587\u672c\u5206\u7c7b\u7b49\u3002 GPT-3\u4e8e2020\u5e745\u6708\u65e9\u4e9b\u65f6\u5019\u7531Open AI\u63a8\u51fa\uff0c\u4f5c\u4e3a\u5176\u5148\u524d\u8bed\u8a00\u6a21\u578b (LM) GPT-2 \u7684\u7ee7\u627f\u8005\u3002 \u5b83\u88ab\u8ba4\u4e3a\u6bd4GPT-2\u66f4\u597d\u3001\u66f4\u5927\u3002\u4e8b\u5b9e\u4e0a\uff0c\u4e0e\u4ed6\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\uff0cOpenAI GPT-3 \u7684\u5b8c\u6574\u7248\u62e5\u6709\u5927\u7ea6 1750 \u4ebf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u662f\u8fc4\u4eca\u4e3a\u6b62\u8bad\u7ec3\u7684\u6700\u5927\u6a21\u578b\uff0c\u8fd9\u4efd 72 \u9875\u7684 \u7814\u7a76\u8bba\u6587 \u975e\u5e38\u8be6\u7ec6\u5730\u63cf\u8ff0\u4e86\u8be5\u6a21\u578b\u7684\u7279\u6027\u3001\u529f\u80fd\u3001\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002 \u4e0b\u56fe\u4e3a\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u8bad\u7ec3\u53c2\u6570\u7684\u5bf9\u6bd4\uff1a 2 GPT-3\u7684\u6a21\u578b\u7ec6\u8282 \u00b6 2.1 GPT-3\u8bad\u7ec3\u6570\u636e\u96c6 \u00b6 \u4e00\u822c\u6765\u8bf4\uff0c\u6a21\u578b\u7684\u53c2\u6570\u8d8a\u591a\uff0c\u8bad\u7ec3\u6a21\u578b\u6240\u9700\u7684\u6570\u636e\u5c31\u8d8a\u591a\u3002GPT-3\u5171\u8bad\u7ec3\u4e865\u4e2a\u4e0d\u540c\u7684\u8bed\u6599\u5927\u7ea6 45 TB \u7684\u6587\u672c\u6570\u636e\uff0c\u5206\u522b\u662f\u4f4e\u8d28\u91cf\u7684Common Crawl\uff0c\u9ad8\u8d28\u91cf\u7684WebText2\uff0cBooks1\uff0cBooks2\u548cWikipedia\uff0cGPT-3\u6839\u636e\u6570\u636e\u96c6\u7684\u4e0d\u540c\u7684\u8d28\u91cf\u8d4b\u4e88\u4e86\u4e0d\u540c\u7684\u6743\u503c\uff0c\u6743\u503c\u8d8a\u9ad8\u7684\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u8d8a\u5bb9\u6613\u62bd\u6837\u5230\uff0c\u5982\u4e0b\u8868\u6240\u793a\u3002 \u6570\u636e\u96c6 \u6570\u91cf\uff08tokens\uff09 \u8bad\u7ec3\u6570\u636e\u5360\u6bd4 Common Crawl\uff08filterd\uff09 4100\u4ebf 60% Web Text2 190\u4ebf 22% BOOK1 120\u4ebf 8% BOOK2 550\u4ebf 8% Wikipedia 30\u4ebf 2% \u4e0d\u540c\u6570\u636e\u7684\u4ecb\u7ecd\uff1a Common Crawl\u8bed\u6599\u5e93\u5305\u542b\u5728 8 \u5e74\u7684\u7f51\u7edc\u722c\u884c\u4e2d\u6536\u96c6\u7684 PB \u7ea7\u6570\u636e\u3002\u8bed\u6599\u5e93\u5305\u542b\u539f\u59cb\u7f51\u9875\u6570\u636e\u3001\u5143\u6570\u636e\u63d0\u53d6\u548c\u5e26\u6709\u5149\u8fc7\u6ee4\u7684\u6587\u672c\u63d0\u53d6\u3002 WebText2\u662f\u6765\u81ea\u5177\u6709 3+ upvotes \u7684\u5e16\u5b50\u7684\u6240\u6709\u51fa\u7ad9 Reddit \u94fe\u63a5\u7684\u7f51\u9875\u6587\u672c\u3002 Books1\u548cBooks2\u662f\u4e24\u4e2a\u57fa\u4e8e\u4e92\u8054\u7f51\u7684\u56fe\u4e66\u8bed\u6599\u5e93\u3002 \u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u9875\u9762 \u4e5f\u662f\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u4e00\u90e8\u5206\u3002 2.2 GPT-3\u6a21\u578b\u67b6\u6784 \u00b6 GPT-3 \u4e0d\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u6a21\u578b\uff0c\u800c\u662f\u4e00\u4e2a\u6a21\u578b\u7cfb\u5217\u3002\u7cfb\u5217\u4e2d\u7684\u6bcf\u4e2a\u6a21\u578b\u90fd\u6709\u4e0d\u540c\u6570\u91cf\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u4e0b\u8868\u663e\u793a\u4e86\u6bcf\u4e2a\u6a21\u578b\u3001\u4f53\u7cfb\u7ed3\u6784\u53ca\u5176\u5bf9\u5e94\u7684\u53c2\u6570\uff1a \u4e8b\u5b9e\u4e0a\uff0cOpenAI GPT-3 \u7cfb\u5217\u6a21\u578b\u4e0e GPT-2 \u6a21\u578b\u76f8\u67b6\u6784\u5b8c\u5168\u4e00\u81f4\u3002 \u6700\u5927\u7248\u672c GPT-3 175B \u6216\u201cGPT-3\u201d\u5177\u6709175\u4e2aB\u53c2\u6570\u300196\u5c42\u7684\u591a\u5934Transformer\u3001Head size\u4e3a96\u3001\u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a12288\u3001\u6587\u672c\u957f\u5ea6\u5927\u5c0f\u4e3a2048\u3002 2.3 GPT-3\u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f \u00b6 \u4efb\u4f55\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6267\u884c\u7684\u5404\u79cd\u4efb\u52a1\u53d6\u51b3\u4e8e\u5b83\u662f\u5982\u4f55\u5fae\u8c03/\u66f4\u65b0\u7684\u3002\u4f7f\u7528 GPT-3\uff0c\u53ef\u4ee5\u5b8c\u6210\u524d\u9762\u8ba8\u8bba\u7684\u8bb8\u591a NLP \u4efb\u52a1\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u3001\u68af\u5ea6\u6216\u53c2\u6570\u66f4\u65b0\uff0c\u8fd9\u4f7f\u5f97\u8be5\u6a21\u578b**\u4e0e\u4efb\u52a1\u65e0\u5173**\u3002\u56e0\u6b64\uff0cOpenAI GPT-3 \u53ef\u4ee5\u5728\u5f88\u5c11\u6216\u6ca1\u6709\u793a\u4f8b\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u8ba9\u6211\u4eec\u4e86\u89e3\u4e0e\u6a21\u578b\u76f8\u5173\u7684Few-shot/One-shot/Zero-shot\u4efb\u52a1\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u4e00\u4e9b\u793a\u4f8b\u4e86\u89e3\u5982\u4f55\u4e0e\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\u3002 Few-shot\u3001One-shot\u3001Zero-shot Learning\u7b56\u7565\u4e3b\u8981\u662f\u7528\u4e8e\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u5c11\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002 \u4ee5\u4ece\u82f1\u8bed\u5230\u6cd5\u8bed\u7684\u7ffb\u8bd1\u4efb\u52a1\u4e3a\u4f8b\uff0c\u5206\u522b\u5bf9\u6bd4\u4f20\u7edf\u7684\u5fae\u8c03\u7b56\u7565\u548cGPT-3\u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f\u3002 \u4e0b\u56fe\u662f\u4f20\u7edf\u7684\u5fae\u8c03\u7b56\u7565\uff1a \u4f20\u7edf\u7684\u5fae\u8c03\u7b56\u7565\u5b58\u5728\u95ee\u9898\uff1a \u5fae\u8c03\u9700\u8981\u5bf9\u6bcf\u4e00\u4e2a\u4efb\u52a1\u6709\u4e00\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u548c\u4efb\u52a1\u76f8\u5173\u7684\u5fae\u8c03\u3002 \u9700\u8981\u4e00\u4e2a\u76f8\u5173\u4efb\u52a1\u5927\u7684\u6570\u636e\u96c6\uff0c\u800c\u4e14\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u6807\u6ce8 \u5f53\u4e00\u4e2a\u6837\u672c\u6ca1\u6709\u51fa\u73b0\u5728\u6570\u636e\u5206\u5e03\u7684\u65f6\u5019\uff0c\u6cdb\u5316\u6027\u4e0d\u89c1\u5f97\u6bd4\u5c0f\u6a21\u578b\u8981\u597d \u4e0b\u56fe\u663e\u793a\u4e86 GPT-3 \u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f: \u5728zero-shot\u7684\u8bbe\u7f6e\u6761\u4ef6\u4e0b\uff1a\u5148\u7ed9\u51fa\u4efb\u52a1\u7684\u63cf\u8ff0\uff0c\u4e4b\u540e\u7ed9\u51fa\u4e00\u4e2a\u6d4b\u8bd5\u6570\u636e\u5bf9\u5176\u8fdb\u884c\u6d4b\u8bd5\uff0c\u76f4\u63a5\u8ba9\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u53bb\u8fdb\u884c\u4efb\u52a1\u6d4b\u8bd5\u3002 \u5728one-shot\u7684\u8bbe\u7f6e\u6761\u4ef6\u4e0b\uff1a\u5728\u9884\u8bad\u7ec3\u548c\u771f\u6b63\u7ffb\u8bd1\u7684\u6837\u672c\u4e4b\u95f4\uff0c\u63d2\u5165\u4e00\u4e2a\u6837\u672c\u505a\u6307\u5bfc\u3002\u597d\u6bd4\u8bf4\u5728\u9884\u8bad\u7ec3\u597d\u7684\u7ed3\u679c\u548c\u6240\u8981\u6267\u884c\u7684\u4efb\u52a1\u4e4b\u95f4\uff0c\u7ed9\u4e00\u4e2a\u4f8b\u5b50\uff0c\u544a\u8bc9\u6a21\u578b\u82f1\u8bed\u7ffb\u8bd1\u4e3a\u6cd5\u8bed\uff0c\u5e94\u8be5\u8fd9\u4e48\u7ffb\u8bd1\u3002 \u5728few-shot\u7684\u8bbe\u7f6e\u6761\u4ef6\u4e0b\uff1a\u5728\u9884\u8bad\u7ec3\u548c\u771f\u6b63\u7ffb\u8bd1\u7684\u6837\u672c\u4e4b\u95f4\uff0c\u63d2\u5165\u591a\u4e2a\u6837\u672c\u505a\u6307\u5bfc\u3002\u597d\u6bd4\u8bf4\u5728\u9884\u8bad\u7ec3\u597d\u7684\u7ed3\u679c\u548c\u6240\u8981\u6267\u884c\u7684\u4efb\u52a1\u4e4b\u95f4\uff0c\u7ed9\u591a\u4e2a\u4f8b\u5b50\uff0c\u544a\u8bc9\u6a21\u578b\u5e94\u8be5\u5982\u4f55\u5de5\u4f5c\u3002 \u4e0a\u8ff0\u8868\u683c\u663e\u793a\u4e86 GPT-3 \u6a21\u578b\u5728\u6267\u884c\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u548c\u5c11\u6837\u672c\u7ffb\u8bd1\u4efb\u52a1\u65f6\u7684Blue\u5206\u6570\u5bf9\u6bd4\uff08\u7528\u4e8e\u5ea6\u91cf\u540c\u4e00\u6e90\u8bed\u53e5\u7684\u81ea\u52a8\u7ffb\u8bd1\u4e0e\u4eba\u5de5\u521b\u5efa\u7684\u53c2\u8003\u7ffb\u8bd1\u4e4b\u95f4\u7684\u5dee\u5f02\uff09\u3002\u4ece\u8868\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c GPT-3 \u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f\u4e0d\u7528\u5fae\u8c03\u3001\u68af\u5ea6\u6216\u53c2\u6570\u66f4\u65b0\uff0c\u6a21\u578b\u4f9d\u7136\u53ef\u4ee5\u8fbe\u5230\u5f88\u597d\u7684\u6548\u679c\uff0c\u5176\u4e2dGPT-3\u7684few-shot\u8fd8\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524dSOTA\u3002 3 GPT-3\u548cChatGPT\u7684\u533a\u522b\u548c\u8054\u7cfb \u00b6 ChatGPT\u662f\u4e00\u79cd\u57fa\u4e8eGPT-3\u7684\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u3002\u5b83\u65e8\u5728\u4f7f\u7528 GPT-3 \u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u4e0e\u7528\u6237\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u3002\u4f8b\u5982\uff0c\u7528\u6237\u53ef\u4ee5\u5411 ChatGPT \u53d1\u9001\u6d88\u606f\uff0c\u7136\u540e ChatGPT \u4f1a\u6839\u636e\u6d88\u606f\u751f\u6210\u4e00\u6761\u56de\u590d\u3002 GPT-3 \u662f\u4e00\u4e2a\u66f4\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\uff0c\u800c ChatGPT \u5219\u662f\u4f7f\u7528 GPT-3 \u6765\u6784\u5efa\u7684\u804a\u5929\u673a\u5668\u4eba\u3002\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u662f ChatGPT \u4f9d\u8d56\u4e8e GPT-3 \u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u8fdb\u884c\u5bf9\u8bdd\u3002 4 python\u8c03\u7528ChatGPT\u6a21\u578b \u00b6 \u8981\u4f7f\u7528GPT-3\u6216ChatGPT\u6a21\u578b\uff0c\u60a8\u9700\u8981\u5148\u8bbf\u95eeOpenAI\u7684API\u7f51\u7ad9( https://beta.openai.com/docs/quickstart )\uff0c\u7136\u540e\u4eceAPI\u7f51\u7ad9\u83b7\u53d6\u4f60\u7684 API \u5bc6\u94a5\uff0c\u7136\u540e\u5219\u53ef\u4ee5\u4f7f\u7528Python\u8c03\u7528GPT-3\u6216ChatGPT\u6a21\u578b\u3002 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5b89\u88c5OpenAI\u7684Python\u5e93\uff0c\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b8c\u6210\uff1a pip install openai \u8981\u4f7f\u7528 ChatGPT \u6a21\u578b\uff0c\u60a8\u9700\u8981\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\uff1a import openai openai . api_key = \"YOUR API KEY\" model_engine = \"text-davinci-002\" # \u4f7f\u7528GPT3: model_engine =\"davinci\" prompt = \"Hi, how are you doing today?\" completions = openai . Completion . create ( engine = model_engine , prompt = prompt , max_tokens = 1024 , n = 1 , stop = None , temperature = 0.7 , ) message = completions . choices [ 0 ] . text print ( message ) \u8bf7\u6ce8\u610f\uff0c\u4e0a\u8ff0\u4ee3\u7801\u4ec5\u662f\u793a\u4f8b\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u9700\u8981\u8c03\u6574\u4ee3\u7801\u4ee5\u83b7\u5f97\u6240\u9700\u7684\u7ed3\u679c 5 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fGPT-3: GPT-3 \u662f\u7531OpenAI\u5f00\u53d1\u7684\u4e00\u79cd\u5927\u578b\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u578b\uff0c\u662f\u8fc4\u4eca\u4e3a\u6b62\u8bad\u7ec3\u7684\u6700\u5927\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c\u3002GPT-3\u80fd\u591f\u6267\u884c\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u5982\u7ffb\u8bd1\u3001\u95ee\u7b54\u3001\u6458\u8981\u751f\u6210\u3001\u6587\u672c\u5206\u7c7b\u3002 GPT3\u662f\u5728GPT2\u57fa\u7840\u4e0a\u53d1\u5c55\u5904\u7684\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. \u4e86\u89e3\u4e86GPT3\u7684\u5de5\u4f5c\u7ec6\u8282: GPT-3\u5171\u8bad\u7ec3\u4e865\u4e2a\u4e0d\u540c\u7684\u8bed\u6599\u5927\u7ea6 45 TB \u7684\u6587\u672c\u6570\u636e OpenAI GPT-3\u7cfb\u5217\u6a21\u578b\u4e0eGPT-2\u6a21\u578b\u76f8\u67b6\u6784\u5b8c\u5168\u4e00\u81f4\u3002\u6700\u5927\u7248\u672cGPT-3\u5177\u67091750 \u4ebf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\u300196\u5c42\u7684\u591a\u5934Transformer\u3001Head size\u4e3a96\u3001\u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a12288\u3001\u6587\u672c\u957f\u5ea6\u5927\u5c0f\u4e3a2048\u3002 Few-shot\u3001One-shot\u3001Zero-shot Learning\u7b56\u7565\u4e3b\u8981\u662f\u7528\u4e8e\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u5c11\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002 \u5b66\u4e60\u4e86ChatGPT\u548cGPT3\u7684\u533a\u522b\u548c\u8054\u7cfb: ChatGPT\u662f\u4e00\u79cd\u57fa\u4e8eGPT-3\u7684\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u3002\u5b83\u65e8\u5728\u4f7f\u7528GPT-3\u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u4e0e\u7528\u6237\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u3002\u4f8b\u5982\uff0c\u7528\u6237\u53ef\u4ee5\u5411ChatGPT \u53d1\u9001\u6d88\u606f\uff0c\u7136\u540eChatGPT\u4f1a\u6839\u636e\u6d88\u606f\u751f\u6210\u4e00\u6761\u56de\u590d\u3002 GPT-3\u662f\u4e00\u4e2a\u66f4\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\uff0c\u800cChatGPT\u5219\u662f\u4f7f\u7528GPT-3\u6765\u6784\u5efa\u7684\u804a\u5929\u673a\u5668\u4eba\u3002\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u662fChatGPT\u4f9d\u8d56\u4e8eGPT-3\u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u8fdb\u884c\u5bf9\u8bdd\u3002 \u5b66\u4e60\u4e86\u5982\u4f55\u7528python\u8c03\u7528ChatGPT\u6a21\u578b: pip install openai","title":"11 GPT3\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u8ba4\u8bc6GPT-3\u6a21\u578b \u4e86\u89e3GPT-3\u6a21\u578b\u7684\u76f8\u5173\u7ec6\u8282\u5185\u5bb9 \u4e86\u89e3Chat-GPT\u6a21\u578b\u4e0eGPT-3\u6a21\u578b\u7684\u533a\u522b\u548c\u8054\u7cfb \u638c\u63e1\u5bf9Chat_GPT\u6a21\u578bAPI\u63a5\u53e3\u7684\u4f7f\u7528","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-gpt-3","text":"GPT-3 (Generative Pre-training Transformer 3) \u662f\u7531OpenAI\u5f00\u53d1\u7684\u4e00\u79cd\u5927\u578b\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u578b\uff0c\u5177\u6709\u975e\u5e38\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u80fd\u529b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c\u3002GPT-3\u80fd\u591f\u6267\u884c\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u5982\u7ffb\u8bd1\u3001\u95ee\u7b54\u3001\u6458\u8981\u751f\u6210\u3001\u6587\u672c\u5206\u7c7b\u7b49\u3002 GPT-3\u4e8e2020\u5e745\u6708\u65e9\u4e9b\u65f6\u5019\u7531Open AI\u63a8\u51fa\uff0c\u4f5c\u4e3a\u5176\u5148\u524d\u8bed\u8a00\u6a21\u578b (LM) GPT-2 \u7684\u7ee7\u627f\u8005\u3002 \u5b83\u88ab\u8ba4\u4e3a\u6bd4GPT-2\u66f4\u597d\u3001\u66f4\u5927\u3002\u4e8b\u5b9e\u4e0a\uff0c\u4e0e\u4ed6\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\uff0cOpenAI GPT-3 \u7684\u5b8c\u6574\u7248\u62e5\u6709\u5927\u7ea6 1750 \u4ebf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u662f\u8fc4\u4eca\u4e3a\u6b62\u8bad\u7ec3\u7684\u6700\u5927\u6a21\u578b\uff0c\u8fd9\u4efd 72 \u9875\u7684 \u7814\u7a76\u8bba\u6587 \u975e\u5e38\u8be6\u7ec6\u5730\u63cf\u8ff0\u4e86\u8be5\u6a21\u578b\u7684\u7279\u6027\u3001\u529f\u80fd\u3001\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002 \u4e0b\u56fe\u4e3a\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u8bad\u7ec3\u53c2\u6570\u7684\u5bf9\u6bd4\uff1a","title":"1 GPT-3\u7684\u4ecb\u7ecd"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-gpt-3","text":"","title":"2 GPT-3\u7684\u6a21\u578b\u7ec6\u8282"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#21-gpt-3","text":"\u4e00\u822c\u6765\u8bf4\uff0c\u6a21\u578b\u7684\u53c2\u6570\u8d8a\u591a\uff0c\u8bad\u7ec3\u6a21\u578b\u6240\u9700\u7684\u6570\u636e\u5c31\u8d8a\u591a\u3002GPT-3\u5171\u8bad\u7ec3\u4e865\u4e2a\u4e0d\u540c\u7684\u8bed\u6599\u5927\u7ea6 45 TB \u7684\u6587\u672c\u6570\u636e\uff0c\u5206\u522b\u662f\u4f4e\u8d28\u91cf\u7684Common Crawl\uff0c\u9ad8\u8d28\u91cf\u7684WebText2\uff0cBooks1\uff0cBooks2\u548cWikipedia\uff0cGPT-3\u6839\u636e\u6570\u636e\u96c6\u7684\u4e0d\u540c\u7684\u8d28\u91cf\u8d4b\u4e88\u4e86\u4e0d\u540c\u7684\u6743\u503c\uff0c\u6743\u503c\u8d8a\u9ad8\u7684\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u8d8a\u5bb9\u6613\u62bd\u6837\u5230\uff0c\u5982\u4e0b\u8868\u6240\u793a\u3002 \u6570\u636e\u96c6 \u6570\u91cf\uff08tokens\uff09 \u8bad\u7ec3\u6570\u636e\u5360\u6bd4 Common Crawl\uff08filterd\uff09 4100\u4ebf 60% Web Text2 190\u4ebf 22% BOOK1 120\u4ebf 8% BOOK2 550\u4ebf 8% Wikipedia 30\u4ebf 2% \u4e0d\u540c\u6570\u636e\u7684\u4ecb\u7ecd\uff1a Common Crawl\u8bed\u6599\u5e93\u5305\u542b\u5728 8 \u5e74\u7684\u7f51\u7edc\u722c\u884c\u4e2d\u6536\u96c6\u7684 PB \u7ea7\u6570\u636e\u3002\u8bed\u6599\u5e93\u5305\u542b\u539f\u59cb\u7f51\u9875\u6570\u636e\u3001\u5143\u6570\u636e\u63d0\u53d6\u548c\u5e26\u6709\u5149\u8fc7\u6ee4\u7684\u6587\u672c\u63d0\u53d6\u3002 WebText2\u662f\u6765\u81ea\u5177\u6709 3+ upvotes \u7684\u5e16\u5b50\u7684\u6240\u6709\u51fa\u7ad9 Reddit \u94fe\u63a5\u7684\u7f51\u9875\u6587\u672c\u3002 Books1\u548cBooks2\u662f\u4e24\u4e2a\u57fa\u4e8e\u4e92\u8054\u7f51\u7684\u56fe\u4e66\u8bed\u6599\u5e93\u3002 \u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u9875\u9762 \u4e5f\u662f\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u4e00\u90e8\u5206\u3002","title":"2.1 GPT-3\u8bad\u7ec3\u6570\u636e\u96c6"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#22-gpt-3","text":"GPT-3 \u4e0d\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u6a21\u578b\uff0c\u800c\u662f\u4e00\u4e2a\u6a21\u578b\u7cfb\u5217\u3002\u7cfb\u5217\u4e2d\u7684\u6bcf\u4e2a\u6a21\u578b\u90fd\u6709\u4e0d\u540c\u6570\u91cf\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u4e0b\u8868\u663e\u793a\u4e86\u6bcf\u4e2a\u6a21\u578b\u3001\u4f53\u7cfb\u7ed3\u6784\u53ca\u5176\u5bf9\u5e94\u7684\u53c2\u6570\uff1a \u4e8b\u5b9e\u4e0a\uff0cOpenAI GPT-3 \u7cfb\u5217\u6a21\u578b\u4e0e GPT-2 \u6a21\u578b\u76f8\u67b6\u6784\u5b8c\u5168\u4e00\u81f4\u3002 \u6700\u5927\u7248\u672c GPT-3 175B \u6216\u201cGPT-3\u201d\u5177\u6709175\u4e2aB\u53c2\u6570\u300196\u5c42\u7684\u591a\u5934Transformer\u3001Head size\u4e3a96\u3001\u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a12288\u3001\u6587\u672c\u957f\u5ea6\u5927\u5c0f\u4e3a2048\u3002","title":"2.2 GPT-3\u6a21\u578b\u67b6\u6784"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#23-gpt-3","text":"\u4efb\u4f55\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6267\u884c\u7684\u5404\u79cd\u4efb\u52a1\u53d6\u51b3\u4e8e\u5b83\u662f\u5982\u4f55\u5fae\u8c03/\u66f4\u65b0\u7684\u3002\u4f7f\u7528 GPT-3\uff0c\u53ef\u4ee5\u5b8c\u6210\u524d\u9762\u8ba8\u8bba\u7684\u8bb8\u591a NLP \u4efb\u52a1\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u3001\u68af\u5ea6\u6216\u53c2\u6570\u66f4\u65b0\uff0c\u8fd9\u4f7f\u5f97\u8be5\u6a21\u578b**\u4e0e\u4efb\u52a1\u65e0\u5173**\u3002\u56e0\u6b64\uff0cOpenAI GPT-3 \u53ef\u4ee5\u5728\u5f88\u5c11\u6216\u6ca1\u6709\u793a\u4f8b\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u8ba9\u6211\u4eec\u4e86\u89e3\u4e0e\u6a21\u578b\u76f8\u5173\u7684Few-shot/One-shot/Zero-shot\u4efb\u52a1\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u4e00\u4e9b\u793a\u4f8b\u4e86\u89e3\u5982\u4f55\u4e0e\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\u3002 Few-shot\u3001One-shot\u3001Zero-shot Learning\u7b56\u7565\u4e3b\u8981\u662f\u7528\u4e8e\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u5c11\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002 \u4ee5\u4ece\u82f1\u8bed\u5230\u6cd5\u8bed\u7684\u7ffb\u8bd1\u4efb\u52a1\u4e3a\u4f8b\uff0c\u5206\u522b\u5bf9\u6bd4\u4f20\u7edf\u7684\u5fae\u8c03\u7b56\u7565\u548cGPT-3\u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f\u3002 \u4e0b\u56fe\u662f\u4f20\u7edf\u7684\u5fae\u8c03\u7b56\u7565\uff1a \u4f20\u7edf\u7684\u5fae\u8c03\u7b56\u7565\u5b58\u5728\u95ee\u9898\uff1a \u5fae\u8c03\u9700\u8981\u5bf9\u6bcf\u4e00\u4e2a\u4efb\u52a1\u6709\u4e00\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u548c\u4efb\u52a1\u76f8\u5173\u7684\u5fae\u8c03\u3002 \u9700\u8981\u4e00\u4e2a\u76f8\u5173\u4efb\u52a1\u5927\u7684\u6570\u636e\u96c6\uff0c\u800c\u4e14\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u6807\u6ce8 \u5f53\u4e00\u4e2a\u6837\u672c\u6ca1\u6709\u51fa\u73b0\u5728\u6570\u636e\u5206\u5e03\u7684\u65f6\u5019\uff0c\u6cdb\u5316\u6027\u4e0d\u89c1\u5f97\u6bd4\u5c0f\u6a21\u578b\u8981\u597d \u4e0b\u56fe\u663e\u793a\u4e86 GPT-3 \u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f: \u5728zero-shot\u7684\u8bbe\u7f6e\u6761\u4ef6\u4e0b\uff1a\u5148\u7ed9\u51fa\u4efb\u52a1\u7684\u63cf\u8ff0\uff0c\u4e4b\u540e\u7ed9\u51fa\u4e00\u4e2a\u6d4b\u8bd5\u6570\u636e\u5bf9\u5176\u8fdb\u884c\u6d4b\u8bd5\uff0c\u76f4\u63a5\u8ba9\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u53bb\u8fdb\u884c\u4efb\u52a1\u6d4b\u8bd5\u3002 \u5728one-shot\u7684\u8bbe\u7f6e\u6761\u4ef6\u4e0b\uff1a\u5728\u9884\u8bad\u7ec3\u548c\u771f\u6b63\u7ffb\u8bd1\u7684\u6837\u672c\u4e4b\u95f4\uff0c\u63d2\u5165\u4e00\u4e2a\u6837\u672c\u505a\u6307\u5bfc\u3002\u597d\u6bd4\u8bf4\u5728\u9884\u8bad\u7ec3\u597d\u7684\u7ed3\u679c\u548c\u6240\u8981\u6267\u884c\u7684\u4efb\u52a1\u4e4b\u95f4\uff0c\u7ed9\u4e00\u4e2a\u4f8b\u5b50\uff0c\u544a\u8bc9\u6a21\u578b\u82f1\u8bed\u7ffb\u8bd1\u4e3a\u6cd5\u8bed\uff0c\u5e94\u8be5\u8fd9\u4e48\u7ffb\u8bd1\u3002 \u5728few-shot\u7684\u8bbe\u7f6e\u6761\u4ef6\u4e0b\uff1a\u5728\u9884\u8bad\u7ec3\u548c\u771f\u6b63\u7ffb\u8bd1\u7684\u6837\u672c\u4e4b\u95f4\uff0c\u63d2\u5165\u591a\u4e2a\u6837\u672c\u505a\u6307\u5bfc\u3002\u597d\u6bd4\u8bf4\u5728\u9884\u8bad\u7ec3\u597d\u7684\u7ed3\u679c\u548c\u6240\u8981\u6267\u884c\u7684\u4efb\u52a1\u4e4b\u95f4\uff0c\u7ed9\u591a\u4e2a\u4f8b\u5b50\uff0c\u544a\u8bc9\u6a21\u578b\u5e94\u8be5\u5982\u4f55\u5de5\u4f5c\u3002 \u4e0a\u8ff0\u8868\u683c\u663e\u793a\u4e86 GPT-3 \u6a21\u578b\u5728\u6267\u884c\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u548c\u5c11\u6837\u672c\u7ffb\u8bd1\u4efb\u52a1\u65f6\u7684Blue\u5206\u6570\u5bf9\u6bd4\uff08\u7528\u4e8e\u5ea6\u91cf\u540c\u4e00\u6e90\u8bed\u53e5\u7684\u81ea\u52a8\u7ffb\u8bd1\u4e0e\u4eba\u5de5\u521b\u5efa\u7684\u53c2\u8003\u7ffb\u8bd1\u4e4b\u95f4\u7684\u5dee\u5f02\uff09\u3002\u4ece\u8868\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c GPT-3 \u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f\u4e0d\u7528\u5fae\u8c03\u3001\u68af\u5ea6\u6216\u53c2\u6570\u66f4\u65b0\uff0c\u6a21\u578b\u4f9d\u7136\u53ef\u4ee5\u8fbe\u5230\u5f88\u597d\u7684\u6548\u679c\uff0c\u5176\u4e2dGPT-3\u7684few-shot\u8fd8\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524dSOTA\u3002","title":"2.3 GPT-3\u4e09\u79cd\u8bc4\u4f30\u65b9\u5f0f"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-gpt-3chatgpt","text":"ChatGPT\u662f\u4e00\u79cd\u57fa\u4e8eGPT-3\u7684\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u3002\u5b83\u65e8\u5728\u4f7f\u7528 GPT-3 \u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u4e0e\u7528\u6237\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u3002\u4f8b\u5982\uff0c\u7528\u6237\u53ef\u4ee5\u5411 ChatGPT \u53d1\u9001\u6d88\u606f\uff0c\u7136\u540e ChatGPT \u4f1a\u6839\u636e\u6d88\u606f\u751f\u6210\u4e00\u6761\u56de\u590d\u3002 GPT-3 \u662f\u4e00\u4e2a\u66f4\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\uff0c\u800c ChatGPT \u5219\u662f\u4f7f\u7528 GPT-3 \u6765\u6784\u5efa\u7684\u804a\u5929\u673a\u5668\u4eba\u3002\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u662f ChatGPT \u4f9d\u8d56\u4e8e GPT-3 \u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u8fdb\u884c\u5bf9\u8bdd\u3002","title":"3 GPT-3\u548cChatGPT\u7684\u533a\u522b\u548c\u8054\u7cfb"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4-pythonchatgpt","text":"\u8981\u4f7f\u7528GPT-3\u6216ChatGPT\u6a21\u578b\uff0c\u60a8\u9700\u8981\u5148\u8bbf\u95eeOpenAI\u7684API\u7f51\u7ad9( https://beta.openai.com/docs/quickstart )\uff0c\u7136\u540e\u4eceAPI\u7f51\u7ad9\u83b7\u53d6\u4f60\u7684 API \u5bc6\u94a5\uff0c\u7136\u540e\u5219\u53ef\u4ee5\u4f7f\u7528Python\u8c03\u7528GPT-3\u6216ChatGPT\u6a21\u578b\u3002 \u9996\u5148\uff0c\u60a8\u9700\u8981\u5b89\u88c5OpenAI\u7684Python\u5e93\uff0c\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b8c\u6210\uff1a pip install openai \u8981\u4f7f\u7528 ChatGPT \u6a21\u578b\uff0c\u60a8\u9700\u8981\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\uff1a import openai openai . api_key = \"YOUR API KEY\" model_engine = \"text-davinci-002\" # \u4f7f\u7528GPT3: model_engine =\"davinci\" prompt = \"Hi, how are you doing today?\" completions = openai . Completion . create ( engine = model_engine , prompt = prompt , max_tokens = 1024 , n = 1 , stop = None , temperature = 0.7 , ) message = completions . choices [ 0 ] . text print ( message ) \u8bf7\u6ce8\u610f\uff0c\u4e0a\u8ff0\u4ee3\u7801\u4ec5\u662f\u793a\u4f8b\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u9700\u8981\u8c03\u6574\u4ee3\u7801\u4ee5\u83b7\u5f97\u6240\u9700\u7684\u7ed3\u679c","title":"4 python\u8c03\u7528ChatGPT\u6a21\u578b"},{"location":"06_mkdocs_pretrained_model_old/11%20GPT3%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#5","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fGPT-3: GPT-3 \u662f\u7531OpenAI\u5f00\u53d1\u7684\u4e00\u79cd\u5927\u578b\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u578b\uff0c\u662f\u8fc4\u4eca\u4e3a\u6b62\u8bad\u7ec3\u7684\u6700\u5927\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c\u3002GPT-3\u80fd\u591f\u6267\u884c\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff0c\u5982\u7ffb\u8bd1\u3001\u95ee\u7b54\u3001\u6458\u8981\u751f\u6210\u3001\u6587\u672c\u5206\u7c7b\u3002 GPT3\u662f\u5728GPT2\u57fa\u7840\u4e0a\u53d1\u5c55\u5904\u7684\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. \u4e86\u89e3\u4e86GPT3\u7684\u5de5\u4f5c\u7ec6\u8282: GPT-3\u5171\u8bad\u7ec3\u4e865\u4e2a\u4e0d\u540c\u7684\u8bed\u6599\u5927\u7ea6 45 TB \u7684\u6587\u672c\u6570\u636e OpenAI GPT-3\u7cfb\u5217\u6a21\u578b\u4e0eGPT-2\u6a21\u578b\u76f8\u67b6\u6784\u5b8c\u5168\u4e00\u81f4\u3002\u6700\u5927\u7248\u672cGPT-3\u5177\u67091750 \u4ebf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\u300196\u5c42\u7684\u591a\u5934Transformer\u3001Head size\u4e3a96\u3001\u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e3a12288\u3001\u6587\u672c\u957f\u5ea6\u5927\u5c0f\u4e3a2048\u3002 Few-shot\u3001One-shot\u3001Zero-shot Learning\u7b56\u7565\u4e3b\u8981\u662f\u7528\u4e8e\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u5c11\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002 \u5b66\u4e60\u4e86ChatGPT\u548cGPT3\u7684\u533a\u522b\u548c\u8054\u7cfb: ChatGPT\u662f\u4e00\u79cd\u57fa\u4e8eGPT-3\u7684\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u3002\u5b83\u65e8\u5728\u4f7f\u7528GPT-3\u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u4e0e\u7528\u6237\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u3002\u4f8b\u5982\uff0c\u7528\u6237\u53ef\u4ee5\u5411ChatGPT \u53d1\u9001\u6d88\u606f\uff0c\u7136\u540eChatGPT\u4f1a\u6839\u636e\u6d88\u606f\u751f\u6210\u4e00\u6761\u56de\u590d\u3002 GPT-3\u662f\u4e00\u4e2a\u66f4\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\uff0c\u800cChatGPT\u5219\u662f\u4f7f\u7528GPT-3\u6765\u6784\u5efa\u7684\u804a\u5929\u673a\u5668\u4eba\u3002\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u662fChatGPT\u4f9d\u8d56\u4e8eGPT-3\u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\u6765\u8fdb\u884c\u5bf9\u8bdd\u3002 \u5b66\u4e60\u4e86\u5982\u4f55\u7528python\u8c03\u7528ChatGPT\u6a21\u578b: pip install openai","title":"5 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/12%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u95f4\u7684\u4e0d\u540c\u70b9. \u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u6bd4\u8f83\u4e0b\u7684\u5404\u81ea\u4f18\u70b9\u548c\u7f3a\u70b9. 1 BERT, GPT, ELMo\u4e4b\u95f4\u7684\u4e0d\u540c\u70b9 \u00b6 \u5173\u4e8e\u7279\u5f81\u63d0\u53d6\u5668: ELMo\u91c7\u7528\u4e24\u90e8\u5206\u53cc\u5c42\u53cc\u5411LSTM\u8fdb\u884c\u7279\u5f81\u63d0\u53d6, \u7136\u540e\u518d\u8fdb\u884c\u7279\u5f81\u62fc\u63a5\u6765\u878d\u5408\u8bed\u4e49\u4fe1\u606f. GPT\u548cBERT\u91c7\u7528Transformer\u8fdb\u884c\u7279\u5f81\u63d0\u53d6. \u5f88\u591aNLP\u4efb\u52a1\u8868\u660eTransformer\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a\u4e8eLSTM, \u5bf9\u4e8eELMo\u800c\u8a00, \u91c7\u75281\u5c42\u9759\u6001token embedding + 2\u5c42LSTM, \u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u5355/\u53cc\u5411\u8bed\u8a00\u6a21\u578b: \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709GPT\u91c7\u7528\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u800cELMo\u548cBERT\u90fd\u91c7\u7528\u53cc\u5411\u8bed\u8a00\u6a21\u578b. ELMo\u867d\u7136\u88ab\u8ba4\u4e3a\u91c7\u7528\u4e86\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u7136\u540e\u8fdb\u884c\u7279\u5f81\u62fc\u63a5, \u8fd9\u79cd\u878d\u5408\u7279\u5f81\u7684\u80fd\u529b\u6bd4BERT\u4e00\u4f53\u5316\u7684\u878d\u5408\u7279\u5f81\u65b9\u5f0f\u5f31. \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709ELMo\u6ca1\u6709\u91c7\u7528Transformer. GPT\u548cBERT\u90fd\u6e90\u4e8eTransformer\u67b6\u6784, GPT\u7684\u5355\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86\u7ecf\u8fc7\u4fee\u6539\u540e\u7684Decoder\u6a21\u5757, Decoder\u91c7\u7528\u4e86look-ahead mask, \u53ea\u80fd\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u672a\u6765\u7684\u4fe1\u606f\u90fd\u88abmask\u6389\u4e86. \u800cBERT\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86Encoder\u6a21\u5757, Encoder\u53ea\u91c7\u7528\u4e86padding mask, \u53ef\u4ee5\u540c\u65f6\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u4ee5\u53cacontext after\u4e0b\u6587\u4fe1\u606f. 2 BERT, GPT, ELMo\u5404\u81ea\u7684\u4f18\u70b9\u548c\u7f3a\u70b9 \u00b6 ELMo: \u4f18\u70b9: \u4ece\u65e9\u671f\u7684Word2Vec\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u5927\u7f3a\u70b9\u51fa\u53d1, \u8fdb\u884c\u6539\u8fdb, \u8fd9\u4e00\u7f3a\u70b9\u5c31\u662f\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. ELMo\u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574word embedding, \u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u7f3a\u70b9: ELMo\u4f7f\u7528LSTM\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. ELMo\u4f7f\u7528\u5411\u91cf\u62fc\u63a5\u7684\u65b9\u5f0f\u878d\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. GPT: \u4f18\u70b9: GPT\u4f7f\u7528\u4e86Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u7f3a\u70b9: GPT\u53ea\u4f7f\u7528\u4e86\u5355\u5411Decoder, \u65e0\u6cd5\u878d\u5408\u672a\u6765\u7684\u4fe1\u606f. BERT: \u4f18\u70b9: BERT\u4f7f\u7528\u4e86\u53cc\u5411Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u6dfb\u52a0\u4e86\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1, MLM + NSP\u7684\u591a\u4efb\u52a1\u65b9\u5f0f\u8fdb\u884c\u6a21\u578b\u9884\u8bad\u7ec3. \u7f3a\u70b9: \u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u91cf\u592a\u591a, \u9700\u8981\u7684\u6570\u636e\u548c\u7b97\u529b\u8981\u6c42\u8fc7\u9ad8, \u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e94\u7528\u573a\u666f\u8981\u6c42\u9ad8. \u66f4\u9002\u5408\u7528\u4e8e\u8bed\u8a00\u5d4c\u5165\u8868\u8fbe, \u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u4efb\u52a1, \u4e0d\u9002\u5408\u7528\u4e8e\u751f\u6210\u5f0f\u7684\u4efb\u52a1. 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86BERT, GPT, ELMo\u4e4b\u95f4\u7684\u533a\u522b: * \u4e09\u8005\u6240\u9009\u53d6\u7684\u7279\u5f81\u63d0\u53d6\u5668\u4e0d\u540c. * BERT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757. * GPT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757. * ELMo\u91c7\u7528\u7684\u53cc\u5c42\u53cc\u5411LSTM\u6a21\u5757. \u4e09\u8005\u6240\u91c7\u7528\u7684\u8bed\u8a00\u6a21\u578b\u5355/\u53cc\u5411\u4e0d\u540c. BERT\u91c7\u7528\u7684\u662f\u6700\u5f7b\u5e95\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u53ef\u4ee5\u540c\u65f6\u5173\u6ce8context before\u548ccontext after. GPT\u91c7\u7528\u7684\u662f\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5373Transformer\u4e2d\u7684Decoder, \u7531\u4e8e\u91c7\u7528\u4e86mask\u673a\u5236, \u6240\u4ee5\u672a\u6765\u4fe1\u606fcontext after\u90fd\u4e0d\u53ef\u89c1. ELMo\u8868\u9762\u4e0a\u88ab\u8ba4\u4e3a\u662f\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411LSTM\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u5728\u8fdb\u884c\u7b80\u5355\u7684\u62fc\u63a5\u878d\u5408.","title":"12 BERT GPT ELMo\u6a21\u578b\u7684\u5bf9\u6bd4"},{"location":"06_mkdocs_pretrained_model_old/12%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#_1","text":"\u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u95f4\u7684\u4e0d\u540c\u70b9. \u7406\u89e3BERT, GPT, ELMo\u76f8\u4e92\u6bd4\u8f83\u4e0b\u7684\u5404\u81ea\u4f18\u70b9\u548c\u7f3a\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/12%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#1-bert-gpt-elmo","text":"\u5173\u4e8e\u7279\u5f81\u63d0\u53d6\u5668: ELMo\u91c7\u7528\u4e24\u90e8\u5206\u53cc\u5c42\u53cc\u5411LSTM\u8fdb\u884c\u7279\u5f81\u63d0\u53d6, \u7136\u540e\u518d\u8fdb\u884c\u7279\u5f81\u62fc\u63a5\u6765\u878d\u5408\u8bed\u4e49\u4fe1\u606f. GPT\u548cBERT\u91c7\u7528Transformer\u8fdb\u884c\u7279\u5f81\u63d0\u53d6. \u5f88\u591aNLP\u4efb\u52a1\u8868\u660eTransformer\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a\u4e8eLSTM, \u5bf9\u4e8eELMo\u800c\u8a00, \u91c7\u75281\u5c42\u9759\u6001token embedding + 2\u5c42LSTM, \u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u6709\u9650. \u5355/\u53cc\u5411\u8bed\u8a00\u6a21\u578b: \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709GPT\u91c7\u7528\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u800cELMo\u548cBERT\u90fd\u91c7\u7528\u53cc\u5411\u8bed\u8a00\u6a21\u578b. ELMo\u867d\u7136\u88ab\u8ba4\u4e3a\u91c7\u7528\u4e86\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u7136\u540e\u8fdb\u884c\u7279\u5f81\u62fc\u63a5, \u8fd9\u79cd\u878d\u5408\u7279\u5f81\u7684\u80fd\u529b\u6bd4BERT\u4e00\u4f53\u5316\u7684\u878d\u5408\u7279\u5f81\u65b9\u5f0f\u5f31. \u4e09\u8005\u4e4b\u4e2d, \u53ea\u6709ELMo\u6ca1\u6709\u91c7\u7528Transformer. GPT\u548cBERT\u90fd\u6e90\u4e8eTransformer\u67b6\u6784, GPT\u7684\u5355\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86\u7ecf\u8fc7\u4fee\u6539\u540e\u7684Decoder\u6a21\u5757, Decoder\u91c7\u7528\u4e86look-ahead mask, \u53ea\u80fd\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u672a\u6765\u7684\u4fe1\u606f\u90fd\u88abmask\u6389\u4e86. \u800cBERT\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u4e86Encoder\u6a21\u5757, Encoder\u53ea\u91c7\u7528\u4e86padding mask, \u53ef\u4ee5\u540c\u65f6\u770b\u5230context before\u4e0a\u6587\u4fe1\u606f, \u4ee5\u53cacontext after\u4e0b\u6587\u4fe1\u606f.","title":"1 BERT, GPT, ELMo\u4e4b\u95f4\u7684\u4e0d\u540c\u70b9"},{"location":"06_mkdocs_pretrained_model_old/12%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#2-bert-gpt-elmo","text":"ELMo: \u4f18\u70b9: \u4ece\u65e9\u671f\u7684Word2Vec\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u5927\u7f3a\u70b9\u51fa\u53d1, \u8fdb\u884c\u6539\u8fdb, \u8fd9\u4e00\u7f3a\u70b9\u5c31\u662f\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. ELMo\u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574word embedding, \u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u7f3a\u70b9: ELMo\u4f7f\u7528LSTM\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. ELMo\u4f7f\u7528\u5411\u91cf\u62fc\u63a5\u7684\u65b9\u5f0f\u878d\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u80fd\u529b\u5f31\u4e8eTransformer. GPT: \u4f18\u70b9: GPT\u4f7f\u7528\u4e86Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u7f3a\u70b9: GPT\u53ea\u4f7f\u7528\u4e86\u5355\u5411Decoder, \u65e0\u6cd5\u878d\u5408\u672a\u6765\u7684\u4fe1\u606f. BERT: \u4f18\u70b9: BERT\u4f7f\u7528\u4e86\u53cc\u5411Transformer\u63d0\u53d6\u7279\u5f81, \u4f7f\u5f97\u6a21\u578b\u80fd\u529b\u5927\u5e45\u63d0\u5347. \u6dfb\u52a0\u4e86\u4e24\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1, MLM + NSP\u7684\u591a\u4efb\u52a1\u65b9\u5f0f\u8fdb\u884c\u6a21\u578b\u9884\u8bad\u7ec3. \u7f3a\u70b9: \u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u91cf\u592a\u591a, \u9700\u8981\u7684\u6570\u636e\u548c\u7b97\u529b\u8981\u6c42\u8fc7\u9ad8, \u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5e94\u7528\u573a\u666f\u8981\u6c42\u9ad8. \u66f4\u9002\u5408\u7528\u4e8e\u8bed\u8a00\u5d4c\u5165\u8868\u8fbe, \u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u4efb\u52a1, \u4e0d\u9002\u5408\u7528\u4e8e\u751f\u6210\u5f0f\u7684\u4efb\u52a1.","title":"2 BERT, GPT, ELMo\u5404\u81ea\u7684\u4f18\u70b9\u548c\u7f3a\u70b9"},{"location":"06_mkdocs_pretrained_model_old/12%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html#3","text":"\u5b66\u4e60\u4e86BERT, GPT, ELMo\u4e4b\u95f4\u7684\u533a\u522b: * \u4e09\u8005\u6240\u9009\u53d6\u7684\u7279\u5f81\u63d0\u53d6\u5668\u4e0d\u540c. * BERT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757. * GPT\u91c7\u7528\u7684\u662fTransformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757. * ELMo\u91c7\u7528\u7684\u53cc\u5c42\u53cc\u5411LSTM\u6a21\u5757. \u4e09\u8005\u6240\u91c7\u7528\u7684\u8bed\u8a00\u6a21\u578b\u5355/\u53cc\u5411\u4e0d\u540c. BERT\u91c7\u7528\u7684\u662f\u6700\u5f7b\u5e95\u7684\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u53ef\u4ee5\u540c\u65f6\u5173\u6ce8context before\u548ccontext after. GPT\u91c7\u7528\u7684\u662f\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5373Transformer\u4e2d\u7684Decoder, \u7531\u4e8e\u91c7\u7528\u4e86mask\u673a\u5236, \u6240\u4ee5\u672a\u6765\u4fe1\u606fcontext after\u90fd\u4e0d\u53ef\u89c1. ELMo\u8868\u9762\u4e0a\u88ab\u8ba4\u4e3a\u662f\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4f46\u5b9e\u9645\u4e0a\u662f\u5de6\u53f3\u4e24\u4e2a\u5355\u5411LSTM\u6a21\u578b\u5206\u522b\u63d0\u53d6\u7279\u5f81, \u5728\u8fdb\u884c\u7b80\u5355\u7684\u62fc\u63a5\u878d\u5408.","title":"3 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1\u5176\u4ed6\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 1 Encoder\u6a21\u5757 \u00b6 1.1 Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u00b6 \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42. 1.2 \u5173\u4e8eEncoder Block \u00b6 \u5728Transformer\u67b6\u6784\u4e2d, 6\u4e2a\u4e00\u6a21\u4e00\u6837\u7684Encoder Block\u5c42\u5c42\u5806\u53e0\u5728\u4e00\u8d77, \u5171\u540c\u7ec4\u6210\u5b8c\u6574\u7684Encoder, \u56e0\u6b64\u5256\u6790\u4e00\u4e2aBlock\u5c31\u53ef\u4ee5\u5bf9\u6574\u4e2aEncoder\u7684\u5185\u90e8\u7ed3\u6784\u6709\u6e05\u6670\u7684\u8ba4\u8bc6. 1.3 \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42(self-attention) \u00b6 \u9996\u5148\u6765\u770bself-attention\u7684\u8ba1\u7b97\u89c4\u5219\u56fe: \u4e0a\u8ff0attention\u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u5c06query\u548ckey-value\u952e\u503c\u5bf9\u7684\u4e00\u7ec4\u96c6\u5408\u6620\u5c04\u5230\u8f93\u51fa, \u8f93\u51fa\u88ab\u8ba1\u7b97\u4e3avalues\u7684\u52a0\u6743\u548c, \u5176\u4e2d\u5206\u914d\u7ed9\u6bcf\u4e2avalue\u7684\u6743\u91cd\u7531query\u4e0e\u5bf9\u5e94key\u7684\u76f8\u4f3c\u6027\u51fd\u6570\u8ba1\u7b97\u5f97\u6765. \u8fd9\u79cdattention\u7684\u5f62\u5f0f\u88ab\u79f0\u4e3aScaled Dot-Product Attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V \u6240\u8c13\u7684\u591a\u5934self-attention\u5c42, \u5219\u662f\u5148\u5c06Q, K, V\u7ecf\u8fc7\u53c2\u6570\u77e9\u9635\u8fdb\u884c\u6620\u5c04, \u518d\u505aself-attention, \u6700\u540e\u5c06\u7ed3\u679c\u62fc\u63a5\u8d77\u6765\u9001\u5165\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u5373\u53ef. \u4e0a\u8ff0\u7684\u591a\u5934self-attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: MultiHead(Q,K,V)=Concat(head_1,\\cdots , head_h)W^O\\\\\\\\ where \\; head_i= Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})\\\\\\\\ \u5176\u4e2d\\; W_{i}^Q \\in \\Bbb{R}^{d_{model}\\times d_k} ,W_{i}^K \\in \\Bbb{R}^{d_{model}\\times d_k} , W_{i}^V \\in \\Bbb{R}^{d_{model}\\times d_v} ,W_{i}^O \\in \\Bbb{R}^{hd_v\\times d_{model}} MultiHead(Q,K,V)=Concat(head_1,\\cdots , head_h)W^O\\\\\\\\ where \\; head_i= Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})\\\\\\\\ \u5176\u4e2d\\; W_{i}^Q \\in \\Bbb{R}^{d_{model}\\times d_k} ,W_{i}^K \\in \\Bbb{R}^{d_{model}\\times d_k} , W_{i}^V \\in \\Bbb{R}^{d_{model}\\times d_v} ,W_{i}^O \\in \\Bbb{R}^{hd_v\\times d_{model}} \u591a\u5934self-attention\u5c42\u7684\u4f5c\u7528: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Multi-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u603b\u4f53\u8ba1\u7b97\u91cf\u548c\u5355\u4e00head\u76f8\u540c\u7684\u60c5\u51b5\u4e0b, \u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757, \u7531\u4e24\u4e2a\u7ebf\u6027\u53d8\u6362\u7ec4\u6210, \u4e2d\u95f4\u6709\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: $$ FFN(x)=\\max(0,xW_1+b_1)W_2+b_2 $$ \u6ce8\u610f: \u539f\u7248\u8bba\u6587\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ef4\u5ea6\u5747\u4e3ad_model = 512, \u5c42\u5185\u7684\u8fde\u63a5\u7ef4\u5ea6d_ff = 2048, \u5747\u91c7\u75284\u500d\u7684\u5927\u5c0f\u5173\u7cfb. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u5355\u7eaf\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5e76\u4e0d\u8db3\u4ee5\u63d0\u53d6\u5230\u7406\u60f3\u7684\u7279\u5f81, \u56e0\u6b64\u589e\u52a0\u5168\u8fde\u63a5\u5c42\u6765\u63d0\u5347\u7f51\u7edc\u7684\u80fd\u529b. 1.4 Decoder\u6a21\u5757 \u00b6 Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b\u4e09\u4e2a\u5b50\u5c42. \u4e00\u4e2a\u591a\u5934self-attention\u5c42 \u4e00\u4e2aEncoder-Decoder attention\u5c42 \u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42 Decoder Block\u4e2d\u7684\u591a\u5934self-attention\u5c42 Decoder\u4e2d\u7684\u591a\u5934self-attention\u5c42\u4e0eEncoder\u6a21\u5757\u4e00\u81f4, \u4f46\u9700\u8981\u6ce8\u610f\u7684\u662fDecoder\u6a21\u5757\u7684\u591a\u5934self-attention\u9700\u8981\u505alook-ahead-mask, \u56e0\u4e3a\u5728\u9884\u6d4b\u7684\u65f6\u5019\"\u4e0d\u80fd\u770b\u89c1\u672a\u6765\u7684\u4fe1\u606f\", \u6240\u4ee5\u8981\u5c06\u5f53\u524d\u7684token\u548c\u4e4b\u540e\u7684token\u5168\u90e8mask. Decoder Block\u4e2d\u7684Encoder-Decoder attention\u5c42 \u8fd9\u4e00\u5c42\u533a\u522b\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684Q = K = V, \u6b64\u5904\u77e9\u9635Q\u6765\u6e90\u4e8eDecoder\u7aef\u7ecf\u8fc7\u4e0a\u4e00\u4e2aDecoder Block\u7684\u8f93\u51fa, \u800c\u77e9\u9635K, V\u5219\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa, \u9020\u6210\u4e86Q != K = V\u7684\u60c5\u51b5. \u8fd9\u6837\u8bbe\u8ba1\u662f\u4e3a\u4e86\u8ba9Decoder\u7aef\u7684token\u80fd\u591f\u7ed9\u4e88Encoder\u7aef\u5bf9\u5e94\u7684token\u66f4\u591a\u7684\u5173\u6ce8. Decoder Block\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6b64\u5904\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u6a21\u5757\u4e2d\u7684\u5b8c\u5168\u4e00\u6837. Decoder Block\u4e2d\u67092\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u4f5c\u7528: \u591a\u5934self-attention\u5c42\u662f\u4e3a\u4e86\u62df\u5408Decoder\u7aef\u81ea\u8eab\u7684\u4fe1\u606f, \u800cEncoder-Decoder attention\u5c42\u662f\u4e3a\u4e86\u6574\u5408Encoder\u548cDecoder\u7684\u4fe1\u606f. 1.5 Add & Norm\u6a21\u5757 \u00b6 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5177\u4f53\u6765\u8bf4Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, Norm\u8868\u793aLayerNorm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5177\u4f53\u7684\u6570\u5b66\u8868\u8fbe\u5f62\u5f0f\u4e3a: LayerNorm(x + Sublayer(x)), \u5176\u4e2dSublayer(x)\u4e3a\u5b50\u5c42\u7684\u8f93\u51fa. Add\u6b8b\u5dee\u8fde\u63a5\u7684\u4f5c\u7528: \u548c\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u8fde\u63a5\u4f5c\u7528\u4e00\u81f4, \u90fd\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u4f20\u9012\u7684\u66f4\u6df1, \u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. \u8bd5\u9a8c\u8868\u660e\u6b8b\u5dee\u8fde\u63a5\u7684\u786e\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u73b0. Norm\u7684\u4f5c\u7528: \u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u989d\u589e\u52a0, \u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u5927, \u8fc7\u5c0f, \u65b9\u5dee\u53d8\u5927\u7b49\u73b0\u8c61, \u8fd9\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38, \u6a21\u578b\u7684\u6536\u655b\u975e\u5e38\u6162. \u56e0\u6b64\u5bf9\u6bcf\u4e00\u5c42\u8ba1\u7b97\u540e\u7684\u6570\u503c\u8fdb\u884c\u89c4\u8303\u5316\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u8868\u73b0. 1.6 \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding \u00b6 Transformer\u4e2d\u76f4\u63a5\u91c7\u7528\u6b63\u5f26\u51fd\u6570\u548c\u4f59\u5f26\u51fd\u6570\u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f, \u5982\u4e0b\u56fe\u6240\u793a: $$ PE_{(pos, 2i)}=\\sin(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ PE_{(pos, 2i+1)}=\\cos(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ $$ \u9700\u8981\u6ce8\u610f: \u4e09\u89d2\u51fd\u6570\u5e94\u7528\u5728\u6b64\u5904\u7684\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u70b9, \u56e0\u4e3a\u5bf9\u4e8e\u4efb\u610f\u7684PE(pos+k), \u90fd\u53ef\u4ee5\u8868\u793a\u4e3aPE(pos)\u7684\u7ebf\u6027\u51fd\u6570, \u5927\u5927\u65b9\u4fbf\u8ba1\u7b97. \u800c\u4e14\u5468\u671f\u6027\u51fd\u6570\u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u4e5f\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b. $$ \\sin(\\alpha+\\beta)=\\sin(\\alpha)\\cos(\\beta)+\\cos(\\alpha)\\sin(\\beta)\\\\ \\cos(\\alpha+\\beta)=\\cos(\\alpha)\\cos(\\beta)-\\sin(\\alpha)\\sin(\\beta) $$ 2 \u5c0f\u7ed3 \u00b6 Encoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e24\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u7684\u662f\u4e00\u79cdScaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Mul ti-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u6bd4\u5355\u4e00head\u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u662f\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u7ec4\u6210, \u7ebf\u6027\u53d8\u6362\u4e2d\u95f4\u589e\u6dfb\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5177\u4f53\u7684\u7ef4\u5ea6\u91c7\u75284\u500d\u5173\u7cfb, \u5373\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684d_model=512, \u5219\u5c42\u5185\u7684\u53d8\u6362\u7ef4\u5ea6d_ff=2048. Decoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b3\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, Encoder-Decoder Attention\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u548cEncoder\u6a21\u5757\u4e00\u6837\u7684Scaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u6700\u5927\u7684 \u533a\u522b\u5728\u4e8e\u9700\u8981\u6dfb\u52a0look-ahead-mask, \u5373\u906e\u63a9\"\u672a\u6765\u7684\u4fe1\u606f\". Encoder-Decoder Attention\u5c42\u548c\u4e0a\u4e00\u5c42\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u6700\u4e3b\u8981\u7684\u533a\u522b\u5728\u4e8eQ != K = V, \u77e9\u9635Q\u6765\u6e90\u4e8e\u4e0a\u4e00\u5c42Decoder Block\u7684\u8f93\u51fa, \u540c\u65f6K, V\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u4e2d\u5b8c\u5168\u4e00\u6837. Add & Norm\u6a21\u5757 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, \u4f5c\u7528\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u65e0\u635f\u8017\u7684\u4f20\u9012\u7684\u66f4\u6df1, \u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. Norm\u8868\u793aLayerNorm, \u5c42\u7ea7\u522b\u7684\u6570\u503c\u6807\u51c6\u5316\u64cd\u4f5c, \u4f5c\u7528\u662f\u9632\u6b62\u53c2\u6570\u8fc7\u5927\u8fc7\u5c0f\u5bfc\u81f4\u7684\u5b66\u4e60\u8fc7\u7a0b\u5f02\u5e38, \u6a21\u578b\u6536\u655b\u7279\u522b\u6162\u7684\u95ee\u9898. \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding Transformer\u4e2d\u91c7\u7528\u4e09\u89d2\u51fd\u6570\u6765\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801. \u56e0\u4e3a\u4e09\u89d2\u51fd\u6570\u662f\u5468\u671f\u6027\u51fd\u6570, \u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u800c\u4e14\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u53ef\u4ee5\u5bf9\u5e8f\u5217\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u7f16\u7801\u7684\u91cd\u8981\u7a0b\u5ea6\u540c\u7b49\u770b\u5f85. \u672c\u8282\u5e38\u89c1\u95ee\u7b54 Transformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"2 Transformer \u5404\u5b50\u6a21\u5757\u4f5c\u7528 formd\u516c\u5f0fbak"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#_1","text":"\u638c\u63e1Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1\u5176\u4ed6\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#1-encoder","text":"","title":"1 Encoder\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#11-encoder","text":"\u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42.","title":"1.1 Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528:"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#12-encoder-block","text":"\u5728Transformer\u67b6\u6784\u4e2d, 6\u4e2a\u4e00\u6a21\u4e00\u6837\u7684Encoder Block\u5c42\u5c42\u5806\u53e0\u5728\u4e00\u8d77, \u5171\u540c\u7ec4\u6210\u5b8c\u6574\u7684Encoder, \u56e0\u6b64\u5256\u6790\u4e00\u4e2aBlock\u5c31\u53ef\u4ee5\u5bf9\u6574\u4e2aEncoder\u7684\u5185\u90e8\u7ed3\u6784\u6709\u6e05\u6670\u7684\u8ba4\u8bc6.","title":"1.2 \u5173\u4e8eEncoder Block"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#13-self-attention","text":"\u9996\u5148\u6765\u770bself-attention\u7684\u8ba1\u7b97\u89c4\u5219\u56fe: \u4e0a\u8ff0attention\u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u5c06query\u548ckey-value\u952e\u503c\u5bf9\u7684\u4e00\u7ec4\u96c6\u5408\u6620\u5c04\u5230\u8f93\u51fa, \u8f93\u51fa\u88ab\u8ba1\u7b97\u4e3avalues\u7684\u52a0\u6743\u548c, \u5176\u4e2d\u5206\u914d\u7ed9\u6bcf\u4e2avalue\u7684\u6743\u91cd\u7531query\u4e0e\u5bf9\u5e94key\u7684\u76f8\u4f3c\u6027\u51fd\u6570\u8ba1\u7b97\u5f97\u6765. \u8fd9\u79cdattention\u7684\u5f62\u5f0f\u88ab\u79f0\u4e3aScaled Dot-Product Attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V \u6240\u8c13\u7684\u591a\u5934self-attention\u5c42, \u5219\u662f\u5148\u5c06Q, K, V\u7ecf\u8fc7\u53c2\u6570\u77e9\u9635\u8fdb\u884c\u6620\u5c04, \u518d\u505aself-attention, \u6700\u540e\u5c06\u7ed3\u679c\u62fc\u63a5\u8d77\u6765\u9001\u5165\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u5373\u53ef. \u4e0a\u8ff0\u7684\u591a\u5934self-attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: MultiHead(Q,K,V)=Concat(head_1,\\cdots , head_h)W^O\\\\\\\\ where \\; head_i= Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})\\\\\\\\ \u5176\u4e2d\\; W_{i}^Q \\in \\Bbb{R}^{d_{model}\\times d_k} ,W_{i}^K \\in \\Bbb{R}^{d_{model}\\times d_k} , W_{i}^V \\in \\Bbb{R}^{d_{model}\\times d_v} ,W_{i}^O \\in \\Bbb{R}^{hd_v\\times d_{model}} MultiHead(Q,K,V)=Concat(head_1,\\cdots , head_h)W^O\\\\\\\\ where \\; head_i= Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})\\\\\\\\ \u5176\u4e2d\\; W_{i}^Q \\in \\Bbb{R}^{d_{model}\\times d_k} ,W_{i}^K \\in \\Bbb{R}^{d_{model}\\times d_k} , W_{i}^V \\in \\Bbb{R}^{d_{model}\\times d_v} ,W_{i}^O \\in \\Bbb{R}^{hd_v\\times d_{model}} \u591a\u5934self-attention\u5c42\u7684\u4f5c\u7528: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Multi-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u603b\u4f53\u8ba1\u7b97\u91cf\u548c\u5355\u4e00head\u76f8\u540c\u7684\u60c5\u51b5\u4e0b, \u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757, \u7531\u4e24\u4e2a\u7ebf\u6027\u53d8\u6362\u7ec4\u6210, \u4e2d\u95f4\u6709\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: $$ FFN(x)=\\max(0,xW_1+b_1)W_2+b_2 $$ \u6ce8\u610f: \u539f\u7248\u8bba\u6587\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ef4\u5ea6\u5747\u4e3ad_model = 512, \u5c42\u5185\u7684\u8fde\u63a5\u7ef4\u5ea6d_ff = 2048, \u5747\u91c7\u75284\u500d\u7684\u5927\u5c0f\u5173\u7cfb. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u5355\u7eaf\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5e76\u4e0d\u8db3\u4ee5\u63d0\u53d6\u5230\u7406\u60f3\u7684\u7279\u5f81, \u56e0\u6b64\u589e\u52a0\u5168\u8fde\u63a5\u5c42\u6765\u63d0\u5347\u7f51\u7edc\u7684\u80fd\u529b.","title":"1.3 \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42(self-attention)"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#14-decoder","text":"Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b\u4e09\u4e2a\u5b50\u5c42. \u4e00\u4e2a\u591a\u5934self-attention\u5c42 \u4e00\u4e2aEncoder-Decoder attention\u5c42 \u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42 Decoder Block\u4e2d\u7684\u591a\u5934self-attention\u5c42 Decoder\u4e2d\u7684\u591a\u5934self-attention\u5c42\u4e0eEncoder\u6a21\u5757\u4e00\u81f4, \u4f46\u9700\u8981\u6ce8\u610f\u7684\u662fDecoder\u6a21\u5757\u7684\u591a\u5934self-attention\u9700\u8981\u505alook-ahead-mask, \u56e0\u4e3a\u5728\u9884\u6d4b\u7684\u65f6\u5019\"\u4e0d\u80fd\u770b\u89c1\u672a\u6765\u7684\u4fe1\u606f\", \u6240\u4ee5\u8981\u5c06\u5f53\u524d\u7684token\u548c\u4e4b\u540e\u7684token\u5168\u90e8mask. Decoder Block\u4e2d\u7684Encoder-Decoder attention\u5c42 \u8fd9\u4e00\u5c42\u533a\u522b\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684Q = K = V, \u6b64\u5904\u77e9\u9635Q\u6765\u6e90\u4e8eDecoder\u7aef\u7ecf\u8fc7\u4e0a\u4e00\u4e2aDecoder Block\u7684\u8f93\u51fa, \u800c\u77e9\u9635K, V\u5219\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa, \u9020\u6210\u4e86Q != K = V\u7684\u60c5\u51b5. \u8fd9\u6837\u8bbe\u8ba1\u662f\u4e3a\u4e86\u8ba9Decoder\u7aef\u7684token\u80fd\u591f\u7ed9\u4e88Encoder\u7aef\u5bf9\u5e94\u7684token\u66f4\u591a\u7684\u5173\u6ce8. Decoder Block\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6b64\u5904\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u6a21\u5757\u4e2d\u7684\u5b8c\u5168\u4e00\u6837. Decoder Block\u4e2d\u67092\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u4f5c\u7528: \u591a\u5934self-attention\u5c42\u662f\u4e3a\u4e86\u62df\u5408Decoder\u7aef\u81ea\u8eab\u7684\u4fe1\u606f, \u800cEncoder-Decoder attention\u5c42\u662f\u4e3a\u4e86\u6574\u5408Encoder\u548cDecoder\u7684\u4fe1\u606f.","title":"1.4 Decoder\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#15-add-norm","text":"Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5177\u4f53\u6765\u8bf4Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, Norm\u8868\u793aLayerNorm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5177\u4f53\u7684\u6570\u5b66\u8868\u8fbe\u5f62\u5f0f\u4e3a: LayerNorm(x + Sublayer(x)), \u5176\u4e2dSublayer(x)\u4e3a\u5b50\u5c42\u7684\u8f93\u51fa. Add\u6b8b\u5dee\u8fde\u63a5\u7684\u4f5c\u7528: \u548c\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u8fde\u63a5\u4f5c\u7528\u4e00\u81f4, \u90fd\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u4f20\u9012\u7684\u66f4\u6df1, \u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. \u8bd5\u9a8c\u8868\u660e\u6b8b\u5dee\u8fde\u63a5\u7684\u786e\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u73b0. Norm\u7684\u4f5c\u7528: \u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u989d\u589e\u52a0, \u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u5927, \u8fc7\u5c0f, \u65b9\u5dee\u53d8\u5927\u7b49\u73b0\u8c61, \u8fd9\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38, \u6a21\u578b\u7684\u6536\u655b\u975e\u5e38\u6162. \u56e0\u6b64\u5bf9\u6bcf\u4e00\u5c42\u8ba1\u7b97\u540e\u7684\u6570\u503c\u8fdb\u884c\u89c4\u8303\u5316\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u8868\u73b0.","title":"1.5 Add &amp; Norm\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#16-positional-encoding","text":"Transformer\u4e2d\u76f4\u63a5\u91c7\u7528\u6b63\u5f26\u51fd\u6570\u548c\u4f59\u5f26\u51fd\u6570\u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f, \u5982\u4e0b\u56fe\u6240\u793a: $$ PE_{(pos, 2i)}=\\sin(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ PE_{(pos, 2i+1)}=\\cos(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ $$ \u9700\u8981\u6ce8\u610f: \u4e09\u89d2\u51fd\u6570\u5e94\u7528\u5728\u6b64\u5904\u7684\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u70b9, \u56e0\u4e3a\u5bf9\u4e8e\u4efb\u610f\u7684PE(pos+k), \u90fd\u53ef\u4ee5\u8868\u793a\u4e3aPE(pos)\u7684\u7ebf\u6027\u51fd\u6570, \u5927\u5927\u65b9\u4fbf\u8ba1\u7b97. \u800c\u4e14\u5468\u671f\u6027\u51fd\u6570\u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u4e5f\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b. $$ \\sin(\\alpha+\\beta)=\\sin(\\alpha)\\cos(\\beta)+\\cos(\\alpha)\\sin(\\beta)\\\\ \\cos(\\alpha+\\beta)=\\cos(\\alpha)\\cos(\\beta)-\\sin(\\alpha)\\sin(\\beta) $$","title":"1.6 \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8-formd%E5%85%AC%E5%BC%8Fbak.html#2","text":"Encoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e24\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u7684\u662f\u4e00\u79cdScaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Mul ti-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u6bd4\u5355\u4e00head\u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u662f\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u7ec4\u6210, \u7ebf\u6027\u53d8\u6362\u4e2d\u95f4\u589e\u6dfb\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5177\u4f53\u7684\u7ef4\u5ea6\u91c7\u75284\u500d\u5173\u7cfb, \u5373\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684d_model=512, \u5219\u5c42\u5185\u7684\u53d8\u6362\u7ef4\u5ea6d_ff=2048. Decoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b3\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, Encoder-Decoder Attention\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u548cEncoder\u6a21\u5757\u4e00\u6837\u7684Scaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u6700\u5927\u7684 \u533a\u522b\u5728\u4e8e\u9700\u8981\u6dfb\u52a0look-ahead-mask, \u5373\u906e\u63a9\"\u672a\u6765\u7684\u4fe1\u606f\". Encoder-Decoder Attention\u5c42\u548c\u4e0a\u4e00\u5c42\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u6700\u4e3b\u8981\u7684\u533a\u522b\u5728\u4e8eQ != K = V, \u77e9\u9635Q\u6765\u6e90\u4e8e\u4e0a\u4e00\u5c42Decoder Block\u7684\u8f93\u51fa, \u540c\u65f6K, V\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u4e2d\u5b8c\u5168\u4e00\u6837. Add & Norm\u6a21\u5757 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, \u4f5c\u7528\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u65e0\u635f\u8017\u7684\u4f20\u9012\u7684\u66f4\u6df1, \u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. Norm\u8868\u793aLayerNorm, \u5c42\u7ea7\u522b\u7684\u6570\u503c\u6807\u51c6\u5316\u64cd\u4f5c, \u4f5c\u7528\u662f\u9632\u6b62\u53c2\u6570\u8fc7\u5927\u8fc7\u5c0f\u5bfc\u81f4\u7684\u5b66\u4e60\u8fc7\u7a0b\u5f02\u5e38, \u6a21\u578b\u6536\u655b\u7279\u522b\u6162\u7684\u95ee\u9898. \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding Transformer\u4e2d\u91c7\u7528\u4e09\u89d2\u51fd\u6570\u6765\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801. \u56e0\u4e3a\u4e09\u89d2\u51fd\u6570\u662f\u5468\u671f\u6027\u51fd\u6570, \u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u800c\u4e14\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u53ef\u4ee5\u5bf9\u5e8f\u5217\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u7f16\u7801\u7684\u91cd\u8981\u7a0b\u5ea6\u540c\u7b49\u770b\u5f85. \u672c\u8282\u5e38\u89c1\u95ee\u7b54 Transformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"2 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1\u5176\u4ed6\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u601d\u8003\u9898\uff1aTransformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528? 1 Encoder\u6a21\u5757 \u00b6 1.1 Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u00b6 \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42. 1.2 \u5173\u4e8eEncoder Block \u00b6 \u5728Transformer\u67b6\u6784\u4e2d, 6\u4e2a\u4e00\u6a21\u4e00\u6837\u7684Encoder Block\u5c42\u5c42\u5806\u53e0\u5728\u4e00\u8d77, \u5171\u540c\u7ec4\u6210\u5b8c\u6574\u7684Encoder, \u56e0\u6b64\u5256\u6790\u4e00\u4e2aBlock\u5c31\u53ef\u4ee5\u5bf9\u6574\u4e2aEncoder\u7684\u5185\u90e8\u7ed3\u6784\u6709\u6e05\u6670\u7684\u8ba4\u8bc6. 1.3 \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42(self-attention) \u00b6 \u9996\u5148\u6765\u770bself-attention\u7684\u8ba1\u7b97\u89c4\u5219\u56fe: \u4e0a\u8ff0attention\u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u5c06query\u548ckey-value\u952e\u503c\u5bf9\u7684\u4e00\u7ec4\u96c6\u5408\u6620\u5c04\u5230\u8f93\u51fa, \u8f93\u51fa\u88ab\u8ba1\u7b97\u4e3avalues\u7684\u52a0\u6743\u548c, \u5176\u4e2d\u5206\u914d\u7ed9\u6bcf\u4e2avalue\u7684\u6743\u91cd\u7531query\u4e0e\u5bf9\u5e94key\u7684\u76f8\u4f3c\u6027\u51fd\u6570\u8ba1\u7b97\u5f97\u6765. \u8fd9\u79cdattention\u7684\u5f62\u5f0f\u88ab\u79f0\u4e3aScaled Dot-Product Attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V \u6240\u8c13\u7684\u591a\u5934self-attention\u5c42, \u5219\u662f\u5148\u5c06Q, K, V\u7ecf\u8fc7\u53c2\u6570\u77e9\u9635\u8fdb\u884c\u6620\u5c04, \u518d\u505aself-attention, \u6700\u540e\u5c06\u7ed3\u679c\u62fc\u63a5\u8d77\u6765\u9001\u5165\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u5373\u53ef. \u4e0a\u8ff0\u7684\u591a\u5934self-attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: \u591a\u5934self-attention\u5c42\u7684\u4f5c\u7528: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Multi-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u603b\u4f53\u8ba1\u7b97\u91cf\u548c\u5355\u4e00head\u76f8\u540c\u7684\u60c5\u51b5\u4e0b, \u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757, \u7531\u4e24\u4e2a\u7ebf\u6027\u53d8\u6362\u7ec4\u6210, \u4e2d\u95f4\u6709\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: $$ FFN(x)=\\max(0,xW_1+b_1)W_2+b_2 $$ \u6ce8\u610f: \u539f\u7248\u8bba\u6587\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ef4\u5ea6\u5747\u4e3ad_model = 512, \u5c42\u5185\u7684\u8fde\u63a5\u7ef4\u5ea6d_ff = 2048, \u5747\u91c7\u75284\u500d\u7684\u5927\u5c0f\u5173\u7cfb. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u5355\u7eaf\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5e76\u4e0d\u8db3\u4ee5\u63d0\u53d6\u5230\u7406\u60f3\u7684\u7279\u5f81, \u56e0\u6b64\u589e\u52a0\u5168\u8fde\u63a5\u5c42\u6765\u63d0\u5347\u7f51\u7edc\u7684\u80fd\u529b. 1.4 Decoder\u6a21\u5757 \u00b6 Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b\u4e09\u4e2a\u5b50\u5c42. \u4e00\u4e2a\u591a\u5934self-attention\u5c42 \u4e00\u4e2aEncoder-Decoder attention\u5c42 \u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42 Decoder Block\u4e2d\u7684\u591a\u5934self-attention\u5c42 Decoder\u4e2d\u7684\u591a\u5934self-attention\u5c42\u4e0eEncoder\u6a21\u5757\u4e00\u81f4, \u4f46\u9700\u8981\u6ce8\u610f\u7684\u662fDecoder\u6a21\u5757\u7684\u591a\u5934self-attention\u9700\u8981\u505alook-ahead-mask, \u56e0\u4e3a\u5728\u9884\u6d4b\u7684\u65f6\u5019\"\u4e0d\u80fd\u770b\u89c1\u672a\u6765\u7684\u4fe1\u606f\", \u6240\u4ee5\u8981\u5c06\u5f53\u524d\u7684token\u548c\u4e4b\u540e\u7684token\u5168\u90e8mask. Decoder Block\u4e2d\u7684Encoder-Decoder attention\u5c42 \u8fd9\u4e00\u5c42\u533a\u522b\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684Q = K = V, \u6b64\u5904\u77e9\u9635Q\u6765\u6e90\u4e8eDecoder\u7aef\u7ecf\u8fc7\u4e0a\u4e00\u4e2aDecoder Block\u7684\u8f93\u51fa, \u800c\u77e9\u9635K, V\u5219\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa, \u9020\u6210\u4e86Q != K = V\u7684\u60c5\u51b5. \u8fd9\u6837\u8bbe\u8ba1\u662f\u4e3a\u4e86\u8ba9Decoder\u7aef\u7684token\u80fd\u591f\u7ed9\u4e88Encoder\u7aef\u5bf9\u5e94\u7684token\u66f4\u591a\u7684\u5173\u6ce8. Decoder Block\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6b64\u5904\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u6a21\u5757\u4e2d\u7684\u5b8c\u5168\u4e00\u6837. Decoder Block\u4e2d\u67092\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u4f5c\u7528: \u591a\u5934self-attention\u5c42\u662f\u4e3a\u4e86\u62df\u5408Decoder\u7aef\u81ea\u8eab\u7684\u4fe1\u606f, \u800cEncoder-Decoder attention\u5c42\u662f\u4e3a\u4e86\u6574\u5408Encoder\u548cDecoder\u7684\u4fe1\u606f. 1.5 Add & Norm\u6a21\u5757 \u00b6 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5177\u4f53\u6765\u8bf4Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, Norm\u8868\u793aLayerNorm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5177\u4f53\u7684\u6570\u5b66\u8868\u8fbe\u5f62\u5f0f\u4e3a: LayerNorm(x + Sublayer(x)), \u5176\u4e2dSublayer(x)\u4e3a\u5b50\u5c42\u7684\u8f93\u51fa. Add\u6b8b\u5dee\u8fde\u63a5\u7684\u4f5c\u7528: \u548c\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u8fde\u63a5\u4f5c\u7528\u4e00\u81f4, \u90fd\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u4f20\u9012\u7684\u66f4\u6df1, \u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. \u8bd5\u9a8c\u8868\u660e\u6b8b\u5dee\u8fde\u63a5\u7684\u786e\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u73b0. Norm\u7684\u4f5c\u7528: \u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u989d\u589e\u52a0, \u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u5927, \u8fc7\u5c0f, \u65b9\u5dee\u53d8\u5927\u7b49\u73b0\u8c61, \u8fd9\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38, \u6a21\u578b\u7684\u6536\u655b\u975e\u5e38\u6162. \u56e0\u6b64\u5bf9\u6bcf\u4e00\u5c42\u8ba1\u7b97\u540e\u7684\u6570\u503c\u8fdb\u884c\u89c4\u8303\u5316\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u8868\u73b0. 1.6 \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding \u00b6 Transformer\u4e2d\u76f4\u63a5\u91c7\u7528\u6b63\u5f26\u51fd\u6570\u548c\u4f59\u5f26\u51fd\u6570\u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f, \u5982\u4e0b\u56fe\u6240\u793a: $$ PE_{(pos, 2i)}=\\sin(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ PE_{(pos, 2i+1)}=\\cos(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ $$ \u9700\u8981\u6ce8\u610f: \u4e09\u89d2\u51fd\u6570\u5e94\u7528\u5728\u6b64\u5904\u7684\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u70b9, \u56e0\u4e3a\u5bf9\u4e8e\u4efb\u610f\u7684PE(pos+k), \u90fd\u53ef\u4ee5\u8868\u793a\u4e3aPE(pos)\u7684\u7ebf\u6027\u51fd\u6570, \u5927\u5927\u65b9\u4fbf\u8ba1\u7b97. \u800c\u4e14\u5468\u671f\u6027\u51fd\u6570\u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u4e5f\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b. $$ \\sin(\\alpha+\\beta)=\\sin(\\alpha)\\cos(\\beta)+\\cos(\\alpha)\\sin(\\beta)\\\\ \\cos(\\alpha+\\beta)=\\cos(\\alpha)\\cos(\\beta)-\\sin(\\alpha)\\sin(\\beta) $$ 2 \u5c0f\u7ed3 \u00b6 Encoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e24\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u7684\u662f\u4e00\u79cdScaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Mul ti-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u6bd4\u5355\u4e00head\u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u662f\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u7ec4\u6210, \u7ebf\u6027\u53d8\u6362\u4e2d\u95f4\u589e\u6dfb\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5177\u4f53\u7684\u7ef4\u5ea6\u91c7\u75284\u500d\u5173\u7cfb, \u5373\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684d_model=512, \u5219\u5c42\u5185\u7684\u53d8\u6362\u7ef4\u5ea6d_ff=2048. Decoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b3\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, Encoder-Decoder Attention\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u548cEncoder\u6a21\u5757\u4e00\u6837\u7684Scaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u6700\u5927\u7684 \u533a\u522b\u5728\u4e8e\u9700\u8981\u6dfb\u52a0look-ahead-mask, \u5373\u906e\u63a9\"\u672a\u6765\u7684\u4fe1\u606f\". Encoder-Decoder Attention\u5c42\u548c\u4e0a\u4e00\u5c42\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u6700\u4e3b\u8981\u7684\u533a\u522b\u5728\u4e8eQ != K = V, \u77e9\u9635Q\u6765\u6e90\u4e8e\u4e0a\u4e00\u5c42Decoder Block\u7684\u8f93\u51fa, \u540c\u65f6K, V\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u4e2d\u5b8c\u5168\u4e00\u6837. Add & Norm\u6a21\u5757 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, \u4f5c\u7528\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u65e0\u635f\u8017\u7684\u4f20\u9012\u7684\u66f4\u6df1, \u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. Norm\u8868\u793aLayerNorm, \u5c42\u7ea7\u522b\u7684\u6570\u503c\u6807\u51c6\u5316\u64cd\u4f5c, \u4f5c\u7528\u662f\u9632\u6b62\u53c2\u6570\u8fc7\u5927\u8fc7\u5c0f\u5bfc\u81f4\u7684\u5b66\u4e60\u8fc7\u7a0b\u5f02\u5e38, \u6a21\u578b\u6536\u655b\u7279\u522b\u6162\u7684\u95ee\u9898. \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding Transformer\u4e2d\u91c7\u7528\u4e09\u89d2\u51fd\u6570\u6765\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801. \u56e0\u4e3a\u4e09\u89d2\u51fd\u6570\u662f\u5468\u671f\u6027\u51fd\u6570, \u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u800c\u4e14\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u53ef\u4ee5\u5bf9\u5e8f\u5217\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u7f16\u7801\u7684\u91cd\u8981\u7a0b\u5ea6\u540c\u7b49\u770b\u5f85. \u672c\u8282\u5e38\u89c1\u95ee\u7b54 Transformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"2 Transformer \u5404\u5b50\u6a21\u5757\u4f5c\u7528"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#_1","text":"\u638c\u63e1Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1\u5176\u4ed6\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u601d\u8003\u9898\uff1aTransformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#1-encoder","text":"","title":"1 Encoder\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#11-encoder","text":"\u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42.","title":"1.1 Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528:"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#12-encoder-block","text":"\u5728Transformer\u67b6\u6784\u4e2d, 6\u4e2a\u4e00\u6a21\u4e00\u6837\u7684Encoder Block\u5c42\u5c42\u5806\u53e0\u5728\u4e00\u8d77, \u5171\u540c\u7ec4\u6210\u5b8c\u6574\u7684Encoder, \u56e0\u6b64\u5256\u6790\u4e00\u4e2aBlock\u5c31\u53ef\u4ee5\u5bf9\u6574\u4e2aEncoder\u7684\u5185\u90e8\u7ed3\u6784\u6709\u6e05\u6670\u7684\u8ba4\u8bc6.","title":"1.2 \u5173\u4e8eEncoder Block"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#13-self-attention","text":"\u9996\u5148\u6765\u770bself-attention\u7684\u8ba1\u7b97\u89c4\u5219\u56fe: \u4e0a\u8ff0attention\u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u5c06query\u548ckey-value\u952e\u503c\u5bf9\u7684\u4e00\u7ec4\u96c6\u5408\u6620\u5c04\u5230\u8f93\u51fa, \u8f93\u51fa\u88ab\u8ba1\u7b97\u4e3avalues\u7684\u52a0\u6743\u548c, \u5176\u4e2d\u5206\u914d\u7ed9\u6bcf\u4e2avalue\u7684\u6743\u91cd\u7531query\u4e0e\u5bf9\u5e94key\u7684\u76f8\u4f3c\u6027\u51fd\u6570\u8ba1\u7b97\u5f97\u6765. \u8fd9\u79cdattention\u7684\u5f62\u5f0f\u88ab\u79f0\u4e3aScaled Dot-Product Attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V \u6240\u8c13\u7684\u591a\u5934self-attention\u5c42, \u5219\u662f\u5148\u5c06Q, K, V\u7ecf\u8fc7\u53c2\u6570\u77e9\u9635\u8fdb\u884c\u6620\u5c04, \u518d\u505aself-attention, \u6700\u540e\u5c06\u7ed3\u679c\u62fc\u63a5\u8d77\u6765\u9001\u5165\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u5373\u53ef. \u4e0a\u8ff0\u7684\u591a\u5934self-attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: \u591a\u5934self-attention\u5c42\u7684\u4f5c\u7528: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Multi-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u603b\u4f53\u8ba1\u7b97\u91cf\u548c\u5355\u4e00head\u76f8\u540c\u7684\u60c5\u51b5\u4e0b, \u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757, \u7531\u4e24\u4e2a\u7ebf\u6027\u53d8\u6362\u7ec4\u6210, \u4e2d\u95f4\u6709\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: $$ FFN(x)=\\max(0,xW_1+b_1)W_2+b_2 $$ \u6ce8\u610f: \u539f\u7248\u8bba\u6587\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ef4\u5ea6\u5747\u4e3ad_model = 512, \u5c42\u5185\u7684\u8fde\u63a5\u7ef4\u5ea6d_ff = 2048, \u5747\u91c7\u75284\u500d\u7684\u5927\u5c0f\u5173\u7cfb. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u5355\u7eaf\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5e76\u4e0d\u8db3\u4ee5\u63d0\u53d6\u5230\u7406\u60f3\u7684\u7279\u5f81, \u56e0\u6b64\u589e\u52a0\u5168\u8fde\u63a5\u5c42\u6765\u63d0\u5347\u7f51\u7edc\u7684\u80fd\u529b.","title":"1.3 \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42(self-attention)"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#14-decoder","text":"Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b\u4e09\u4e2a\u5b50\u5c42. \u4e00\u4e2a\u591a\u5934self-attention\u5c42 \u4e00\u4e2aEncoder-Decoder attention\u5c42 \u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42 Decoder Block\u4e2d\u7684\u591a\u5934self-attention\u5c42 Decoder\u4e2d\u7684\u591a\u5934self-attention\u5c42\u4e0eEncoder\u6a21\u5757\u4e00\u81f4, \u4f46\u9700\u8981\u6ce8\u610f\u7684\u662fDecoder\u6a21\u5757\u7684\u591a\u5934self-attention\u9700\u8981\u505alook-ahead-mask, \u56e0\u4e3a\u5728\u9884\u6d4b\u7684\u65f6\u5019\"\u4e0d\u80fd\u770b\u89c1\u672a\u6765\u7684\u4fe1\u606f\", \u6240\u4ee5\u8981\u5c06\u5f53\u524d\u7684token\u548c\u4e4b\u540e\u7684token\u5168\u90e8mask. Decoder Block\u4e2d\u7684Encoder-Decoder attention\u5c42 \u8fd9\u4e00\u5c42\u533a\u522b\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684Q = K = V, \u6b64\u5904\u77e9\u9635Q\u6765\u6e90\u4e8eDecoder\u7aef\u7ecf\u8fc7\u4e0a\u4e00\u4e2aDecoder Block\u7684\u8f93\u51fa, \u800c\u77e9\u9635K, V\u5219\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa, \u9020\u6210\u4e86Q != K = V\u7684\u60c5\u51b5. \u8fd9\u6837\u8bbe\u8ba1\u662f\u4e3a\u4e86\u8ba9Decoder\u7aef\u7684token\u80fd\u591f\u7ed9\u4e88Encoder\u7aef\u5bf9\u5e94\u7684token\u66f4\u591a\u7684\u5173\u6ce8. Decoder Block\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6b64\u5904\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u6a21\u5757\u4e2d\u7684\u5b8c\u5168\u4e00\u6837. Decoder Block\u4e2d\u67092\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u4f5c\u7528: \u591a\u5934self-attention\u5c42\u662f\u4e3a\u4e86\u62df\u5408Decoder\u7aef\u81ea\u8eab\u7684\u4fe1\u606f, \u800cEncoder-Decoder attention\u5c42\u662f\u4e3a\u4e86\u6574\u5408Encoder\u548cDecoder\u7684\u4fe1\u606f.","title":"1.4 Decoder\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#15-add-norm","text":"Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5177\u4f53\u6765\u8bf4Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, Norm\u8868\u793aLayerNorm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5177\u4f53\u7684\u6570\u5b66\u8868\u8fbe\u5f62\u5f0f\u4e3a: LayerNorm(x + Sublayer(x)), \u5176\u4e2dSublayer(x)\u4e3a\u5b50\u5c42\u7684\u8f93\u51fa. Add\u6b8b\u5dee\u8fde\u63a5\u7684\u4f5c\u7528: \u548c\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u8fde\u63a5\u4f5c\u7528\u4e00\u81f4, \u90fd\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u4f20\u9012\u7684\u66f4\u6df1, \u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. \u8bd5\u9a8c\u8868\u660e\u6b8b\u5dee\u8fde\u63a5\u7684\u786e\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u73b0. Norm\u7684\u4f5c\u7528: \u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u989d\u589e\u52a0, \u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u5927, \u8fc7\u5c0f, \u65b9\u5dee\u53d8\u5927\u7b49\u73b0\u8c61, \u8fd9\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38, \u6a21\u578b\u7684\u6536\u655b\u975e\u5e38\u6162. \u56e0\u6b64\u5bf9\u6bcf\u4e00\u5c42\u8ba1\u7b97\u540e\u7684\u6570\u503c\u8fdb\u884c\u89c4\u8303\u5316\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u8868\u73b0.","title":"1.5 Add &amp; Norm\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#16-positional-encoding","text":"Transformer\u4e2d\u76f4\u63a5\u91c7\u7528\u6b63\u5f26\u51fd\u6570\u548c\u4f59\u5f26\u51fd\u6570\u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f, \u5982\u4e0b\u56fe\u6240\u793a: $$ PE_{(pos, 2i)}=\\sin(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ PE_{(pos, 2i+1)}=\\cos(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ $$ \u9700\u8981\u6ce8\u610f: \u4e09\u89d2\u51fd\u6570\u5e94\u7528\u5728\u6b64\u5904\u7684\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u70b9, \u56e0\u4e3a\u5bf9\u4e8e\u4efb\u610f\u7684PE(pos+k), \u90fd\u53ef\u4ee5\u8868\u793a\u4e3aPE(pos)\u7684\u7ebf\u6027\u51fd\u6570, \u5927\u5927\u65b9\u4fbf\u8ba1\u7b97. \u800c\u4e14\u5468\u671f\u6027\u51fd\u6570\u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u4e5f\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b. $$ \\sin(\\alpha+\\beta)=\\sin(\\alpha)\\cos(\\beta)+\\cos(\\alpha)\\sin(\\beta)\\\\ \\cos(\\alpha+\\beta)=\\cos(\\alpha)\\cos(\\beta)-\\sin(\\alpha)\\sin(\\beta) $$","title":"1.6 \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding"},{"location":"06_mkdocs_pretrained_model_old/2%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#2","text":"Encoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e24\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u7684\u662f\u4e00\u79cdScaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Mul ti-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u6bd4\u5355\u4e00head\u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u662f\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u7ec4\u6210, \u7ebf\u6027\u53d8\u6362\u4e2d\u95f4\u589e\u6dfb\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5177\u4f53\u7684\u7ef4\u5ea6\u91c7\u75284\u500d\u5173\u7cfb, \u5373\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684d_model=512, \u5219\u5c42\u5185\u7684\u53d8\u6362\u7ef4\u5ea6d_ff=2048. Decoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b3\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, Encoder-Decoder Attention\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u548cEncoder\u6a21\u5757\u4e00\u6837\u7684Scaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u6700\u5927\u7684 \u533a\u522b\u5728\u4e8e\u9700\u8981\u6dfb\u52a0look-ahead-mask, \u5373\u906e\u63a9\"\u672a\u6765\u7684\u4fe1\u606f\". Encoder-Decoder Attention\u5c42\u548c\u4e0a\u4e00\u5c42\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u6700\u4e3b\u8981\u7684\u533a\u522b\u5728\u4e8eQ != K = V, \u77e9\u9635Q\u6765\u6e90\u4e8e\u4e0a\u4e00\u5c42Decoder Block\u7684\u8f93\u51fa, \u540c\u65f6K, V\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u4e2d\u5b8c\u5168\u4e00\u6837. Add & Norm\u6a21\u5757 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, \u4f5c\u7528\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u65e0\u635f\u8017\u7684\u4f20\u9012\u7684\u66f4\u6df1, \u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. Norm\u8868\u793aLayerNorm, \u5c42\u7ea7\u522b\u7684\u6570\u503c\u6807\u51c6\u5316\u64cd\u4f5c, \u4f5c\u7528\u662f\u9632\u6b62\u53c2\u6570\u8fc7\u5927\u8fc7\u5c0f\u5bfc\u81f4\u7684\u5b66\u4e60\u8fc7\u7a0b\u5f02\u5e38, \u6a21\u578b\u6536\u655b\u7279\u522b\u6162\u7684\u95ee\u9898. \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding Transformer\u4e2d\u91c7\u7528\u4e09\u89d2\u51fd\u6570\u6765\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801. \u56e0\u4e3a\u4e09\u89d2\u51fd\u6570\u662f\u5468\u671f\u6027\u51fd\u6570, \u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u800c\u4e14\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u53ef\u4ee5\u5bf9\u5e8f\u5217\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u7f16\u7801\u7684\u91cd\u8981\u7a0b\u5ea6\u540c\u7b49\u770b\u5f85. \u672c\u8282\u5e38\u89c1\u95ee\u7b54 Transformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"2 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/3%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u7684\u8f93\u5165\u5f20\u91cf\u7279\u70b9\u548c\u542b\u4e49. \u638c\u63e1Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u638c\u63e1Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u601d\u8003\u9898\uff1aTransformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u5177\u4f53\u8f93\u5165\u662f\u4ec0\u4e48? \u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u9884\u6d4b\u9636\u6bb5\u4e00\u81f4\u5417? 1 Decoder\u7aef\u7684\u8f93\u5165\u89e3\u6790 \u00b6 1.1 Decoder\u7aef\u7684\u67b6\u6784 \u00b6 Transformer\u539f\u59cb\u8bba\u6587\u4e2d\u7684Decoder\u6a21\u5757\u662f\u7531N=6\u4e2a\u76f8\u540c\u7684Decoder Block\u5806\u53e0\u800c\u6210, \u5176\u4e2d\u6bcf\u4e00\u4e2aBlock\u662f\u75313\u4e2a\u5b50\u6a21\u5757\u6784\u6210, \u5206\u522b\u662f\u591a\u5934self-attention\u6a21\u5757, Encoder-Decoder attention\u6a21\u5757, \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757. 6\u4e2aBlock\u7684\u8f93\u5165\u4e0d\u5b8c\u5168\u76f8\u540c: \u6700\u4e0b\u9762\u7684\u4e00\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u662f\u7ecf\u5386\u4e86MASK\u4e4b\u540e\u7684Decoder\u7aef\u7684\u8f93\u5165 + Encoder\u7aef\u7684\u8f93\u51fa. \u5176\u4ed65\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u90fd\u662f\u524d\u4e00\u5c42Block\u7684\u8f93\u51fa + Encoder\u7aef\u7684\u8f93\u51fa. 1.2 Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790 \u00b6 \u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u8bf4, \u5047\u8bbe\u73b0\u5728\u7684\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u7b49\u4e8e\"How are you?\", \u5f53time step=1\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\u4e00\u4e2a\u7279\u6b8a\u7684token, \u6bd4\u5982\"SOS\"; \u5f53time step=2\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How\"; \u5f53time step=3\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How are\", \u4ee5\u6b64\u7c7b\u63a8... \u6ce8\u610f: \u5728\u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u8bad\u7ec3\u9636\u6bb5\u4e0d\u4f1a\u8fd9\u6837\u52a8\u6001\u8f93\u5165, \u800c\u662f\u4e00\u6b21\u6027\u7684\u628a\u76ee\u6807\u5e8f\u5217\u5168\u90e8\u8f93\u5165\u7ed9\u7b2c\u4e00\u5c42\u7684Block, \u7136\u540e\u901a\u8fc7\u591a\u5934self-attention\u4e2d\u7684MASK\u673a\u5236\u5bf9\u5e8f\u5217\u8fdb\u884c\u540c\u6837\u7684\u906e\u63a9\u5373\u53ef. 1.3 Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790 \u00b6 \u540c\u7406\u4e8e\u8bad\u7ec3\u9636\u6bb5, \u9884\u6d4b\u65f6\u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u56e0\u4e3a\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u90fd\u4f1a\u6709Encoder\u7684\u8f93\u51fa\u5f20\u91cf, \u56e0\u6b64\u8fd9\u91cc\u4e0d\u505a\u7279\u6b8a\u8ba8\u8bba, \u53ea\u4e13\u6ce8\u4e8e\u7eaf\u7cb9\u4eceDecoder\u7aef\u63a5\u6536\u7684\u8f93\u5165. \u9884\u6d4b\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0, input_tensor=\"SOS\"\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u8f93\u51fa\u7684\u7d2f\u8ba1\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u8bf4: \u5f53time step=1\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"What\"; \u5f53time step=2\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"is\"; \u5f53time step=3\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"the\"; \u5f53time step=4\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"matter\"; \u5f53time step=5\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"?\"; \u5f53time step=6\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter ?\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"EOS\", \u4ee3\u8868\u53e5\u5b50\u7684\u7ed3\u675f\u7b26, \u8bf4\u660e\u89e3\u7801\u7ed3\u675f, \u9884\u6d4b\u7ed3\u675f. 2 \u5c0f\u7ed3 \u00b6 \u5728Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u7684\u8f93\u5165, \u533a\u5206\u4e8e\u4e0d\u540c\u7684Block, \u6700\u5e95\u5c42\u7684Block\u8f93\u5165\u6709\u5176\u7279\u6b8a\u7684\u5730\u65b9. \u7b2c\u4e8c\u5c42\u5230\u7b2c\u516d\u5c42\u7684\u8f93\u5165\u4e00\u81f4, \u90fd\u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u548cEncoder\u7684\u8f93\u51fa. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u770b, \u5c31\u662f\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f, \u4e0d\u65ad\u7684\u5c06\u4e4b\u524d\u7684\u8f93\u5165\u878d\u5408\u8fdb\u6765. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u91c7\u7528\u7684\u662fMASK\u673a\u5236\u6765\u6a21\u62df\u8f93\u5165\u5e8f\u5217\u4e0d\u65ad\u6dfb\u52a0\u7684\u8fc7\u7a0b. \u6700\u5e95\u5c42\u7684Block\u5728\u9884\u6d4b\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c\u7684\u7d2f\u79ef\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u770b, \u4e5f\u662f\u968f\u7740\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f. \u76f8\u6bd4\u4e8e\u8bad\u7ec3\u9636\u6bb5\u6700\u5927\u7684\u4e0d\u540c\u662f\u8fd9\u91cc\u4e0d\u65ad\u62fc\u63a5\u8fdb\u6765\u7684token\u662f\u6bcf\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c, \u800c\u4e0d\u662f\u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u53d6\u5f97\u7684groud truth\u503c.","title":"3 Transformer Decoder\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/3%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#_1","text":"\u638c\u63e1Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u7684\u8f93\u5165\u5f20\u91cf\u7279\u70b9\u548c\u542b\u4e49. \u638c\u63e1Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u638c\u63e1Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u601d\u8003\u9898\uff1aTransformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u5177\u4f53\u8f93\u5165\u662f\u4ec0\u4e48? \u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u9884\u6d4b\u9636\u6bb5\u4e00\u81f4\u5417?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/3%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#1-decoder","text":"","title":"1 Decoder\u7aef\u7684\u8f93\u5165\u89e3\u6790"},{"location":"06_mkdocs_pretrained_model_old/3%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#11-decoder","text":"Transformer\u539f\u59cb\u8bba\u6587\u4e2d\u7684Decoder\u6a21\u5757\u662f\u7531N=6\u4e2a\u76f8\u540c\u7684Decoder Block\u5806\u53e0\u800c\u6210, \u5176\u4e2d\u6bcf\u4e00\u4e2aBlock\u662f\u75313\u4e2a\u5b50\u6a21\u5757\u6784\u6210, \u5206\u522b\u662f\u591a\u5934self-attention\u6a21\u5757, Encoder-Decoder attention\u6a21\u5757, \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757. 6\u4e2aBlock\u7684\u8f93\u5165\u4e0d\u5b8c\u5168\u76f8\u540c: \u6700\u4e0b\u9762\u7684\u4e00\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u662f\u7ecf\u5386\u4e86MASK\u4e4b\u540e\u7684Decoder\u7aef\u7684\u8f93\u5165 + Encoder\u7aef\u7684\u8f93\u51fa. \u5176\u4ed65\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u90fd\u662f\u524d\u4e00\u5c42Block\u7684\u8f93\u51fa + Encoder\u7aef\u7684\u8f93\u51fa.","title":"1.1 Decoder\u7aef\u7684\u67b6\u6784"},{"location":"06_mkdocs_pretrained_model_old/3%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#12-decoder","text":"\u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u8bf4, \u5047\u8bbe\u73b0\u5728\u7684\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u7b49\u4e8e\"How are you?\", \u5f53time step=1\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\u4e00\u4e2a\u7279\u6b8a\u7684token, \u6bd4\u5982\"SOS\"; \u5f53time step=2\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How\"; \u5f53time step=3\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How are\", \u4ee5\u6b64\u7c7b\u63a8... \u6ce8\u610f: \u5728\u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u8bad\u7ec3\u9636\u6bb5\u4e0d\u4f1a\u8fd9\u6837\u52a8\u6001\u8f93\u5165, \u800c\u662f\u4e00\u6b21\u6027\u7684\u628a\u76ee\u6807\u5e8f\u5217\u5168\u90e8\u8f93\u5165\u7ed9\u7b2c\u4e00\u5c42\u7684Block, \u7136\u540e\u901a\u8fc7\u591a\u5934self-attention\u4e2d\u7684MASK\u673a\u5236\u5bf9\u5e8f\u5217\u8fdb\u884c\u540c\u6837\u7684\u906e\u63a9\u5373\u53ef.","title":"1.2 Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790"},{"location":"06_mkdocs_pretrained_model_old/3%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#13-decoder","text":"\u540c\u7406\u4e8e\u8bad\u7ec3\u9636\u6bb5, \u9884\u6d4b\u65f6\u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u56e0\u4e3a\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u90fd\u4f1a\u6709Encoder\u7684\u8f93\u51fa\u5f20\u91cf, \u56e0\u6b64\u8fd9\u91cc\u4e0d\u505a\u7279\u6b8a\u8ba8\u8bba, \u53ea\u4e13\u6ce8\u4e8e\u7eaf\u7cb9\u4eceDecoder\u7aef\u63a5\u6536\u7684\u8f93\u5165. \u9884\u6d4b\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0, input_tensor=\"SOS\"\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u8f93\u51fa\u7684\u7d2f\u8ba1\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u8bf4: \u5f53time step=1\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"What\"; \u5f53time step=2\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"is\"; \u5f53time step=3\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"the\"; \u5f53time step=4\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"matter\"; \u5f53time step=5\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"?\"; \u5f53time step=6\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter ?\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"EOS\", \u4ee3\u8868\u53e5\u5b50\u7684\u7ed3\u675f\u7b26, \u8bf4\u660e\u89e3\u7801\u7ed3\u675f, \u9884\u6d4b\u7ed3\u675f.","title":"1.3 Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790"},{"location":"06_mkdocs_pretrained_model_old/3%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#2","text":"\u5728Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u7684\u8f93\u5165, \u533a\u5206\u4e8e\u4e0d\u540c\u7684Block, \u6700\u5e95\u5c42\u7684Block\u8f93\u5165\u6709\u5176\u7279\u6b8a\u7684\u5730\u65b9. \u7b2c\u4e8c\u5c42\u5230\u7b2c\u516d\u5c42\u7684\u8f93\u5165\u4e00\u81f4, \u90fd\u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u548cEncoder\u7684\u8f93\u51fa. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u770b, \u5c31\u662f\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f, \u4e0d\u65ad\u7684\u5c06\u4e4b\u524d\u7684\u8f93\u5165\u878d\u5408\u8fdb\u6765. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u91c7\u7528\u7684\u662fMASK\u673a\u5236\u6765\u6a21\u62df\u8f93\u5165\u5e8f\u5217\u4e0d\u65ad\u6dfb\u52a0\u7684\u8fc7\u7a0b. \u6700\u5e95\u5c42\u7684Block\u5728\u9884\u6d4b\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c\u7684\u7d2f\u79ef\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u770b, \u4e5f\u662f\u968f\u7740\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f. \u76f8\u6bd4\u4e8e\u8bad\u7ec3\u9636\u6bb5\u6700\u5927\u7684\u4e0d\u540c\u662f\u8fd9\u91cc\u4e0d\u65ad\u62fc\u63a5\u8fdb\u6765\u7684token\u662f\u6bcf\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c, \u800c\u4e0d\u662f\u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u53d6\u5f97\u7684groud truth\u503c.","title":"2 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1self-attention\u7684\u673a\u5236\u548c\u539f\u7406. \u638c\u63e1\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u6765\u8ba1\u7b97self-attention. \u7406\u89e3softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. \u7406\u89e3softmax\u51fd\u6570\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u68af\u5ea6\u6c42\u5bfc\u7684\u6570\u5b66\u8fc7\u7a0b. \u7406\u89e3softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u539f\u56e0. \u7406\u89e3self-attention\u8ba1\u7b97\u89c4\u5219\u4e2d\u5f52\u4e00\u5316\u7684\u539f\u56e0. \u601d\u8003\u98981\uff1a Transformer\u4e2d\u4e00\u76f4\u5f3a\u8c03\u7684self-attention\u662f\u4ec0\u4e48? \u4e3a\u4ec0\u4e48\u80fd\u53d1\u6325\u5982\u6b64\u5927\u7684\u4f5c\u7528? \u8ba1\u7b97\u7684\u65f6\u5019\u5982\u679c\u4e0d\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V), \u800c\u4ec5\u4ec5\u4f7f\u7528(Q, V)\u6216\u8005(K, V)\u6216\u8005(V)\u884c\u4e0d\u884c? \u601d\u8003\u98982\uff1aself-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled? 1 Self-attention\u7684\u673a\u5236\u548c\u539f\u7406 \u00b6 self-attention\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u8eab\u548c\u81ea\u8eab\u8fdb\u884c\u5173\u8054\u7684attention\u673a\u5236, \u4ece\u800c\u5f97\u5230\u66f4\u597d\u7684representation\u6765\u8868\u8fbe\u81ea\u8eab. self-attention\u662fattention\u673a\u5236\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u5728self-attention\u4e2d, Q=K=V, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd(token)\u90fd\u548c\u8be5\u5e8f\u5217\u4e2d\u7684\u5176\u4ed6\u6240\u6709\u5355\u8bcd(token)\u8fdb\u884cattention\u89c4\u5219\u7684\u8ba1\u7b97. attention\u673a\u5236\u8ba1\u7b97\u7684\u7279\u70b9\u5728\u4e8e, \u53ef\u4ee5\u76f4\u63a5\u8de8\u8d8a\u4e00\u53e5\u8bdd\u4e2d\u4e0d\u540c\u8ddd\u79bb\u7684token, \u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u5b66\u4e60\u5230\u5e8f\u5217\u7684\u77e5\u8bc6\u4f9d\u8d56\u548c\u8bed\u5e8f\u7ed3\u6784. \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, self-attention\u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u6355\u6349\u5230\u8bed\u4e49\u5c42\u9762\u7684\u7279\u5f81(its\u7684\u6307\u4ee3\u5bf9\u8c61\u662fLaw). \u5e94\u7528\u4f20\u7edf\u7684RNN, LSTM, \u5728\u83b7\u53d6\u957f\u8ddd\u79bb\u8bed\u4e49\u7279\u5f81\u548c\u7ed3\u6784\u7279\u5f81\u7684\u65f6\u5019, \u9700\u8981\u6309\u7167\u5e8f\u5217\u987a\u5e8f\u4f9d\u6b21\u8ba1\u7b97, \u8ddd\u79bb\u8d8a\u8fdc\u7684\u8054\u7cfb\u4fe1\u606f\u7684\u635f\u8017\u8d8a\u5927, \u6709\u6548\u63d0\u53d6\u548c\u6355\u83b7\u7684\u53ef\u80fd\u6027\u8d8a\u5c0f. \u4f46\u662f\u5e94\u7528self-attention\u65f6, \u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4f1a\u76f4\u63a5\u5c06\u53e5\u5b50\u4e2d\u4efb\u610f\u4e24\u4e2atoken\u7684\u8054\u7cfb\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u6b65\u9aa4\u76f4\u63a5\u8054\u7cfb\u8d77\u6765, \u5173\u4e8eself-attention\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528(Q, K, V)\u4e09\u5143\u7ec4\u800c\u4e0d\u662f\u5176\u4ed6\u5f62\u5f0f: \u9996\u5148\u4e00\u6761\u5c31\u662f\u4ece\u5206\u6790\u7684\u89d2\u5ea6\u770b, \u67e5\u8be2Query\u662f\u4e00\u6761\u72ec\u7acb\u7684\u5e8f\u5217\u4fe1\u606f, \u901a\u8fc7\u5173\u952e\u8bcdKey\u7684\u63d0\u793a\u4f5c\u7528, \u5f97\u5230\u6700\u7ec8\u8bed\u4e49\u7684\u771f\u5b9e\u503cValue\u8868\u8fbe, \u6570\u5b66\u610f\u4e49\u66f4\u5145\u5206, \u5b8c\u5907. \u8fd9\u91cc\u4e0d\u4f7f\u7528(K, V)\u6216\u8005(V)\u6ca1\u6709\u4ec0\u4e48\u5fc5\u987b\u7684\u7406\u7531, \u4e5f\u6ca1\u6709\u76f8\u5173\u7684\u8bba\u6587\u6765\u4e25\u683c\u9610\u8ff0\u6bd4\u8f83\u8bd5\u9a8c\u7684\u7ed3\u679c\u5dee\u5f02, \u6240\u4ee5\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u653e\u6027\u95ee\u9898\u672a\u6765\u53bb\u63a2\u7d22, \u53ea\u8981\u660e\u786e\u5728\u7ecf\u5178self-attention\u5b9e\u73b0\u4e2d\u7528\u7684\u662f\u4e09\u5143\u7ec4\u5c31\u597d. self-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled? 2 Self-attention\u4e2d\u7684\u5f52\u4e00\u5316\u6982\u8ff0 \u00b6 \u8bad\u7ec3\u4e0a\u7684\u610f\u4e49: \u968f\u7740\u8bcd\u5d4c\u5165\u7ef4\u5ea6d_k\u7684\u589e\u5927, q * k \u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4e5f\u4f1a\u589e\u5927, \u5728\u8bad\u7ec3\u65f6\u4f1a\u5c06\u5e26\u6709\u9971\u548c\u533a\u95f4\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u6bd4\u5982\uff1asigmoid\u6fc0\u6d3b\u51fd\u6570\u3001tanh\u6fc0\u6d3b\u51fd\u6570\u3001\u903b\u8f91\u56de\u5f52softmax\uff09\u63a8\u5165\u68af\u5ea6\u975e\u5e38\u5c0f\u7684\u533a\u57df, \u53ef\u80fd\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u73b0\u8c61, \u9020\u6210\u6a21\u578b\u6536\u655b\u56f0\u96be. \u6570\u5b66\u4e0a\u7684\u610f\u4e49: \u5047\u8bbeq\u548ck\u7684\u7edf\u8ba1\u53d8\u91cf\u662f\u6ee1\u8db3\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u72ec\u7acb\u968f\u673a\u53d8\u91cf, \u610f\u5473\u7740q\u548ck\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1. \u90a3\u4e48q\u548ck\u7684\u70b9\u79ef\u7ed3\u679c\u5c31\u662f\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3ad_k, \u4e3a\u4e86\u62b5\u6d88\u8fd9\u79cd\u65b9\u5dee\u88ab\u653e\u5927d_k\u500d\u7684\u5f71\u54cd, \u5728\u8ba1\u7b97\u4e2d\u4e3b\u52a8\u5c06\u70b9\u79ef\u7f29\u653e1/sqrt(d_k), \u8fd9\u6837\u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4f9d\u7136\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1. 3 softmax\u7684\u68af\u5ea6\u53d8\u5316 \u00b6 \u8fd9\u91cc\u6211\u4eec\u52063\u4e2a\u6b65\u9aa4\u6765\u89e3\u91casoftmax\u7684\u68af\u5ea6\u95ee\u9898: \u7b2c\u4e00\u6b65: softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684. \u7b2c\u4e8c\u6b65: softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u7b2c\u4e09\u6b65: softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0. 3.1 softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684 \u00b6 \u5bf9\u4e8e\u4e00\u4e2a\u8f93\u5165\u5411\u91cfx, softmax\u51fd\u6570\u5c06\u5176\u505a\u4e86\u4e00\u4e2a\u5f52\u4e00\u5316\u7684\u6620\u5c04, \u9996\u5148\u901a\u8fc7\u81ea\u7136\u5e95\u6570e\u5c06\u8f93\u5165\u5143\u7d20\u4e4b\u95f4\u7684\u5dee\u8ddd\u5148\"\u62c9\u5927\", \u7136\u540e\u518d\u5f52\u4e00\u5316\u4e3a\u4e00\u4e2a\u65b0\u7684\u5206\u5e03. \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5047\u8bbe\u67d0\u4e2a\u8f93\u5165x\u4e2d\u6700\u5927\u7684\u5143\u7d20\u4e0b\u6807\u662fk, \u5982\u679c\u8f93\u5165\u7684\u6570\u91cf\u7ea7\u53d8\u5927(\u5c31\u662fx\u4e2d\u7684\u6bcf\u4e2a\u5206\u91cf\u7edd\u5bf9\u503c\u90fd\u5f88\u5927), \u90a3\u4e48\u5728\u6570\u5b66\u4e0a\u4f1a\u9020\u6210y_k\u7684\u503c\u975e\u5e38\u63a5\u8fd11. \u5177\u4f53\u7528\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a, \u5047\u8bbe\u8f93\u5165\u7684\u5411\u91cfx = [a, a, 2a], \u90a3\u4e48\u968f\u4fbf\u7ed9\u51e0\u4e2a\u4e0d\u540c\u6570\u91cf\u7ea7\u7684\u503c\u6765\u770b\u770b\u5bf9y3\u4ea7\u751f\u7684\u5f71\u54cd a = 1\u65f6, y3 = 0.5761168847658291 # e^2 / (e^1 + e^1 + e^2)) a = 10\u65f6, y3 = 0.9999092083843412 # e^20 / (e^10 + e^10 + e^20)) a = 100\u65f6, y3 = 1.0 # e^200 / (e^100 + e^100 + e^200)) \u91c7\u7528\u4e00\u6bb5\u5b9e\u4f8b\u4ee3\u7801\u5c06a\u5728\u4e0d\u540c\u53d6\u503c\u4e0b, \u5bf9\u5e94\u7684y3\u5168\u90e8\u753b\u51fa\u6765, \u4ee5\u66f2\u7ebf\u7684\u5f62\u5f0f\u5c55\u793a: from math import exp from matplotlib import pyplot as plt import numpy as np f = lambda x : exp ( x * 2 ) / ( exp ( x ) + exp ( x ) + exp ( x * 2 )) x = np . linspace ( 0 , 100 , 100 ) y_3 = [ f ( x_i ) for x_i in x ] plt . plot ( x , y_3 ) plt . show () \u5f97\u5230\u5982\u4e0b\u7684\u66f2\u7ebf: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927 \u7ed3\u8bba\uff1a \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6\uff0csoftmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e\u3002\u901a\u4fd7\u7684\u8bb2\uff1a\u6570\u636e\u7684\u65b9\u5dee\u53d8\u5927\uff08\u79bb\u6563\u7a0b\u5ea6\u53d8\u5927\uff09\uff0c\u6700\u5927\u503c\u5f3a\u5360\u4e86\u6240\u6709\u6982\u7387\u3002 3.2 softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684 \u00b6 softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u5bb9\u6613\u68af\u5ea6\u6d88\u5931\uff0c\u6240\u4ee5\u8981\u770b\u4e00\u770bsoftmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u662f\u5982\u4f55\u6c42\u5bfc\u7684\u3002 \u9996\u5148\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u548c\u8f93\u51fa: \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u53cd\u5411\u4f20\u64ad\u5c31\u662f\u8f93\u51fa\u7aef\u7684\u635f\u5931\u51fd\u6570\u5bf9\u8f93\u5165\u7aef\u6c42\u504f\u5bfc\u7684\u8fc7\u7a0b, \u8fd9\u91cc\u8981\u5206\u4e24\u79cd\u60c5\u51b5, \u7b2c\u4e00\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \u7b2c\u4e8c\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \u7ecf\u8fc7\u5bf9\u4e24\u79cd\u60c5\u51b5\u5206\u522b\u7684\u6c42\u5bfc\u8ba1\u7b97, \u53ef\u4ee5\u5f97\u51fa\u6700\u7ec8\u7684\u7ed3\u8bba\u5982\u4e0b: \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \u628a\u62bd\u8c61\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u6620\u5c04\u6210\u77e9\u9635\u8868\u793a\uff0c\u89c13.3\u8282\u8868\u793a\uff08i=j\u65f6\uff0c\u4e24\u4e2a\u77e9\u9635\u5bf9\u89d2\u7ebf - \u5bf9\u89d2\u7ebf \uff1bi!=j\u65f6\uff0c\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u51cf\uff09\u3002 3.3 softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0 \u00b6 \u6839\u636e\u7b2c\u4e8c\u6b65\u4e2dsoftmax\u51fd\u6570\u7684\u6c42\u5bfc\u7ed3\u679c, \u53ef\u4ee5\u5c06\u6700\u7ec8\u7684\u7ed3\u679c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c55\u5f00\u5982\u4e0b: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \u6839\u636e\u7b2c\u4e00\u6b65\u4e2d\u7684\u8ba8\u8bba\u7ed3\u679c, \u5f53\u8f93\u5165x\u7684\u5206\u91cf\u503c\u8f83\u5927\u65f6, softmax\u51fd\u6570\u4f1a\u5c06\u5927\u90e8\u5206\u6982\u7387\u5206\u914d\u7ed9\u6700\u5927\u7684\u5143\u7d20, \u5047\u8bbe\u6700\u5927\u5143\u7d20\u662fx1, \u90a3\u4e48softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c06\u4ea7\u751f\u4e00\u4e2a\u63a5\u8fd1one-hot\u7684\u7ed3\u679c\u5f20\u91cfy_ = [1, 0, 0,..., 0], \u6b64\u65f6\u7ed3\u679c\u77e9\u9635\u53d8\u4e3a: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \u7ed3\u8bba: \u7efc\u4e0a\u53ef\u4ee5\u5f97\u51fa, \u6240\u6709\u7684\u68af\u5ea6\u90fd\u6d88\u5931\u4e3a0(\u63a5\u8fd1\u4e8e0), \u53c2\u6570\u51e0\u4e4e\u65e0\u6cd5\u66f4\u65b0, \u6a21\u578b\u6536\u655b\u56f0\u96be. 4 \u7ef4\u5ea6\u4e0e\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb \u00b6 \u9488\u5bf9\u4e3a\u4ec0\u4e48\u7ef4\u5ea6\u4f1a\u5f71\u54cd\u70b9\u79ef\u7684\u5927\u5c0f, \u539f\u59cb\u8bba\u6587\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u70b9\u89e3\u91ca\u5982\u4e0b: To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their doct product, q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k. \u6211\u4eec\u5206\u4e24\u6b65\u5bf9\u5176\u8fdb\u884c\u4e00\u4e2a\u63a8\u5bfc, \u9996\u5148\u5c31\u662f\u5047\u8bbe\u5411\u91cfq\u548ck\u7684\u5404\u4e2a\u5206\u91cf\u662f\u76f8\u4e92\u72ec\u7acb\u7684\u968f\u673a\u53d8\u91cf, X = q_i, Y = k_i, X\u548cY\u5404\u81ea\u6709d_k\u4e2a\u5206\u91cf, \u4e5f\u5c31\u662f\u5411\u91cf\u7684\u7ef4\u5ea6\u7b49\u4e8ed_k, \u6709E(X) = E(Y) = 0, \u4ee5\u53caD(X) = D(Y) = 1. \u53ef\u4ee5\u5f97\u5230E(XY) = E(X)E(Y) = 0 * 0 = 0 \u540c\u7406, \u5bf9\u4e8eD(XY)\u63a8\u5bfc\u5982\u4e0b: \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \u6839\u636e\u671f\u671b\u548c\u65b9\u5dee\u7684\u6027\u8d28, \u5bf9\u4e8e\u4e92\u76f8\u72ec\u7acb\u7684\u53d8\u91cf\u6ee1\u8db3\u4e0b\u5f0f: E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) \u4e0a\u8ff0\u516c\u5f0f\uff0c\u7b80\u8bfb\u4e3a\uff1a\u548c\u7684\u671f\u671b\uff0c\u7b49\u4e8e\u671f\u671b\u7684\u548c\uff1b\u548c\u7684\u65b9\u5dee\u7b49\u4e8e\u65b9\u5dee\u7684\u548c \u6839\u636e\u4e0a\u9762\u7684\u516c\u5f0f, \u53ef\u4ee5\u5f88\u8f7b\u677e\u7684\u5f97\u51faq*k\u7684\u5747\u503c\u4e3aE(qk) = 0, D(qk) = d_k. \u6240\u4ee5\u65b9\u5dee\u8d8a\u5927, \u5bf9\u5e94\u7684qk\u7684\u70b9\u79ef\u5c31\u8d8a\u5927, \u8fd9\u6837softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c31\u4f1a\u66f4\u504f\u5411\u6700\u5927\u503c\u6240\u5728\u7684\u5206\u91cf. \u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5c06\u70b9\u79ef\u9664\u4ee5sqrt(d_k), \u5c06\u65b9\u5dee\u5728\u6570\u5b66\u4e0a\u91cd\u65b0\"\u62c9\u56de1\", \u5982\u4e0b\u6240\u793a: D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 \u6700\u7ec8\u7684\u7ed3\u8bba: \u901a\u8fc7\u6570\u5b66\u4e0a\u7684\u6280\u5de7\u5c06\u65b9\u5dee\u63a7\u5236\u57281, \u4e5f\u5c31\u6709\u6548\u7684\u63a7\u5236\u4e86\u70b9\u79ef\u7ed3\u679c\u7684\u53d1\u6563, \u4e5f\u5c31\u63a7\u5236\u4e86\u5bf9\u5e94\u7684\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898! 5 \u5c0f\u7ed3 \u00b6 self-attention\u673a\u5236\u7684\u91cd\u70b9\u662f\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u53c2\u4e0e\u89c4\u5219\u8fd0\u7b97, \u8fd9\u91cc\u9762Q=K=V. self-attention\u6700\u5927\u7684\u4f18\u52bf\u662f\u53ef\u4ee5\u65b9\u4fbf\u6709\u6548\u7684\u63d0\u53d6\u8fdc\u8ddd\u79bb\u4f9d\u8d56\u7684\u7279\u5f81\u548c\u7ed3\u6784\u4fe1\u606f, \u4e0d\u5fc5\u5411RNN\u90a3\u6837\u4f9d\u6b21\u8ba1\u7b97\u4ea7\u751f\u4f20\u9012\u635f\u8017. \u5173\u4e8eself-attention\u91c7\u7528\u4e09\u5143\u7ec4\u7684\u539f\u56e0, \u7ecf\u5178\u5b9e\u73b0\u7684\u65b9\u5f0f\u6570\u5b66\u610f\u4e49\u660e\u786e, \u7406\u7531\u5145\u5206, \u81f3\u4e8e\u5176\u4ed6\u65b9\u5f0f\u7684\u53ef\u884c\u6027\u6682\u65f6\u6ca1\u6709\u8bba\u6587\u505a\u5145\u5206\u7684\u5bf9\u6bd4\u8bd5\u9a8c\u7814\u7a76. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. softmax\u51fd\u6570\u672c\u8d28\u662f\u5bf9\u8f93\u5165\u7684\u6570\u636e\u5206\u5e03\u505a\u4e00\u6b21\u5f52\u4e00\u5316\u5904\u7406, \u4f46\u662f\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927. \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6, softmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u5177\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b\u89c1\u8bb2\u4e49\u6b63\u6587\u90e8\u5206, \u6ce8\u610f\u8981\u5206\u4e24\u79cd\u60c5\u51b5\u8ba8\u8bba, \u5206\u522b\u5904\u7406. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0. \u7ed3\u5408\u7b2c\u4e00\u6b65, \u7b2c\u4e8c\u6b65\u7684\u7ed3\u8bba, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u6700\u7ec8\u7684\u68af\u5ea6\u77e9\u9635\u63a5\u8fd1\u4e8e\u96f6\u77e9\u9635, \u8fd9\u6837\u5728\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\u7684\u65f6\u5019\u5c31\u4f1a\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u73b0\u8c61. \u5b66\u4e60\u4e86\u7ef4\u5ea6\u548c\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb\u63a8\u5bfc. \u901a\u8fc7\u671f\u671b\u548c\u65b9\u5dee\u7684\u63a8\u5bfc\u7406\u89e3\u4e86\u4e3a\u4ec0\u4e48\u70b9\u79ef\u4f1a\u9020\u6210\u65b9\u5dee\u53d8\u5927. \u7406\u89e3\u4e86\u901a\u8fc7\u6570\u5b66\u6280\u5de7\u9664\u4ee5sqrt(d_k)\u5c31\u53ef\u4ee5\u8ba9\u65b9\u5dee\u6062\u590d\u62101.","title":"4 Self attention\u673a\u5236\u8be6\u89e3"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#_1","text":"\u638c\u63e1self-attention\u7684\u673a\u5236\u548c\u539f\u7406. \u638c\u63e1\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u6765\u8ba1\u7b97self-attention. \u7406\u89e3softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. \u7406\u89e3softmax\u51fd\u6570\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u68af\u5ea6\u6c42\u5bfc\u7684\u6570\u5b66\u8fc7\u7a0b. \u7406\u89e3softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u539f\u56e0. \u7406\u89e3self-attention\u8ba1\u7b97\u89c4\u5219\u4e2d\u5f52\u4e00\u5316\u7684\u539f\u56e0. \u601d\u8003\u98981\uff1a Transformer\u4e2d\u4e00\u76f4\u5f3a\u8c03\u7684self-attention\u662f\u4ec0\u4e48? \u4e3a\u4ec0\u4e48\u80fd\u53d1\u6325\u5982\u6b64\u5927\u7684\u4f5c\u7528? \u8ba1\u7b97\u7684\u65f6\u5019\u5982\u679c\u4e0d\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V), \u800c\u4ec5\u4ec5\u4f7f\u7528(Q, V)\u6216\u8005(K, V)\u6216\u8005(V)\u884c\u4e0d\u884c? \u601d\u8003\u98982\uff1aself-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#1-self-attention","text":"self-attention\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u8eab\u548c\u81ea\u8eab\u8fdb\u884c\u5173\u8054\u7684attention\u673a\u5236, \u4ece\u800c\u5f97\u5230\u66f4\u597d\u7684representation\u6765\u8868\u8fbe\u81ea\u8eab. self-attention\u662fattention\u673a\u5236\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u5728self-attention\u4e2d, Q=K=V, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd(token)\u90fd\u548c\u8be5\u5e8f\u5217\u4e2d\u7684\u5176\u4ed6\u6240\u6709\u5355\u8bcd(token)\u8fdb\u884cattention\u89c4\u5219\u7684\u8ba1\u7b97. attention\u673a\u5236\u8ba1\u7b97\u7684\u7279\u70b9\u5728\u4e8e, \u53ef\u4ee5\u76f4\u63a5\u8de8\u8d8a\u4e00\u53e5\u8bdd\u4e2d\u4e0d\u540c\u8ddd\u79bb\u7684token, \u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u5b66\u4e60\u5230\u5e8f\u5217\u7684\u77e5\u8bc6\u4f9d\u8d56\u548c\u8bed\u5e8f\u7ed3\u6784. \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, self-attention\u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u6355\u6349\u5230\u8bed\u4e49\u5c42\u9762\u7684\u7279\u5f81(its\u7684\u6307\u4ee3\u5bf9\u8c61\u662fLaw). \u5e94\u7528\u4f20\u7edf\u7684RNN, LSTM, \u5728\u83b7\u53d6\u957f\u8ddd\u79bb\u8bed\u4e49\u7279\u5f81\u548c\u7ed3\u6784\u7279\u5f81\u7684\u65f6\u5019, \u9700\u8981\u6309\u7167\u5e8f\u5217\u987a\u5e8f\u4f9d\u6b21\u8ba1\u7b97, \u8ddd\u79bb\u8d8a\u8fdc\u7684\u8054\u7cfb\u4fe1\u606f\u7684\u635f\u8017\u8d8a\u5927, \u6709\u6548\u63d0\u53d6\u548c\u6355\u83b7\u7684\u53ef\u80fd\u6027\u8d8a\u5c0f. \u4f46\u662f\u5e94\u7528self-attention\u65f6, \u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4f1a\u76f4\u63a5\u5c06\u53e5\u5b50\u4e2d\u4efb\u610f\u4e24\u4e2atoken\u7684\u8054\u7cfb\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u6b65\u9aa4\u76f4\u63a5\u8054\u7cfb\u8d77\u6765, \u5173\u4e8eself-attention\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528(Q, K, V)\u4e09\u5143\u7ec4\u800c\u4e0d\u662f\u5176\u4ed6\u5f62\u5f0f: \u9996\u5148\u4e00\u6761\u5c31\u662f\u4ece\u5206\u6790\u7684\u89d2\u5ea6\u770b, \u67e5\u8be2Query\u662f\u4e00\u6761\u72ec\u7acb\u7684\u5e8f\u5217\u4fe1\u606f, \u901a\u8fc7\u5173\u952e\u8bcdKey\u7684\u63d0\u793a\u4f5c\u7528, \u5f97\u5230\u6700\u7ec8\u8bed\u4e49\u7684\u771f\u5b9e\u503cValue\u8868\u8fbe, \u6570\u5b66\u610f\u4e49\u66f4\u5145\u5206, \u5b8c\u5907. \u8fd9\u91cc\u4e0d\u4f7f\u7528(K, V)\u6216\u8005(V)\u6ca1\u6709\u4ec0\u4e48\u5fc5\u987b\u7684\u7406\u7531, \u4e5f\u6ca1\u6709\u76f8\u5173\u7684\u8bba\u6587\u6765\u4e25\u683c\u9610\u8ff0\u6bd4\u8f83\u8bd5\u9a8c\u7684\u7ed3\u679c\u5dee\u5f02, \u6240\u4ee5\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u653e\u6027\u95ee\u9898\u672a\u6765\u53bb\u63a2\u7d22, \u53ea\u8981\u660e\u786e\u5728\u7ecf\u5178self-attention\u5b9e\u73b0\u4e2d\u7528\u7684\u662f\u4e09\u5143\u7ec4\u5c31\u597d. self-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled?","title":"1 Self-attention\u7684\u673a\u5236\u548c\u539f\u7406"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#2-self-attention","text":"\u8bad\u7ec3\u4e0a\u7684\u610f\u4e49: \u968f\u7740\u8bcd\u5d4c\u5165\u7ef4\u5ea6d_k\u7684\u589e\u5927, q * k \u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4e5f\u4f1a\u589e\u5927, \u5728\u8bad\u7ec3\u65f6\u4f1a\u5c06\u5e26\u6709\u9971\u548c\u533a\u95f4\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u6bd4\u5982\uff1asigmoid\u6fc0\u6d3b\u51fd\u6570\u3001tanh\u6fc0\u6d3b\u51fd\u6570\u3001\u903b\u8f91\u56de\u5f52softmax\uff09\u63a8\u5165\u68af\u5ea6\u975e\u5e38\u5c0f\u7684\u533a\u57df, \u53ef\u80fd\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u73b0\u8c61, \u9020\u6210\u6a21\u578b\u6536\u655b\u56f0\u96be. \u6570\u5b66\u4e0a\u7684\u610f\u4e49: \u5047\u8bbeq\u548ck\u7684\u7edf\u8ba1\u53d8\u91cf\u662f\u6ee1\u8db3\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u72ec\u7acb\u968f\u673a\u53d8\u91cf, \u610f\u5473\u7740q\u548ck\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1. \u90a3\u4e48q\u548ck\u7684\u70b9\u79ef\u7ed3\u679c\u5c31\u662f\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3ad_k, \u4e3a\u4e86\u62b5\u6d88\u8fd9\u79cd\u65b9\u5dee\u88ab\u653e\u5927d_k\u500d\u7684\u5f71\u54cd, \u5728\u8ba1\u7b97\u4e2d\u4e3b\u52a8\u5c06\u70b9\u79ef\u7f29\u653e1/sqrt(d_k), \u8fd9\u6837\u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4f9d\u7136\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1.","title":"2 Self-attention\u4e2d\u7684\u5f52\u4e00\u5316\u6982\u8ff0"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#3-softmax","text":"\u8fd9\u91cc\u6211\u4eec\u52063\u4e2a\u6b65\u9aa4\u6765\u89e3\u91casoftmax\u7684\u68af\u5ea6\u95ee\u9898: \u7b2c\u4e00\u6b65: softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684. \u7b2c\u4e8c\u6b65: softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u7b2c\u4e09\u6b65: softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0.","title":"3 softmax\u7684\u68af\u5ea6\u53d8\u5316"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#31-softmax","text":"\u5bf9\u4e8e\u4e00\u4e2a\u8f93\u5165\u5411\u91cfx, softmax\u51fd\u6570\u5c06\u5176\u505a\u4e86\u4e00\u4e2a\u5f52\u4e00\u5316\u7684\u6620\u5c04, \u9996\u5148\u901a\u8fc7\u81ea\u7136\u5e95\u6570e\u5c06\u8f93\u5165\u5143\u7d20\u4e4b\u95f4\u7684\u5dee\u8ddd\u5148\"\u62c9\u5927\", \u7136\u540e\u518d\u5f52\u4e00\u5316\u4e3a\u4e00\u4e2a\u65b0\u7684\u5206\u5e03. \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5047\u8bbe\u67d0\u4e2a\u8f93\u5165x\u4e2d\u6700\u5927\u7684\u5143\u7d20\u4e0b\u6807\u662fk, \u5982\u679c\u8f93\u5165\u7684\u6570\u91cf\u7ea7\u53d8\u5927(\u5c31\u662fx\u4e2d\u7684\u6bcf\u4e2a\u5206\u91cf\u7edd\u5bf9\u503c\u90fd\u5f88\u5927), \u90a3\u4e48\u5728\u6570\u5b66\u4e0a\u4f1a\u9020\u6210y_k\u7684\u503c\u975e\u5e38\u63a5\u8fd11. \u5177\u4f53\u7528\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a, \u5047\u8bbe\u8f93\u5165\u7684\u5411\u91cfx = [a, a, 2a], \u90a3\u4e48\u968f\u4fbf\u7ed9\u51e0\u4e2a\u4e0d\u540c\u6570\u91cf\u7ea7\u7684\u503c\u6765\u770b\u770b\u5bf9y3\u4ea7\u751f\u7684\u5f71\u54cd a = 1\u65f6, y3 = 0.5761168847658291 # e^2 / (e^1 + e^1 + e^2)) a = 10\u65f6, y3 = 0.9999092083843412 # e^20 / (e^10 + e^10 + e^20)) a = 100\u65f6, y3 = 1.0 # e^200 / (e^100 + e^100 + e^200)) \u91c7\u7528\u4e00\u6bb5\u5b9e\u4f8b\u4ee3\u7801\u5c06a\u5728\u4e0d\u540c\u53d6\u503c\u4e0b, \u5bf9\u5e94\u7684y3\u5168\u90e8\u753b\u51fa\u6765, \u4ee5\u66f2\u7ebf\u7684\u5f62\u5f0f\u5c55\u793a: from math import exp from matplotlib import pyplot as plt import numpy as np f = lambda x : exp ( x * 2 ) / ( exp ( x ) + exp ( x ) + exp ( x * 2 )) x = np . linspace ( 0 , 100 , 100 ) y_3 = [ f ( x_i ) for x_i in x ] plt . plot ( x , y_3 ) plt . show () \u5f97\u5230\u5982\u4e0b\u7684\u66f2\u7ebf: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927 \u7ed3\u8bba\uff1a \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6\uff0csoftmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e\u3002\u901a\u4fd7\u7684\u8bb2\uff1a\u6570\u636e\u7684\u65b9\u5dee\u53d8\u5927\uff08\u79bb\u6563\u7a0b\u5ea6\u53d8\u5927\uff09\uff0c\u6700\u5927\u503c\u5f3a\u5360\u4e86\u6240\u6709\u6982\u7387\u3002","title":"3.1 softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#32-softmax","text":"softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u5bb9\u6613\u68af\u5ea6\u6d88\u5931\uff0c\u6240\u4ee5\u8981\u770b\u4e00\u770bsoftmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u662f\u5982\u4f55\u6c42\u5bfc\u7684\u3002 \u9996\u5148\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u548c\u8f93\u51fa: \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u53cd\u5411\u4f20\u64ad\u5c31\u662f\u8f93\u51fa\u7aef\u7684\u635f\u5931\u51fd\u6570\u5bf9\u8f93\u5165\u7aef\u6c42\u504f\u5bfc\u7684\u8fc7\u7a0b, \u8fd9\u91cc\u8981\u5206\u4e24\u79cd\u60c5\u51b5, \u7b2c\u4e00\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \u7b2c\u4e8c\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \u7ecf\u8fc7\u5bf9\u4e24\u79cd\u60c5\u51b5\u5206\u522b\u7684\u6c42\u5bfc\u8ba1\u7b97, \u53ef\u4ee5\u5f97\u51fa\u6700\u7ec8\u7684\u7ed3\u8bba\u5982\u4e0b: \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \u628a\u62bd\u8c61\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u6620\u5c04\u6210\u77e9\u9635\u8868\u793a\uff0c\u89c13.3\u8282\u8868\u793a\uff08i=j\u65f6\uff0c\u4e24\u4e2a\u77e9\u9635\u5bf9\u89d2\u7ebf - \u5bf9\u89d2\u7ebf \uff1bi!=j\u65f6\uff0c\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u51cf\uff09\u3002","title":"3.2 softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#33-softmax","text":"\u6839\u636e\u7b2c\u4e8c\u6b65\u4e2dsoftmax\u51fd\u6570\u7684\u6c42\u5bfc\u7ed3\u679c, \u53ef\u4ee5\u5c06\u6700\u7ec8\u7684\u7ed3\u679c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c55\u5f00\u5982\u4e0b: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \u6839\u636e\u7b2c\u4e00\u6b65\u4e2d\u7684\u8ba8\u8bba\u7ed3\u679c, \u5f53\u8f93\u5165x\u7684\u5206\u91cf\u503c\u8f83\u5927\u65f6, softmax\u51fd\u6570\u4f1a\u5c06\u5927\u90e8\u5206\u6982\u7387\u5206\u914d\u7ed9\u6700\u5927\u7684\u5143\u7d20, \u5047\u8bbe\u6700\u5927\u5143\u7d20\u662fx1, \u90a3\u4e48softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c06\u4ea7\u751f\u4e00\u4e2a\u63a5\u8fd1one-hot\u7684\u7ed3\u679c\u5f20\u91cfy_ = [1, 0, 0,..., 0], \u6b64\u65f6\u7ed3\u679c\u77e9\u9635\u53d8\u4e3a: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \u7ed3\u8bba: \u7efc\u4e0a\u53ef\u4ee5\u5f97\u51fa, \u6240\u6709\u7684\u68af\u5ea6\u90fd\u6d88\u5931\u4e3a0(\u63a5\u8fd1\u4e8e0), \u53c2\u6570\u51e0\u4e4e\u65e0\u6cd5\u66f4\u65b0, \u6a21\u578b\u6536\u655b\u56f0\u96be.","title":"3.3 softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#4","text":"\u9488\u5bf9\u4e3a\u4ec0\u4e48\u7ef4\u5ea6\u4f1a\u5f71\u54cd\u70b9\u79ef\u7684\u5927\u5c0f, \u539f\u59cb\u8bba\u6587\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u70b9\u89e3\u91ca\u5982\u4e0b: To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their doct product, q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k. \u6211\u4eec\u5206\u4e24\u6b65\u5bf9\u5176\u8fdb\u884c\u4e00\u4e2a\u63a8\u5bfc, \u9996\u5148\u5c31\u662f\u5047\u8bbe\u5411\u91cfq\u548ck\u7684\u5404\u4e2a\u5206\u91cf\u662f\u76f8\u4e92\u72ec\u7acb\u7684\u968f\u673a\u53d8\u91cf, X = q_i, Y = k_i, X\u548cY\u5404\u81ea\u6709d_k\u4e2a\u5206\u91cf, \u4e5f\u5c31\u662f\u5411\u91cf\u7684\u7ef4\u5ea6\u7b49\u4e8ed_k, \u6709E(X) = E(Y) = 0, \u4ee5\u53caD(X) = D(Y) = 1. \u53ef\u4ee5\u5f97\u5230E(XY) = E(X)E(Y) = 0 * 0 = 0 \u540c\u7406, \u5bf9\u4e8eD(XY)\u63a8\u5bfc\u5982\u4e0b: \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \u6839\u636e\u671f\u671b\u548c\u65b9\u5dee\u7684\u6027\u8d28, \u5bf9\u4e8e\u4e92\u76f8\u72ec\u7acb\u7684\u53d8\u91cf\u6ee1\u8db3\u4e0b\u5f0f: E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) \u4e0a\u8ff0\u516c\u5f0f\uff0c\u7b80\u8bfb\u4e3a\uff1a\u548c\u7684\u671f\u671b\uff0c\u7b49\u4e8e\u671f\u671b\u7684\u548c\uff1b\u548c\u7684\u65b9\u5dee\u7b49\u4e8e\u65b9\u5dee\u7684\u548c \u6839\u636e\u4e0a\u9762\u7684\u516c\u5f0f, \u53ef\u4ee5\u5f88\u8f7b\u677e\u7684\u5f97\u51faq*k\u7684\u5747\u503c\u4e3aE(qk) = 0, D(qk) = d_k. \u6240\u4ee5\u65b9\u5dee\u8d8a\u5927, \u5bf9\u5e94\u7684qk\u7684\u70b9\u79ef\u5c31\u8d8a\u5927, \u8fd9\u6837softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c31\u4f1a\u66f4\u504f\u5411\u6700\u5927\u503c\u6240\u5728\u7684\u5206\u91cf. \u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5c06\u70b9\u79ef\u9664\u4ee5sqrt(d_k), \u5c06\u65b9\u5dee\u5728\u6570\u5b66\u4e0a\u91cd\u65b0\"\u62c9\u56de1\", \u5982\u4e0b\u6240\u793a: D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 \u6700\u7ec8\u7684\u7ed3\u8bba: \u901a\u8fc7\u6570\u5b66\u4e0a\u7684\u6280\u5de7\u5c06\u65b9\u5dee\u63a7\u5236\u57281, \u4e5f\u5c31\u6709\u6548\u7684\u63a7\u5236\u4e86\u70b9\u79ef\u7ed3\u679c\u7684\u53d1\u6563, \u4e5f\u5c31\u63a7\u5236\u4e86\u5bf9\u5e94\u7684\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898!","title":"4 \u7ef4\u5ea6\u4e0e\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb"},{"location":"06_mkdocs_pretrained_model_old/4%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#5","text":"self-attention\u673a\u5236\u7684\u91cd\u70b9\u662f\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u53c2\u4e0e\u89c4\u5219\u8fd0\u7b97, \u8fd9\u91cc\u9762Q=K=V. self-attention\u6700\u5927\u7684\u4f18\u52bf\u662f\u53ef\u4ee5\u65b9\u4fbf\u6709\u6548\u7684\u63d0\u53d6\u8fdc\u8ddd\u79bb\u4f9d\u8d56\u7684\u7279\u5f81\u548c\u7ed3\u6784\u4fe1\u606f, \u4e0d\u5fc5\u5411RNN\u90a3\u6837\u4f9d\u6b21\u8ba1\u7b97\u4ea7\u751f\u4f20\u9012\u635f\u8017. \u5173\u4e8eself-attention\u91c7\u7528\u4e09\u5143\u7ec4\u7684\u539f\u56e0, \u7ecf\u5178\u5b9e\u73b0\u7684\u65b9\u5f0f\u6570\u5b66\u610f\u4e49\u660e\u786e, \u7406\u7531\u5145\u5206, \u81f3\u4e8e\u5176\u4ed6\u65b9\u5f0f\u7684\u53ef\u884c\u6027\u6682\u65f6\u6ca1\u6709\u8bba\u6587\u505a\u5145\u5206\u7684\u5bf9\u6bd4\u8bd5\u9a8c\u7814\u7a76. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. softmax\u51fd\u6570\u672c\u8d28\u662f\u5bf9\u8f93\u5165\u7684\u6570\u636e\u5206\u5e03\u505a\u4e00\u6b21\u5f52\u4e00\u5316\u5904\u7406, \u4f46\u662f\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927. \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6, softmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u5177\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b\u89c1\u8bb2\u4e49\u6b63\u6587\u90e8\u5206, \u6ce8\u610f\u8981\u5206\u4e24\u79cd\u60c5\u51b5\u8ba8\u8bba, \u5206\u522b\u5904\u7406. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0. \u7ed3\u5408\u7b2c\u4e00\u6b65, \u7b2c\u4e8c\u6b65\u7684\u7ed3\u8bba, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u6700\u7ec8\u7684\u68af\u5ea6\u77e9\u9635\u63a5\u8fd1\u4e8e\u96f6\u77e9\u9635, \u8fd9\u6837\u5728\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\u7684\u65f6\u5019\u5c31\u4f1a\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u73b0\u8c61. \u5b66\u4e60\u4e86\u7ef4\u5ea6\u548c\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb\u63a8\u5bfc. \u901a\u8fc7\u671f\u671b\u548c\u65b9\u5dee\u7684\u63a8\u5bfc\u7406\u89e3\u4e86\u4e3a\u4ec0\u4e48\u70b9\u79ef\u4f1a\u9020\u6210\u65b9\u5dee\u53d8\u5927. \u7406\u89e3\u4e86\u901a\u8fc7\u6570\u5b66\u6280\u5de7\u9664\u4ee5sqrt(d_k)\u5c31\u53ef\u4ee5\u8ba9\u65b9\u5dee\u6062\u590d\u62101.","title":"5 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/5%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Transformer\u4e2d\u5e94\u7528\u591a\u5934\u6ce8\u610f\u529b\u7684\u539f\u56e0. \u638c\u63e1Transformer\u4e2d\u591a\u5934\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u65b9\u5f0f. \u601d\u8003\u9898\uff1aTransformer\u4e3a\u4ec0\u4e48\u9700\u8981\u8fdb\u884cMulti-head Attention? Multi-head Attention\u7684\u8ba1\u7b97\u8fc7\u7a0b\u662f\u4ec0\u4e48? 1 \u91c7\u7528Multi-head Attention\u7684\u539f\u56e0 \u00b6 \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u8fdb\u884cMulti-head Attention\u7684\u539f\u56e0\u662f\u5c06\u6a21\u578b\u5206\u4e3a\u591a\u4e2a\u5934, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f, \u6700\u540e\u518d\u5c06\u5404\u4e2a\u65b9\u9762\u7684\u4fe1\u606f\u7efc\u5408\u8d77\u6765\u5f97\u5230\u66f4\u597d\u7684\u6548\u679c. \u591a\u4e2a\u5934\u8fdb\u884cattention\u8ba1\u7b97\u6700\u540e\u518d\u7efc\u5408\u8d77\u6765, \u7c7b\u4f3c\u4e8eCNN\u4e2d\u91c7\u7528\u591a\u4e2a\u5377\u79ef\u6838\u7684\u4f5c\u7528, \u4e0d\u540c\u7684\u5377\u79ef\u6838\u63d0\u53d6\u4e0d\u540c\u7684\u7279\u5f81, \u5173\u6ce8\u4e0d\u540c\u7684\u90e8\u5206, \u6700\u540e\u518d\u8fdb\u884c\u878d\u5408. \u76f4\u89c2\u4e0a\u8bb2, \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f. 2 Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f \u00b6 Multi-head Attention\u548c\u5355\u4e00head\u7684Attention\u552f\u4e00\u7684\u533a\u522b\u5c31\u5728\u4e8e, \u5176\u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684embedding_dim=512\u8fdb\u884c\u5207\u5272\u6210head=8, \u8fd9\u6837\u6bcf\u4e00\u4e2ahead\u7684\u5d4c\u5165\u7ef4\u5ea6\u5c31\u662f512/8=64, \u540e\u7eed\u7684Attention\u8ba1\u7b97\u516c\u5f0f\u5b8c\u5168\u4e00\u81f4, \u53ea\u4e0d\u8fc7\u662f\u572864\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u77e9\u9635\u8fd0\u7b97\u800c\u5df2. \u5728head=8\u4e2a\u5934\u4e0a\u5206\u522b\u8fdb\u884c\u6ce8\u610f\u529b\u89c4\u5219\u7684\u8fd0\u7b97\u540e, \u7b80\u5355\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u5bf9\u7ed3\u679c\u5f20\u91cf\u8fdb\u884c\u878d\u5408\u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u7ed3\u679c. 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u91c7\u7528Multi-head Attention\u7684\u539f\u56e0. \u5c06\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u5934, \u5206\u522b\u8fdb\u884cAttention\u8ba1\u7b97, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f\u7279\u5f81, \u66f4\u597d\u7684\u63d0\u5347\u6a21\u578b\u7684\u6548\u679c. \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f. \u5b66\u4e60\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f. \u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6embedding_dim\u8fdb\u884c\u5207\u5272, \u5207\u5272\u540e\u7684\u8ba1\u7b97\u89c4\u5219\u548c\u5355\u4e00head\u5b8c\u5168\u4e00\u81f4. \u5728\u4e0d\u540c\u7684head\u4e0a\u5e94\u7528\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u540e, \u5f97\u5230\u7684\u7ed3\u679c\u5f20\u91cf\u76f4\u63a5\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u8fdb\u884c\u878d\u5408, \u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u7ed3\u679c\u5f20\u91cf.","title":"5 Multi head Attention\u8be6\u89e3"},{"location":"06_mkdocs_pretrained_model_old/5%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#_1","text":"\u638c\u63e1Transformer\u4e2d\u5e94\u7528\u591a\u5934\u6ce8\u610f\u529b\u7684\u539f\u56e0. \u638c\u63e1Transformer\u4e2d\u591a\u5934\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u65b9\u5f0f. \u601d\u8003\u9898\uff1aTransformer\u4e3a\u4ec0\u4e48\u9700\u8981\u8fdb\u884cMulti-head Attention? Multi-head Attention\u7684\u8ba1\u7b97\u8fc7\u7a0b\u662f\u4ec0\u4e48?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/5%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#1-multi-head-attention","text":"\u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u8fdb\u884cMulti-head Attention\u7684\u539f\u56e0\u662f\u5c06\u6a21\u578b\u5206\u4e3a\u591a\u4e2a\u5934, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f, \u6700\u540e\u518d\u5c06\u5404\u4e2a\u65b9\u9762\u7684\u4fe1\u606f\u7efc\u5408\u8d77\u6765\u5f97\u5230\u66f4\u597d\u7684\u6548\u679c. \u591a\u4e2a\u5934\u8fdb\u884cattention\u8ba1\u7b97\u6700\u540e\u518d\u7efc\u5408\u8d77\u6765, \u7c7b\u4f3c\u4e8eCNN\u4e2d\u91c7\u7528\u591a\u4e2a\u5377\u79ef\u6838\u7684\u4f5c\u7528, \u4e0d\u540c\u7684\u5377\u79ef\u6838\u63d0\u53d6\u4e0d\u540c\u7684\u7279\u5f81, \u5173\u6ce8\u4e0d\u540c\u7684\u90e8\u5206, \u6700\u540e\u518d\u8fdb\u884c\u878d\u5408. \u76f4\u89c2\u4e0a\u8bb2, \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f.","title":"1 \u91c7\u7528Multi-head Attention\u7684\u539f\u56e0"},{"location":"06_mkdocs_pretrained_model_old/5%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#2-multi-head-attention","text":"Multi-head Attention\u548c\u5355\u4e00head\u7684Attention\u552f\u4e00\u7684\u533a\u522b\u5c31\u5728\u4e8e, \u5176\u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684embedding_dim=512\u8fdb\u884c\u5207\u5272\u6210head=8, \u8fd9\u6837\u6bcf\u4e00\u4e2ahead\u7684\u5d4c\u5165\u7ef4\u5ea6\u5c31\u662f512/8=64, \u540e\u7eed\u7684Attention\u8ba1\u7b97\u516c\u5f0f\u5b8c\u5168\u4e00\u81f4, \u53ea\u4e0d\u8fc7\u662f\u572864\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u77e9\u9635\u8fd0\u7b97\u800c\u5df2. \u5728head=8\u4e2a\u5934\u4e0a\u5206\u522b\u8fdb\u884c\u6ce8\u610f\u529b\u89c4\u5219\u7684\u8fd0\u7b97\u540e, \u7b80\u5355\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u5bf9\u7ed3\u679c\u5f20\u91cf\u8fdb\u884c\u878d\u5408\u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u7ed3\u679c.","title":"2 Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f"},{"location":"06_mkdocs_pretrained_model_old/5%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#3","text":"\u5b66\u4e60\u4e86Transformer\u67b6\u6784\u91c7\u7528Multi-head Attention\u7684\u539f\u56e0. \u5c06\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u5934, \u5206\u522b\u8fdb\u884cAttention\u8ba1\u7b97, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f\u7279\u5f81, \u66f4\u597d\u7684\u63d0\u5347\u6a21\u578b\u7684\u6548\u679c. \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f. \u5b66\u4e60\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f. \u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6embedding_dim\u8fdb\u884c\u5207\u5272, \u5207\u5272\u540e\u7684\u8ba1\u7b97\u89c4\u5219\u548c\u5355\u4e00head\u5b8c\u5168\u4e00\u81f4. \u5728\u4e0d\u540c\u7684head\u4e0a\u5e94\u7528\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u540e, \u5f97\u5230\u7684\u7ed3\u679c\u5f20\u91cf\u76f4\u63a5\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u8fdb\u884c\u878d\u5408, \u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u7ed3\u679c\u5f20\u91cf.","title":"3 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u80cc\u540e\u7684\u539f\u56e0. \u638c\u63e1Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684. \u7406\u89e3\u4e3a\u4ec0\u4e48\u91c7\u7528\u8fd9\u6837\u7684\u65b9\u5f0f\u53ef\u4ee5\u5b9e\u73b0Transformer\u7684\u5e76\u884c\u5316. \u638c\u63e1Transformer\u53ef\u4ee5\u66ff\u4ee3seq2seq\u7684\u6838\u5fc3\u539f\u56e0. \u601d\u8003\u9898\uff1atransformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684? \u5177\u4f53\u4f53\u73b0\u5728\u54ea\u91cc? \u601d\u8003\u9898\uff1aTransformer\u76f8\u6bd4\u4e8eRNN/LSTM\u6709\u4ec0\u4e48\u4f18\u52bf? \u4e3a\u4ec0\u4e48? \u601d\u8003\u9898\uff1a\u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq? 1 Transformer\u7684\u5e76\u884c\u8ba1\u7b97 \u00b6 \u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e00\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b. \u5bf9\u4e8eRNN\u6765\u8bf4, \u4efb\u610f\u65f6\u523bt\u7684\u8f93\u5165\u662f\u65f6\u523bt\u7684\u8f93\u5165x(t)\u548c\u4e0a\u4e00\u65f6\u523b\u7684\u9690\u85cf\u5c42\u8f93\u51fah(t-1), \u7ecf\u8fc7\u8fd0\u7b97\u540e\u5f97\u5230\u5f53\u524d\u65f6\u523b\u9690\u85cf\u5c42\u7684\u8f93\u51fah(t), \u8fd9\u4e2ah(t)\u4e5f\u5373\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523bt+1\u7684\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u8fd9\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u662fRNN\u7684\u672c\u8d28\u7279\u5f81, RNN\u7684\u5386\u53f2\u4fe1\u606f\u662f\u9700\u8981\u901a\u8fc7\u8fd9\u4e2a\u65f6\u95f4\u6b65\u4e00\u6b65\u4e00\u6b65\u5411\u540e\u4f20\u9012\u7684. \u800c\u8fd9\u5c31\u610f\u5473\u7740RNN\u5e8f\u5217\u540e\u9762\u7684\u4fe1\u606f\u53ea\u80fd\u7b49\u5230\u524d\u9762\u7684\u8ba1\u7b97\u7ed3\u675f\u540e, \u5c06\u5386\u53f2\u4fe1\u606f\u901a\u8fc7hidden state\u4f20\u9012\u7ed9\u540e\u9762\u624d\u80fd\u5f00\u59cb\u8ba1\u7b97, \u5f62\u6210\u94fe\u5f0f\u7684\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u5b9e\u73b0\u5e76\u884c. \u5bf9\u4e8eTransformer\u7ed3\u6784\u6765\u8bf4, \u5728self-attention\u5c42, \u65e0\u8bba\u5e8f\u5217\u7684\u957f\u5ea6\u662f\u591a\u5c11, \u90fd\u53ef\u4ee5\u4e00\u6b21\u6027\u8ba1\u7b97\u6240\u6709\u5355\u8bcd\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u5173\u7cfb, \u8fd9\u4e2aattention\u7684\u8ba1\u7b97\u662f\u540c\u6b65\u7684, \u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c. 2 Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u8fc7\u7a0b \u00b6 2.1 Transformer\u67b6\u6784\u4e2dEncoder\u7684\u5e76\u884c\u5316 \u00b6 \u9996\u5148Transformer\u7684\u5e76\u884c\u5316\u4e3b\u8981\u4f53\u73b0\u5728Encoder\u6a21\u5757\u4e0a. \u4e0a\u56fe\u6700\u5e95\u5c42\u7eff\u8272\u7684\u90e8\u5206, \u6574\u4e2a\u5e8f\u5217\u6240\u6709\u7684token\u53ef\u4ee5\u5e76\u884c\u7684\u8fdb\u884cEmbedding\u64cd\u4f5c, \u8fd9\u4e00\u5c42\u7684\u5904\u7406\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684. \u4e0a\u56fe\u7b2c\u4e8c\u5c42\u571f\u9ec4\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662fTransformer\u4e2d\u6700\u91cd\u8981\u7684self-attention\u90e8\u5206, \u8fd9\u91cc\u5bf9\u4e8e\u4efb\u610f\u4e00\u4e2a\u5355\u8bcd\u6bd4\u5982x1, \u8981\u8ba1\u7b97x1\u5bf9\u4e8e\u5176\u4ed6\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5206\u5e03, \u5f97\u5230z1. \u8fd9\u4e2a\u8fc7\u7a0b\u662f\u5177\u6709\u4f9d\u8d56\u6027\u7684, \u5fc5\u987b\u7b49\u5230\u5e8f\u5217\u4e2d\u6240\u6709\u7684\u5355\u8bcd\u5b8c\u6210Embedding\u624d\u53ef\u4ee5\u8fdb\u884c. \u56e0\u6b64\u8fd9\u4e00\u6b65\u662f\u4e0d\u80fd\u5e76\u884c\u5904\u7406\u7684. \u4f46\u662f\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\u770b, \u6211\u4eec\u771f\u5b9e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u5e03\u7684\u65f6\u5019, \u91c7\u7528\u7684\u90fd\u662f\u77e9\u9635\u8fd0\u7b97, \u4e5f\u5c31\u662f\u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u8ba1\u7b97\u51fa\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5f20\u91cf, \u4ece\u8fd9\u4e2a\u89d2\u5ea6\u770b\u4e5f\u7b97\u662f\u5b9e\u73b0\u4e86\u5e76\u884c, \u53ea\u662f\u77e9\u9635\u8fd0\u7b97\u7684\"\u5e76\u884c\"\u548c\u8bcd\u5d4c\u5165\u7684\"\u5e76\u884c\"\u6982\u5ff5\u4e0a\u4e0d\u540c\u800c\u5df2. \u4e0a\u56fe\u7b2c\u4e09\u5c42\u84dd\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u5bf9\u4e8e\u4e0d\u540c\u7684\u5411\u91cfz\u4e4b\u95f4\u4e5f\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684, \u6240\u4ee5\u8fd9\u4e00\u5c42\u662f\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316\u5904\u7406\u7684. \u4e5f\u5c31\u662f\u6240\u6709\u7684\u5411\u91cfz\u8f93\u5165Feed Forward\u7f51\u7edc\u7684\u8ba1\u7b97\u53ef\u4ee5\u540c\u6b65\u8fdb\u884c, \u4e92\u4e0d\u5e72\u6270. 2.2 Transformer\u67b6\u6784\u4e2dDecoder\u7684\u5e76\u884c\u5316 \u00b6 \u5176\u6b21Transformer\u7684\u5e76\u884c\u5316\u4e5f\u90e8\u5206\u7684\u4f53\u73b0\u5728Decoder\u6a21\u5757\u4e0a. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u5176\u4e2dSelf-Attention\u548cEncoder-Decoder Attention\u4e24\u4e2a\u5b50\u5c42\u7684\u5e76\u884c\u5316\u4e5f\u662f\u5728\u8fdb\u884c\u77e9\u9635\u4e58\u6cd5, \u548cEncoder\u7684\u7406\u89e3\u662f\u4e00\u81f4\u7684. \u5728\u8fdb\u884cEmbedding\u548cFeed Forward\u7684\u5904\u7406\u65f6, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb, \u6240\u4ee5\u4e5f\u662f\u53ef\u4ee5\u5b8c\u5168\u5e76\u884c\u5316\u5904\u7406\u7684, \u8fd9\u91cc\u548cEncoder\u7684\u7406\u89e3\u4e5f\u662f\u4e00\u81f4\u7684. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u9636\u6bb5\u57fa\u672c\u4e0a\u4e0d\u8ba4\u4e3a\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u56e0\u4e3a\u7b2c\u4e00\u4e2atime step\u7684\u8f93\u5165\u53ea\u662f\u4e00\u4e2a\"SOS\", \u540e\u7eed\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u4e5f\u53ea\u662f\u4f9d\u6b21\u6dfb\u52a0\u4e4b\u524d\u6240\u6709\u7684\u9884\u6d4btoken. \u6ce8\u610f: \u6700\u91cd\u8981\u7684\u533a\u522b\u662f\u8bad\u7ec3\u9636\u6bb5\u76ee\u6807\u6587\u672c\u5982\u679c\u670920\u4e2atoken, \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u662f\u4e00\u6b21\u6027\u7684\u8f93\u5165\u7ed9Decoder\u7aef, \u53ef\u4ee5\u505a\u5230\u4e00\u4e9b\u5b50\u5c42\u7684\u5e76\u884c\u5316\u5904\u7406. \u4f46\u662f\u5728\u9884\u6d4b\u9636\u6bb5, \u5982\u679c\u9884\u6d4b\u7684\u7ed3\u679c\u8bed\u53e5\u603b\u5171\u670920\u4e2atoken, \u5219\u9700\u8981\u91cd\u590d\u5904\u740620\u6b21\u5faa\u73af\u7684\u8fc7\u7a0b, \u6bcf\u6b21\u7684\u8f93\u5165\u6dfb\u52a0\u8fdb\u53bb\u4e00\u4e2atoken, \u6bcf\u6b21\u7684\u8f93\u5165\u5e8f\u5217\u6bd4\u4e0a\u4e00\u6b21\u591a\u4e00\u4e2atoken, \u6240\u4ee5\u4e0d\u8ba4\u4e3a\u662f\u5e76\u884c\u5904\u7406. 3 Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b \u00b6 \u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e8c\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b. Transformer\u56e0\u4e3a\u91c7\u7528\u4e86Multi-head Attention\u7ed3\u6784\u548c\u8ba1\u7b97\u673a\u5236, \u62e5\u6709\u6bd4RNN/LSTM\u66f4\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b, \u8fd9\u91cc\u5e76\u4e0d\u4ec5\u4ec5\u7531\u7406\u8bba\u5206\u6790\u5f97\u6765, \u800c\u662f\u5927\u91cf\u7684\u8bd5\u9a8c\u6570\u636e\u548c\u5bf9\u6bd4\u7ed3\u679c, \u6e05\u695a\u7684\u5c55\u793a\u4e86Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b\u8fdc\u8fdc\u80dc\u4e8eRNN/LSTM. \u6ce8\u610f: \u4e0d\u662f\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u5c31\u8d8a\u65e0\u654c, \u5728\u5f88\u591a\u5177\u4f53\u7684\u5e94\u7528\u4e2dRNN/LSTM\u4f9d\u7136\u5927\u6709\u7528\u6b66\u4e4b\u5730, \u8981\u5177\u4f53\u95ee\u9898\u5177\u4f53\u5206\u6790. 4 \u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq? \u00b6 4.1 seq2seq\u7684\u4e24\u5927\u7f3a\u9677 \u00b6 seq2seq\u67b6\u6784\u7684\u7b2c\u4e00\u5927\u7f3a\u9677\u662f\u5c06Encoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u8bed\u4e49\u5411\u91cf\u4e2d, \u7528\u8fd9\u4e2a\u56fa\u5b9a\u7684\u5411\u91cf\u6765\u4ee3\u8868\u7f16\u7801\u5668\u7aef\u7684\u5168\u90e8\u4fe1\u606f. \u8fd9\u6837\u65e2\u4f1a\u9020\u6210\u4fe1\u606f\u7684\u635f\u8017, \u4e5f\u65e0\u6cd5\u8ba9Decoder\u7aef\u5728\u89e3\u7801\u7684\u65f6\u5019\u53bb\u7528\u6ce8\u610f\u529b\u805a\u7126\u54ea\u4e9b\u662f\u66f4\u91cd\u8981\u7684\u4fe1\u606f. seq2seq\u67b6\u6784\u7684\u7b2c\u4e8c\u5927\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c, \u672c\u8d28\u4e0a\u548cRNN/LSTM\u65e0\u6cd5\u5e76\u884c\u7684\u539f\u56e0\u4e00\u6837. 4.2 Transformer\u7684\u6539\u8fdb \u00b6 Transformer\u67b6\u6784\u540c\u65f6\u89e3\u51b3\u4e86seq2seq\u7684\u4e24\u5927\u7f3a\u9677, \u65e2\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97, \u53c8\u5e94\u7528Multi-head Attention\u673a\u5236\u6765\u89e3\u51b3Encoder\u56fa\u5b9a\u7f16\u7801\u7684\u95ee\u9898, \u8ba9Decoder\u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u53bb\u5173\u6ce8\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u6700\u91cd\u8981\u7684\u90a3\u4e9b\u90e8\u5206. 5 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u539f\u56e0. \u7b2c\u4e00\u5927\u4f18\u52bf\u662f\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf. \u7b2c\u4e8c\u5927\u4f18\u52bf\u662f\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dEncoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Encoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u6d4b\u8bd5\u9636\u6bb5\u90fd\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u76f8\u540c\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dDecoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u7684Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u8ba1\u7b97\u4e0d\u80fd\u5e76\u884c\u5316\u5904\u7406. \u5b66\u4e60\u4e86seq2seq\u67b6\u6784\u7684\u4e24\u5927\u7f3a\u9677. \u7b2c\u4e00\u4e2a\u7f3a\u9677\u662fEncoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u88ab\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u7684\u8f93\u51fa\u5f20\u91cf, \u5f53\u5e8f\u5217\u957f\u5ea6\u8f83\u957f\u65f6\u4f1a\u9020\u6210\u6bd4\u8f83\u4e25\u91cd\u7684\u4fe1\u606f\u635f\u8017. \u7b2c\u4e8c\u4e2a\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c\u8ba1\u7b97. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u5bf9seq2seq\u4e24\u5927\u7f3a\u9677\u7684\u6539\u8fdb. Transformer\u5e94\u7528Multi-head Attention\u673a\u5236\u8ba9\u7f16\u7801\u5668\u4fe1\u606f\u53ef\u4ee5\u66f4\u597d\u7684\u5c55\u793a\u7ed9\u89e3\u7801\u5668. Transformer\u53ef\u4ee5\u5b9e\u73b0Encoder\u7aef\u7684\u5e76\u884c\u8ba1\u7b97.","title":"6 Transformer\u4f18\u52bf"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#_1","text":"\u638c\u63e1Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u80cc\u540e\u7684\u539f\u56e0. \u638c\u63e1Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684. \u7406\u89e3\u4e3a\u4ec0\u4e48\u91c7\u7528\u8fd9\u6837\u7684\u65b9\u5f0f\u53ef\u4ee5\u5b9e\u73b0Transformer\u7684\u5e76\u884c\u5316. \u638c\u63e1Transformer\u53ef\u4ee5\u66ff\u4ee3seq2seq\u7684\u6838\u5fc3\u539f\u56e0. \u601d\u8003\u9898\uff1atransformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684? \u5177\u4f53\u4f53\u73b0\u5728\u54ea\u91cc? \u601d\u8003\u9898\uff1aTransformer\u76f8\u6bd4\u4e8eRNN/LSTM\u6709\u4ec0\u4e48\u4f18\u52bf? \u4e3a\u4ec0\u4e48? \u601d\u8003\u9898\uff1a\u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#1-transformer","text":"\u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e00\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b. \u5bf9\u4e8eRNN\u6765\u8bf4, \u4efb\u610f\u65f6\u523bt\u7684\u8f93\u5165\u662f\u65f6\u523bt\u7684\u8f93\u5165x(t)\u548c\u4e0a\u4e00\u65f6\u523b\u7684\u9690\u85cf\u5c42\u8f93\u51fah(t-1), \u7ecf\u8fc7\u8fd0\u7b97\u540e\u5f97\u5230\u5f53\u524d\u65f6\u523b\u9690\u85cf\u5c42\u7684\u8f93\u51fah(t), \u8fd9\u4e2ah(t)\u4e5f\u5373\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523bt+1\u7684\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u8fd9\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u662fRNN\u7684\u672c\u8d28\u7279\u5f81, RNN\u7684\u5386\u53f2\u4fe1\u606f\u662f\u9700\u8981\u901a\u8fc7\u8fd9\u4e2a\u65f6\u95f4\u6b65\u4e00\u6b65\u4e00\u6b65\u5411\u540e\u4f20\u9012\u7684. \u800c\u8fd9\u5c31\u610f\u5473\u7740RNN\u5e8f\u5217\u540e\u9762\u7684\u4fe1\u606f\u53ea\u80fd\u7b49\u5230\u524d\u9762\u7684\u8ba1\u7b97\u7ed3\u675f\u540e, \u5c06\u5386\u53f2\u4fe1\u606f\u901a\u8fc7hidden state\u4f20\u9012\u7ed9\u540e\u9762\u624d\u80fd\u5f00\u59cb\u8ba1\u7b97, \u5f62\u6210\u94fe\u5f0f\u7684\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u5b9e\u73b0\u5e76\u884c. \u5bf9\u4e8eTransformer\u7ed3\u6784\u6765\u8bf4, \u5728self-attention\u5c42, \u65e0\u8bba\u5e8f\u5217\u7684\u957f\u5ea6\u662f\u591a\u5c11, \u90fd\u53ef\u4ee5\u4e00\u6b21\u6027\u8ba1\u7b97\u6240\u6709\u5355\u8bcd\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u5173\u7cfb, \u8fd9\u4e2aattention\u7684\u8ba1\u7b97\u662f\u540c\u6b65\u7684, \u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c.","title":"1 Transformer\u7684\u5e76\u884c\u8ba1\u7b97"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#2-transformer","text":"","title":"2 Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u8fc7\u7a0b"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#21-transformerencoder","text":"\u9996\u5148Transformer\u7684\u5e76\u884c\u5316\u4e3b\u8981\u4f53\u73b0\u5728Encoder\u6a21\u5757\u4e0a. \u4e0a\u56fe\u6700\u5e95\u5c42\u7eff\u8272\u7684\u90e8\u5206, \u6574\u4e2a\u5e8f\u5217\u6240\u6709\u7684token\u53ef\u4ee5\u5e76\u884c\u7684\u8fdb\u884cEmbedding\u64cd\u4f5c, \u8fd9\u4e00\u5c42\u7684\u5904\u7406\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684. \u4e0a\u56fe\u7b2c\u4e8c\u5c42\u571f\u9ec4\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662fTransformer\u4e2d\u6700\u91cd\u8981\u7684self-attention\u90e8\u5206, \u8fd9\u91cc\u5bf9\u4e8e\u4efb\u610f\u4e00\u4e2a\u5355\u8bcd\u6bd4\u5982x1, \u8981\u8ba1\u7b97x1\u5bf9\u4e8e\u5176\u4ed6\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5206\u5e03, \u5f97\u5230z1. \u8fd9\u4e2a\u8fc7\u7a0b\u662f\u5177\u6709\u4f9d\u8d56\u6027\u7684, \u5fc5\u987b\u7b49\u5230\u5e8f\u5217\u4e2d\u6240\u6709\u7684\u5355\u8bcd\u5b8c\u6210Embedding\u624d\u53ef\u4ee5\u8fdb\u884c. \u56e0\u6b64\u8fd9\u4e00\u6b65\u662f\u4e0d\u80fd\u5e76\u884c\u5904\u7406\u7684. \u4f46\u662f\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\u770b, \u6211\u4eec\u771f\u5b9e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u5e03\u7684\u65f6\u5019, \u91c7\u7528\u7684\u90fd\u662f\u77e9\u9635\u8fd0\u7b97, \u4e5f\u5c31\u662f\u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u8ba1\u7b97\u51fa\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5f20\u91cf, \u4ece\u8fd9\u4e2a\u89d2\u5ea6\u770b\u4e5f\u7b97\u662f\u5b9e\u73b0\u4e86\u5e76\u884c, \u53ea\u662f\u77e9\u9635\u8fd0\u7b97\u7684\"\u5e76\u884c\"\u548c\u8bcd\u5d4c\u5165\u7684\"\u5e76\u884c\"\u6982\u5ff5\u4e0a\u4e0d\u540c\u800c\u5df2. \u4e0a\u56fe\u7b2c\u4e09\u5c42\u84dd\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u5bf9\u4e8e\u4e0d\u540c\u7684\u5411\u91cfz\u4e4b\u95f4\u4e5f\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684, \u6240\u4ee5\u8fd9\u4e00\u5c42\u662f\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316\u5904\u7406\u7684. \u4e5f\u5c31\u662f\u6240\u6709\u7684\u5411\u91cfz\u8f93\u5165Feed Forward\u7f51\u7edc\u7684\u8ba1\u7b97\u53ef\u4ee5\u540c\u6b65\u8fdb\u884c, \u4e92\u4e0d\u5e72\u6270.","title":"2.1 Transformer\u67b6\u6784\u4e2dEncoder\u7684\u5e76\u884c\u5316"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#22-transformerdecoder","text":"\u5176\u6b21Transformer\u7684\u5e76\u884c\u5316\u4e5f\u90e8\u5206\u7684\u4f53\u73b0\u5728Decoder\u6a21\u5757\u4e0a. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u5176\u4e2dSelf-Attention\u548cEncoder-Decoder Attention\u4e24\u4e2a\u5b50\u5c42\u7684\u5e76\u884c\u5316\u4e5f\u662f\u5728\u8fdb\u884c\u77e9\u9635\u4e58\u6cd5, \u548cEncoder\u7684\u7406\u89e3\u662f\u4e00\u81f4\u7684. \u5728\u8fdb\u884cEmbedding\u548cFeed Forward\u7684\u5904\u7406\u65f6, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb, \u6240\u4ee5\u4e5f\u662f\u53ef\u4ee5\u5b8c\u5168\u5e76\u884c\u5316\u5904\u7406\u7684, \u8fd9\u91cc\u548cEncoder\u7684\u7406\u89e3\u4e5f\u662f\u4e00\u81f4\u7684. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u9636\u6bb5\u57fa\u672c\u4e0a\u4e0d\u8ba4\u4e3a\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u56e0\u4e3a\u7b2c\u4e00\u4e2atime step\u7684\u8f93\u5165\u53ea\u662f\u4e00\u4e2a\"SOS\", \u540e\u7eed\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u4e5f\u53ea\u662f\u4f9d\u6b21\u6dfb\u52a0\u4e4b\u524d\u6240\u6709\u7684\u9884\u6d4btoken. \u6ce8\u610f: \u6700\u91cd\u8981\u7684\u533a\u522b\u662f\u8bad\u7ec3\u9636\u6bb5\u76ee\u6807\u6587\u672c\u5982\u679c\u670920\u4e2atoken, \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u662f\u4e00\u6b21\u6027\u7684\u8f93\u5165\u7ed9Decoder\u7aef, \u53ef\u4ee5\u505a\u5230\u4e00\u4e9b\u5b50\u5c42\u7684\u5e76\u884c\u5316\u5904\u7406. \u4f46\u662f\u5728\u9884\u6d4b\u9636\u6bb5, \u5982\u679c\u9884\u6d4b\u7684\u7ed3\u679c\u8bed\u53e5\u603b\u5171\u670920\u4e2atoken, \u5219\u9700\u8981\u91cd\u590d\u5904\u740620\u6b21\u5faa\u73af\u7684\u8fc7\u7a0b, \u6bcf\u6b21\u7684\u8f93\u5165\u6dfb\u52a0\u8fdb\u53bb\u4e00\u4e2atoken, \u6bcf\u6b21\u7684\u8f93\u5165\u5e8f\u5217\u6bd4\u4e0a\u4e00\u6b21\u591a\u4e00\u4e2atoken, \u6240\u4ee5\u4e0d\u8ba4\u4e3a\u662f\u5e76\u884c\u5904\u7406.","title":"2.2 Transformer\u67b6\u6784\u4e2dDecoder\u7684\u5e76\u884c\u5316"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#3-transformer","text":"\u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e8c\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b. Transformer\u56e0\u4e3a\u91c7\u7528\u4e86Multi-head Attention\u7ed3\u6784\u548c\u8ba1\u7b97\u673a\u5236, \u62e5\u6709\u6bd4RNN/LSTM\u66f4\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b, \u8fd9\u91cc\u5e76\u4e0d\u4ec5\u4ec5\u7531\u7406\u8bba\u5206\u6790\u5f97\u6765, \u800c\u662f\u5927\u91cf\u7684\u8bd5\u9a8c\u6570\u636e\u548c\u5bf9\u6bd4\u7ed3\u679c, \u6e05\u695a\u7684\u5c55\u793a\u4e86Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b\u8fdc\u8fdc\u80dc\u4e8eRNN/LSTM. \u6ce8\u610f: \u4e0d\u662f\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u5c31\u8d8a\u65e0\u654c, \u5728\u5f88\u591a\u5177\u4f53\u7684\u5e94\u7528\u4e2dRNN/LSTM\u4f9d\u7136\u5927\u6709\u7528\u6b66\u4e4b\u5730, \u8981\u5177\u4f53\u95ee\u9898\u5177\u4f53\u5206\u6790.","title":"3 Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#4-transformerseq2seq","text":"","title":"4 \u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq?"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#41-seq2seq","text":"seq2seq\u67b6\u6784\u7684\u7b2c\u4e00\u5927\u7f3a\u9677\u662f\u5c06Encoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u8bed\u4e49\u5411\u91cf\u4e2d, \u7528\u8fd9\u4e2a\u56fa\u5b9a\u7684\u5411\u91cf\u6765\u4ee3\u8868\u7f16\u7801\u5668\u7aef\u7684\u5168\u90e8\u4fe1\u606f. \u8fd9\u6837\u65e2\u4f1a\u9020\u6210\u4fe1\u606f\u7684\u635f\u8017, \u4e5f\u65e0\u6cd5\u8ba9Decoder\u7aef\u5728\u89e3\u7801\u7684\u65f6\u5019\u53bb\u7528\u6ce8\u610f\u529b\u805a\u7126\u54ea\u4e9b\u662f\u66f4\u91cd\u8981\u7684\u4fe1\u606f. seq2seq\u67b6\u6784\u7684\u7b2c\u4e8c\u5927\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c, \u672c\u8d28\u4e0a\u548cRNN/LSTM\u65e0\u6cd5\u5e76\u884c\u7684\u539f\u56e0\u4e00\u6837.","title":"4.1 seq2seq\u7684\u4e24\u5927\u7f3a\u9677"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#42-transformer","text":"Transformer\u67b6\u6784\u540c\u65f6\u89e3\u51b3\u4e86seq2seq\u7684\u4e24\u5927\u7f3a\u9677, \u65e2\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97, \u53c8\u5e94\u7528Multi-head Attention\u673a\u5236\u6765\u89e3\u51b3Encoder\u56fa\u5b9a\u7f16\u7801\u7684\u95ee\u9898, \u8ba9Decoder\u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u53bb\u5173\u6ce8\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u6700\u91cd\u8981\u7684\u90a3\u4e9b\u90e8\u5206.","title":"4.2 Transformer\u7684\u6539\u8fdb"},{"location":"06_mkdocs_pretrained_model_old/6%20Transformer%E4%BC%98%E5%8A%BF.html#5","text":"\u5b66\u4e60\u4e86Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u539f\u56e0. \u7b2c\u4e00\u5927\u4f18\u52bf\u662f\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf. \u7b2c\u4e8c\u5927\u4f18\u52bf\u662f\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dEncoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Encoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u6d4b\u8bd5\u9636\u6bb5\u90fd\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u76f8\u540c\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dDecoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u7684Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u8ba1\u7b97\u4e0d\u80fd\u5e76\u884c\u5316\u5904\u7406. \u5b66\u4e60\u4e86seq2seq\u67b6\u6784\u7684\u4e24\u5927\u7f3a\u9677. \u7b2c\u4e00\u4e2a\u7f3a\u9677\u662fEncoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u88ab\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u7684\u8f93\u51fa\u5f20\u91cf, \u5f53\u5e8f\u5217\u957f\u5ea6\u8f83\u957f\u65f6\u4f1a\u9020\u6210\u6bd4\u8f83\u4e25\u91cd\u7684\u4fe1\u606f\u635f\u8017. \u7b2c\u4e8c\u4e2a\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c\u8ba1\u7b97. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u5bf9seq2seq\u4e24\u5927\u7f3a\u9677\u7684\u6539\u8fdb. Transformer\u5e94\u7528Multi-head Attention\u673a\u5236\u8ba9\u7f16\u7801\u5668\u4fe1\u606f\u53ef\u4ee5\u66f4\u597d\u7684\u5c55\u793a\u7ed9\u89e3\u7801\u5668. Transformer\u53ef\u4ee5\u5b9e\u73b0Encoder\u7aef\u7684\u5e76\u884c\u8ba1\u7b97.","title":"5 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u7406\u89e3BERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u539f\u56e0. \u7406\u89e3BERT\u6a21\u578b\u7684\u7f3a\u70b9\u548c\u539f\u56e0. \u7406\u89e3\u5728MLM\u4efb\u52a1\u4e2d\u91c7\u752880%, 10%, 10%\u7b56\u7565\u7684\u539f\u56e0. \u638c\u63e1\u5229\u7528BERT\u5904\u7406\u957f\u6587\u672c\u7684\u4efb\u52a1\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u601d\u8003\u9898\uff1aBERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u7f3a\u70b9? \u601d\u8003\u9898\uff1aBERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565? \u601d\u8003\u9898\uff1a\u957f\u6587\u672c\u9884\u6d4b\u4efb\u52a1\u5982\u679c\u60f3\u7528BERT\u6765\u5b9e\u73b0, \u8981\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c? 1 BERT\u6a21\u578b\u4f18\u7f3a\u70b9 \u00b6 1.1 BERT\u7684\u4f18\u70b9 \u00b6 \u901a\u8fc7\u9884\u8bad\u7ec3, \u52a0\u4e0aFine-tunning, \u572811\u9879NLP\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c. BERT\u7684\u6839\u57fa\u6e90\u4e8eTransformer, \u76f8\u6bd4\u4f20\u7edfRNN\u66f4\u52a0\u9ad8\u6548, \u53ef\u4ee5\u5e76\u884c\u5316\u5904\u7406\u540c\u65f6\u80fd\u6355\u6349\u957f\u8ddd\u79bb\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757, \u4e0d\u4ec5\u4ec5\u83b7\u5f97\u4e86\u771f\u6b63\u610f\u4e49\u4e0a\u7684bidirectional context, \u800c\u4e14\u4e3a\u540e\u7eed\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u4e86\u8db3\u591f\u7684\u8c03\u6574\u7a7a\u95f4. 1.2 BERT\u7684\u7f3a\u70b9 \u00b6 BERT\u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u592a\u591a, \u4e0d\u5229\u4e8e\u8d44\u6e90\u7d27\u5f20\u7684\u5e94\u7528\u573a\u666f, \u4e5f\u4e0d\u5229\u4e8e\u4e0a\u7ebf\u7684\u5b9e\u65f6\u5904\u7406. BERT\u76ee\u524d\u7ed9\u51fa\u7684\u4e2d\u6587\u6a21\u578b\u4e2d, \u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u5f88\u591a\u9700\u8981\u8bcd\u5411\u91cf\u7684\u5e94\u7528\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528. \u540c\u65f6\u8be5\u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u5f88\u591a\u751f\u50fb\u8bcd, \u53ea\u80fd\u4ee5UNK\u4ee3\u66ff. BERT\u4e2d\u7b2c\u4e00\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1MLM\u4e2d, [MASK]\u6807\u8bb0\u53ea\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u800c\u5728\u9884\u6d4b\u9636\u6bb5\u4e0d\u4f1a\u51fa\u73b0, \u8fd9\u5c31\u9020\u6210\u4e86\u4e00\u5b9a\u7684\u4fe1\u606f\u504f\u5dee, \u56e0\u6b64\u8bad\u7ec3\u65f6\u4e0d\u80fd\u8fc7\u591a\u7684\u4f7f\u7528[MASK], \u5426\u5219\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u8868\u73b0. \u6309\u7167BERT\u7684MLM\u4efb\u52a1\u4e2d\u7684\u7ea6\u5b9a, \u6bcf\u4e2abatch\u6570\u636e\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u88ab\u6a21\u578b\u5b66\u4e60\u548c\u9884\u6d4b, \u6240\u4ee5BERT\u6536\u655b\u7684\u901f\u5ea6\u6bd4left-to-right\u6a21\u578b\u8981\u6162\u5f88\u591a(left-to-right\u6a21\u578b\u4e2d\u6bcf\u4e00\u4e2atoken\u90fd\u4f1a\u53c2\u4e0e\u8bad\u7ec3). 2 BERT\u7684MLM\u4efb\u52a1 \u00b6 2.1 BERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565? \u00b6 \u9996\u5148, \u5982\u679c\u6240\u6709\u53c2\u4e0e\u8bad\u7ec3\u7684token\u88ab100%\u7684[MASK], \u90a3\u4e48\u5728fine-tunning\u7684\u65f6\u5019\u6240\u6709\u5355\u8bcd\u90fd\u662f\u5df2\u77e5\u7684, \u4e0d\u5b58\u5728[MASK], \u90a3\u4e48\u6a21\u578b\u5c31\u53ea\u80fd\u6839\u636e\u5176\u4ed6token\u7684\u4fe1\u606f\u548c\u8bed\u5e8f\u7ed3\u6784\u6765\u9884\u6d4b\u5f53\u524d\u8bcd, \u800c\u65e0\u6cd5\u5229\u7528\u5230\u8fd9\u4e2a\u8bcd\u672c\u8eab\u7684\u4fe1\u606f, \u56e0\u4e3a\u5b83\u4eec\u4ece\u672a\u51fa\u73b0\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u7b49\u4e8e\u6a21\u578b\u4ece\u672a\u63a5\u89e6\u5230\u5b83\u4eec\u7684\u4fe1\u606f, \u7b49\u4e8e\u6574\u4e2a\u8bed\u4e49\u7a7a\u95f4\u635f\u5931\u4e86\u90e8\u5206\u4fe1\u606f. \u91c7\u752880%\u7684\u6982\u7387\u4e0b\u5e94\u7528[MASK], \u65e2\u53ef\u4ee5\u8ba9\u6a21\u578b\u53bb\u5b66\u7740\u9884\u6d4b\u8fd9\u4e9b\u5355\u8bcd, \u53c8\u4ee520%\u7684\u6982\u7387\u4fdd\u7559\u4e86\u8bed\u4e49\u4fe1\u606f\u5c55\u793a\u7ed9\u6a21\u578b. \u4fdd\u7559\u4e0b\u6765\u7684\u4fe1\u606f\u5982\u679c\u5168\u90e8\u4f7f\u7528\u539f\u59cbtoken, \u90a3\u4e48\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u5077\u61d2, \u76f4\u63a5\u7167\u6284\u5f53\u524dtoken\u4fe1\u606f. \u91c7\u752810%\u6982\u7387\u4e0brandom token\u6765\u968f\u673a\u66ff\u6362\u5f53\u524dtoken, \u4f1a\u8ba9\u6a21\u578b\u4e0d\u80fd\u53bb\u6b7b\u8bb0\u786c\u80cc\u5f53\u524d\u7684token, \u800c\u53bb\u5c3d\u529b\u5b66\u4e60\u5355\u8bcd\u5468\u8fb9\u7684\u8bed\u4e49\u8868\u8fbe\u548c\u8fdc\u8ddd\u79bb\u7684\u4fe1\u606f\u4f9d\u8d56, \u5c1d\u8bd5\u5efa\u6a21\u5b8c\u6574\u7684\u8bed\u8a00\u4fe1\u606f. \u6700\u540e\u518d\u4ee510%\u7684\u6982\u7387\u4fdd\u7559\u539f\u59cb\u7684token, \u610f\u4e49\u5c31\u662f\u4fdd\u7559\u8bed\u8a00\u672c\u6765\u7684\u9762\u8c8c, \u8ba9\u4fe1\u606f\u4e0d\u81f3\u4e8e\u5b8c\u5168\u88ab\u906e\u63a9, \u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\"\u770b\u6e05\"\u771f\u5b9e\u7684\u8bed\u8a00\u9762\u8c8c. 3 BERT\u5904\u7406\u957f\u6587\u672c\u7684\u65b9\u6cd5 \u00b6 \u9996\u9009\u8981\u660e\u786e\u4e00\u70b9, BERT\u9884\u8bad\u7ec3\u6a21\u578b\u6240\u63a5\u6536\u7684\u6700\u5927sequence\u957f\u5ea6\u662f512. \u90a3\u4e48\u5bf9\u4e8e\u957f\u6587\u672c(\u6587\u672c\u957f\u5ea6\u8d85\u8fc7512\u7684\u53e5\u5b50), \u5c31\u9700\u8981\u7279\u6b8a\u7684\u65b9\u5f0f\u6765\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u6838\u5fc3\u5c31\u662f\u5982\u4f55\u8fdb\u884c\u622a\u65ad. head-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5934\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u524d510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). tail-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5c3e\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u6700\u540e510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). head+only\u65b9\u5f0f: \u9009\u62e9\u524d128\u4e2atoken\u548c\u6700\u540e382\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5728800\u4ee5\u5185), \u6216\u8005\u524d256\u4e2atoken\u548c\u6700\u540e254\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5927\u4e8e800). 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86BERT\u6a21\u578b\u76843\u4e2a\u4f18\u70b9: \u572811\u4e2aNLP\u4efb\u52a1\u4e0a\u53d6\u5f97SOAT\u6210\u7ee9. \u5229\u7528\u4e86Transformer\u7684\u5e76\u884c\u5316\u80fd\u529b\u4ee5\u53ca\u957f\u8bed\u53e5\u6355\u6349\u8bed\u4e49\u4f9d\u8d56\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u5b9e\u73b0\u4e86\u53cc\u5411Transformer\u5e76\u4e3a\u540e\u7eed\u7684\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u8db3\u591f\u7684\u7a7a\u95f4. \u5b66\u4e60\u4e86BERT\u6a21\u578b\u76844\u4e2a\u7f3a\u70b9: BERT\u6a21\u578b\u592a\u5927, \u592a\u6162. BERT\u6a21\u578b\u4e2d\u7684\u4e2d\u6587\u6a21\u578b\u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u65e0\u6cd5\u5229\u7528\u8bcd\u5411\u91cf, \u65e0\u6cd5\u8bc6\u522b\u751f\u50fb\u8bcd. BERT\u6a21\u578b\u4e2d\u7684MLM\u4efb\u52a1, [MASK]\u6807\u8bb0\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u9884\u6d4b\u9636\u6bb5\u4e0d\u51fa\u73b0, \u8fd9\u79cd\u504f\u5dee\u4f1a\u5bf9\u6a21\u578b\u6709\u4e00\u5b9a\u5f71\u54cd. BERT\u6a21\u578b\u7684MLM\u4efb\u52a1, \u6bcf\u4e2abatch\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u9020\u6210\u5927\u91cf\u6587\u672c\u6570\u636e\u7684\"\u65e0\u7528\", \u6536\u655b\u901f\u5ea6\u6162, \u9700\u8981\u7684\u7b97\u529b\u548c\u7b97\u65f6\u90fd\u5927\u5927\u63d0\u9ad8. \u5b66\u4e60\u4e86\u957f\u6587\u672c\u5904\u7406\u5982\u679c\u8981\u5229\u7528BERT\u7684\u8bdd, \u9700\u8981\u8fdb\u884c\u622a\u65ad\u5904\u7406. \u7b2c\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u524d\u9762510\u4e2atoken. \u7b2c\u4e8c\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u540e\u9762510\u4e2atoken. \u7b2c\u4e09\u79cd\u65b9\u5f0f\u5c31\u662f\u524d\u540e\u5206\u522b\u4fdd\u7559\u4e00\u90e8\u5206token, \u603b\u6570\u662f510. BERT\u4e2dMLM\u4efb\u52a1\u4e2d\u7684[MASK]\u662f\u4ee5\u4e00\u79cd\u663e\u793a\u7684\u65b9\u5f0f\u544a\u8bc9\u6a21\u578b\"\u8fd9\u4e2a\u8bcd\u6211\u4e0d\u544a\u8bc9\u4f60, \u4f60\u81ea\u5df1\u4ece\u4e0a\u4e0b\u6587\u91cc\u731c\", \u975e\u5e38\u7c7b\u4f3c\u4e8e\u540c\u5b66\u4eec\u5728\u505a\u5b8c\u5f62\u586b\u7a7a. \u5982\u679c[MASK]\u4ee5\u5916\u7684\u90e8\u5206\u5168\u90e8\u90fd\u7528\u539f\u59cbtoken, \u6a21\u578b\u4f1a\u5b66\u4e60\u5230\"\u5982\u679c\u5f53\u524d\u8bcd\u662f[MASK], \u5c31\u6839\u636e\u5176\u4ed6\u8bcd\u7684\u4fe1\u606f\u63a8\u65ad\u8fd9\u4e2a\u8bcd; \u5982\u679c\u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u6b63\u5e38\u7684\u5355\u8bcd, \u5c31\u76f4\u63a5\u7167\u6284\". \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6240\u6709\u5355\u8bcd\u90fd\u662f\u6b63\u5e38\u5355\u8bcd\u4e86, \u6a21\u578b\u5c31\u4f1a\u7167\u6284\u6240\u6709\u5355\u8bcd, \u4e0d\u518d\u63d0\u53d6\u5355\u8bcd\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4e86. BERT\u4e2dMLM\u4efb\u52a1\u4ee510%\u7684\u6982\u7387\u586b\u5165random token, \u5c31\u662f\u8ba9\u6a21\u578b\u65f6\u523b\u5904\u4e8e\"\u7d27\u5f20\u60c5\u7eea\"\u4e2d, \u8ba9\u6a21\u578b\u641e\u4e0d\u6e05\u695a\u5f53\u524d\u770b\u5230\u7684token\u662f\u771f\u5b9e\u7684\u5355\u8bcd\u8fd8\u662f\u88ab\u968f\u673a\u66ff\u6362\u6389\u7684\u5355\u8bcd, \u8fd9\u6837\u6a21\u578b\u5728\u4efb\u610f\u7684token\u4f4d\u7f6e\u5c31\u53ea\u80fd\u628a\u5f53\u524dtoken\u7684\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7ed3\u5408\u8d77\u6765\u505a\u7efc\u5408\u7684\u5224\u65ad\u548c\u5efa\u6a21. \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6a21\u578b\u4e5f\u4f1a\u540c\u65f6\u63d0\u53d6\u8fd9\u4e24\u65b9\u9762\u7684\u4fe1\u606f, \u56e0\u4e3a\u6a21\u578b\"\u5fc3\u7406\u5f88\u7d27\u5f20\", \u5b83\u4e0d\u77e5\u9053\u5f53\u524d\u770b\u5230\u7684\u8fd9\u4e2atoken, \u6240\u8c13\u7684\"\u6b63\u5e38\u5355\u8bcd\"\u5230\u5e95\u6709\u6ca1\u6709\"\u63d0\u524d\u88ab\u52a8\u8fc7\u624b\u811a\".","title":"7 BERT\u6a21\u578b\u7279\u70b9"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#_1","text":"\u7406\u89e3BERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u539f\u56e0. \u7406\u89e3BERT\u6a21\u578b\u7684\u7f3a\u70b9\u548c\u539f\u56e0. \u7406\u89e3\u5728MLM\u4efb\u52a1\u4e2d\u91c7\u752880%, 10%, 10%\u7b56\u7565\u7684\u539f\u56e0. \u638c\u63e1\u5229\u7528BERT\u5904\u7406\u957f\u6587\u672c\u7684\u4efb\u52a1\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u601d\u8003\u9898\uff1aBERT\u6a21\u578b\u7684\u4f18\u70b9\u548c\u7f3a\u70b9? \u601d\u8003\u9898\uff1aBERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565? \u601d\u8003\u9898\uff1a\u957f\u6587\u672c\u9884\u6d4b\u4efb\u52a1\u5982\u679c\u60f3\u7528BERT\u6765\u5b9e\u73b0, \u8981\u5982\u4f55\u6784\u9020\u8bad\u7ec3\u6837\u672c?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#1-bert","text":"","title":"1 BERT\u6a21\u578b\u4f18\u7f3a\u70b9"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#11-bert","text":"\u901a\u8fc7\u9884\u8bad\u7ec3, \u52a0\u4e0aFine-tunning, \u572811\u9879NLP\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c. BERT\u7684\u6839\u57fa\u6e90\u4e8eTransformer, \u76f8\u6bd4\u4f20\u7edfRNN\u66f4\u52a0\u9ad8\u6548, \u53ef\u4ee5\u5e76\u884c\u5316\u5904\u7406\u540c\u65f6\u80fd\u6355\u6349\u957f\u8ddd\u79bb\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757, \u4e0d\u4ec5\u4ec5\u83b7\u5f97\u4e86\u771f\u6b63\u610f\u4e49\u4e0a\u7684bidirectional context, \u800c\u4e14\u4e3a\u540e\u7eed\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u4e86\u8db3\u591f\u7684\u8c03\u6574\u7a7a\u95f4.","title":"1.1 BERT\u7684\u4f18\u70b9"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#12-bert","text":"BERT\u6a21\u578b\u8fc7\u4e8e\u5e9e\u5927, \u53c2\u6570\u592a\u591a, \u4e0d\u5229\u4e8e\u8d44\u6e90\u7d27\u5f20\u7684\u5e94\u7528\u573a\u666f, \u4e5f\u4e0d\u5229\u4e8e\u4e0a\u7ebf\u7684\u5b9e\u65f6\u5904\u7406. BERT\u76ee\u524d\u7ed9\u51fa\u7684\u4e2d\u6587\u6a21\u578b\u4e2d, \u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u5f88\u591a\u9700\u8981\u8bcd\u5411\u91cf\u7684\u5e94\u7528\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528. \u540c\u65f6\u8be5\u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u5f88\u591a\u751f\u50fb\u8bcd, \u53ea\u80fd\u4ee5UNK\u4ee3\u66ff. BERT\u4e2d\u7b2c\u4e00\u4e2a\u9884\u8bad\u7ec3\u4efb\u52a1MLM\u4e2d, [MASK]\u6807\u8bb0\u53ea\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u800c\u5728\u9884\u6d4b\u9636\u6bb5\u4e0d\u4f1a\u51fa\u73b0, \u8fd9\u5c31\u9020\u6210\u4e86\u4e00\u5b9a\u7684\u4fe1\u606f\u504f\u5dee, \u56e0\u6b64\u8bad\u7ec3\u65f6\u4e0d\u80fd\u8fc7\u591a\u7684\u4f7f\u7528[MASK], \u5426\u5219\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u8868\u73b0. \u6309\u7167BERT\u7684MLM\u4efb\u52a1\u4e2d\u7684\u7ea6\u5b9a, \u6bcf\u4e2abatch\u6570\u636e\u4e2d\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u88ab\u6a21\u578b\u5b66\u4e60\u548c\u9884\u6d4b, \u6240\u4ee5BERT\u6536\u655b\u7684\u901f\u5ea6\u6bd4left-to-right\u6a21\u578b\u8981\u6162\u5f88\u591a(left-to-right\u6a21\u578b\u4e2d\u6bcf\u4e00\u4e2atoken\u90fd\u4f1a\u53c2\u4e0e\u8bad\u7ec3).","title":"1.2 BERT\u7684\u7f3a\u70b9"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#2-bertmlm","text":"","title":"2 BERT\u7684MLM\u4efb\u52a1"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#21-bertmlm80-10-10","text":"\u9996\u5148, \u5982\u679c\u6240\u6709\u53c2\u4e0e\u8bad\u7ec3\u7684token\u88ab100%\u7684[MASK], \u90a3\u4e48\u5728fine-tunning\u7684\u65f6\u5019\u6240\u6709\u5355\u8bcd\u90fd\u662f\u5df2\u77e5\u7684, \u4e0d\u5b58\u5728[MASK], \u90a3\u4e48\u6a21\u578b\u5c31\u53ea\u80fd\u6839\u636e\u5176\u4ed6token\u7684\u4fe1\u606f\u548c\u8bed\u5e8f\u7ed3\u6784\u6765\u9884\u6d4b\u5f53\u524d\u8bcd, \u800c\u65e0\u6cd5\u5229\u7528\u5230\u8fd9\u4e2a\u8bcd\u672c\u8eab\u7684\u4fe1\u606f, \u56e0\u4e3a\u5b83\u4eec\u4ece\u672a\u51fa\u73b0\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u7b49\u4e8e\u6a21\u578b\u4ece\u672a\u63a5\u89e6\u5230\u5b83\u4eec\u7684\u4fe1\u606f, \u7b49\u4e8e\u6574\u4e2a\u8bed\u4e49\u7a7a\u95f4\u635f\u5931\u4e86\u90e8\u5206\u4fe1\u606f. \u91c7\u752880%\u7684\u6982\u7387\u4e0b\u5e94\u7528[MASK], \u65e2\u53ef\u4ee5\u8ba9\u6a21\u578b\u53bb\u5b66\u7740\u9884\u6d4b\u8fd9\u4e9b\u5355\u8bcd, \u53c8\u4ee520%\u7684\u6982\u7387\u4fdd\u7559\u4e86\u8bed\u4e49\u4fe1\u606f\u5c55\u793a\u7ed9\u6a21\u578b. \u4fdd\u7559\u4e0b\u6765\u7684\u4fe1\u606f\u5982\u679c\u5168\u90e8\u4f7f\u7528\u539f\u59cbtoken, \u90a3\u4e48\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u5077\u61d2, \u76f4\u63a5\u7167\u6284\u5f53\u524dtoken\u4fe1\u606f. \u91c7\u752810%\u6982\u7387\u4e0brandom token\u6765\u968f\u673a\u66ff\u6362\u5f53\u524dtoken, \u4f1a\u8ba9\u6a21\u578b\u4e0d\u80fd\u53bb\u6b7b\u8bb0\u786c\u80cc\u5f53\u524d\u7684token, \u800c\u53bb\u5c3d\u529b\u5b66\u4e60\u5355\u8bcd\u5468\u8fb9\u7684\u8bed\u4e49\u8868\u8fbe\u548c\u8fdc\u8ddd\u79bb\u7684\u4fe1\u606f\u4f9d\u8d56, \u5c1d\u8bd5\u5efa\u6a21\u5b8c\u6574\u7684\u8bed\u8a00\u4fe1\u606f. \u6700\u540e\u518d\u4ee510%\u7684\u6982\u7387\u4fdd\u7559\u539f\u59cb\u7684token, \u610f\u4e49\u5c31\u662f\u4fdd\u7559\u8bed\u8a00\u672c\u6765\u7684\u9762\u8c8c, \u8ba9\u4fe1\u606f\u4e0d\u81f3\u4e8e\u5b8c\u5168\u88ab\u906e\u63a9, \u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\"\u770b\u6e05\"\u771f\u5b9e\u7684\u8bed\u8a00\u9762\u8c8c.","title":"2.1 BERT\u7684MLM\u4efb\u52a1\u4e2d\u4e3a\u4ec0\u4e48\u91c7\u7528\u4e8680%, 10%, 10%\u7684\u7b56\u7565?"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#3-bert","text":"\u9996\u9009\u8981\u660e\u786e\u4e00\u70b9, BERT\u9884\u8bad\u7ec3\u6a21\u578b\u6240\u63a5\u6536\u7684\u6700\u5927sequence\u957f\u5ea6\u662f512. \u90a3\u4e48\u5bf9\u4e8e\u957f\u6587\u672c(\u6587\u672c\u957f\u5ea6\u8d85\u8fc7512\u7684\u53e5\u5b50), \u5c31\u9700\u8981\u7279\u6b8a\u7684\u65b9\u5f0f\u6765\u6784\u9020\u8bad\u7ec3\u6837\u672c. \u6838\u5fc3\u5c31\u662f\u5982\u4f55\u8fdb\u884c\u622a\u65ad. head-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5934\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u524d510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). tail-only\u65b9\u5f0f: \u8fd9\u662f\u53ea\u4fdd\u7559\u957f\u6587\u672c\u5c3e\u90e8\u4fe1\u606f\u7684\u622a\u65ad\u65b9\u5f0f, \u5177\u4f53\u4e3a\u4fdd\u5b58\u6700\u540e510\u4e2atoken (\u8981\u7559\u4e24\u4e2a\u4f4d\u7f6e\u7ed9[CLS]\u548c[SEP]). head+only\u65b9\u5f0f: \u9009\u62e9\u524d128\u4e2atoken\u548c\u6700\u540e382\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5728800\u4ee5\u5185), \u6216\u8005\u524d256\u4e2atoken\u548c\u6700\u540e254\u4e2atoken (\u6587\u672c\u603b\u957f\u5ea6\u5927\u4e8e800).","title":"3 BERT\u5904\u7406\u957f\u6587\u672c\u7684\u65b9\u6cd5"},{"location":"06_mkdocs_pretrained_model_old/7%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html#4","text":"\u5b66\u4e60\u4e86BERT\u6a21\u578b\u76843\u4e2a\u4f18\u70b9: \u572811\u4e2aNLP\u4efb\u52a1\u4e0a\u53d6\u5f97SOAT\u6210\u7ee9. \u5229\u7528\u4e86Transformer\u7684\u5e76\u884c\u5316\u80fd\u529b\u4ee5\u53ca\u957f\u8bed\u53e5\u6355\u6349\u8bed\u4e49\u4f9d\u8d56\u548c\u7ed3\u6784\u4f9d\u8d56. BERT\u5b9e\u73b0\u4e86\u53cc\u5411Transformer\u5e76\u4e3a\u540e\u7eed\u7684\u5fae\u8c03\u4efb\u52a1\u7559\u51fa\u8db3\u591f\u7684\u7a7a\u95f4. \u5b66\u4e60\u4e86BERT\u6a21\u578b\u76844\u4e2a\u7f3a\u70b9: BERT\u6a21\u578b\u592a\u5927, \u592a\u6162. BERT\u6a21\u578b\u4e2d\u7684\u4e2d\u6587\u6a21\u578b\u662f\u4ee5\u5b57\u4e3a\u57fa\u672ctoken\u5355\u4f4d\u7684, \u65e0\u6cd5\u5229\u7528\u8bcd\u5411\u91cf, \u65e0\u6cd5\u8bc6\u522b\u751f\u50fb\u8bcd. BERT\u6a21\u578b\u4e2d\u7684MLM\u4efb\u52a1, [MASK]\u6807\u8bb0\u5728\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0, \u9884\u6d4b\u9636\u6bb5\u4e0d\u51fa\u73b0, \u8fd9\u79cd\u504f\u5dee\u4f1a\u5bf9\u6a21\u578b\u6709\u4e00\u5b9a\u5f71\u54cd. BERT\u6a21\u578b\u7684MLM\u4efb\u52a1, \u6bcf\u4e2abatch\u53ea\u670915%\u7684token\u53c2\u4e0e\u4e86\u8bad\u7ec3, \u9020\u6210\u5927\u91cf\u6587\u672c\u6570\u636e\u7684\"\u65e0\u7528\", \u6536\u655b\u901f\u5ea6\u6162, \u9700\u8981\u7684\u7b97\u529b\u548c\u7b97\u65f6\u90fd\u5927\u5927\u63d0\u9ad8. \u5b66\u4e60\u4e86\u957f\u6587\u672c\u5904\u7406\u5982\u679c\u8981\u5229\u7528BERT\u7684\u8bdd, \u9700\u8981\u8fdb\u884c\u622a\u65ad\u5904\u7406. \u7b2c\u4e00\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u524d\u9762510\u4e2atoken. \u7b2c\u4e8c\u79cd\u65b9\u5f0f\u5c31\u662f\u53ea\u4fdd\u7559\u540e\u9762510\u4e2atoken. \u7b2c\u4e09\u79cd\u65b9\u5f0f\u5c31\u662f\u524d\u540e\u5206\u522b\u4fdd\u7559\u4e00\u90e8\u5206token, \u603b\u6570\u662f510. BERT\u4e2dMLM\u4efb\u52a1\u4e2d\u7684[MASK]\u662f\u4ee5\u4e00\u79cd\u663e\u793a\u7684\u65b9\u5f0f\u544a\u8bc9\u6a21\u578b\"\u8fd9\u4e2a\u8bcd\u6211\u4e0d\u544a\u8bc9\u4f60, \u4f60\u81ea\u5df1\u4ece\u4e0a\u4e0b\u6587\u91cc\u731c\", \u975e\u5e38\u7c7b\u4f3c\u4e8e\u540c\u5b66\u4eec\u5728\u505a\u5b8c\u5f62\u586b\u7a7a. \u5982\u679c[MASK]\u4ee5\u5916\u7684\u90e8\u5206\u5168\u90e8\u90fd\u7528\u539f\u59cbtoken, \u6a21\u578b\u4f1a\u5b66\u4e60\u5230\"\u5982\u679c\u5f53\u524d\u8bcd\u662f[MASK], \u5c31\u6839\u636e\u5176\u4ed6\u8bcd\u7684\u4fe1\u606f\u63a8\u65ad\u8fd9\u4e2a\u8bcd; \u5982\u679c\u5f53\u524d\u8bcd\u662f\u4e00\u4e2a\u6b63\u5e38\u7684\u5355\u8bcd, \u5c31\u76f4\u63a5\u7167\u6284\". \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6240\u6709\u5355\u8bcd\u90fd\u662f\u6b63\u5e38\u5355\u8bcd\u4e86, \u6a21\u578b\u5c31\u4f1a\u7167\u6284\u6240\u6709\u5355\u8bcd, \u4e0d\u518d\u63d0\u53d6\u5355\u8bcd\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4e86. BERT\u4e2dMLM\u4efb\u52a1\u4ee510%\u7684\u6982\u7387\u586b\u5165random token, \u5c31\u662f\u8ba9\u6a21\u578b\u65f6\u523b\u5904\u4e8e\"\u7d27\u5f20\u60c5\u7eea\"\u4e2d, \u8ba9\u6a21\u578b\u641e\u4e0d\u6e05\u695a\u5f53\u524d\u770b\u5230\u7684token\u662f\u771f\u5b9e\u7684\u5355\u8bcd\u8fd8\u662f\u88ab\u968f\u673a\u66ff\u6362\u6389\u7684\u5355\u8bcd, \u8fd9\u6837\u6a21\u578b\u5728\u4efb\u610f\u7684token\u4f4d\u7f6e\u5c31\u53ea\u80fd\u628a\u5f53\u524dtoken\u7684\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7ed3\u5408\u8d77\u6765\u505a\u7efc\u5408\u7684\u5224\u65ad\u548c\u5efa\u6a21. \u8fd9\u6837\u4e00\u6765, \u5230\u4e86fine-tunning\u9636\u6bb5, \u6a21\u578b\u4e5f\u4f1a\u540c\u65f6\u63d0\u53d6\u8fd9\u4e24\u65b9\u9762\u7684\u4fe1\u606f, \u56e0\u4e3a\u6a21\u578b\"\u5fc3\u7406\u5f88\u7d27\u5f20\", \u5b83\u4e0d\u77e5\u9053\u5f53\u524d\u770b\u5230\u7684\u8fd9\u4e2atoken, \u6240\u8c13\u7684\"\u6b63\u5e38\u5355\u8bcd\"\u5230\u5e95\u6709\u6ca1\u6709\"\u63d0\u524d\u88ab\u52a8\u8fc7\u624b\u811a\".","title":"4 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662fELMo. \u638c\u63e1ELMo\u7684\u67b6\u6784. \u638c\u63e1ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u4e86\u89e3ELMo\u7684\u6548\u679c\u548c\u6210\u7ee9. \u4e86\u89e3ELMo\u7684\u4f18\u7f3a\u70b9. 1 ELMo\u7b80\u4ecb \u00b6 ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. ELMo\u7684\u5168\u79f0\u662fEmbeddings from Language Models. ELMo\u6a21\u578b\u7684\u63d0\u51fa\u6e90\u4e8e\u8bba\u6587 << Deep Contextualized Word Representations >> . ELMo\u6a21\u578b\u63d0\u51fa\u7684\u52a8\u673a\u6e90\u4e8e\u7814\u7a76\u4eba\u5458\u8ba4\u4e3a\u4e00\u4e2a\u597d\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5e94\u8be5\u80fd\u591f\u5305\u542b\u4e30\u5bcc\u7684\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f, \u5e76\u4e14\u80fd\u591f\u5bf9\u591a\u4e49\u8bcd\u8fdb\u884c\u5efa\u6a21. \u800c\u4f20\u7edf\u7684\u8bcd\u5411\u91cf(2013\u5e74\u7684word2vec, 2014\u5e74\u7684GloVe)\u90fd\u662f\u4e0a\u4e0b\u6587\u65e0\u5173\u7684, \u4e5f\u5c31\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf. \u6700\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\"apple\"\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b, \u5e94\u8be5\u53ef\u4ee5\u8868\u793a\u6c34\u679c\u6216\u516c\u53f8, \u4f46\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf\u663e\u7136\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9. \u56e0\u6b64\u7814\u7a76\u56e2\u961f\u5229\u7528\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e00\u4e2a\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u9884\u8bad\u7ec3\u6a21\u578b, \u6210\u4e3aELMo, \u5e76\u57286\u4e2aNLP\u4efb\u52a1\u4e0a\u83b7\u5f97\u63d0\u5347. 2 ELMo\u7684\u67b6\u6784 \u00b6 2.1 \u603b\u4f53\u67b6\u6784 \u00b6 \u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aELMo\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684\u4e24\u90e8\u5206\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757. 2.2 Embedding\u6a21\u5757 \u00b6 ELMo\u6700\u5e95\u5c42\u7684\u8bcd\u5d4c\u5165\u91c7\u7528CNN\u5bf9\u5b57\u7b26\u7ea7\u8fdb\u884c\u7f16\u7801, \u672c\u8d28\u5c31\u662f\u83b7\u5f97\u4e00\u4e2a\u9759\u6001\u7684\u8bcd\u5d4c\u5165\u5411\u91cf\u4f5c\u4e3a\u7f51\u7edc\u7684\u5e95\u5c42\u8f93\u5165. 2.3 \u4e24\u90e8\u5206\u7684\u53cc\u5c42LSTM\u6a21\u5757 \u00b6 \u8fd9\u662f\u6574\u4e2aELMo\u4e2d\u6700\u91cd\u8981\u7684\u90e8\u5206, \u67b6\u6784\u4e2d\u5206\u6210\u5de6\u4fa7\u7684\u524d\u5411LSTM\u7f51\u7edc, \u548c\u53f3\u4fa7\u7684\u53cd\u5411LSTM\u7f51\u7edc. ELMo\u7684\u505a\u6cd5\u662f\u6211\u4eec\u53ea\u9884\u8bad\u7ec3\u4e00\u4e2aLanguage Model, \u800cword embedding\u662f\u901a\u8fc7\u8f93\u5165\u7684\u53e5\u5b50\u5b9e\u65f6\u7ed9\u51fa\u7684, \u8fd9\u6837\u5355\u8bcd\u7684\u5d4c\u5165\u5411\u91cf\u5c31\u5305\u542b\u4e86\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f, \u4e5f\u5c31\u5f7b\u5e95\u6539\u53d8\u4e86Word2Vec\u548cGloVe\u7684\u9759\u6001\u8bcd\u5411\u91cf\u7684\u505a\u6cd5. ELMo\u7684\u8fd9\u4e00\u6a21\u5757\u5206\u4e3a\u5de6\u53f3\u4e24\u90e8\u5206, \u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u53cc\u5411LM, \u5bf9\u4e8e\u5de6\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t1, t2, ..., tN), Language Model\u901a\u8fc7\u524d\u9762k-1\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u524d\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) \u540c\u7406, \u5bf9\u4e8e\u67b6\u6784\u4e2d\u7684\u53f3\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t(k+1), t(k+2), ..., t(N)), Language Model\u901a\u8fc7\u540e\u9762N-k\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u540e\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) ELMo\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u76ee\u6807\u51fd\u6570\u5c31\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u516c\u5f0f: \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s)) \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s)) 2.4 \u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757 \u00b6 \u56e0\u4e3aELMo\u662f\u4e2a\u8bed\u8a00\u6a21\u578b, \u5bf9\u4e8e\u6bcf\u4e2atoken, \u901a\u8fc7\u4e00\u4e2aL\u5c42\u7684\u53cc\u5411LSTM\u7f51\u7edc\u53ef\u4ee5\u8ba1\u7b97\u51fa2L+1\u4e2a\u8868\u793a\u5411\u91cf\u5982\u4e0b: R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} \u4ece\u4e0a\u9762\u7684\u516c\u5f0f\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230, \u67093\u4e2a\u4e0d\u540c\u7684\u7ec4\u6210\u90e8\u5206, \u7b2c\u4e00\u4e2a\u5c31\u662f\u5bf9token\u76f4\u63a5\u8fdb\u884cCNN\u7f16\u7801\u7684\u7ed3\u679c, \u4e5f\u662fELMo\u6700\u5e95\u5c42\u6a21\u5757\u7684\u8f93\u51fa; \u7b2c\u4e8c\u4e2a\u5c31\u662f\u524d\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7b2c\u4e09\u4e2a\u5c31\u662f\u540e\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7efc\u5408\u4e09\u90e8\u5206\u7684\u8f93\u51fa\u52a0\u5728\u4e00\u8d77, \u5c31\u662f2L+1\u4e2a\u8f93\u51fa\u5411\u91cf. \u901a\u8fc7\u6574\u4e2a\u7f51\u7edc, \u6bcf\u4e00\u4e2atoken\u5f97\u5230\u4e862L+1\u4e2a\u8868\u793a\u5411\u91cf, \u4f46\u662f\u6211\u4eec\u5e0c\u671b\u6bcf\u4e00\u4e2atoken\u80fd\u5bf9\u5e94\u4e00\u4e2a\u5411\u91cf. \u6700\u7b80\u5355\u7684\u505a\u6cd5\u5c31\u662f\u53d6\u6700\u4e0a\u5c42\u7684\u8f93\u51fa\u7ed3\u679c\u4f5c\u4e3atoken\u7684\u8868\u793a\u5411\u91cf, \u66f4\u901a\u7528\u7684\u505a\u6cd5\u662f\u52a0\u5165\u82e5\u5e72\u53c2\u6570\u6765\u878d\u5408\u6240\u6709\u5c42\u7684\u4fe1\u606f, \u5982\u4e0b\u6240\u793a: ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} \u4e0a\u5f0f\u7684\u610f\u601d\u662f\u5bf9\u4e8e2L+1\u4e2a\u5411\u91cf, \u6bcf\u4e00\u4e2a\u524d\u9762\u90fd\u52a0\u4e0a\u4e00\u4e2a\u6743\u91cd\u7a00\u758f, \u7136\u540e\u76f4\u63a5\u878d\u5408\u6210\u4e00\u4e2a\u5411\u91cf, \u6700\u540e\u518d\u4e58\u4e00\u4e2a\u7cfb\u6570\u4f5c\u4e3a\u6700\u7ec8\u8be5token\u7684\u8bcd\u5411\u91cf. \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u6700\u524d\u9762\u7684\u90a3\u4e2a\u7cfb\u6570, \u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u53d6\u4e0d\u540c\u7684\u503c\u6548\u679c\u4f1a\u6709\u8f83\u5927\u7684\u5dee\u5f02, \u9700\u8981\u6ce8\u610f\u5728SQuAD\u4e2d\u8bbe\u7f6e\u4e3a0.01\u53d6\u5f97\u7684\u6548\u679c\u8981\u597d\u4e8e\u8bbe\u7f6e\u4e3a1. \u539f\u59cb\u8bba\u6587\u4e2d\u5728\u8fdb\u884c\u5e95\u5c42token\u7f16\u7801\u65f6, \u7528CNN\u5f62\u6210\u4e86\u4e00\u4e2a512\u7ef4\u7684\u5217\u5411\u91cf, \u4e5f\u5c31\u662f\u521d\u59cb\u5d4c\u5165\u7ef4\u5ea6\u7b49\u4e8e512. \u4e2d\u95f4\u5c42\u4f7f\u7528\u4e86\u53cc\u5c42\u7684LSTM\u5206\u522b\u8fdb\u884c\u524d\u5411\u7f16\u7801\u548c\u540e\u5411\u7f16\u7801, \u6bcf\u5c42\u7684\u5355\u4e2aLSTM\u8f93\u5165\u7ef4\u5ea6\u662f512, \u8f93\u51fa\u7ef4\u5ea6\u4e5f\u662f512, \u4fdd\u6301\u4e00\u81f4. \u56e0\u4e3a\u662f\u53cc\u5411\u7f16\u7801\u5e76\u4e14\u5206\u5de6\u53f3\u4e24\u90e8\u5206, \u6240\u4ee5\u6bcf\u5c42\u7684\u8f93\u51fa\u7ef4\u5ea6\u662f512*2=1024, \u6700\u540e\u8fdb\u884c\u6743\u91cd\u878d\u5408\u540e\u7684\u5411\u91cf\u7ef4\u5ea6\u5c31\u662f1024. 3 ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1 \u00b6 3.1 ELMo\u7684\u672c\u8d28\u601d\u60f3 \u00b6 \u9996\u5148\u7528\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5b66\u597d\u4e00\u4e2a\u5355\u8bcd\u7684word embedding, \u6b64\u65f6\u662f\u65e0\u6cd5\u533a\u5206\u591a\u4e49\u8bcd\u7684, \u4f46\u6ca1\u5173\u7cfb. \u5f53\u5b9e\u9645\u4f7f\u7528word embedding\u7684\u65f6\u5019, \u8be5\u5355\u8bcd\u5df2\u7ecf\u5177\u5907\u4e86\u7279\u5b9a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f, \u8fd9\u4e2a\u65f6\u5019\u53ef\u4ee5\u6839\u636e\u4e0a\u4e0b\u6587\u5355\u8bcd\u7684\u8bed\u4e49\u53bb\u8c03\u6574\u5355\u8bcd\u7684word embedding\u8868\u793a, \u8fd9\u6837\u7ecf\u8fc7\u8c03\u6574\u540e\u5f97\u5230\u7684word embedding\u5411\u91cf\u5c31\u53ef\u4ee5\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u5728\u5f53\u524d\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u5b9e\u542b\u4e49\u4e86, \u4e5f\u5c31\u81ea\u7136\u7684\u89e3\u51b3\u4e86\u591a\u4e49\u8bcd\u95ee\u9898. \u7ed3\u8bba\u5c31\u662fELMo\u6a21\u578b\u662f\u4e2a\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b. 3.2 ELMo\u7684\u9884\u8bad\u7ec3\u91c7\u7528\u4e86\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b \u00b6 \u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u505a\u4e0b\u6e38\u4efb\u52a1\u65f6, \u4ece\u9884\u8bad\u7ec3\u7f51\u7edc\u4e2d\u63d0\u53d6\u5bf9\u5e94\u5355\u8bcd\u7684\u7f51\u7edc\u5404\u5c42\u7684word embedding\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u5230\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u7b2c\u4e00\u9636\u6bb5: \u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3. \u518d\u6b21\u56de\u5230ELMo\u7684\u603b\u4f53\u67b6\u6784\u56fe, \u7f51\u7edc\u7ed3\u6784\u91c7\u7528\u4e86\u53cc\u5c42\u53cc\u5411LSTM. \u76ee\u524d\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u4efb\u52a1\u76ee\u6807\u662f\u6839\u636e\u5355\u8bcdWi\u7684\u4e0a\u4e0b\u6587\u53bb\u6b63\u786e\u9884\u6d4b\u5355\u8bcdWi, Wi\u4e4b\u524d\u7684\u5355\u8bcd\u5e8f\u5217context-before\u79f0\u4e3a\u4e0a\u6587, Wi\u4e4b\u540e\u7684\u5355\u8bcd\u5e8f\u5217context-after\u79f0\u4e3a\u4e0b\u6587. \u67b6\u6784\u56fe\u4e0a\u5de6\u4fa7\u7684\u524d\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u6b63\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u5de6\u5411\u53f3\u987a\u5e8f\u7684\u9664\u4e86\u9884\u6d4b\u5355\u8bcdWi\u4e4b\u5916\u7684\u4e0a\u6587context-before; \u53f3\u4fa7\u7684\u53cd\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u53cd\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u53f3\u5411\u5de6\u7684\u9006\u5e8f\u7684\u4e0b\u6587context-after; \u6bcf\u4e2a\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u90fd\u662fL=2, \u5373\u53cc\u5c42LSTM\u53e0\u52a0. \u4f7f\u7528\u4e0a\u8ff0\u7684\u7f51\u7edc\u7ed3\u6784\u5229\u7528\u5927\u91cf\u8bed\u6599\u505a\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u5c31\u80fd\u9884\u8bad\u7ec3\u597d\u8fd9\u4e2a\u7f51\u7edc. \u5f53\u8f93\u5165\u4e00\u4e2a\u65b0\u53e5\u5b50S_new\u65f6, \u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u90fd\u80fd\u5f97\u5230\u5bf9\u5e94\u76843\u4e2aembedding\u5411\u91cf: 1-\u6700\u5e95\u5c42\u7684\u5355\u8bcd\u7684word embedding. 2-\u4e2d\u95f4\u7b2c\u4e00\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u53e5\u6cd5\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. 3-\u4e2d\u95f4\u7b2c\u4e8c\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u8bed\u4e49\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. ELMo\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u4ec5\u4ec5\u5b66\u4f1a\u4e86\u5355\u8bcd\u7684word embedding, \u8fd8\u5b66\u4e60\u4e86\u4e00\u4e2a\u53cc\u5c42\u53cc\u5411\u7684LSTM\u7f51\u7edc, \u8fd9\u4e24\u8005\u540e\u7eed\u90fd\u4f1a\u7528\u5230, \u662f\u6574\u4e2aELMo\u9884\u8bad\u7ec3\u7684\u4e24\u5927\u4ea7\u51fa\u7ed3\u679c. \u7b2c\u4e8c\u9636\u6bb5: \u4e0b\u6e38\u4efb\u52a1\u7684\u8c03\u6574. \u6bd4\u5982\u6211\u4eec\u7684\u4e0b\u6e38\u4efb\u52a1\u662fQA\u95ee\u9898. \u5bf9\u4e8e\u95ee\u53e5X, \u53ef\u4ee5\u5148\u5c06\u53e5\u5b50X\u4f5c\u4e3a\u9884\u8bad\u7ec3\u597d\u7684ELMo\u7f51\u7edc\u7684\u8f93\u5165, \u8fd9\u6837X\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5728ELMo\u4e2d\u90fd\u80fd\u83b7\u5f973\u4e2a\u5bf9\u5e94\u7684embedding\u5411\u91cf. \u4e4b\u540e\u8d4b\u7ed9\u8fd93\u4e2a\u5411\u91cf\u5404\u81ea\u4e00\u4e2a\u6743\u91cda, \u8fd9\u4e2a\u6743\u91cda\u65e2\u53ef\u4ee5\u662f\u5b66\u4e60\u5f97\u6765\u7684\u4e5f\u53ef\u4ee5\u662f\u6700\u7b80\u5355\u7684\u5e73\u5747\u5206\u5e03\u8d4b\u503c, \u7136\u540e\u628a3\u4e2a\u5411\u91cf\u52a0\u6743\u6c42\u548c, \u6574\u4e2a\u6210\u4e00\u4e2a\u8bcd\u5411\u91cf. \u6700\u540e\u5c06\u6574\u5408\u540e\u7684\u8bcd\u5411\u91cf\u4f5c\u4e3aX\u5728\u81ea\u5df1\u4efb\u52a1\u7684\u90a3\u4e2a\u7f51\u7edc\u7ed3\u6784\u4e2d\u5bf9\u5e94\u5355\u8bcd\u7684\u8f93\u5165, \u4ee5\u6b64\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u8fdb\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u5bf9\u4e8e\u56de\u7b54Y\u53ef\u4ee5\u540c\u6837\u5904\u7406. \u56e0\u4e3aELMo\u7ed9\u4e0b\u6e38\u63d0\u4f9b\u7684\u662f\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u5f62\u5f0f, \u6240\u4ee5\u8fd9\u4e00\u7c7b\u9884\u8bad\u7ec3\u65b9\u6cd5\u88ab\u79f0\u4e3a\"Feature-based Pre-Training\". 4 ELMo\u6a21\u578b\u7684\u6548\u679c \u00b6 ELMo\u5bf9\u4e8e\u591a\u4e49\u8bcd\u95ee\u9898\u7684\u89e3\u51b3\u7ed3\u679c: \u524d\u9762\u63d0\u5230\u9759\u6001\u7684word embedding\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898, \u90a3\u4e48ELMo\u5f15\u5165\u4e0a\u4e0b\u6587\u52a8\u6001\u8bed\u4e49\u8c03\u6574\u540e\u7684embedding word\u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u95ee\u9898\u5417? \u7b54\u6848\u6b63\u5982\u4e0a\u56fe\u6240\u793a, \u800c\u4e14\u6bd4\u6211\u4eec\u671f\u5f85\u7684\u89e3\u51b3\u6548\u679c\u8981\u66f4\u597d. \u4e0a\u56fe\u4e2d\u7684\u4f8b\u5b50, \u5bf9\u4e8eGloVe\u8bad\u7ec3\u51fa\u6765\u7684word embedding\u6765\u8bf4, \u591a\u4e49\u8bcd\u6bd4\u5982play, \u6839\u636e\u5b83\u7684embedding\u627e\u51fa\u6700\u63a5\u8fd1\u5176\u8bed\u4e49\u7684\u5355\u8bcd, \u53d1\u73b0\u7ed3\u679c\u96c6\u5408\u51e0\u4e4e\u5168\u90e8\u90fd\u5728\u4f53\u80b2\u9886\u57df, \u8fd9\u5f88\u660e\u663e\u662f\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u4e2d\u5305\u542bplay\u7684\u8bed\u53e5\u4e2d\u4f53\u80b2\u9886\u57df\u7684\u6570\u91cf\u660e\u663e\u5360\u591a\u6570\u5bfc\u81f4\u7684. \u518d\u6765\u770b\u4f7f\u7528ELMo\u540e\u7684\u6548\u679c, \u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u540e\u7684embedding word\u4e0d\u4ec5\u4ec5\u80fd\u627e\u51fa\u5bf9\u5e94\u4e8e\"play\":\"\u6f14\u51fa\"\u7684\u76f8\u540c\u8bed\u4e49\u7684\u53e5\u5b50, \u800c\u4e14\u8fd8\u53ef\u4ee5\u4fdd\u8bc1\u627e\u51fa\u7684\u53e5\u5b50\u4e2d\u7684play\u5bf9\u5e94\u7684\u8bcd\u6027\u4e5f\u662f\u76f8\u540c\u7684, \u8fd9\u771f\u7684\u662f\u8d85\u51fa\u671f\u5f85\u4e4b\u5916\u7684\u60ca\u559c! \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230ELMo\u7684\u8bd5\u9a8c\u6548\u679c, \u57286\u4e2aNLP\u4e3b\u6d41\u4efb\u52a1\u4e2d\u6027\u80fd\u90fd\u6709\u4e0d\u540c\u5e45\u5ea6\u7684\u63d0\u5347, \u6700\u9ad8\u7684\u63d0\u5347\u8fbe\u523025%, \u4efb\u52a1\u7684\u8986\u76d6\u8303\u56f4\u5f88\u5e7f, \u5305\u542b\u53e5\u5b50\u8bed\u4e49\u5173\u7cfb\u5224\u65ad, \u5206\u7c7b\u4efb\u52a1, \u9605\u8bfb\u7406\u89e3\u7b49\u7b49. 5 ELMo\u7684\u5f85\u6539\u8fdb\u70b9 \u00b6 ELMo\u5728\u4f20\u7edf\u9759\u6001word embedding\u65b9\u6cd5(Word2Vec, GloVe)\u7684\u57fa\u7840\u4e0a\u63d0\u5347\u4e86\u5f88\u591a, \u4f46\u662f\u4f9d\u7136\u5b58\u5728\u7f3a\u9677, \u6709\u5f88\u5927\u7684\u6539\u8fdb\u4f59\u5730. \u7b2c\u4e00\u70b9: \u4e00\u4e2a\u5f88\u660e\u663e\u7684\u7f3a\u70b9\u5728\u4e8e\u7279\u5f81\u63d0\u53d6\u5668\u7684\u9009\u62e9\u4e0a, ELMo\u4f7f\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM, \u800c\u4e0d\u662f\u73b0\u5728\u6a2a\u626b\u5343\u519b\u7684Transformer, \u5728\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0a\u80af\u5b9a\u662f\u8981\u5f31\u4e00\u4e9b\u7684. \u8bbe\u60f3\u5982\u679cELMo\u7684\u63d0\u5347\u63d0\u53d6\u5668\u9009\u7528Transformer, \u90a3\u4e48\u540e\u6765\u7684BERT\u7684\u53cd\u54cd\u5c06\u8fdc\u4e0d\u5982\u5f53\u65f6\u90a3\u4e48\u706b\u7206\u4e86. \u7b2c\u4e8c\u70b9: ELMo\u9009\u7528\u53cc\u5411\u62fc\u63a5\u7684\u65b9\u5f0f\u8fdb\u884c\u7279\u5f81\u878d\u5408, \u8fd9\u79cd\u65b9\u6cd5\u80af\u5b9a\u4e0d\u5982BERT\u4e00\u4f53\u5316\u7684\u53cc\u5411\u63d0\u53d6\u7279\u5f81\u597d. 6 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fELMo. ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. ELMo\u57286\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u6709\u5f88\u5927\u7684\u63d0\u5347\u8868\u73b0. \u5b66\u4e60\u4e86ELMo\u7684\u7ed3\u6784. ELMo\u67b6\u6784\u603b\u4f53\u4e0a\u91c7\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u7684\u53cc\u5411\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757. \u5b66\u4e60\u4e86ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. ELMo\u7684\u672c\u8d28\u601d\u60f3\u5c31\u662f\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u8fdb\u884c\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b. ELMo\u7684\u9884\u8bad\u7ec3\u662f\u4e00\u4e2a\u660e\u663e\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b. \u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3, \u5f97\u5230\u57fa\u7840\u9759\u6001\u8bcd\u5411\u91cf\u548c\u53cc\u5411\u53cc\u5c42LSTM\u7f51\u7edc. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u62e5\u6709\u4e0a\u4e0b\u6587\u7684\u73af\u5883\u4e2d, \u5c06\u4e0a\u4e0b\u6587\u8f93\u5165\u53cc\u5411\u53cc\u5c42LSTM\u4e2d, \u5f97\u5230\u52a8\u6001\u8c03\u6574\u540e\u7684word embedding, \u7b49\u4e8e\u5c06\u5355\u8bcd\u878d\u5408\u8fdb\u4e86\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u53ef\u4ee5\u66f4\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u7684\u771f\u5b9e\u542b\u4e49. \u5b66\u4e60\u4e86ELMo\u7684\u6548\u679c. \u7ecf\u8fc7\u4e0eGloVe\u9759\u6001\u8bcd\u5411\u91cf\u7684\u5bf9\u6bd4, \u660e\u663e\u53ef\u4ee5\u770b\u51faELMo\u7684\u8bcd\u5411\u91cf\u53ef\u4ee5\u66f4\u597d\u7684\u8868\u8fbe\u771f\u5b9e\u8bed\u4e49, \u66f4\u597d\u7684\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u5b66\u4e60\u4e86ELMo\u7684\u5f85\u6539\u8fdb\u70b9. ELMo\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6ca1\u6709\u9009\u7528\u66f4\u5f3a\u5927\u7684Transformer, \u5728\u63d0\u53d6\u7279\u5f81\u4e0a\u80af\u5b9a\u5f31\u4e8e\u73b0\u5728\u7684\u6700\u4f18\u7ed3\u679c.","title":"8 ELMo\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fELMo. \u638c\u63e1ELMo\u7684\u67b6\u6784. \u638c\u63e1ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u4e86\u89e3ELMo\u7684\u6548\u679c\u548c\u6210\u7ee9. \u4e86\u89e3ELMo\u7684\u4f18\u7f3a\u70b9.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-elmo","text":"ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u6a21\u578b. ELMo\u7684\u5168\u79f0\u662fEmbeddings from Language Models. ELMo\u6a21\u578b\u7684\u63d0\u51fa\u6e90\u4e8e\u8bba\u6587 << Deep Contextualized Word Representations >> . ELMo\u6a21\u578b\u63d0\u51fa\u7684\u52a8\u673a\u6e90\u4e8e\u7814\u7a76\u4eba\u5458\u8ba4\u4e3a\u4e00\u4e2a\u597d\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5e94\u8be5\u80fd\u591f\u5305\u542b\u4e30\u5bcc\u7684\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f, \u5e76\u4e14\u80fd\u591f\u5bf9\u591a\u4e49\u8bcd\u8fdb\u884c\u5efa\u6a21. \u800c\u4f20\u7edf\u7684\u8bcd\u5411\u91cf(2013\u5e74\u7684word2vec, 2014\u5e74\u7684GloVe)\u90fd\u662f\u4e0a\u4e0b\u6587\u65e0\u5173\u7684, \u4e5f\u5c31\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf. \u6700\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\"apple\"\u5728\u4e0d\u540c\u7684\u8bed\u5883\u4e0b, \u5e94\u8be5\u53ef\u4ee5\u8868\u793a\u6c34\u679c\u6216\u516c\u53f8, \u4f46\u662f\u56fa\u5b9a\u7684\u8bcd\u5411\u91cf\u663e\u7136\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9. \u56e0\u6b64\u7814\u7a76\u56e2\u961f\u5229\u7528\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e00\u4e2a\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u9884\u8bad\u7ec3\u6a21\u578b, \u6210\u4e3aELMo, \u5e76\u57286\u4e2aNLP\u4efb\u52a1\u4e0a\u83b7\u5f97\u63d0\u5347.","title":"1 ELMo\u7b80\u4ecb"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-elmo","text":"","title":"2 ELMo\u7684\u67b6\u6784"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#21","text":"\u4ece\u4e0a\u9762\u7684\u67b6\u6784\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, \u5b8f\u89c2\u4e0aELMo\u5206\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757. \u6700\u5e95\u5c42\u9ec4\u8272\u6807\u8bb0\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u84dd\u8272\u6807\u8bb0\u7684\u4e24\u90e8\u5206\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7eff\u8272\u6807\u8bb0\u7684\u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757.","title":"2.1 \u603b\u4f53\u67b6\u6784"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#22-embedding","text":"ELMo\u6700\u5e95\u5c42\u7684\u8bcd\u5d4c\u5165\u91c7\u7528CNN\u5bf9\u5b57\u7b26\u7ea7\u8fdb\u884c\u7f16\u7801, \u672c\u8d28\u5c31\u662f\u83b7\u5f97\u4e00\u4e2a\u9759\u6001\u7684\u8bcd\u5d4c\u5165\u5411\u91cf\u4f5c\u4e3a\u7f51\u7edc\u7684\u5e95\u5c42\u8f93\u5165.","title":"2.2 Embedding\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#23-lstm","text":"\u8fd9\u662f\u6574\u4e2aELMo\u4e2d\u6700\u91cd\u8981\u7684\u90e8\u5206, \u67b6\u6784\u4e2d\u5206\u6210\u5de6\u4fa7\u7684\u524d\u5411LSTM\u7f51\u7edc, \u548c\u53f3\u4fa7\u7684\u53cd\u5411LSTM\u7f51\u7edc. ELMo\u7684\u505a\u6cd5\u662f\u6211\u4eec\u53ea\u9884\u8bad\u7ec3\u4e00\u4e2aLanguage Model, \u800cword embedding\u662f\u901a\u8fc7\u8f93\u5165\u7684\u53e5\u5b50\u5b9e\u65f6\u7ed9\u51fa\u7684, \u8fd9\u6837\u5355\u8bcd\u7684\u5d4c\u5165\u5411\u91cf\u5c31\u5305\u542b\u4e86\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f, \u4e5f\u5c31\u5f7b\u5e95\u6539\u53d8\u4e86Word2Vec\u548cGloVe\u7684\u9759\u6001\u8bcd\u5411\u91cf\u7684\u505a\u6cd5. ELMo\u7684\u8fd9\u4e00\u6a21\u5757\u5206\u4e3a\u5de6\u53f3\u4e24\u90e8\u5206, \u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u53cc\u5411LM, \u5bf9\u4e8e\u5de6\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t1, t2, ..., tN), Language Model\u901a\u8fc7\u524d\u9762k-1\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u524d\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,\\cdots,t_{k-1}) \u540c\u7406, \u5bf9\u4e8e\u67b6\u6784\u4e2d\u7684\u53f3\u534a\u90e8\u5206, \u7ed9\u5b9a\u4e86N\u4e2atokens(t(k+1), t(k+2), ..., t(N)), Language Model\u901a\u8fc7\u540e\u9762N-k\u4e2a\u4f4d\u7f6e\u7684token\u5e8f\u5217\u6765\u8ba1\u7b97\u7b2ck\u4e2atoken\u51fa\u73b0\u7684\u6982\u7387, \u6784\u6210\u540e\u5411\u53cc\u5c42LSTM\u6a21\u578b. p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) p(t_1,t_2,\\cdots,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},\\cdots,t_{N}) ELMo\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u76ee\u6807\u51fd\u6570\u5c31\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u516c\u5f0f: \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s)) \\sum_{k=1}^{N}(\\log p(t_k|t_1,\\cdots,t_{k-1};\\Theta_x,\\overset{\\to}\\Theta_{LSTM},\\Theta_s)+\\log p(t_k|t_{k+1},\\cdots,t_{N};\\Theta_x,\\overset{\\leftarrow}\\Theta_{LSTM},\\Theta_s))","title":"2.3 \u4e24\u90e8\u5206\u7684\u53cc\u5c42LSTM\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#24","text":"\u56e0\u4e3aELMo\u662f\u4e2a\u8bed\u8a00\u6a21\u578b, \u5bf9\u4e8e\u6bcf\u4e2atoken, \u901a\u8fc7\u4e00\u4e2aL\u5c42\u7684\u53cc\u5411LSTM\u7f51\u7edc\u53ef\u4ee5\u8ba1\u7b97\u51fa2L+1\u4e2a\u8868\u793a\u5411\u91cf\u5982\u4e0b: R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} R_k = \\left\\{x_k^{LM}, \\overset{\\to LM}{h_{k,j}}, \\overset{\\leftarrow LM}{h_{k,j}} |j=1,\\cdots,L\\right\\}=\\left\\{ h_{k,j}^{LM}|j=0,\\cdots,L\\right\\} \u4ece\u4e0a\u9762\u7684\u516c\u5f0f\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230, \u67093\u4e2a\u4e0d\u540c\u7684\u7ec4\u6210\u90e8\u5206, \u7b2c\u4e00\u4e2a\u5c31\u662f\u5bf9token\u76f4\u63a5\u8fdb\u884cCNN\u7f16\u7801\u7684\u7ed3\u679c, \u4e5f\u662fELMo\u6700\u5e95\u5c42\u6a21\u5757\u7684\u8f93\u51fa; \u7b2c\u4e8c\u4e2a\u5c31\u662f\u524d\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7b2c\u4e09\u4e2a\u5c31\u662f\u540e\u5411LSTM\u7684\u8f93\u51fa\u7ed3\u679c, \u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u4e00\u4e2a\u8f93\u51fa, \u603b\u5171L\u5c42\u5c31\u4f1a\u6709L\u4e2a\u8f93\u51fa; \u7efc\u5408\u4e09\u90e8\u5206\u7684\u8f93\u51fa\u52a0\u5728\u4e00\u8d77, \u5c31\u662f2L+1\u4e2a\u8f93\u51fa\u5411\u91cf. \u901a\u8fc7\u6574\u4e2a\u7f51\u7edc, \u6bcf\u4e00\u4e2atoken\u5f97\u5230\u4e862L+1\u4e2a\u8868\u793a\u5411\u91cf, \u4f46\u662f\u6211\u4eec\u5e0c\u671b\u6bcf\u4e00\u4e2atoken\u80fd\u5bf9\u5e94\u4e00\u4e2a\u5411\u91cf. \u6700\u7b80\u5355\u7684\u505a\u6cd5\u5c31\u662f\u53d6\u6700\u4e0a\u5c42\u7684\u8f93\u51fa\u7ed3\u679c\u4f5c\u4e3atoken\u7684\u8868\u793a\u5411\u91cf, \u66f4\u901a\u7528\u7684\u505a\u6cd5\u662f\u52a0\u5165\u82e5\u5e72\u53c2\u6570\u6765\u878d\u5408\u6240\u6709\u5c42\u7684\u4fe1\u606f, \u5982\u4e0b\u6240\u793a: ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} ELMo_k^{task}=E(R_k;\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM} \u4e0a\u5f0f\u7684\u610f\u601d\u662f\u5bf9\u4e8e2L+1\u4e2a\u5411\u91cf, \u6bcf\u4e00\u4e2a\u524d\u9762\u90fd\u52a0\u4e0a\u4e00\u4e2a\u6743\u91cd\u7a00\u758f, \u7136\u540e\u76f4\u63a5\u878d\u5408\u6210\u4e00\u4e2a\u5411\u91cf, \u6700\u540e\u518d\u4e58\u4e00\u4e2a\u7cfb\u6570\u4f5c\u4e3a\u6700\u7ec8\u8be5token\u7684\u8bcd\u5411\u91cf. \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u6700\u524d\u9762\u7684\u90a3\u4e2a\u7cfb\u6570, \u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u53d6\u4e0d\u540c\u7684\u503c\u6548\u679c\u4f1a\u6709\u8f83\u5927\u7684\u5dee\u5f02, \u9700\u8981\u6ce8\u610f\u5728SQuAD\u4e2d\u8bbe\u7f6e\u4e3a0.01\u53d6\u5f97\u7684\u6548\u679c\u8981\u597d\u4e8e\u8bbe\u7f6e\u4e3a1. \u539f\u59cb\u8bba\u6587\u4e2d\u5728\u8fdb\u884c\u5e95\u5c42token\u7f16\u7801\u65f6, \u7528CNN\u5f62\u6210\u4e86\u4e00\u4e2a512\u7ef4\u7684\u5217\u5411\u91cf, \u4e5f\u5c31\u662f\u521d\u59cb\u5d4c\u5165\u7ef4\u5ea6\u7b49\u4e8e512. \u4e2d\u95f4\u5c42\u4f7f\u7528\u4e86\u53cc\u5c42\u7684LSTM\u5206\u522b\u8fdb\u884c\u524d\u5411\u7f16\u7801\u548c\u540e\u5411\u7f16\u7801, \u6bcf\u5c42\u7684\u5355\u4e2aLSTM\u8f93\u5165\u7ef4\u5ea6\u662f512, \u8f93\u51fa\u7ef4\u5ea6\u4e5f\u662f512, \u4fdd\u6301\u4e00\u81f4. \u56e0\u4e3a\u662f\u53cc\u5411\u7f16\u7801\u5e76\u4e14\u5206\u5de6\u53f3\u4e24\u90e8\u5206, \u6240\u4ee5\u6bcf\u5c42\u7684\u8f93\u51fa\u7ef4\u5ea6\u662f512*2=1024, \u6700\u540e\u8fdb\u884c\u6743\u91cd\u878d\u5408\u540e\u7684\u5411\u91cf\u7ef4\u5ea6\u5c31\u662f1024.","title":"2.4 \u8bcd\u5411\u91cf\u8868\u5f81\u6a21\u5757"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-elmo","text":"","title":"3 ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#31-elmo","text":"\u9996\u5148\u7528\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5b66\u597d\u4e00\u4e2a\u5355\u8bcd\u7684word embedding, \u6b64\u65f6\u662f\u65e0\u6cd5\u533a\u5206\u591a\u4e49\u8bcd\u7684, \u4f46\u6ca1\u5173\u7cfb. \u5f53\u5b9e\u9645\u4f7f\u7528word embedding\u7684\u65f6\u5019, \u8be5\u5355\u8bcd\u5df2\u7ecf\u5177\u5907\u4e86\u7279\u5b9a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f, \u8fd9\u4e2a\u65f6\u5019\u53ef\u4ee5\u6839\u636e\u4e0a\u4e0b\u6587\u5355\u8bcd\u7684\u8bed\u4e49\u53bb\u8c03\u6574\u5355\u8bcd\u7684word embedding\u8868\u793a, \u8fd9\u6837\u7ecf\u8fc7\u8c03\u6574\u540e\u5f97\u5230\u7684word embedding\u5411\u91cf\u5c31\u53ef\u4ee5\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u5728\u5f53\u524d\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u5b9e\u542b\u4e49\u4e86, \u4e5f\u5c31\u81ea\u7136\u7684\u89e3\u51b3\u4e86\u591a\u4e49\u8bcd\u95ee\u9898. \u7ed3\u8bba\u5c31\u662fELMo\u6a21\u578b\u662f\u4e2a\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b.","title":"3.1 ELMo\u7684\u672c\u8d28\u601d\u60f3"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#32-elmo","text":"\u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u505a\u4e0b\u6e38\u4efb\u52a1\u65f6, \u4ece\u9884\u8bad\u7ec3\u7f51\u7edc\u4e2d\u63d0\u53d6\u5bf9\u5e94\u5355\u8bcd\u7684\u7f51\u7edc\u5404\u5c42\u7684word embedding\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u5230\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u7b2c\u4e00\u9636\u6bb5: \u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3. \u518d\u6b21\u56de\u5230ELMo\u7684\u603b\u4f53\u67b6\u6784\u56fe, \u7f51\u7edc\u7ed3\u6784\u91c7\u7528\u4e86\u53cc\u5c42\u53cc\u5411LSTM. \u76ee\u524d\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u4efb\u52a1\u76ee\u6807\u662f\u6839\u636e\u5355\u8bcdWi\u7684\u4e0a\u4e0b\u6587\u53bb\u6b63\u786e\u9884\u6d4b\u5355\u8bcdWi, Wi\u4e4b\u524d\u7684\u5355\u8bcd\u5e8f\u5217context-before\u79f0\u4e3a\u4e0a\u6587, Wi\u4e4b\u540e\u7684\u5355\u8bcd\u5e8f\u5217context-after\u79f0\u4e3a\u4e0b\u6587. \u67b6\u6784\u56fe\u4e0a\u5de6\u4fa7\u7684\u524d\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u6b63\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u5de6\u5411\u53f3\u987a\u5e8f\u7684\u9664\u4e86\u9884\u6d4b\u5355\u8bcdWi\u4e4b\u5916\u7684\u4e0a\u6587context-before; \u53f3\u4fa7\u7684\u53cd\u5411\u53cc\u5c42LSTM\u4ee3\u8868\u53cd\u65b9\u5411\u7f16\u7801\u5668, \u8f93\u5165\u7684\u662f\u4ece\u53f3\u5411\u5de6\u7684\u9006\u5e8f\u7684\u4e0b\u6587context-after; \u6bcf\u4e2a\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u90fd\u662fL=2, \u5373\u53cc\u5c42LSTM\u53e0\u52a0. \u4f7f\u7528\u4e0a\u8ff0\u7684\u7f51\u7edc\u7ed3\u6784\u5229\u7528\u5927\u91cf\u8bed\u6599\u505a\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u5c31\u80fd\u9884\u8bad\u7ec3\u597d\u8fd9\u4e2a\u7f51\u7edc. \u5f53\u8f93\u5165\u4e00\u4e2a\u65b0\u53e5\u5b50S_new\u65f6, \u53e5\u5b50\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u90fd\u80fd\u5f97\u5230\u5bf9\u5e94\u76843\u4e2aembedding\u5411\u91cf: 1-\u6700\u5e95\u5c42\u7684\u5355\u8bcd\u7684word embedding. 2-\u4e2d\u95f4\u7b2c\u4e00\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u53e5\u6cd5\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. 3-\u4e2d\u95f4\u7b2c\u4e8c\u5c42\u53cc\u5411LSTM\u4e2d\u5bf9\u5e94\u5355\u8bcd\u4f4d\u7f6e\u7684embedding, \u8fd9\u5c42\u7f16\u7801\u5bf9\u5e94\u5355\u8bcd\u7684\u8bed\u4e49\u4fe1\u606f\u66f4\u591a\u4e00\u4e9b. ELMo\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e0d\u4ec5\u4ec5\u5b66\u4f1a\u4e86\u5355\u8bcd\u7684word embedding, \u8fd8\u5b66\u4e60\u4e86\u4e00\u4e2a\u53cc\u5c42\u53cc\u5411\u7684LSTM\u7f51\u7edc, \u8fd9\u4e24\u8005\u540e\u7eed\u90fd\u4f1a\u7528\u5230, \u662f\u6574\u4e2aELMo\u9884\u8bad\u7ec3\u7684\u4e24\u5927\u4ea7\u51fa\u7ed3\u679c. \u7b2c\u4e8c\u9636\u6bb5: \u4e0b\u6e38\u4efb\u52a1\u7684\u8c03\u6574. \u6bd4\u5982\u6211\u4eec\u7684\u4e0b\u6e38\u4efb\u52a1\u662fQA\u95ee\u9898. \u5bf9\u4e8e\u95ee\u53e5X, \u53ef\u4ee5\u5148\u5c06\u53e5\u5b50X\u4f5c\u4e3a\u9884\u8bad\u7ec3\u597d\u7684ELMo\u7f51\u7edc\u7684\u8f93\u5165, \u8fd9\u6837X\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u5728ELMo\u4e2d\u90fd\u80fd\u83b7\u5f973\u4e2a\u5bf9\u5e94\u7684embedding\u5411\u91cf. \u4e4b\u540e\u8d4b\u7ed9\u8fd93\u4e2a\u5411\u91cf\u5404\u81ea\u4e00\u4e2a\u6743\u91cda, \u8fd9\u4e2a\u6743\u91cda\u65e2\u53ef\u4ee5\u662f\u5b66\u4e60\u5f97\u6765\u7684\u4e5f\u53ef\u4ee5\u662f\u6700\u7b80\u5355\u7684\u5e73\u5747\u5206\u5e03\u8d4b\u503c, \u7136\u540e\u628a3\u4e2a\u5411\u91cf\u52a0\u6743\u6c42\u548c, \u6574\u4e2a\u6210\u4e00\u4e2a\u8bcd\u5411\u91cf. \u6700\u540e\u5c06\u6574\u5408\u540e\u7684\u8bcd\u5411\u91cf\u4f5c\u4e3aX\u5728\u81ea\u5df1\u4efb\u52a1\u7684\u90a3\u4e2a\u7f51\u7edc\u7ed3\u6784\u4e2d\u5bf9\u5e94\u5355\u8bcd\u7684\u8f93\u5165, \u4ee5\u6b64\u4f5c\u4e3a\u65b0\u7279\u5f81\u8865\u5145\u8fdb\u4e0b\u6e38\u4efb\u52a1\u4e2d. \u5bf9\u4e8e\u56de\u7b54Y\u53ef\u4ee5\u540c\u6837\u5904\u7406. \u56e0\u4e3aELMo\u7ed9\u4e0b\u6e38\u63d0\u4f9b\u7684\u662f\u6bcf\u4e2a\u5355\u8bcd\u7684\u7279\u5f81\u5f62\u5f0f, \u6240\u4ee5\u8fd9\u4e00\u7c7b\u9884\u8bad\u7ec3\u65b9\u6cd5\u88ab\u79f0\u4e3a\"Feature-based Pre-Training\".","title":"3.2 ELMo\u7684\u9884\u8bad\u7ec3\u91c7\u7528\u4e86\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4-elmo","text":"ELMo\u5bf9\u4e8e\u591a\u4e49\u8bcd\u95ee\u9898\u7684\u89e3\u51b3\u7ed3\u679c: \u524d\u9762\u63d0\u5230\u9759\u6001\u7684word embedding\u65e0\u6cd5\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898, \u90a3\u4e48ELMo\u5f15\u5165\u4e0a\u4e0b\u6587\u52a8\u6001\u8bed\u4e49\u8c03\u6574\u540e\u7684embedding word\u53ef\u4ee5\u89e3\u51b3\u591a\u4e49\u8bcd\u95ee\u9898\u5417? \u7b54\u6848\u6b63\u5982\u4e0a\u56fe\u6240\u793a, \u800c\u4e14\u6bd4\u6211\u4eec\u671f\u5f85\u7684\u89e3\u51b3\u6548\u679c\u8981\u66f4\u597d. \u4e0a\u56fe\u4e2d\u7684\u4f8b\u5b50, \u5bf9\u4e8eGloVe\u8bad\u7ec3\u51fa\u6765\u7684word embedding\u6765\u8bf4, \u591a\u4e49\u8bcd\u6bd4\u5982play, \u6839\u636e\u5b83\u7684embedding\u627e\u51fa\u6700\u63a5\u8fd1\u5176\u8bed\u4e49\u7684\u5355\u8bcd, \u53d1\u73b0\u7ed3\u679c\u96c6\u5408\u51e0\u4e4e\u5168\u90e8\u90fd\u5728\u4f53\u80b2\u9886\u57df, \u8fd9\u5f88\u660e\u663e\u662f\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u4e2d\u5305\u542bplay\u7684\u8bed\u53e5\u4e2d\u4f53\u80b2\u9886\u57df\u7684\u6570\u91cf\u660e\u663e\u5360\u591a\u6570\u5bfc\u81f4\u7684. \u518d\u6765\u770b\u4f7f\u7528ELMo\u540e\u7684\u6548\u679c, \u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u540e\u7684embedding word\u4e0d\u4ec5\u4ec5\u80fd\u627e\u51fa\u5bf9\u5e94\u4e8e\"play\":\"\u6f14\u51fa\"\u7684\u76f8\u540c\u8bed\u4e49\u7684\u53e5\u5b50, \u800c\u4e14\u8fd8\u53ef\u4ee5\u4fdd\u8bc1\u627e\u51fa\u7684\u53e5\u5b50\u4e2d\u7684play\u5bf9\u5e94\u7684\u8bcd\u6027\u4e5f\u662f\u76f8\u540c\u7684, \u8fd9\u771f\u7684\u662f\u8d85\u51fa\u671f\u5f85\u4e4b\u5916\u7684\u60ca\u559c! \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230ELMo\u7684\u8bd5\u9a8c\u6548\u679c, \u57286\u4e2aNLP\u4e3b\u6d41\u4efb\u52a1\u4e2d\u6027\u80fd\u90fd\u6709\u4e0d\u540c\u5e45\u5ea6\u7684\u63d0\u5347, \u6700\u9ad8\u7684\u63d0\u5347\u8fbe\u523025%, \u4efb\u52a1\u7684\u8986\u76d6\u8303\u56f4\u5f88\u5e7f, \u5305\u542b\u53e5\u5b50\u8bed\u4e49\u5173\u7cfb\u5224\u65ad, \u5206\u7c7b\u4efb\u52a1, \u9605\u8bfb\u7406\u89e3\u7b49\u7b49.","title":"4 ELMo\u6a21\u578b\u7684\u6548\u679c"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#5-elmo","text":"ELMo\u5728\u4f20\u7edf\u9759\u6001word embedding\u65b9\u6cd5(Word2Vec, GloVe)\u7684\u57fa\u7840\u4e0a\u63d0\u5347\u4e86\u5f88\u591a, \u4f46\u662f\u4f9d\u7136\u5b58\u5728\u7f3a\u9677, \u6709\u5f88\u5927\u7684\u6539\u8fdb\u4f59\u5730. \u7b2c\u4e00\u70b9: \u4e00\u4e2a\u5f88\u660e\u663e\u7684\u7f3a\u70b9\u5728\u4e8e\u7279\u5f81\u63d0\u53d6\u5668\u7684\u9009\u62e9\u4e0a, ELMo\u4f7f\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM, \u800c\u4e0d\u662f\u73b0\u5728\u6a2a\u626b\u5343\u519b\u7684Transformer, \u5728\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0a\u80af\u5b9a\u662f\u8981\u5f31\u4e00\u4e9b\u7684. \u8bbe\u60f3\u5982\u679cELMo\u7684\u63d0\u5347\u63d0\u53d6\u5668\u9009\u7528Transformer, \u90a3\u4e48\u540e\u6765\u7684BERT\u7684\u53cd\u54cd\u5c06\u8fdc\u4e0d\u5982\u5f53\u65f6\u90a3\u4e48\u706b\u7206\u4e86. \u7b2c\u4e8c\u70b9: ELMo\u9009\u7528\u53cc\u5411\u62fc\u63a5\u7684\u65b9\u5f0f\u8fdb\u884c\u7279\u5f81\u878d\u5408, \u8fd9\u79cd\u65b9\u6cd5\u80af\u5b9a\u4e0d\u5982BERT\u4e00\u4f53\u5316\u7684\u53cc\u5411\u63d0\u53d6\u7279\u5f81\u597d.","title":"5 ELMo\u7684\u5f85\u6539\u8fdb\u70b9"},{"location":"06_mkdocs_pretrained_model_old/8%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#6","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fELMo. ELMo\u662f2018\u5e743\u6708\u7531\u534e\u76db\u987f\u5927\u5b66\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. ELMo\u57286\u79cdNLP\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u6709\u5f88\u5927\u7684\u63d0\u5347\u8868\u73b0. \u5b66\u4e60\u4e86ELMo\u7684\u7ed3\u6784. ELMo\u67b6\u6784\u603b\u4f53\u4e0a\u91c7\u7528\u4e86\u53cc\u5411\u53cc\u5c42LSTM\u7684\u7ed3\u6784. \u6700\u5e95\u5c42\u7684Embedding\u6a21\u5757. \u4e2d\u95f4\u5c42\u7684\u53cc\u5411\u53cc\u5c42LSTM\u6a21\u5757. \u6700\u4e0a\u5c42\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757. \u5b66\u4e60\u4e86ELMo\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. ELMo\u7684\u672c\u8d28\u601d\u60f3\u5c31\u662f\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u5bf9word embedding\u8fdb\u884c\u52a8\u6001\u8c03\u6574\u7684\u8bed\u8a00\u6a21\u578b. ELMo\u7684\u9884\u8bad\u7ec3\u662f\u4e00\u4e2a\u660e\u663e\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b. \u7b2c\u4e00\u9636\u6bb5: \u5229\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3, \u5f97\u5230\u57fa\u7840\u9759\u6001\u8bcd\u5411\u91cf\u548c\u53cc\u5411\u53cc\u5c42LSTM\u7f51\u7edc. \u7b2c\u4e8c\u9636\u6bb5: \u5728\u62e5\u6709\u4e0a\u4e0b\u6587\u7684\u73af\u5883\u4e2d, \u5c06\u4e0a\u4e0b\u6587\u8f93\u5165\u53cc\u5411\u53cc\u5c42LSTM\u4e2d, \u5f97\u5230\u52a8\u6001\u8c03\u6574\u540e\u7684word embedding, \u7b49\u4e8e\u5c06\u5355\u8bcd\u878d\u5408\u8fdb\u4e86\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49, \u53ef\u4ee5\u66f4\u51c6\u786e\u7684\u8868\u8fbe\u5355\u8bcd\u7684\u771f\u5b9e\u542b\u4e49. \u5b66\u4e60\u4e86ELMo\u7684\u6548\u679c. \u7ecf\u8fc7\u4e0eGloVe\u9759\u6001\u8bcd\u5411\u91cf\u7684\u5bf9\u6bd4, \u660e\u663e\u53ef\u4ee5\u770b\u51faELMo\u7684\u8bcd\u5411\u91cf\u53ef\u4ee5\u66f4\u597d\u7684\u8868\u8fbe\u771f\u5b9e\u8bed\u4e49, \u66f4\u597d\u7684\u89e3\u51b3\u591a\u4e49\u8bcd\u7684\u95ee\u9898. \u5b66\u4e60\u4e86ELMo\u7684\u5f85\u6539\u8fdb\u70b9. ELMo\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6ca1\u6709\u9009\u7528\u66f4\u5f3a\u5927\u7684Transformer, \u5728\u63d0\u53d6\u7279\u5f81\u4e0a\u80af\u5b9a\u5f31\u4e8e\u73b0\u5728\u7684\u6700\u4f18\u7ed3\u679c.","title":"6 \u5c0f\u7ed3"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3\u4ec0\u4e48\u662fGPT. \u638c\u63e1GPT\u7684\u67b6\u6784. \u638c\u63e1GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. 1 GPT\u4ecb\u7ecd \u00b6 GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. OpenAI\u5728\u8bba\u6587 << Improving Language Understanding by Generative Pre-Training >> \u4e2d\u63d0\u51faGPT\u6a21\u578b. OpenAI\u540e\u7eed\u53c8\u5728\u8bba\u6587 << Language Models are Unsupervised Multitask Learners >> \u4e2d\u63d0\u51faGPT2\u6a21\u578b. GPT\u548cGPT2\u6a21\u578b\u7ed3\u6784\u5dee\u522b\u4e0d\u5927, \u4f46\u662fGPT2\u91c7\u7528\u4e86\u66f4\u5927\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3. OpenAI GPT\u6a21\u578b\u662f\u5728Google BERT\u6a21\u578b\u4e4b\u524d\u63d0\u51fa\u7684, \u4e0eBERT\u6700\u5927\u7684\u533a\u522b\u5728\u4e8eGPT\u91c7\u7528\u4e86\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3, \u5373\u4f7f\u7528\u5355\u8bcd\u7684\u4e0a\u6587\u6765\u9884\u6d4b\u5355\u8bcd, \u800cBERT\u662f\u91c7\u7528\u4e86\u53cc\u5411\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f\u5171\u540c\u6765\u9884\u6d4b\u5355\u8bcd. \u6b63\u662f\u56e0\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u4e0a\u7684\u533a\u522b, \u4f7f\u5f97GPT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1(NLG), \u800cBERT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1(NLU). 2 GPT\u7684\u67b6\u6784 \u00b6 \u770b\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6bd4\u67b6\u6784\u56fe, \u4e2d\u95f4\u7684\u5c31\u662fGPT: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230GPT\u91c7\u7528\u7684\u662f\u5355\u5411Transformer\u6a21\u578b, \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50[u1, u2, ..., un], GPT\u5728\u9884\u6d4b\u5355\u8bcdui\u7684\u65f6\u5019\u53ea\u4f1a\u5229\u7528[u1, u2, ..., u(i-1)]\u7684\u4fe1\u606f, \u800cBERT\u4f1a\u540c\u65f6\u5229\u7528\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f[u1, u2, ..., u(i-1), u(i+1), ..., un]. \u4f5c\u4e3a\u4e24\u5927\u6a21\u578b\u7684\u76f4\u63a5\u5bf9\u6bd4, BERT\u91c7\u7528\u4e86Transformer\u7684Encoder\u6a21\u5757, \u800cGPT\u91c7\u7528\u4e86Transformer\u7684Decoder\u6a21\u5757. \u5e76\u4e14GPT\u7684Decoder Block\u548c\u7ecf\u5178Transformer Decoder Block\u8fd8\u6709\u6240\u4e0d\u540c, \u5982\u4e0b\u56fe\u6240\u793a: \u5982\u4e0a\u56fe\u6240\u793a, \u7ecf\u5178\u7684Transformer Decoder Block\u5305\u542b3\u4e2a\u5b50\u5c42, \u5206\u522b\u662fMasked Multi-Head Attention\u5c42, encoder-decoder attention\u5c42, \u4ee5\u53caFeed Forward\u5c42. \u4f46\u662f\u5728GPT\u4e2d\u53d6\u6d88\u4e86\u7b2c\u4e8c\u4e2aencoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5c42, \u548cFeed Forward\u5c42. \u4f5c\u4e3a\u5355\u5411Transformer Decoder\u6a21\u578b, GPT\u5229\u7528\u53e5\u5b50\u5e8f\u5217\u4fe1\u606f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u65f6\u5019, \u8981\u4f7f\u7528Masked Multi-Head Attention\u5bf9\u5355\u8bcd\u7684\u4e0b\u6587\u8fdb\u884c\u906e\u63a9(look ahead mask), \u6765\u9632\u6b62\u672a\u6765\u4fe1\u606f\u7684\u63d0\u524d\u6cc4\u9732. \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50\u5305\u542b4\u4e2a\u5355\u8bcd[A, B, C, D], GPT\u9700\u8981\u7528[A]\u9884\u6d4bB, \u7528[A, B]\u9884\u6d4bC, \u7528[A, B, C]\u9884\u6d4bD. \u5f88\u663e\u7136\u7684\u5c31\u662f\u5f53\u8981\u9884\u6d4bB\u65f6, \u9700\u8981\u5c06[B, C, D]\u906e\u63a9\u8d77\u6765. \u5177\u4f53\u7684\u906e\u63a9\u64cd\u4f5c\u662f\u5728slef-attention\u8fdb\u884csoftmax\u4e4b\u524d\u8fdb\u884c\u7684, \u4e00\u822c\u7684\u5b9e\u73b0\u662f\u5c06MASK\u7684\u4f4d\u7f6e\u7528\u4e00\u4e2a\u65e0\u7a77\u5c0f\u7684\u6570\u503c-inf\u6765\u66ff\u6362, \u66ff\u6362\u540e\u6267\u884csoftmax\u8ba1\u7b97\u5f97\u5230\u65b0\u7684\u7ed3\u679c\u77e9\u9635. \u8fd9\u6837-inf\u7684\u4f4d\u7f6e\u5c31\u53d8\u6210\u4e860. \u5982\u4e0a\u56fe\u6240\u793a, \u6700\u540e\u7684\u77e9\u9635\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u505a\u5230\u5f53\u5229\u7528A\u9884\u6d4bB\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A\u7684\u4fe1\u606f; \u5f53\u5229\u7528[A, B]\u9884\u6d4bC\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A, B\u7684\u4fe1\u606f. \u6ce8\u610f: \u5bf9\u6bd4\u4e8e\u7ecf\u5178\u7684Transformer\u67b6\u6784, \u89e3\u7801\u5668\u6a21\u5757\u91c7\u7528\u4e866\u4e2aDecoder Block; GPT\u7684\u67b6\u6784\u4e2d\u91c7\u7528\u4e8612\u4e2aDecoder Block. 3 GPT\u8bad\u7ec3\u8fc7\u7a0b \u00b6 GPT\u7684\u8bad\u7ec3\u4e5f\u662f\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b: \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning. 3.1 \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b \u00b6 \u7ed9\u5b9a\u53e5\u5b50U = [u1, u2, ..., un], GPT\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u4f3c\u7136\u51fd\u6570: $$ L_1(U)=\\sum_i\\log P(u_i|u_{i-k},\\cdots,u_{i-1};\\Theta) $$ \u6709\u4e0a\u8ff0\u516c\u5f0f\u53ef\u77e5, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5047\u8bbe\u8f93\u5165\u5f20\u91cf\u7528h0\u8868\u793a, \u5219\u8ba1\u7b97\u516c\u5f0f\u5982\u4e0b: $$ h_0 = UW_e + W_p $$ \u5176\u4e2dWp\u662f\u5355\u8bcd\u7684\u4f4d\u7f6e\u7f16\u7801, We\u662f\u5355\u8bcd\u672c\u8eab\u7684word embedding. Wp\u7684\u5f62\u72b6\u662f[max_seq_len, embedding_dim], We\u7684\u5f62\u72b6\u662f[vocab_size, embedding_dim]. \u5f97\u5230\u8f93\u5165\u5f20\u91cfh0\u540e, \u8981\u5c06h0\u4f20\u5165GPT\u7684Decoder Block\u4e2d, \u4f9d\u6b21\u5f97\u5230ht: $$ h_t = transformer\\_block(h_{l-1})\\;\\;\\;\\;l\\in[1,t] $$ \u6700\u540e\u901a\u8fc7\u5f97\u5230\u7684ht\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd: $$ P(u)=softmax(h_tW_e^T) $$ 3.2 \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning \u00b6 GPT\u7ecf\u8fc7\u9884\u8bad\u7ec3\u540e, \u4f1a\u9488\u5bf9\u5177\u4f53\u7684\u4e0b\u6e38\u4efb\u52a1\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03. \u5fae\u8c03\u91c7\u7528\u7684\u662f\u6709\u76d1\u7763\u5b66\u4e60, \u8bad\u7ec3\u6837\u672c\u5305\u62ec\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u548clabel y. GPT\u5fae\u8c03\u7684\u76ee\u6807\u4efb\u52a1\u662f\u6839\u636e\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u9884\u6d4b\u6807\u7b7ey. P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) \u5176\u4e2d W_y W_y \u8868\u793a\u9884\u6d4b\u8f93\u51fa\u7684\u77e9\u9635\u53c2\u6570, \u5fae\u8c03\u4efb\u52a1\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u51fd\u6570: L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) \u7efc\u5408\u4e24\u4e2a\u9636\u6bb5\u7684\u76ee\u6807\u4efb\u52a1\u51fd\u6570, \u53ef\u77e5GPT\u7684\u6700\u7ec8\u4f18\u5316\u51fd\u6570\u4e3a: L_3 = L_2 + \\lambda L_1 L_3 = L_2 + \\lambda L_1 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86\u4ec0\u4e48\u662fGPT. GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u672c\u8d28\u4e0a\u6765\u8bf4, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b. \u5b66\u4e60\u4e86GPT\u7684\u67b6\u6784. GPT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684\u89e3\u7801\u5668\u6a21\u5757. GPT\u5728\u4f7f\u7528\u89e3\u7801\u5668\u6a21\u5757\u65f6\u505a\u4e86\u4e00\u5b9a\u7684\u6539\u9020, \u5c06\u4f20\u7edf\u76843\u5c42Decoder Block\u53d8\u6210\u4e862\u5c42Block, \u5220\u9664\u4e86encoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5b50\u5c42\u548cFeed Forward\u5b50\u5c42. GPT\u7684\u89e3\u7801\u5668\u603b\u5171\u662f\u753112\u4e2a\u6539\u9020\u540e\u7684Decoder Block\u7ec4\u6210\u7684. \u5b66\u4e60\u4e86GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u53ea\u5229\u7528\u5355\u8bcd\u524d\u9762\u7684\u4fe1\u606f\u6765\u9884\u6d4b\u5f53\u524d\u5355\u8bcd. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning.","title":"9 GPT\u6a21\u578b\u4ecb\u7ecd"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3\u4ec0\u4e48\u662fGPT. \u638c\u63e1GPT\u7684\u67b6\u6784. \u638c\u63e1GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#1-gpt","text":"GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b. OpenAI\u5728\u8bba\u6587 << Improving Language Understanding by Generative Pre-Training >> \u4e2d\u63d0\u51faGPT\u6a21\u578b. OpenAI\u540e\u7eed\u53c8\u5728\u8bba\u6587 << Language Models are Unsupervised Multitask Learners >> \u4e2d\u63d0\u51faGPT2\u6a21\u578b. GPT\u548cGPT2\u6a21\u578b\u7ed3\u6784\u5dee\u522b\u4e0d\u5927, \u4f46\u662fGPT2\u91c7\u7528\u4e86\u66f4\u5927\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3. OpenAI GPT\u6a21\u578b\u662f\u5728Google BERT\u6a21\u578b\u4e4b\u524d\u63d0\u51fa\u7684, \u4e0eBERT\u6700\u5927\u7684\u533a\u522b\u5728\u4e8eGPT\u91c7\u7528\u4e86\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3, \u5373\u4f7f\u7528\u5355\u8bcd\u7684\u4e0a\u6587\u6765\u9884\u6d4b\u5355\u8bcd, \u800cBERT\u662f\u91c7\u7528\u4e86\u53cc\u5411\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f\u5171\u540c\u6765\u9884\u6d4b\u5355\u8bcd. \u6b63\u662f\u56e0\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u4e0a\u7684\u533a\u522b, \u4f7f\u5f97GPT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1(NLG), \u800cBERT\u66f4\u64c5\u957f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1(NLU).","title":"1 GPT\u4ecb\u7ecd"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#2-gpt","text":"\u770b\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6bd4\u67b6\u6784\u56fe, \u4e2d\u95f4\u7684\u5c31\u662fGPT: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230GPT\u91c7\u7528\u7684\u662f\u5355\u5411Transformer\u6a21\u578b, \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50[u1, u2, ..., un], GPT\u5728\u9884\u6d4b\u5355\u8bcdui\u7684\u65f6\u5019\u53ea\u4f1a\u5229\u7528[u1, u2, ..., u(i-1)]\u7684\u4fe1\u606f, \u800cBERT\u4f1a\u540c\u65f6\u5229\u7528\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f[u1, u2, ..., u(i-1), u(i+1), ..., un]. \u4f5c\u4e3a\u4e24\u5927\u6a21\u578b\u7684\u76f4\u63a5\u5bf9\u6bd4, BERT\u91c7\u7528\u4e86Transformer\u7684Encoder\u6a21\u5757, \u800cGPT\u91c7\u7528\u4e86Transformer\u7684Decoder\u6a21\u5757. \u5e76\u4e14GPT\u7684Decoder Block\u548c\u7ecf\u5178Transformer Decoder Block\u8fd8\u6709\u6240\u4e0d\u540c, \u5982\u4e0b\u56fe\u6240\u793a: \u5982\u4e0a\u56fe\u6240\u793a, \u7ecf\u5178\u7684Transformer Decoder Block\u5305\u542b3\u4e2a\u5b50\u5c42, \u5206\u522b\u662fMasked Multi-Head Attention\u5c42, encoder-decoder attention\u5c42, \u4ee5\u53caFeed Forward\u5c42. \u4f46\u662f\u5728GPT\u4e2d\u53d6\u6d88\u4e86\u7b2c\u4e8c\u4e2aencoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5c42, \u548cFeed Forward\u5c42. \u4f5c\u4e3a\u5355\u5411Transformer Decoder\u6a21\u578b, GPT\u5229\u7528\u53e5\u5b50\u5e8f\u5217\u4fe1\u606f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u65f6\u5019, \u8981\u4f7f\u7528Masked Multi-Head Attention\u5bf9\u5355\u8bcd\u7684\u4e0b\u6587\u8fdb\u884c\u906e\u63a9(look ahead mask), \u6765\u9632\u6b62\u672a\u6765\u4fe1\u606f\u7684\u63d0\u524d\u6cc4\u9732. \u4f8b\u5982\u7ed9\u5b9a\u4e00\u4e2a\u53e5\u5b50\u5305\u542b4\u4e2a\u5355\u8bcd[A, B, C, D], GPT\u9700\u8981\u7528[A]\u9884\u6d4bB, \u7528[A, B]\u9884\u6d4bC, \u7528[A, B, C]\u9884\u6d4bD. \u5f88\u663e\u7136\u7684\u5c31\u662f\u5f53\u8981\u9884\u6d4bB\u65f6, \u9700\u8981\u5c06[B, C, D]\u906e\u63a9\u8d77\u6765. \u5177\u4f53\u7684\u906e\u63a9\u64cd\u4f5c\u662f\u5728slef-attention\u8fdb\u884csoftmax\u4e4b\u524d\u8fdb\u884c\u7684, \u4e00\u822c\u7684\u5b9e\u73b0\u662f\u5c06MASK\u7684\u4f4d\u7f6e\u7528\u4e00\u4e2a\u65e0\u7a77\u5c0f\u7684\u6570\u503c-inf\u6765\u66ff\u6362, \u66ff\u6362\u540e\u6267\u884csoftmax\u8ba1\u7b97\u5f97\u5230\u65b0\u7684\u7ed3\u679c\u77e9\u9635. \u8fd9\u6837-inf\u7684\u4f4d\u7f6e\u5c31\u53d8\u6210\u4e860. \u5982\u4e0a\u56fe\u6240\u793a, \u6700\u540e\u7684\u77e9\u9635\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u505a\u5230\u5f53\u5229\u7528A\u9884\u6d4bB\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A\u7684\u4fe1\u606f; \u5f53\u5229\u7528[A, B]\u9884\u6d4bC\u7684\u65f6\u5019, \u53ea\u80fd\u770b\u5230A, B\u7684\u4fe1\u606f. \u6ce8\u610f: \u5bf9\u6bd4\u4e8e\u7ecf\u5178\u7684Transformer\u67b6\u6784, \u89e3\u7801\u5668\u6a21\u5757\u91c7\u7528\u4e866\u4e2aDecoder Block; GPT\u7684\u67b6\u6784\u4e2d\u91c7\u7528\u4e8612\u4e2aDecoder Block.","title":"2 GPT\u7684\u67b6\u6784"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#3-gpt","text":"GPT\u7684\u8bad\u7ec3\u4e5f\u662f\u5178\u578b\u7684\u4e24\u9636\u6bb5\u8fc7\u7a0b: \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning.","title":"3 GPT\u8bad\u7ec3\u8fc7\u7a0b"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#31","text":"\u7ed9\u5b9a\u53e5\u5b50U = [u1, u2, ..., un], GPT\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u4f3c\u7136\u51fd\u6570: $$ L_1(U)=\\sum_i\\log P(u_i|u_{i-k},\\cdots,u_{i-1};\\Theta) $$ \u6709\u4e0a\u8ff0\u516c\u5f0f\u53ef\u77e5, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b, \u5047\u8bbe\u8f93\u5165\u5f20\u91cf\u7528h0\u8868\u793a, \u5219\u8ba1\u7b97\u516c\u5f0f\u5982\u4e0b: $$ h_0 = UW_e + W_p $$ \u5176\u4e2dWp\u662f\u5355\u8bcd\u7684\u4f4d\u7f6e\u7f16\u7801, We\u662f\u5355\u8bcd\u672c\u8eab\u7684word embedding. Wp\u7684\u5f62\u72b6\u662f[max_seq_len, embedding_dim], We\u7684\u5f62\u72b6\u662f[vocab_size, embedding_dim]. \u5f97\u5230\u8f93\u5165\u5f20\u91cfh0\u540e, \u8981\u5c06h0\u4f20\u5165GPT\u7684Decoder Block\u4e2d, \u4f9d\u6b21\u5f97\u5230ht: $$ h_t = transformer\\_block(h_{l-1})\\;\\;\\;\\;l\\in[1,t] $$ \u6700\u540e\u901a\u8fc7\u5f97\u5230\u7684ht\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd: $$ P(u)=softmax(h_tW_e^T) $$","title":"3.1 \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#32-fine-tunning","text":"GPT\u7ecf\u8fc7\u9884\u8bad\u7ec3\u540e, \u4f1a\u9488\u5bf9\u5177\u4f53\u7684\u4e0b\u6e38\u4efb\u52a1\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03. \u5fae\u8c03\u91c7\u7528\u7684\u662f\u6709\u76d1\u7763\u5b66\u4e60, \u8bad\u7ec3\u6837\u672c\u5305\u62ec\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u548clabel y. GPT\u5fae\u8c03\u7684\u76ee\u6807\u4efb\u52a1\u662f\u6839\u636e\u5355\u8bcd\u5e8f\u5217[x1, x2, ..., xn]\u9884\u6d4b\u6807\u7b7ey. P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) P(y|x^1,\\cdots,x^m)=softmax(h_l^mW_y) \u5176\u4e2d W_y W_y \u8868\u793a\u9884\u6d4b\u8f93\u51fa\u7684\u77e9\u9635\u53c2\u6570, \u5fae\u8c03\u4efb\u52a1\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u4e0b\u9762\u7684\u51fd\u6570: L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) L_2=\\sum_{(x,y)}\\log P(y|x^1,\\cdots,x^m) \u7efc\u5408\u4e24\u4e2a\u9636\u6bb5\u7684\u76ee\u6807\u4efb\u52a1\u51fd\u6570, \u53ef\u77e5GPT\u7684\u6700\u7ec8\u4f18\u5316\u51fd\u6570\u4e3a: L_3 = L_2 + \\lambda L_1 L_3 = L_2 + \\lambda L_1","title":"3.2 \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning"},{"location":"06_mkdocs_pretrained_model_old/9%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html#4","text":"\u5b66\u4e60\u4e86\u4ec0\u4e48\u662fGPT. GPT\u662fOpenAI\u516c\u53f8\u63d0\u51fa\u7684\u4e00\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u672c\u8d28\u4e0a\u6765\u8bf4, GPT\u662f\u4e00\u4e2a\u5355\u5411\u8bed\u8a00\u6a21\u578b. \u5b66\u4e60\u4e86GPT\u7684\u67b6\u6784. GPT\u91c7\u7528\u4e86Transformer\u67b6\u6784\u4e2d\u7684\u89e3\u7801\u5668\u6a21\u5757. GPT\u5728\u4f7f\u7528\u89e3\u7801\u5668\u6a21\u5757\u65f6\u505a\u4e86\u4e00\u5b9a\u7684\u6539\u9020, \u5c06\u4f20\u7edf\u76843\u5c42Decoder Block\u53d8\u6210\u4e862\u5c42Block, \u5220\u9664\u4e86encoder-decoder attention\u5b50\u5c42, \u53ea\u4fdd\u7559Masked Multi-Head Attention\u5b50\u5c42\u548cFeed Forward\u5b50\u5c42. GPT\u7684\u89e3\u7801\u5668\u603b\u5171\u662f\u753112\u4e2a\u6539\u9020\u540e\u7684Decoder Block\u7ec4\u6210\u7684. \u5b66\u4e60\u4e86GPT\u7684\u9884\u8bad\u7ec3\u4efb\u52a1. \u7b2c\u4e00\u9636\u6bb5: \u65e0\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b. \u53ea\u5229\u7528\u5355\u8bcd\u524d\u9762\u7684\u4fe1\u606f\u6765\u9884\u6d4b\u5f53\u524d\u5355\u8bcd. \u7b2c\u4e8c\u9636\u6bb5: \u6709\u76d1\u7763\u7684\u4e0b\u6e38\u4efb\u52a1fine-tunning.","title":"4 \u5c0f\u7ed3"},{"location":"07_mkdocs_HMM_CRF/index.html","text":"\u00b6","title":"Index"},{"location":"07_mkdocs_HMM_CRF/index.html#_1","text":"","title":""},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa. \u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u7684\u4f5c\u7528. \u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u7684\u4f7f\u7528\u8fc7\u7a0b. \u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02. \u4e86\u89e3HMM\u548cCRF\u7684\u53d1\u5c55\u73b0\u72b6. 1 HMM\u6a21\u578b\u4ecb\u7ecd \u00b6 1.1 HMM\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa \u00b6 HMM(Hidden Markov Model), \u4e2d\u6587\u79f0\u4f5c\u9690\u542b\u9a6c\u5c14\u79d1\u592b\u6a21\u578b, \u56e0\u4fc4\u56fd\u6570\u5b66\u5bb6\u9a6c\u5c14\u53ef\u592b\u800c\u5f97\u540d. \u5b83\u4e00\u822c\u4ee5\u6587\u672c\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u4ee5\u8be5\u5e8f\u5217\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217\u4e3a\u8f93\u51fa. \u4ec0\u4e48\u662f\u9690\u542b\u5e8f\u5217: * \u5e8f\u5217\u6570\u636e\u4e2d\u6bcf\u4e2a\u5355\u5143\u5305\u542b\u7684\u9690\u6027\u4fe1\u606f, \u8fd9\u4e9b\u9690\u6027\u4fe1\u606f\u4e4b\u95f4\u4e5f\u5b58\u5728\u4e00\u5b9a\u5173\u8054. \u4e3e\u4e2a\u4f8b\u5b50: \u7ed9\u5b9a\u4e00\u6bb5\u6587\u672c: \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" \u6211\u4eec\u770b\u5230\u7684\u8fd9\u53e5\u8bdd\u53ef\u4ee5\u53eb\u505a: \u89c2\u6d4b\u5e8f\u5217 \u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u53e5\u8bdd\u4ee5\u8bcd\u4e3a\u5355\u4f4d\u8fdb\u884c\u5212\u5206\u5f97\u5230: [\"\u4eba\u751f\", \"\u8be5\", \"\u5982\u4f55\", \"\u8d77\u5934\"] \u90a3\u4e48\u6bcf\u4e2a\u8bcd\u5bf9\u5e94\u7684\u8bcd\u6027\u5c31\u662f\u5b83\u7684\u9690\u542b\u5e8f\u5217, \u5982: [\"n\", \"r\", \"r\", \"v\"] 1.2 HMM\u6a21\u578b\u7684\u4f5c\u7528 \u00b6 \u5728NLP\u9886\u57df, HMM\u7528\u6765\u89e3\u51b3\u6587\u672c\u5e8f\u5217\u6807\u6ce8\u95ee\u9898. \u5982\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u90fd\u53ef\u4ee5\u770b\u4f5c\u662f\u5e8f\u5217\u6807\u6ce8\u95ee\u9898. HMM\u7684\u4e24\u5927\u5047\u8bbe: \u9f50\u6b21\u9a6c\u5c14\u79d1\u592b\u5047\u8bbe: \u4efb\u4e00\u65f6\u523b t \u7684\u67d0\u4e00\u72b6\u6001\u53ea\u4f9d\u8d56\u4e8e\u5176\u524d\u4e00\u65f6\u523b\u7684\u72b6\u6001, \u4e0e\u5176\u5b83\u65f6\u523b\u7684\u72b6\u6001\u53ca\u89c2\u6d4b\u65e0\u5173, \u4e5f\u4e0e\u65f6\u523b t \u65e0\u5173. \u89c2\u6d4b\u72ec\u7acb\u6027\u5047\u8bbe: \u4efb\u610f\u65f6\u523b\u7684\u89c2\u6d4b\u72b6\u6001\u53ea\u4f9d\u8d56\u4e8e\u8be5\u65f6\u523b\u7684\u9690\u85cf\u72b6\u6001, \u4e0e\u5176\u4ed6\u72b6\u6001\u65e0\u5173. 1.3 HMM\u6a21\u578b\u4f7f\u7528\u8fc7\u7a0b\u7b80\u8ff0 \u00b6 \u9996\u5148, HMM\u6a21\u578b\u8868\u793a\u4e3a: lambda = HMM(A, B, pi), \u5176\u4e2dA, B, pi\u90fd\u662f\u6a21\u578b\u7684\u53c2\u6570, \u5206\u522b\u79f0\u4f5c: \u8f6c\u79fb\u6982\u7387\u77e9\u9635, \u53d1\u5c04\u6982\u7387\u77e9\u9635\u548c\u521d\u59cb\u6982\u7387\u77e9\u9635. \u63a5\u7740, \u6211\u4eec\u5f00\u59cb\u8bad\u7ec3HMM\u6a21\u578b, \u8bed\u6599\u5c31\u662f\u4e8b\u5148\u51c6\u5907\u597d\u7684\u4e00\u5b9a\u6570\u91cf\u7684\u89c2\u6d4b\u5e8f\u5217\u53ca\u5176\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217, \u901a\u8fc7\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u6c42\u5f97\u4e00\u7ec4\u53c2\u6570, \u4f7f\u7531\u89c2\u6d4b\u5e8f\u5217\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6982\u7387\u6700\u5927. \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u4e3a\u4e86\u7b80\u5316\u8ba1\u7b97, \u9a6c\u5c14\u53ef\u592b\u63d0\u51fa\u4e00\u79cd\u5047\u8bbe: \u9690\u542b\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5355\u5143\u7684\u53ef\u80fd\u6027\u53ea\u4e0e\u4e0a\u4e00\u4e2a\u5355\u5143\u6709\u5173. \u8fd9\u4e2a\u5047\u8bbe\u5c31\u662f\u8457\u540d\u7684\u9690\u542b\u5047\u8bbe. \u8bad\u7ec3\u540e, \u6211\u4eec\u5c31\u5f97\u5230\u4e86\u5177\u5907\u9884\u6d4b\u80fd\u529b\u7684\u65b0\u6a21\u578b: lambda = HMM(A, B, pi), \u5176\u4e2d\u7684\u6a21\u578b\u53c2\u6570\u5df2\u7ecf\u6539\u53d8. \u4e4b\u540e\u7ed9\u5b9a\u8f93\u5165\u5e8f\u5217(x1, x2, ..., xn), \u7ecf\u8fc7\u6a21\u578b\u8ba1\u7b97lambda(x1, x2, ..., xn)\u5f97\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03. \u6700\u540e, \u4f7f\u7528\u7ef4\u7279\u6bd4\u7b97\u6cd5\u4ece\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\u4e2d\u627e\u51fa\u6982\u7387\u6700\u5927\u7684\u4e00\u6761\u5e8f\u5217\u8def\u5f84\u5c31\u662f\u6211\u4eec\u9700\u8981\u7684\u9690\u542b\u5e8f\u5217: (y1, y2, ..., yn). 1.4 HMM\u5e38\u89c1\u4e09\u4e2a\u95ee\u9898 \u00b6 \u8bc4\u4f30\u95ee\u9898: \u5df2\u77e5\u6a21\u578b\u53c2\u6570, \u8ba1\u7b97\u67d0\u4e00\u7279\u5b9a\u8f93\u51fa\u5e8f\u5217\u7684\u6982\u7387. \u901a\u5e38\u4f7f\u7528\u524d\u5411\u6216\u8005\u540e\u5411\u7b97\u6cd5\u89e3\u51b3. \u89e3\u7801\u95ee\u9898: \u5df2\u77e5\u6a21\u578b\u53c2\u6570, \u6c42\u5bf9\u5e94\u7684\u6982\u7387\u6700\u5927\u7684\u9690\u542b\u72b6\u6001\u5e8f\u5217. \u901a\u5e38\u4f7f\u7528Viterbi\u7b97\u6cd5\u89e3\u51b3. \u5b66\u4e60\u95ee\u9898: \u5df2\u77e5\u89c2\u6d4b\u5e8f\u5217, \u6c42\u4f7f\u5f97\u8be5\u5e8f\u5217\u6982\u7387\u6700\u5927\u7684\u6a21\u578b\u53c2\u6570, \u901a\u5e38\u4f7f\u7528EM\u7b97\u6cd5\u6216\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u89e3\u51b3. 2 CRF\u6a21\u578b\u4ecb\u7ecd \u00b6 2.1 CRF\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa \u00b6 CRF(Conditional Random Fields), \u4e2d\u6587\u79f0\u4f5c\u6761\u4ef6\u968f\u673a\u573a, \u540cHMM\u4e00\u6837, \u5b83\u4e00\u822c\u4e5f\u4ee5\u6587\u672c\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u4ee5\u8be5\u5e8f\u5217\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217\u4e3a\u8f93\u51fa. 2.2 CRF\u6a21\u578b\u7684\u4f5c\u7528 \u00b6 \u540cHMM\u4e00\u6837, \u5728NLP\u9886\u57df, CRF\u7528\u6765\u89e3\u51b3\u6587\u672c\u5e8f\u5217\u6807\u6ce8\u95ee\u9898. \u5982\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b. 2.3 CRF\u6a21\u578b\u4f7f\u7528\u8fc7\u7a0b\u7b80\u8ff0 \u00b6 \u9996\u5148, CRF\u6a21\u578b\u8868\u793a\u4e3a: lambda = CRF(w1, w2, ..., wn), \u5176\u4e2dw1\u5230wn\u662f\u6a21\u578b\u53c2\u6570. \u63a5\u7740, \u6211\u4eec\u5f00\u59cb\u8bad\u7ec3CRF\u6a21\u578b, \u8bed\u6599\u540c\u6837\u662f\u4e8b\u5148\u51c6\u5907\u597d\u7684\u4e00\u5b9a\u6570\u91cf\u7684\u89c2\u6d4b\u5e8f\u5217\u53ca\u5176\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217. \u4e0e\u6b64\u540c\u65f6\u6211\u4eec\u8fd8\u9700\u8981\u505a\u4eba\u5de5\u7279\u5f81\u5de5\u7a0b, \u7136\u540e\u901a\u8fc7\u4e0d\u65ad\u8bad\u7ec3\u6c42\u5f97\u4e00\u7ec4\u53c2\u6570, \u4f7f\u7531\u89c2\u6d4b\u5e8f\u5217\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6982\u7387\u6700\u5927. \u8bad\u7ec3\u540e, \u6211\u4eec\u5c31\u5f97\u5230\u4e86\u5177\u5907\u9884\u6d4b\u80fd\u529b\u7684\u65b0\u6a21\u578b: lambda = CRF(w1, w2, ..., wn), \u5176\u4e2d\u7684\u6a21\u578b\u53c2\u6570\u5df2\u7ecf\u6539\u53d8. \u4e4b\u540e\u7ed9\u5b9a\u8f93\u5165\u5e8f\u5217(x1, x2, ..., xn), \u7ecf\u8fc7\u6a21\u578b\u8ba1\u7b97lambda(x1, x2, ..., xn)\u5f97\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03. \u6700\u540e, \u8fd8\u662f\u4f7f\u7528\u7ef4\u7279\u6bd4\u7b97\u6cd5\u4ece\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\u4e2d\u627e\u51fa\u6982\u7387\u6700\u5927\u7684\u4e00\u6761\u5e8f\u5217\u8def\u5f84\u5c31\u662f\u6211\u4eec\u9700\u8981\u7684\u9690\u542b\u5e8f\u5217: (y1, y2, ..., yn). 3 HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u5dee\u5f02\u548c\u73b0\u72b6 \u00b6 3.1 HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u5dee\u5f02 \u00b6 HMM\u5c5e\u4e8e\u751f\u6210\u5f0f\u6a21\u578b\uff0cCRF\u5c5e\u4e8e\u5224\u522b\u5f0f\u6a21\u578b HMM\u6a21\u578b\u5b58\u5728\u9690\u9a6c\u5047\u8bbe, \u800cCRF\u4e0d\u5b58\u5728, \u56e0\u6b64HMM\u7684\u8ba1\u7b97\u901f\u5ea6\u8981\u6bd4CRF\u6a21\u578b\u5feb\u5f88\u591a, \u9002\u7528\u4e8e\u5bf9\u9884\u6d4b\u6027\u80fd\u8981\u6c42\u8f83\u9ad8\u7684\u573a\u5408. \u540c\u6837\u56e0\u4e3a\u9690\u9a6c\u5047\u8bbe, \u5f53\u9884\u6d4b\u95ee\u9898\u4e2d\u9690\u542b\u5e8f\u5217\u5355\u5143\u5e76\u4e0d\u662f\u53ea\u4e0e\u4e0a\u4e00\u4e2a\u5355\u5143\u6709\u5173\u65f6, HMM\u7684\u51c6\u786e\u7387\u4f1a\u5927\u5927\u964d\u4f4e, \u800cCRF\u4e0d\u53d7\u8fd9\u6837\u9650\u5236, \u51c6\u786e\u7387\u660e\u663e\u9ad8\u4e8eHMM. 3.2 HMM\u548cCRF\u7684\u53d1\u5c55\u73b0\u72b6 \u00b6 HMM\u548cCRF\u6a21\u578b\u66fe\u5728\u591a\u79cd\u5e8f\u5217\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272, \u4f34\u968fNLP\u5de5\u7a0b\u5e08\u5ea6\u8fc7\u6f2b\u957f\u7684\u4e00\u6bb5\u65f6\u671f. \u4f46\u7531\u4e8e\u8fd1\u5e74\u6765\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\u8fc5\u901f, \u7ecf\u5178\u5e8f\u5217\u6a21\u578b, \u5982HMM\u548cCRF, \u5df2\u7ecf\u5f00\u59cb\u6162\u6162\u6de1\u51fa\u4eba\u4eec\u7684\u89c6\u91ce. \u56e0\u6b64, \u6211\u4eec\u8fd9\u91cc\u4e5f\u662f\u5bf9\u5176\u505a\u4e86\u7b80\u6d01\u7684\u603b\u7ed3\u77e5\u8bc6, \u8ba9\u5927\u5bb6\u5bf9\u5176\u6709\u4e00\u5b9a\u7684\u57fa\u672c\u8ba4\u8bc6. 4 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa. \u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u7684\u4f5c\u7528. \u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u7684\u4f7f\u7528\u8fc7\u7a0b. \u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02. \u5b66\u4e60\u4e86HMM\u548cCRF\u7684\u53d1\u5c55\u73b0\u72b6.","title":"1 HMM\u548cCRF\u4ecb\u7ecd"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#_1","text":"\u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa. \u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u7684\u4f5c\u7528. \u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u7684\u4f7f\u7528\u8fc7\u7a0b. \u4e86\u89e3HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02. \u4e86\u89e3HMM\u548cCRF\u7684\u53d1\u5c55\u73b0\u72b6.","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#1-hmm","text":"","title":"1 HMM\u6a21\u578b\u4ecb\u7ecd"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#11-hmm","text":"HMM(Hidden Markov Model), \u4e2d\u6587\u79f0\u4f5c\u9690\u542b\u9a6c\u5c14\u79d1\u592b\u6a21\u578b, \u56e0\u4fc4\u56fd\u6570\u5b66\u5bb6\u9a6c\u5c14\u53ef\u592b\u800c\u5f97\u540d. \u5b83\u4e00\u822c\u4ee5\u6587\u672c\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u4ee5\u8be5\u5e8f\u5217\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217\u4e3a\u8f93\u51fa. \u4ec0\u4e48\u662f\u9690\u542b\u5e8f\u5217: * \u5e8f\u5217\u6570\u636e\u4e2d\u6bcf\u4e2a\u5355\u5143\u5305\u542b\u7684\u9690\u6027\u4fe1\u606f, \u8fd9\u4e9b\u9690\u6027\u4fe1\u606f\u4e4b\u95f4\u4e5f\u5b58\u5728\u4e00\u5b9a\u5173\u8054. \u4e3e\u4e2a\u4f8b\u5b50: \u7ed9\u5b9a\u4e00\u6bb5\u6587\u672c: \"\u4eba\u751f\u8be5\u5982\u4f55\u8d77\u5934\" \u6211\u4eec\u770b\u5230\u7684\u8fd9\u53e5\u8bdd\u53ef\u4ee5\u53eb\u505a: \u89c2\u6d4b\u5e8f\u5217 \u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u53e5\u8bdd\u4ee5\u8bcd\u4e3a\u5355\u4f4d\u8fdb\u884c\u5212\u5206\u5f97\u5230: [\"\u4eba\u751f\", \"\u8be5\", \"\u5982\u4f55\", \"\u8d77\u5934\"] \u90a3\u4e48\u6bcf\u4e2a\u8bcd\u5bf9\u5e94\u7684\u8bcd\u6027\u5c31\u662f\u5b83\u7684\u9690\u542b\u5e8f\u5217, \u5982: [\"n\", \"r\", \"r\", \"v\"]","title":"1.1 HMM\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#12-hmm","text":"\u5728NLP\u9886\u57df, HMM\u7528\u6765\u89e3\u51b3\u6587\u672c\u5e8f\u5217\u6807\u6ce8\u95ee\u9898. \u5982\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u90fd\u53ef\u4ee5\u770b\u4f5c\u662f\u5e8f\u5217\u6807\u6ce8\u95ee\u9898. HMM\u7684\u4e24\u5927\u5047\u8bbe: \u9f50\u6b21\u9a6c\u5c14\u79d1\u592b\u5047\u8bbe: \u4efb\u4e00\u65f6\u523b t \u7684\u67d0\u4e00\u72b6\u6001\u53ea\u4f9d\u8d56\u4e8e\u5176\u524d\u4e00\u65f6\u523b\u7684\u72b6\u6001, \u4e0e\u5176\u5b83\u65f6\u523b\u7684\u72b6\u6001\u53ca\u89c2\u6d4b\u65e0\u5173, \u4e5f\u4e0e\u65f6\u523b t \u65e0\u5173. \u89c2\u6d4b\u72ec\u7acb\u6027\u5047\u8bbe: \u4efb\u610f\u65f6\u523b\u7684\u89c2\u6d4b\u72b6\u6001\u53ea\u4f9d\u8d56\u4e8e\u8be5\u65f6\u523b\u7684\u9690\u85cf\u72b6\u6001, \u4e0e\u5176\u4ed6\u72b6\u6001\u65e0\u5173.","title":"1.2 HMM\u6a21\u578b\u7684\u4f5c\u7528"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#13-hmm","text":"\u9996\u5148, HMM\u6a21\u578b\u8868\u793a\u4e3a: lambda = HMM(A, B, pi), \u5176\u4e2dA, B, pi\u90fd\u662f\u6a21\u578b\u7684\u53c2\u6570, \u5206\u522b\u79f0\u4f5c: \u8f6c\u79fb\u6982\u7387\u77e9\u9635, \u53d1\u5c04\u6982\u7387\u77e9\u9635\u548c\u521d\u59cb\u6982\u7387\u77e9\u9635. \u63a5\u7740, \u6211\u4eec\u5f00\u59cb\u8bad\u7ec3HMM\u6a21\u578b, \u8bed\u6599\u5c31\u662f\u4e8b\u5148\u51c6\u5907\u597d\u7684\u4e00\u5b9a\u6570\u91cf\u7684\u89c2\u6d4b\u5e8f\u5217\u53ca\u5176\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217, \u901a\u8fc7\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u6c42\u5f97\u4e00\u7ec4\u53c2\u6570, \u4f7f\u7531\u89c2\u6d4b\u5e8f\u5217\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6982\u7387\u6700\u5927. \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u4e3a\u4e86\u7b80\u5316\u8ba1\u7b97, \u9a6c\u5c14\u53ef\u592b\u63d0\u51fa\u4e00\u79cd\u5047\u8bbe: \u9690\u542b\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5355\u5143\u7684\u53ef\u80fd\u6027\u53ea\u4e0e\u4e0a\u4e00\u4e2a\u5355\u5143\u6709\u5173. \u8fd9\u4e2a\u5047\u8bbe\u5c31\u662f\u8457\u540d\u7684\u9690\u542b\u5047\u8bbe. \u8bad\u7ec3\u540e, \u6211\u4eec\u5c31\u5f97\u5230\u4e86\u5177\u5907\u9884\u6d4b\u80fd\u529b\u7684\u65b0\u6a21\u578b: lambda = HMM(A, B, pi), \u5176\u4e2d\u7684\u6a21\u578b\u53c2\u6570\u5df2\u7ecf\u6539\u53d8. \u4e4b\u540e\u7ed9\u5b9a\u8f93\u5165\u5e8f\u5217(x1, x2, ..., xn), \u7ecf\u8fc7\u6a21\u578b\u8ba1\u7b97lambda(x1, x2, ..., xn)\u5f97\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03. \u6700\u540e, \u4f7f\u7528\u7ef4\u7279\u6bd4\u7b97\u6cd5\u4ece\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\u4e2d\u627e\u51fa\u6982\u7387\u6700\u5927\u7684\u4e00\u6761\u5e8f\u5217\u8def\u5f84\u5c31\u662f\u6211\u4eec\u9700\u8981\u7684\u9690\u542b\u5e8f\u5217: (y1, y2, ..., yn).","title":"1.3 HMM\u6a21\u578b\u4f7f\u7528\u8fc7\u7a0b\u7b80\u8ff0"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#14-hmm","text":"\u8bc4\u4f30\u95ee\u9898: \u5df2\u77e5\u6a21\u578b\u53c2\u6570, \u8ba1\u7b97\u67d0\u4e00\u7279\u5b9a\u8f93\u51fa\u5e8f\u5217\u7684\u6982\u7387. \u901a\u5e38\u4f7f\u7528\u524d\u5411\u6216\u8005\u540e\u5411\u7b97\u6cd5\u89e3\u51b3. \u89e3\u7801\u95ee\u9898: \u5df2\u77e5\u6a21\u578b\u53c2\u6570, \u6c42\u5bf9\u5e94\u7684\u6982\u7387\u6700\u5927\u7684\u9690\u542b\u72b6\u6001\u5e8f\u5217. \u901a\u5e38\u4f7f\u7528Viterbi\u7b97\u6cd5\u89e3\u51b3. \u5b66\u4e60\u95ee\u9898: \u5df2\u77e5\u89c2\u6d4b\u5e8f\u5217, \u6c42\u4f7f\u5f97\u8be5\u5e8f\u5217\u6982\u7387\u6700\u5927\u7684\u6a21\u578b\u53c2\u6570, \u901a\u5e38\u4f7f\u7528EM\u7b97\u6cd5\u6216\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u89e3\u51b3.","title":"1.4 HMM\u5e38\u89c1\u4e09\u4e2a\u95ee\u9898"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#2-crf","text":"","title":"2 CRF\u6a21\u578b\u4ecb\u7ecd"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#21-crf","text":"CRF(Conditional Random Fields), \u4e2d\u6587\u79f0\u4f5c\u6761\u4ef6\u968f\u673a\u573a, \u540cHMM\u4e00\u6837, \u5b83\u4e00\u822c\u4e5f\u4ee5\u6587\u672c\u5e8f\u5217\u6570\u636e\u4e3a\u8f93\u5165, \u4ee5\u8be5\u5e8f\u5217\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217\u4e3a\u8f93\u51fa.","title":"2.1 CRF\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#22-crf","text":"\u540cHMM\u4e00\u6837, \u5728NLP\u9886\u57df, CRF\u7528\u6765\u89e3\u51b3\u6587\u672c\u5e8f\u5217\u6807\u6ce8\u95ee\u9898. \u5982\u5206\u8bcd, \u8bcd\u6027\u6807\u6ce8, \u547d\u540d\u5b9e\u4f53\u8bc6\u522b.","title":"2.2 CRF\u6a21\u578b\u7684\u4f5c\u7528"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#23-crf","text":"\u9996\u5148, CRF\u6a21\u578b\u8868\u793a\u4e3a: lambda = CRF(w1, w2, ..., wn), \u5176\u4e2dw1\u5230wn\u662f\u6a21\u578b\u53c2\u6570. \u63a5\u7740, \u6211\u4eec\u5f00\u59cb\u8bad\u7ec3CRF\u6a21\u578b, \u8bed\u6599\u540c\u6837\u662f\u4e8b\u5148\u51c6\u5907\u597d\u7684\u4e00\u5b9a\u6570\u91cf\u7684\u89c2\u6d4b\u5e8f\u5217\u53ca\u5176\u5bf9\u5e94\u7684\u9690\u542b\u5e8f\u5217. \u4e0e\u6b64\u540c\u65f6\u6211\u4eec\u8fd8\u9700\u8981\u505a\u4eba\u5de5\u7279\u5f81\u5de5\u7a0b, \u7136\u540e\u901a\u8fc7\u4e0d\u65ad\u8bad\u7ec3\u6c42\u5f97\u4e00\u7ec4\u53c2\u6570, \u4f7f\u7531\u89c2\u6d4b\u5e8f\u5217\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6982\u7387\u6700\u5927. \u8bad\u7ec3\u540e, \u6211\u4eec\u5c31\u5f97\u5230\u4e86\u5177\u5907\u9884\u6d4b\u80fd\u529b\u7684\u65b0\u6a21\u578b: lambda = CRF(w1, w2, ..., wn), \u5176\u4e2d\u7684\u6a21\u578b\u53c2\u6570\u5df2\u7ecf\u6539\u53d8. \u4e4b\u540e\u7ed9\u5b9a\u8f93\u5165\u5e8f\u5217(x1, x2, ..., xn), \u7ecf\u8fc7\u6a21\u578b\u8ba1\u7b97lambda(x1, x2, ..., xn)\u5f97\u5230\u5bf9\u5e94\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03. \u6700\u540e, \u8fd8\u662f\u4f7f\u7528\u7ef4\u7279\u6bd4\u7b97\u6cd5\u4ece\u9690\u542b\u5e8f\u5217\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\u4e2d\u627e\u51fa\u6982\u7387\u6700\u5927\u7684\u4e00\u6761\u5e8f\u5217\u8def\u5f84\u5c31\u662f\u6211\u4eec\u9700\u8981\u7684\u9690\u542b\u5e8f\u5217: (y1, y2, ..., yn).","title":"2.3 CRF\u6a21\u578b\u4f7f\u7528\u8fc7\u7a0b\u7b80\u8ff0"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#3-hmmcrf","text":"","title":"3 HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u5dee\u5f02\u548c\u73b0\u72b6"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#31-hmmcrf","text":"HMM\u5c5e\u4e8e\u751f\u6210\u5f0f\u6a21\u578b\uff0cCRF\u5c5e\u4e8e\u5224\u522b\u5f0f\u6a21\u578b HMM\u6a21\u578b\u5b58\u5728\u9690\u9a6c\u5047\u8bbe, \u800cCRF\u4e0d\u5b58\u5728, \u56e0\u6b64HMM\u7684\u8ba1\u7b97\u901f\u5ea6\u8981\u6bd4CRF\u6a21\u578b\u5feb\u5f88\u591a, \u9002\u7528\u4e8e\u5bf9\u9884\u6d4b\u6027\u80fd\u8981\u6c42\u8f83\u9ad8\u7684\u573a\u5408. \u540c\u6837\u56e0\u4e3a\u9690\u9a6c\u5047\u8bbe, \u5f53\u9884\u6d4b\u95ee\u9898\u4e2d\u9690\u542b\u5e8f\u5217\u5355\u5143\u5e76\u4e0d\u662f\u53ea\u4e0e\u4e0a\u4e00\u4e2a\u5355\u5143\u6709\u5173\u65f6, HMM\u7684\u51c6\u786e\u7387\u4f1a\u5927\u5927\u964d\u4f4e, \u800cCRF\u4e0d\u53d7\u8fd9\u6837\u9650\u5236, \u51c6\u786e\u7387\u660e\u663e\u9ad8\u4e8eHMM.","title":"3.1 HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u5dee\u5f02"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#32-hmmcrf","text":"HMM\u548cCRF\u6a21\u578b\u66fe\u5728\u591a\u79cd\u5e8f\u5217\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272, \u4f34\u968fNLP\u5de5\u7a0b\u5e08\u5ea6\u8fc7\u6f2b\u957f\u7684\u4e00\u6bb5\u65f6\u671f. \u4f46\u7531\u4e8e\u8fd1\u5e74\u6765\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\u8fc5\u901f, \u7ecf\u5178\u5e8f\u5217\u6a21\u578b, \u5982HMM\u548cCRF, \u5df2\u7ecf\u5f00\u59cb\u6162\u6162\u6de1\u51fa\u4eba\u4eec\u7684\u89c6\u91ce. \u56e0\u6b64, \u6211\u4eec\u8fd9\u91cc\u4e5f\u662f\u5bf9\u5176\u505a\u4e86\u7b80\u6d01\u7684\u603b\u7ed3\u77e5\u8bc6, \u8ba9\u5927\u5bb6\u5bf9\u5176\u6709\u4e00\u5b9a\u7684\u57fa\u672c\u8ba4\u8bc6.","title":"3.2 HMM\u548cCRF\u7684\u53d1\u5c55\u73b0\u72b6"},{"location":"07_mkdocs_HMM_CRF/1%20HMM%E5%92%8CCRF%E4%BB%8B%E7%BB%8D.html#4","text":"\u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa. \u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u7684\u4f5c\u7528. \u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u7684\u4f7f\u7528\u8fc7\u7a0b. \u5b66\u4e60\u4e86HMM\u4e0eCRF\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02. \u5b66\u4e60\u4e86HMM\u548cCRF\u7684\u53d1\u5c55\u73b0\u72b6.","title":"4 \u5c0f\u7ed3"},{"location":"07_mkdocs_review_model/index.html","text":"","title":"Index"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1\u5176\u4ed6\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u601d\u8003\u9898\uff1aTransformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528? 1 Encoder\u6a21\u5757 \u00b6 1.1 Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u00b6 \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42. 1.2 \u5173\u4e8eEncoder Block \u00b6 \u5728Transformer\u67b6\u6784\u4e2d, 6\u4e2a\u4e00\u6a21\u4e00\u6837\u7684Encoder Block\u5c42\u5c42\u5806\u53e0\u5728\u4e00\u8d77, \u5171\u540c\u7ec4\u6210\u5b8c\u6574\u7684Encoder, \u56e0\u6b64\u5256\u6790\u4e00\u4e2aBlock\u5c31\u53ef\u4ee5\u5bf9\u6574\u4e2aEncoder\u7684\u5185\u90e8\u7ed3\u6784\u6709\u6e05\u6670\u7684\u8ba4\u8bc6. 1.3 \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42(self-attention) \u00b6 \u9996\u5148\u6765\u770bself-attention\u7684\u8ba1\u7b97\u89c4\u5219\u56fe: \u4e0a\u8ff0attention\u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u5c06query\u548ckey-value\u952e\u503c\u5bf9\u7684\u4e00\u7ec4\u96c6\u5408\u6620\u5c04\u5230\u8f93\u51fa, \u8f93\u51fa\u88ab\u8ba1\u7b97\u4e3avalues\u7684\u52a0\u6743\u548c, \u5176\u4e2d\u5206\u914d\u7ed9\u6bcf\u4e2avalue\u7684\u6743\u91cd\u7531query\u4e0e\u5bf9\u5e94key\u7684\u76f8\u4f3c\u6027\u51fd\u6570\u8ba1\u7b97\u5f97\u6765. \u8fd9\u79cdattention\u7684\u5f62\u5f0f\u88ab\u79f0\u4e3aScaled Dot-Product Attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V \u6240\u8c13\u7684\u591a\u5934self-attention\u5c42, \u5219\u662f\u5148\u5c06Q, K, V\u7ecf\u8fc7\u53c2\u6570\u77e9\u9635\u8fdb\u884c\u6620\u5c04, \u518d\u505aself-attention, \u6700\u540e\u5c06\u7ed3\u679c\u62fc\u63a5\u8d77\u6765\u9001\u5165\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u5373\u53ef. \u4e0a\u8ff0\u7684\u591a\u5934self-attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: \u591a\u5934self-attention\u5c42\u7684\u4f5c\u7528: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Multi-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u603b\u4f53\u8ba1\u7b97\u91cf\u548c\u5355\u4e00head\u76f8\u540c\u7684\u60c5\u51b5\u4e0b, \u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757, \u7531\u4e24\u4e2a\u7ebf\u6027\u53d8\u6362\u7ec4\u6210, \u4e2d\u95f4\u6709\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: $$ FFN(x)=\\max(0,xW_1+b_1)W_2+b_2 $$ \u6ce8\u610f: \u539f\u7248\u8bba\u6587\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ef4\u5ea6\u5747\u4e3ad_model = 512, \u5c42\u5185\u7684\u8fde\u63a5\u7ef4\u5ea6d_ff = 2048, \u5747\u91c7\u75284\u500d\u7684\u5927\u5c0f\u5173\u7cfb. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u5355\u7eaf\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5e76\u4e0d\u8db3\u4ee5\u63d0\u53d6\u5230\u7406\u60f3\u7684\u7279\u5f81, \u56e0\u6b64\u589e\u52a0\u5168\u8fde\u63a5\u5c42\u6765\u63d0\u5347\u7f51\u7edc\u7684\u80fd\u529b. 1.4 Decoder\u6a21\u5757 \u00b6 Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b\u4e09\u4e2a\u5b50\u5c42. \u4e00\u4e2a\u591a\u5934self-attention\u5c42 \u4e00\u4e2aEncoder-Decoder attention\u5c42 \u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42 Decoder Block\u4e2d\u7684\u591a\u5934self-attention\u5c42 Decoder\u4e2d\u7684\u591a\u5934self-attention\u5c42\u4e0eEncoder\u6a21\u5757\u4e00\u81f4, \u4f46\u9700\u8981\u6ce8\u610f\u7684\u662fDecoder\u6a21\u5757\u7684\u591a\u5934self-attention\u9700\u8981\u505alook-ahead-mask, \u56e0\u4e3a\u5728\u9884\u6d4b\u7684\u65f6\u5019\"\u4e0d\u80fd\u770b\u89c1\u672a\u6765\u7684\u4fe1\u606f\", \u6240\u4ee5\u8981\u5c06\u5f53\u524d\u7684token\u548c\u4e4b\u540e\u7684token\u5168\u90e8mask. Decoder Block\u4e2d\u7684Encoder-Decoder attention\u5c42 \u8fd9\u4e00\u5c42\u533a\u522b\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684Q = K = V, \u6b64\u5904\u77e9\u9635Q\u6765\u6e90\u4e8eDecoder\u7aef\u7ecf\u8fc7\u4e0a\u4e00\u4e2aDecoder Block\u7684\u8f93\u51fa, \u800c\u77e9\u9635K, V\u5219\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa, \u9020\u6210\u4e86Q != K = V\u7684\u60c5\u51b5. \u8fd9\u6837\u8bbe\u8ba1\u662f\u4e3a\u4e86\u8ba9Decoder\u7aef\u7684token\u80fd\u591f\u7ed9\u4e88Encoder\u7aef\u5bf9\u5e94\u7684token\u66f4\u591a\u7684\u5173\u6ce8. Decoder Block\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6b64\u5904\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u6a21\u5757\u4e2d\u7684\u5b8c\u5168\u4e00\u6837. Decoder Block\u4e2d\u67092\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u4f5c\u7528: \u591a\u5934self-attention\u5c42\u662f\u4e3a\u4e86\u62df\u5408Decoder\u7aef\u81ea\u8eab\u7684\u4fe1\u606f, \u800cEncoder-Decoder attention\u5c42\u662f\u4e3a\u4e86\u6574\u5408Encoder\u548cDecoder\u7684\u4fe1\u606f. 1.5 Add & Norm\u6a21\u5757 \u00b6 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5177\u4f53\u6765\u8bf4Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, Norm\u8868\u793aLayerNorm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5177\u4f53\u7684\u6570\u5b66\u8868\u8fbe\u5f62\u5f0f\u4e3a: LayerNorm(x + Sublayer(x)), \u5176\u4e2dSublayer(x)\u4e3a\u5b50\u5c42\u7684\u8f93\u51fa. Add\u6b8b\u5dee\u8fde\u63a5\u7684\u4f5c\u7528: \u548c\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u8fde\u63a5\u4f5c\u7528\u4e00\u81f4, \u90fd\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u4f20\u9012\u7684\u66f4\u6df1, \u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. \u8bd5\u9a8c\u8868\u660e\u6b8b\u5dee\u8fde\u63a5\u7684\u786e\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u73b0. Norm\u7684\u4f5c\u7528: \u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u989d\u589e\u52a0, \u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u5927, \u8fc7\u5c0f, \u65b9\u5dee\u53d8\u5927\u7b49\u73b0\u8c61, \u8fd9\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38, \u6a21\u578b\u7684\u6536\u655b\u975e\u5e38\u6162. \u56e0\u6b64\u5bf9\u6bcf\u4e00\u5c42\u8ba1\u7b97\u540e\u7684\u6570\u503c\u8fdb\u884c\u89c4\u8303\u5316\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u8868\u73b0. 1.6 \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding \u00b6 Transformer\u4e2d\u76f4\u63a5\u91c7\u7528\u6b63\u5f26\u51fd\u6570\u548c\u4f59\u5f26\u51fd\u6570\u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f, \u5982\u4e0b\u56fe\u6240\u793a: $$ PE_{(pos, 2i)}=\\sin(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ PE_{(pos, 2i+1)}=\\cos(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ $$ \u9700\u8981\u6ce8\u610f: \u4e09\u89d2\u51fd\u6570\u5e94\u7528\u5728\u6b64\u5904\u7684\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u70b9, \u56e0\u4e3a\u5bf9\u4e8e\u4efb\u610f\u7684PE(pos+k), \u90fd\u53ef\u4ee5\u8868\u793a\u4e3aPE(pos)\u7684\u7ebf\u6027\u51fd\u6570, \u5927\u5927\u65b9\u4fbf\u8ba1\u7b97. \u800c\u4e14\u5468\u671f\u6027\u51fd\u6570\u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u4e5f\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b. $$ \\sin(\\alpha+\\beta)=\\sin(\\alpha)\\cos(\\beta)+\\cos(\\alpha)\\sin(\\beta)\\\\ \\cos(\\alpha+\\beta)=\\cos(\\alpha)\\cos(\\beta)-\\sin(\\alpha)\\sin(\\beta) $$ 2 \u5c0f\u7ed3 \u00b6 Encoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e24\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u7684\u662f\u4e00\u79cdScaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Mul ti-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u6bd4\u5355\u4e00head\u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u662f\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u7ec4\u6210, \u7ebf\u6027\u53d8\u6362\u4e2d\u95f4\u589e\u6dfb\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5177\u4f53\u7684\u7ef4\u5ea6\u91c7\u75284\u500d\u5173\u7cfb, \u5373\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684d_model=512, \u5219\u5c42\u5185\u7684\u53d8\u6362\u7ef4\u5ea6d_ff=2048. Decoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b3\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, Encoder-Decoder Attention\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u548cEncoder\u6a21\u5757\u4e00\u6837\u7684Scaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u6700\u5927\u7684 \u533a\u522b\u5728\u4e8e\u9700\u8981\u6dfb\u52a0look-ahead-mask, \u5373\u906e\u63a9\"\u672a\u6765\u7684\u4fe1\u606f\". Encoder-Decoder Attention\u5c42\u548c\u4e0a\u4e00\u5c42\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u6700\u4e3b\u8981\u7684\u533a\u522b\u5728\u4e8eQ != K = V, \u77e9\u9635Q\u6765\u6e90\u4e8e\u4e0a\u4e00\u5c42Decoder Block\u7684\u8f93\u51fa, \u540c\u65f6K, V\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u4e2d\u5b8c\u5168\u4e00\u6837. Add & Norm\u6a21\u5757 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, \u4f5c\u7528\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u65e0\u635f\u8017\u7684\u4f20\u9012\u7684\u66f4\u6df1, \u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. Norm\u8868\u793aLayerNorm, \u5c42\u7ea7\u522b\u7684\u6570\u503c\u6807\u51c6\u5316\u64cd\u4f5c, \u4f5c\u7528\u662f\u9632\u6b62\u53c2\u6570\u8fc7\u5927\u8fc7\u5c0f\u5bfc\u81f4\u7684\u5b66\u4e60\u8fc7\u7a0b\u5f02\u5e38, \u6a21\u578b\u6536\u655b\u7279\u522b\u6162\u7684\u95ee\u9898. \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding Transformer\u4e2d\u91c7\u7528\u4e09\u89d2\u51fd\u6570\u6765\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801. \u56e0\u4e3a\u4e09\u89d2\u51fd\u6570\u662f\u5468\u671f\u6027\u51fd\u6570, \u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u800c\u4e14\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u53ef\u4ee5\u5bf9\u5e8f\u5217\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u7f16\u7801\u7684\u91cd\u8981\u7a0b\u5ea6\u540c\u7b49\u770b\u5f85. \u672c\u8282\u5e38\u89c1\u95ee\u7b54 Transformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"1 Transformer \u5404\u5b50\u6a21\u5757\u4f5c\u7528"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#_1","text":"\u638c\u63e1Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u638c\u63e1\u5176\u4ed6\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528 \u601d\u8003\u9898\uff1aTransformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#1-encoder","text":"","title":"1 Encoder\u6a21\u5757"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#11-encoder","text":"\u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e00\u4e2a\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42.","title":"1.1 Encoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528:"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#12-encoder-block","text":"\u5728Transformer\u67b6\u6784\u4e2d, 6\u4e2a\u4e00\u6a21\u4e00\u6837\u7684Encoder Block\u5c42\u5c42\u5806\u53e0\u5728\u4e00\u8d77, \u5171\u540c\u7ec4\u6210\u5b8c\u6574\u7684Encoder, \u56e0\u6b64\u5256\u6790\u4e00\u4e2aBlock\u5c31\u53ef\u4ee5\u5bf9\u6574\u4e2aEncoder\u7684\u5185\u90e8\u7ed3\u6784\u6709\u6e05\u6670\u7684\u8ba4\u8bc6.","title":"1.2 \u5173\u4e8eEncoder Block"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#13-self-attention","text":"\u9996\u5148\u6765\u770bself-attention\u7684\u8ba1\u7b97\u89c4\u5219\u56fe: \u4e0a\u8ff0attention\u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u5c06query\u548ckey-value\u952e\u503c\u5bf9\u7684\u4e00\u7ec4\u96c6\u5408\u6620\u5c04\u5230\u8f93\u51fa, \u8f93\u51fa\u88ab\u8ba1\u7b97\u4e3avalues\u7684\u52a0\u6743\u548c, \u5176\u4e2d\u5206\u914d\u7ed9\u6bcf\u4e2avalue\u7684\u6743\u91cd\u7531query\u4e0e\u5bf9\u5e94key\u7684\u76f8\u4f3c\u6027\u51fd\u6570\u8ba1\u7b97\u5f97\u6765. \u8fd9\u79cdattention\u7684\u5f62\u5f0f\u88ab\u79f0\u4e3aScaled Dot-Product Attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V Attention(Q,K,V)=Softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_{k}}})\\cdot V \u6240\u8c13\u7684\u591a\u5934self-attention\u5c42, \u5219\u662f\u5148\u5c06Q, K, V\u7ecf\u8fc7\u53c2\u6570\u77e9\u9635\u8fdb\u884c\u6620\u5c04, \u518d\u505aself-attention, \u6700\u540e\u5c06\u7ed3\u679c\u62fc\u63a5\u8d77\u6765\u9001\u5165\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u5373\u53ef. \u4e0a\u8ff0\u7684\u591a\u5934self-attention, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: \u591a\u5934self-attention\u5c42\u7684\u4f5c\u7528: \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Multi-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u603b\u4f53\u8ba1\u7b97\u91cf\u548c\u5355\u4e00head\u76f8\u540c\u7684\u60c5\u51b5\u4e0b, \u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757 \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757, \u7531\u4e24\u4e2a\u7ebf\u6027\u53d8\u6362\u7ec4\u6210, \u4e2d\u95f4\u6709\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5bf9\u5e94\u7684\u6570\u5b66\u516c\u5f0f\u5f62\u5f0f\u5982\u4e0b: $$ FFN(x)=\\max(0,xW_1+b_1)W_2+b_2 $$ \u6ce8\u610f: \u539f\u7248\u8bba\u6587\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u8f93\u5165\u548c\u8f93\u51fa\u7684\u7ef4\u5ea6\u5747\u4e3ad_model = 512, \u5c42\u5185\u7684\u8fde\u63a5\u7ef4\u5ea6d_ff = 2048, \u5747\u91c7\u75284\u500d\u7684\u5927\u5c0f\u5173\u7cfb. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u7684\u4f5c\u7528: \u5355\u7eaf\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5e76\u4e0d\u8db3\u4ee5\u63d0\u53d6\u5230\u7406\u60f3\u7684\u7279\u5f81, \u56e0\u6b64\u589e\u52a0\u5168\u8fde\u63a5\u5c42\u6765\u63d0\u5347\u7f51\u7edc\u7684\u80fd\u529b.","title":"1.3 \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42(self-attention)"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#14-decoder","text":"Decoder\u6a21\u5757\u7684\u7ed3\u6784\u548c\u4f5c\u7528: \u7ecf\u5178\u7684Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b\u4e09\u4e2a\u5b50\u5c42. \u4e00\u4e2a\u591a\u5934self-attention\u5c42 \u4e00\u4e2aEncoder-Decoder attention\u5c42 \u4e00\u4e2a\u524d\u9988\u5168\u8fde\u63a5\u5c42 Decoder Block\u4e2d\u7684\u591a\u5934self-attention\u5c42 Decoder\u4e2d\u7684\u591a\u5934self-attention\u5c42\u4e0eEncoder\u6a21\u5757\u4e00\u81f4, \u4f46\u9700\u8981\u6ce8\u610f\u7684\u662fDecoder\u6a21\u5757\u7684\u591a\u5934self-attention\u9700\u8981\u505alook-ahead-mask, \u56e0\u4e3a\u5728\u9884\u6d4b\u7684\u65f6\u5019\"\u4e0d\u80fd\u770b\u89c1\u672a\u6765\u7684\u4fe1\u606f\", \u6240\u4ee5\u8981\u5c06\u5f53\u524d\u7684token\u548c\u4e4b\u540e\u7684token\u5168\u90e8mask. Decoder Block\u4e2d\u7684Encoder-Decoder attention\u5c42 \u8fd9\u4e00\u5c42\u533a\u522b\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684Q = K = V, \u6b64\u5904\u77e9\u9635Q\u6765\u6e90\u4e8eDecoder\u7aef\u7ecf\u8fc7\u4e0a\u4e00\u4e2aDecoder Block\u7684\u8f93\u51fa, \u800c\u77e9\u9635K, V\u5219\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa, \u9020\u6210\u4e86Q != K = V\u7684\u60c5\u51b5. \u8fd9\u6837\u8bbe\u8ba1\u662f\u4e3a\u4e86\u8ba9Decoder\u7aef\u7684token\u80fd\u591f\u7ed9\u4e88Encoder\u7aef\u5bf9\u5e94\u7684token\u66f4\u591a\u7684\u5173\u6ce8. Decoder Block\u4e2d\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42 \u6b64\u5904\u7684\u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u6a21\u5757\u4e2d\u7684\u5b8c\u5168\u4e00\u6837. Decoder Block\u4e2d\u67092\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u4f5c\u7528: \u591a\u5934self-attention\u5c42\u662f\u4e3a\u4e86\u62df\u5408Decoder\u7aef\u81ea\u8eab\u7684\u4fe1\u606f, \u800cEncoder-Decoder attention\u5c42\u662f\u4e3a\u4e86\u6574\u5408Encoder\u548cDecoder\u7684\u4fe1\u606f.","title":"1.4 Decoder\u6a21\u5757"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#15-add-norm","text":"Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5177\u4f53\u6765\u8bf4Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, Norm\u8868\u793aLayerNorm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5177\u4f53\u7684\u6570\u5b66\u8868\u8fbe\u5f62\u5f0f\u4e3a: LayerNorm(x + Sublayer(x)), \u5176\u4e2dSublayer(x)\u4e3a\u5b50\u5c42\u7684\u8f93\u51fa. Add\u6b8b\u5dee\u8fde\u63a5\u7684\u4f5c\u7528: \u548c\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u6b8b\u5dee\u8fde\u63a5\u4f5c\u7528\u4e00\u81f4, \u90fd\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u4f20\u9012\u7684\u66f4\u6df1, \u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. \u8bd5\u9a8c\u8868\u660e\u6b8b\u5dee\u8fde\u63a5\u7684\u786e\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8868\u73b0. Norm\u7684\u4f5c\u7528: \u968f\u7740\u7f51\u7edc\u5c42\u6570\u7684\u989d\u589e\u52a0, \u901a\u8fc7\u591a\u5c42\u7684\u8ba1\u7b97\u540e\u53c2\u6570\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fc7\u5927, \u8fc7\u5c0f, \u65b9\u5dee\u53d8\u5927\u7b49\u73b0\u8c61, \u8fd9\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8fc7\u7a0b\u51fa\u73b0\u5f02\u5e38, \u6a21\u578b\u7684\u6536\u655b\u975e\u5e38\u6162. \u56e0\u6b64\u5bf9\u6bcf\u4e00\u5c42\u8ba1\u7b97\u540e\u7684\u6570\u503c\u8fdb\u884c\u89c4\u8303\u5316\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u8868\u73b0.","title":"1.5 Add &amp; Norm\u6a21\u5757"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#16-positional-encoding","text":"Transformer\u4e2d\u76f4\u63a5\u91c7\u7528\u6b63\u5f26\u51fd\u6570\u548c\u4f59\u5f26\u51fd\u6570\u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f, \u5982\u4e0b\u56fe\u6240\u793a: $$ PE_{(pos, 2i)}=\\sin(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ PE_{(pos, 2i+1)}=\\cos(\\frac{pos} {10000^{\\frac{2i}{d_{model}}}})\\\\ $$ \u9700\u8981\u6ce8\u610f: \u4e09\u89d2\u51fd\u6570\u5e94\u7528\u5728\u6b64\u5904\u7684\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u70b9, \u56e0\u4e3a\u5bf9\u4e8e\u4efb\u610f\u7684PE(pos+k), \u90fd\u53ef\u4ee5\u8868\u793a\u4e3aPE(pos)\u7684\u7ebf\u6027\u51fd\u6570, \u5927\u5927\u65b9\u4fbf\u8ba1\u7b97. \u800c\u4e14\u5468\u671f\u6027\u51fd\u6570\u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u4e5f\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b. $$ \\sin(\\alpha+\\beta)=\\sin(\\alpha)\\cos(\\beta)+\\cos(\\alpha)\\sin(\\beta)\\\\ \\cos(\\alpha+\\beta)=\\cos(\\alpha)\\cos(\\beta)-\\sin(\\alpha)\\sin(\\beta) $$","title":"1.6 \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding"},{"location":"07_mkdocs_review_model/1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html#2","text":"Encoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Encoder\u6a21\u5757\u5305\u542b6\u4e2aEncoder Block. \u6bcf\u4e2aEncoder Block\u5305\u542b\u4e24\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u7684\u662f\u4e00\u79cdScaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, Mul ti-head\u53ef\u4ee5\u5728\u66f4\u7ec6\u81f4\u7684\u5c42\u9762\u4e0a\u63d0\u53d6\u4e0d\u540chead\u7684\u7279\u5f81, \u6bd4\u5355\u4e00head\u63d0\u53d6\u7279\u5f81\u7684\u6548\u679c\u66f4\u4f73. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u662f\u7531\u4e24\u4e2a\u5168\u8fde\u63a5\u5c42\u7ec4\u6210, \u7ebf\u6027\u53d8\u6362\u4e2d\u95f4\u589e\u6dfb\u4e00\u4e2aRelu\u6fc0\u6d3b\u51fd\u6570, \u5177\u4f53\u7684\u7ef4\u5ea6\u91c7\u75284\u500d\u5173\u7cfb, \u5373\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684d_model=512, \u5219\u5c42\u5185\u7684\u53d8\u6362\u7ef4\u5ea6d_ff=2048. Decoder\u6a21\u5757 \u7ecf\u5178\u7684Transformer\u67b6\u6784\u4e2d\u7684Decoder\u6a21\u5757\u5305\u542b6\u4e2aDecoder Block. \u6bcf\u4e2aDecoder Block\u5305\u542b3\u4e2a\u5b50\u6a21\u5757, \u5206\u522b\u662f\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42, Encoder-Decoder Attention\u5c42, \u548c\u524d\u9988\u5168\u8fde\u63a5\u5c42. \u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u91c7\u7528\u548cEncoder\u6a21\u5757\u4e00\u6837\u7684Scaled Dot-Product Attention\u7684\u8ba1\u7b97\u65b9\u5f0f, \u6700\u5927\u7684 \u533a\u522b\u5728\u4e8e\u9700\u8981\u6dfb\u52a0look-ahead-mask, \u5373\u906e\u63a9\"\u672a\u6765\u7684\u4fe1\u606f\". Encoder-Decoder Attention\u5c42\u548c\u4e0a\u4e00\u5c42\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u6700\u4e3b\u8981\u7684\u533a\u522b\u5728\u4e8eQ != K = V, \u77e9\u9635Q\u6765\u6e90\u4e8e\u4e0a\u4e00\u5c42Decoder Block\u7684\u8f93\u51fa, \u540c\u65f6K, V\u6765\u6e90\u4e8eEncoder\u7aef\u7684\u8f93\u51fa. \u524d\u9988\u5168\u8fde\u63a5\u5c42\u548cEncoder\u4e2d\u5b8c\u5168\u4e00\u6837. Add & Norm\u6a21\u5757 Add & Norm\u6a21\u5757\u63a5\u5728\u6bcf\u4e00\u4e2aEncoder Block\u548cDecoder Block\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5b50\u5c42\u7684\u540e\u9762. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aEncoder Block, \u91cc\u9762\u7684\u4e24\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. \u5bf9\u4e8e\u6bcf\u4e00\u4e2aDecoder Block, \u91cc\u9762\u7684\u4e09\u4e2a\u5b50\u5c42\u540e\u9762\u90fd\u6709Add & Norm. Add\u8868\u793a\u6b8b\u5dee\u8fde\u63a5, \u4f5c\u7528\u662f\u4e3a\u4e86\u5c06\u4fe1\u606f\u65e0\u635f\u8017\u7684\u4f20\u9012\u7684\u66f4\u6df1, \u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b. Norm\u8868\u793aLayerNorm, \u5c42\u7ea7\u522b\u7684\u6570\u503c\u6807\u51c6\u5316\u64cd\u4f5c, \u4f5c\u7528\u662f\u9632\u6b62\u53c2\u6570\u8fc7\u5927\u8fc7\u5c0f\u5bfc\u81f4\u7684\u5b66\u4e60\u8fc7\u7a0b\u5f02\u5e38, \u6a21\u578b\u6536\u655b\u7279\u522b\u6162\u7684\u95ee\u9898. \u4f4d\u7f6e\u7f16\u7801\u5668Positional Encoding Transformer\u4e2d\u91c7\u7528\u4e09\u89d2\u51fd\u6570\u6765\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801. \u56e0\u4e3a\u4e09\u89d2\u51fd\u6570\u662f\u5468\u671f\u6027\u51fd\u6570, \u4e0d\u53d7\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236, \u800c\u4e14\u8fd9\u79cd\u8ba1\u7b97\u65b9\u5f0f\u53ef\u4ee5\u5bf9\u5e8f\u5217\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u7f16\u7801\u7684\u91cd\u8981\u7a0b\u5ea6\u540c\u7b49\u770b\u5f85. \u672c\u8282\u5e38\u89c1\u95ee\u7b54 Transformer\u7684\u7ed3\u6784\u662f\u4ec0\u4e48\u6837\u7684? \u5404\u4e2a\u5b50\u6a21\u5757\u5404\u6709\u4ec0\u4e48\u4f5c\u7528?","title":"2 \u5c0f\u7ed3"},{"location":"07_mkdocs_review_model/2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u7684\u8f93\u5165\u5f20\u91cf\u7279\u70b9\u548c\u542b\u4e49. \u638c\u63e1Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u638c\u63e1Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u601d\u8003\u9898\uff1aTransformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u5177\u4f53\u8f93\u5165\u662f\u4ec0\u4e48? \u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u9884\u6d4b\u9636\u6bb5\u4e00\u81f4\u5417? 1 Decoder\u7aef\u7684\u8f93\u5165\u89e3\u6790 \u00b6 1.1 Decoder\u7aef\u7684\u67b6\u6784 \u00b6 Transformer\u539f\u59cb\u8bba\u6587\u4e2d\u7684Decoder\u6a21\u5757\u662f\u7531N=6\u4e2a\u76f8\u540c\u7684Decoder Block\u5806\u53e0\u800c\u6210, \u5176\u4e2d\u6bcf\u4e00\u4e2aBlock\u662f\u75313\u4e2a\u5b50\u6a21\u5757\u6784\u6210, \u5206\u522b\u662f\u591a\u5934self-attention\u6a21\u5757, Encoder-Decoder attention\u6a21\u5757, \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757. 6\u4e2aBlock\u7684\u8f93\u5165\u4e0d\u5b8c\u5168\u76f8\u540c: \u6700\u4e0b\u9762\u7684\u4e00\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u662f\u7ecf\u5386\u4e86MASK\u4e4b\u540e\u7684Decoder\u7aef\u7684\u8f93\u5165 + Encoder\u7aef\u7684\u8f93\u51fa. \u5176\u4ed65\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u90fd\u662f\u524d\u4e00\u5c42Block\u7684\u8f93\u51fa + Encoder\u7aef\u7684\u8f93\u51fa. 1.2 Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790 \u00b6 \u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u8bf4, \u5047\u8bbe\u73b0\u5728\u7684\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u7b49\u4e8e\"How are you?\", \u5f53time step=1\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\u4e00\u4e2a\u7279\u6b8a\u7684token, \u6bd4\u5982\"SOS\"; \u5f53time step=2\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How\"; \u5f53time step=3\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How are\", \u4ee5\u6b64\u7c7b\u63a8... \u6ce8\u610f: \u5728\u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u8bad\u7ec3\u9636\u6bb5\u4e0d\u4f1a\u8fd9\u6837\u52a8\u6001\u8f93\u5165, \u800c\u662f\u4e00\u6b21\u6027\u7684\u628a\u76ee\u6807\u5e8f\u5217\u5168\u90e8\u8f93\u5165\u7ed9\u7b2c\u4e00\u5c42\u7684Block, \u7136\u540e\u901a\u8fc7\u591a\u5934self-attention\u4e2d\u7684MASK\u673a\u5236\u5bf9\u5e8f\u5217\u8fdb\u884c\u540c\u6837\u7684\u906e\u63a9\u5373\u53ef. 1.3 Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790 \u00b6 \u540c\u7406\u4e8e\u8bad\u7ec3\u9636\u6bb5, \u9884\u6d4b\u65f6\u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u56e0\u4e3a\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u90fd\u4f1a\u6709Encoder\u7684\u8f93\u51fa\u5f20\u91cf, \u56e0\u6b64\u8fd9\u91cc\u4e0d\u505a\u7279\u6b8a\u8ba8\u8bba, \u53ea\u4e13\u6ce8\u4e8e\u7eaf\u7cb9\u4eceDecoder\u7aef\u63a5\u6536\u7684\u8f93\u5165. \u9884\u6d4b\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0, input_tensor=\"SOS\"\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u8f93\u51fa\u7684\u7d2f\u8ba1\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u8bf4: \u5f53time step=1\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"What\"; \u5f53time step=2\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"is\"; \u5f53time step=3\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"the\"; \u5f53time step=4\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"matter\"; \u5f53time step=5\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"?\"; \u5f53time step=6\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter ?\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"EOS\", \u4ee3\u8868\u53e5\u5b50\u7684\u7ed3\u675f\u7b26, \u8bf4\u660e\u89e3\u7801\u7ed3\u675f, \u9884\u6d4b\u7ed3\u675f. 2 \u5c0f\u7ed3 \u00b6 \u5728Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u7684\u8f93\u5165, \u533a\u5206\u4e8e\u4e0d\u540c\u7684Block, \u6700\u5e95\u5c42\u7684Block\u8f93\u5165\u6709\u5176\u7279\u6b8a\u7684\u5730\u65b9. \u7b2c\u4e8c\u5c42\u5230\u7b2c\u516d\u5c42\u7684\u8f93\u5165\u4e00\u81f4, \u90fd\u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u548cEncoder\u7684\u8f93\u51fa. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u770b, \u5c31\u662f\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f, \u4e0d\u65ad\u7684\u5c06\u4e4b\u524d\u7684\u8f93\u5165\u878d\u5408\u8fdb\u6765. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u91c7\u7528\u7684\u662fMASK\u673a\u5236\u6765\u6a21\u62df\u8f93\u5165\u5e8f\u5217\u4e0d\u65ad\u6dfb\u52a0\u7684\u8fc7\u7a0b. \u6700\u5e95\u5c42\u7684Block\u5728\u9884\u6d4b\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c\u7684\u7d2f\u79ef\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u770b, \u4e5f\u662f\u968f\u7740\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f. \u76f8\u6bd4\u4e8e\u8bad\u7ec3\u9636\u6bb5\u6700\u5927\u7684\u4e0d\u540c\u662f\u8fd9\u91cc\u4e0d\u65ad\u62fc\u63a5\u8fdb\u6765\u7684token\u662f\u6bcf\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c, \u800c\u4e0d\u662f\u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u53d6\u5f97\u7684groud truth\u503c.","title":"2 Transformer Decoder\u6a21\u5757"},{"location":"07_mkdocs_review_model/2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#_1","text":"\u638c\u63e1Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u7684\u8f93\u5165\u5f20\u91cf\u7279\u70b9\u548c\u542b\u4e49. \u638c\u63e1Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u638c\u63e1Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u662f\u4ec0\u4e48. \u601d\u8003\u9898\uff1aTransformer\u7ed3\u6784\u4e2d\u7684Decoder\u7aef\u5177\u4f53\u8f93\u5165\u662f\u4ec0\u4e48? \u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u9884\u6d4b\u9636\u6bb5\u4e00\u81f4\u5417?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"07_mkdocs_review_model/2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#1-decoder","text":"","title":"1 Decoder\u7aef\u7684\u8f93\u5165\u89e3\u6790"},{"location":"07_mkdocs_review_model/2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#11-decoder","text":"Transformer\u539f\u59cb\u8bba\u6587\u4e2d\u7684Decoder\u6a21\u5757\u662f\u7531N=6\u4e2a\u76f8\u540c\u7684Decoder Block\u5806\u53e0\u800c\u6210, \u5176\u4e2d\u6bcf\u4e00\u4e2aBlock\u662f\u75313\u4e2a\u5b50\u6a21\u5757\u6784\u6210, \u5206\u522b\u662f\u591a\u5934self-attention\u6a21\u5757, Encoder-Decoder attention\u6a21\u5757, \u524d\u9988\u5168\u8fde\u63a5\u5c42\u6a21\u5757. 6\u4e2aBlock\u7684\u8f93\u5165\u4e0d\u5b8c\u5168\u76f8\u540c: \u6700\u4e0b\u9762\u7684\u4e00\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u662f\u7ecf\u5386\u4e86MASK\u4e4b\u540e\u7684Decoder\u7aef\u7684\u8f93\u5165 + Encoder\u7aef\u7684\u8f93\u51fa. \u5176\u4ed65\u5c42Block\u63a5\u6536\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u90fd\u662f\u524d\u4e00\u5c42Block\u7684\u8f93\u51fa + Encoder\u7aef\u7684\u8f93\u51fa.","title":"1.1 Decoder\u7aef\u7684\u67b6\u6784"},{"location":"07_mkdocs_review_model/2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#12-decoder","text":"\u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u8bf4, \u5047\u8bbe\u73b0\u5728\u7684\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u7b49\u4e8e\"How are you?\", \u5f53time step=1\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\u4e00\u4e2a\u7279\u6b8a\u7684token, \u6bd4\u5982\"SOS\"; \u5f53time step=2\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How\"; \u5f53time step=3\u65f6, \u8f93\u5165\u5f20\u91cf\u4e3a\"SOS How are\", \u4ee5\u6b64\u7c7b\u63a8... \u6ce8\u610f: \u5728\u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u8bad\u7ec3\u9636\u6bb5\u4e0d\u4f1a\u8fd9\u6837\u52a8\u6001\u8f93\u5165, \u800c\u662f\u4e00\u6b21\u6027\u7684\u628a\u76ee\u6807\u5e8f\u5217\u5168\u90e8\u8f93\u5165\u7ed9\u7b2c\u4e00\u5c42\u7684Block, \u7136\u540e\u901a\u8fc7\u591a\u5934self-attention\u4e2d\u7684MASK\u673a\u5236\u5bf9\u5e8f\u5217\u8fdb\u884c\u540c\u6837\u7684\u906e\u63a9\u5373\u53ef.","title":"1.2 Decoder\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790"},{"location":"07_mkdocs_review_model/2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#13-decoder","text":"\u540c\u7406\u4e8e\u8bad\u7ec3\u9636\u6bb5, \u9884\u6d4b\u65f6\u4ece\u7b2c\u4e8c\u5c42Block\u5230\u7b2c\u516d\u5c42Block\u7684\u8f93\u5165\u6a21\u5f0f\u4e00\u81f4, \u65e0\u9700\u7279\u6b8a\u5904\u7406, \u90fd\u662f\u56fa\u5b9a\u64cd\u4f5c\u7684\u5faa\u73af\u5904\u7406. \u805a\u7126\u5728\u7b2c\u4e00\u5c42\u7684Block\u4e0a: \u56e0\u4e3a\u6bcf\u4e00\u6b65\u7684\u8f93\u5165\u90fd\u4f1a\u6709Encoder\u7684\u8f93\u51fa\u5f20\u91cf, \u56e0\u6b64\u8fd9\u91cc\u4e0d\u505a\u7279\u6b8a\u8ba8\u8bba, \u53ea\u4e13\u6ce8\u4e8e\u7eaf\u7cb9\u4eceDecoder\u7aef\u63a5\u6536\u7684\u8f93\u5165. \u9884\u6d4b\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0, input_tensor=\"SOS\"\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u8f93\u51fa\u7684\u7d2f\u8ba1\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u8bf4: \u5f53time step=1\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"What\"; \u5f53time step=2\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"is\"; \u5f53time step=3\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"the\"; \u5f53time step=4\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"matter\"; \u5f53time step=5\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"?\"; \u5f53time step=6\u65f6, \u8f93\u5165\u7684input_tensor=\"SOS What is the matter ?\", \u9884\u6d4b\u51fa\u6765\u7684\u8f93\u51fa\u503c\u662foutput_tensor=\"EOS\", \u4ee3\u8868\u53e5\u5b50\u7684\u7ed3\u675f\u7b26, \u8bf4\u660e\u89e3\u7801\u7ed3\u675f, \u9884\u6d4b\u7ed3\u675f.","title":"1.3 Decoder\u5728\u9884\u6d4b\u9636\u6bb5\u7684\u8f93\u5165\u89e3\u6790"},{"location":"07_mkdocs_review_model/2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html#2","text":"\u5728Transformer\u7ed3\u6784\u4e2d\u7684Decoder\u6a21\u5757\u7684\u8f93\u5165, \u533a\u5206\u4e8e\u4e0d\u540c\u7684Block, \u6700\u5e95\u5c42\u7684Block\u8f93\u5165\u6709\u5176\u7279\u6b8a\u7684\u5730\u65b9. \u7b2c\u4e8c\u5c42\u5230\u7b2c\u516d\u5c42\u7684\u8f93\u5165\u4e00\u81f4, \u90fd\u662f\u4e0a\u4e00\u5c42\u7684\u8f93\u51fa\u548cEncoder\u7684\u8f93\u51fa. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4e0a\u4e00\u4e2atime step\u7684\u8f93\u5165\u52a0\u4e0a\u771f\u5b9e\u6807\u7b7e\u5e8f\u5217\u5411\u540e\u79fb\u4e00\u4f4d. \u5177\u4f53\u6765\u770b, \u5c31\u662f\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f, \u4e0d\u65ad\u7684\u5c06\u4e4b\u524d\u7684\u8f93\u5165\u878d\u5408\u8fdb\u6765. \u6700\u5e95\u5c42\u7684Block\u5728\u8bad\u7ec3\u9636\u6bb5, \u771f\u5b9e\u7684\u4ee3\u7801\u5b9e\u73b0\u4e2d, \u91c7\u7528\u7684\u662fMASK\u673a\u5236\u6765\u6a21\u62df\u8f93\u5165\u5e8f\u5217\u4e0d\u65ad\u6dfb\u52a0\u7684\u8fc7\u7a0b. \u6700\u5e95\u5c42\u7684Block\u5728\u9884\u6d4b\u9636\u6bb5, \u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u662f\u4ecetime step=0\u5f00\u59cb, \u4e00\u76f4\u5230\u4e0a\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c\u7684\u7d2f\u79ef\u62fc\u63a5\u5f20\u91cf. \u5177\u4f53\u6765\u770b, \u4e5f\u662f\u968f\u7740\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u5e8f\u5217\u4f1a\u8d8a\u6765\u8d8a\u957f. \u76f8\u6bd4\u4e8e\u8bad\u7ec3\u9636\u6bb5\u6700\u5927\u7684\u4e0d\u540c\u662f\u8fd9\u91cc\u4e0d\u65ad\u62fc\u63a5\u8fdb\u6765\u7684token\u662f\u6bcf\u4e00\u4e2atime step\u7684\u9884\u6d4b\u503c, \u800c\u4e0d\u662f\u8bad\u7ec3\u9636\u6bb5\u6bcf\u4e00\u4e2atime step\u53d6\u5f97\u7684groud truth\u503c.","title":"2 \u5c0f\u7ed3"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1self-attention\u7684\u673a\u5236\u548c\u539f\u7406. \u638c\u63e1\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u6765\u8ba1\u7b97self-attention. \u7406\u89e3softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. \u7406\u89e3softmax\u51fd\u6570\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u68af\u5ea6\u6c42\u5bfc\u7684\u6570\u5b66\u8fc7\u7a0b. \u7406\u89e3softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u539f\u56e0. \u7406\u89e3self-attention\u8ba1\u7b97\u89c4\u5219\u4e2d\u5f52\u4e00\u5316\u7684\u539f\u56e0. \u601d\u8003\u98981\uff1a Transformer\u4e2d\u4e00\u76f4\u5f3a\u8c03\u7684self-attention\u662f\u4ec0\u4e48? \u4e3a\u4ec0\u4e48\u80fd\u53d1\u6325\u5982\u6b64\u5927\u7684\u4f5c\u7528? \u8ba1\u7b97\u7684\u65f6\u5019\u5982\u679c\u4e0d\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V), \u800c\u4ec5\u4ec5\u4f7f\u7528(Q, V)\u6216\u8005(K, V)\u6216\u8005(V)\u884c\u4e0d\u884c? \u601d\u8003\u98982\uff1aself-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled? 1 Self-attention\u7684\u673a\u5236\u548c\u539f\u7406 \u00b6 self-attention\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u8eab\u548c\u81ea\u8eab\u8fdb\u884c\u5173\u8054\u7684attention\u673a\u5236, \u4ece\u800c\u5f97\u5230\u66f4\u597d\u7684representation\u6765\u8868\u8fbe\u81ea\u8eab. self-attention\u662fattention\u673a\u5236\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u5728self-attention\u4e2d, Q=K=V, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd(token)\u90fd\u548c\u8be5\u5e8f\u5217\u4e2d\u7684\u5176\u4ed6\u6240\u6709\u5355\u8bcd(token)\u8fdb\u884cattention\u89c4\u5219\u7684\u8ba1\u7b97. attention\u673a\u5236\u8ba1\u7b97\u7684\u7279\u70b9\u5728\u4e8e, \u53ef\u4ee5\u76f4\u63a5\u8de8\u8d8a\u4e00\u53e5\u8bdd\u4e2d\u4e0d\u540c\u8ddd\u79bb\u7684token, \u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u5b66\u4e60\u5230\u5e8f\u5217\u7684\u77e5\u8bc6\u4f9d\u8d56\u548c\u8bed\u5e8f\u7ed3\u6784. \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, self-attention\u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u6355\u6349\u5230\u8bed\u4e49\u5c42\u9762\u7684\u7279\u5f81(its\u7684\u6307\u4ee3\u5bf9\u8c61\u662fLaw). \u5e94\u7528\u4f20\u7edf\u7684RNN, LSTM, \u5728\u83b7\u53d6\u957f\u8ddd\u79bb\u8bed\u4e49\u7279\u5f81\u548c\u7ed3\u6784\u7279\u5f81\u7684\u65f6\u5019, \u9700\u8981\u6309\u7167\u5e8f\u5217\u987a\u5e8f\u4f9d\u6b21\u8ba1\u7b97, \u8ddd\u79bb\u8d8a\u8fdc\u7684\u8054\u7cfb\u4fe1\u606f\u7684\u635f\u8017\u8d8a\u5927, \u6709\u6548\u63d0\u53d6\u548c\u6355\u83b7\u7684\u53ef\u80fd\u6027\u8d8a\u5c0f. \u4f46\u662f\u5e94\u7528self-attention\u65f6, \u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4f1a\u76f4\u63a5\u5c06\u53e5\u5b50\u4e2d\u4efb\u610f\u4e24\u4e2atoken\u7684\u8054\u7cfb\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u6b65\u9aa4\u76f4\u63a5\u8054\u7cfb\u8d77\u6765, \u5173\u4e8eself-attention\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528(Q, K, V)\u4e09\u5143\u7ec4\u800c\u4e0d\u662f\u5176\u4ed6\u5f62\u5f0f: \u9996\u5148\u4e00\u6761\u5c31\u662f\u4ece\u5206\u6790\u7684\u89d2\u5ea6\u770b, \u67e5\u8be2Query\u662f\u4e00\u6761\u72ec\u7acb\u7684\u5e8f\u5217\u4fe1\u606f, \u901a\u8fc7\u5173\u952e\u8bcdKey\u7684\u63d0\u793a\u4f5c\u7528, \u5f97\u5230\u6700\u7ec8\u8bed\u4e49\u7684\u771f\u5b9e\u503cValue\u8868\u8fbe, \u6570\u5b66\u610f\u4e49\u66f4\u5145\u5206, \u5b8c\u5907. \u8fd9\u91cc\u4e0d\u4f7f\u7528(K, V)\u6216\u8005(V)\u6ca1\u6709\u4ec0\u4e48\u5fc5\u987b\u7684\u7406\u7531, \u4e5f\u6ca1\u6709\u76f8\u5173\u7684\u8bba\u6587\u6765\u4e25\u683c\u9610\u8ff0\u6bd4\u8f83\u8bd5\u9a8c\u7684\u7ed3\u679c\u5dee\u5f02, \u6240\u4ee5\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u653e\u6027\u95ee\u9898\u672a\u6765\u53bb\u63a2\u7d22, \u53ea\u8981\u660e\u786e\u5728\u7ecf\u5178self-attention\u5b9e\u73b0\u4e2d\u7528\u7684\u662f\u4e09\u5143\u7ec4\u5c31\u597d. self-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled? 2 Self-attention\u4e2d\u7684\u5f52\u4e00\u5316\u6982\u8ff0 \u00b6 \u8bad\u7ec3\u4e0a\u7684\u610f\u4e49: \u968f\u7740\u8bcd\u5d4c\u5165\u7ef4\u5ea6d_k\u7684\u589e\u5927, q * k \u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4e5f\u4f1a\u589e\u5927, \u5728\u8bad\u7ec3\u65f6\u4f1a\u5c06\u5e26\u6709\u9971\u548c\u533a\u95f4\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u6bd4\u5982\uff1asigmoid\u6fc0\u6d3b\u51fd\u6570\u3001tanh\u6fc0\u6d3b\u51fd\u6570\u3001\u903b\u8f91\u56de\u5f52softmax\uff09\u63a8\u5165\u68af\u5ea6\u975e\u5e38\u5c0f\u7684\u533a\u57df, \u53ef\u80fd\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u73b0\u8c61, \u9020\u6210\u6a21\u578b\u6536\u655b\u56f0\u96be. \u6570\u5b66\u4e0a\u7684\u610f\u4e49: \u5047\u8bbeq\u548ck\u7684\u7edf\u8ba1\u53d8\u91cf\u662f\u6ee1\u8db3\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u72ec\u7acb\u968f\u673a\u53d8\u91cf, \u610f\u5473\u7740q\u548ck\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1. \u90a3\u4e48q\u548ck\u7684\u70b9\u79ef\u7ed3\u679c\u5c31\u662f\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3ad_k, \u4e3a\u4e86\u62b5\u6d88\u8fd9\u79cd\u65b9\u5dee\u88ab\u653e\u5927d_k\u500d\u7684\u5f71\u54cd, \u5728\u8ba1\u7b97\u4e2d\u4e3b\u52a8\u5c06\u70b9\u79ef\u7f29\u653e1/sqrt(d_k), \u8fd9\u6837\u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4f9d\u7136\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1. 3 softmax\u7684\u68af\u5ea6\u53d8\u5316 \u00b6 \u8fd9\u91cc\u6211\u4eec\u52063\u4e2a\u6b65\u9aa4\u6765\u89e3\u91casoftmax\u7684\u68af\u5ea6\u95ee\u9898: \u7b2c\u4e00\u6b65: softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684. \u7b2c\u4e8c\u6b65: softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u7b2c\u4e09\u6b65: softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0. 3.1 softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684 \u00b6 \u5bf9\u4e8e\u4e00\u4e2a\u8f93\u5165\u5411\u91cfx, softmax\u51fd\u6570\u5c06\u5176\u505a\u4e86\u4e00\u4e2a\u5f52\u4e00\u5316\u7684\u6620\u5c04, \u9996\u5148\u901a\u8fc7\u81ea\u7136\u5e95\u6570e\u5c06\u8f93\u5165\u5143\u7d20\u4e4b\u95f4\u7684\u5dee\u8ddd\u5148\"\u62c9\u5927\", \u7136\u540e\u518d\u5f52\u4e00\u5316\u4e3a\u4e00\u4e2a\u65b0\u7684\u5206\u5e03. \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5047\u8bbe\u67d0\u4e2a\u8f93\u5165x\u4e2d\u6700\u5927\u7684\u5143\u7d20\u4e0b\u6807\u662fk, \u5982\u679c\u8f93\u5165\u7684\u6570\u91cf\u7ea7\u53d8\u5927(\u5c31\u662fx\u4e2d\u7684\u6bcf\u4e2a\u5206\u91cf\u7edd\u5bf9\u503c\u90fd\u5f88\u5927), \u90a3\u4e48\u5728\u6570\u5b66\u4e0a\u4f1a\u9020\u6210y_k\u7684\u503c\u975e\u5e38\u63a5\u8fd11. \u5177\u4f53\u7528\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a, \u5047\u8bbe\u8f93\u5165\u7684\u5411\u91cfx = [a, a, 2a], \u90a3\u4e48\u968f\u4fbf\u7ed9\u51e0\u4e2a\u4e0d\u540c\u6570\u91cf\u7ea7\u7684\u503c\u6765\u770b\u770b\u5bf9y3\u4ea7\u751f\u7684\u5f71\u54cd a = 1\u65f6, y3 = 0.5761168847658291 # e^2 / (e^1 + e^1 + e^2)) a = 10\u65f6, y3 = 0.9999092083843412 # e^20 / (e^10 + e^10 + e^20)) a = 100\u65f6, y3 = 1.0 # e^200 / (e^100 + e^100 + e^200)) \u91c7\u7528\u4e00\u6bb5\u5b9e\u4f8b\u4ee3\u7801\u5c06a\u5728\u4e0d\u540c\u53d6\u503c\u4e0b, \u5bf9\u5e94\u7684y3\u5168\u90e8\u753b\u51fa\u6765, \u4ee5\u66f2\u7ebf\u7684\u5f62\u5f0f\u5c55\u793a: from math import exp from matplotlib import pyplot as plt import numpy as np f = lambda x : exp ( x * 2 ) / ( exp ( x ) + exp ( x ) + exp ( x * 2 )) x = np . linspace ( 0 , 100 , 100 ) y_3 = [ f ( x_i ) for x_i in x ] plt . plot ( x , y_3 ) plt . show () \u5f97\u5230\u5982\u4e0b\u7684\u66f2\u7ebf: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927 \u7ed3\u8bba\uff1a \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6\uff0csoftmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e\u3002\u901a\u4fd7\u7684\u8bb2\uff1a\u6570\u636e\u7684\u65b9\u5dee\u53d8\u5927\uff08\u79bb\u6563\u7a0b\u5ea6\u53d8\u5927\uff09\uff0c\u6700\u5927\u503c\u5f3a\u5360\u4e86\u6240\u6709\u6982\u7387\u3002 3.2 softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684 \u00b6 softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u5bb9\u6613\u68af\u5ea6\u6d88\u5931\uff0c\u6240\u4ee5\u8981\u770b\u4e00\u770bsoftmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u662f\u5982\u4f55\u6c42\u5bfc\u7684\u3002 \u9996\u5148\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u548c\u8f93\u51fa: \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u53cd\u5411\u4f20\u64ad\u5c31\u662f\u8f93\u51fa\u7aef\u7684\u635f\u5931\u51fd\u6570\u5bf9\u8f93\u5165\u7aef\u6c42\u504f\u5bfc\u7684\u8fc7\u7a0b, \u8fd9\u91cc\u8981\u5206\u4e24\u79cd\u60c5\u51b5, \u7b2c\u4e00\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \u7b2c\u4e8c\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \u7ecf\u8fc7\u5bf9\u4e24\u79cd\u60c5\u51b5\u5206\u522b\u7684\u6c42\u5bfc\u8ba1\u7b97, \u53ef\u4ee5\u5f97\u51fa\u6700\u7ec8\u7684\u7ed3\u8bba\u5982\u4e0b: \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \u628a\u62bd\u8c61\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u6620\u5c04\u6210\u77e9\u9635\u8868\u793a\uff0c\u89c13.3\u8282\u8868\u793a\uff08i=j\u65f6\uff0c\u4e24\u4e2a\u77e9\u9635\u5bf9\u89d2\u7ebf - \u5bf9\u89d2\u7ebf \uff1bi!=j\u65f6\uff0c\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u51cf\uff09\u3002 3.3 softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0 \u00b6 \u6839\u636e\u7b2c\u4e8c\u6b65\u4e2dsoftmax\u51fd\u6570\u7684\u6c42\u5bfc\u7ed3\u679c, \u53ef\u4ee5\u5c06\u6700\u7ec8\u7684\u7ed3\u679c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c55\u5f00\u5982\u4e0b: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \u6839\u636e\u7b2c\u4e00\u6b65\u4e2d\u7684\u8ba8\u8bba\u7ed3\u679c, \u5f53\u8f93\u5165x\u7684\u5206\u91cf\u503c\u8f83\u5927\u65f6, softmax\u51fd\u6570\u4f1a\u5c06\u5927\u90e8\u5206\u6982\u7387\u5206\u914d\u7ed9\u6700\u5927\u7684\u5143\u7d20, \u5047\u8bbe\u6700\u5927\u5143\u7d20\u662fx1, \u90a3\u4e48softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c06\u4ea7\u751f\u4e00\u4e2a\u63a5\u8fd1one-hot\u7684\u7ed3\u679c\u5f20\u91cfy_ = [1, 0, 0,..., 0], \u6b64\u65f6\u7ed3\u679c\u77e9\u9635\u53d8\u4e3a: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \u7ed3\u8bba: \u7efc\u4e0a\u53ef\u4ee5\u5f97\u51fa, \u6240\u6709\u7684\u68af\u5ea6\u90fd\u6d88\u5931\u4e3a0(\u63a5\u8fd1\u4e8e0), \u53c2\u6570\u51e0\u4e4e\u65e0\u6cd5\u66f4\u65b0, \u6a21\u578b\u6536\u655b\u56f0\u96be. 4 \u7ef4\u5ea6\u4e0e\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb \u00b6 \u9488\u5bf9\u4e3a\u4ec0\u4e48\u7ef4\u5ea6\u4f1a\u5f71\u54cd\u70b9\u79ef\u7684\u5927\u5c0f, \u539f\u59cb\u8bba\u6587\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u70b9\u89e3\u91ca\u5982\u4e0b: To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their doct product, q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k. \u6211\u4eec\u5206\u4e24\u6b65\u5bf9\u5176\u8fdb\u884c\u4e00\u4e2a\u63a8\u5bfc, \u9996\u5148\u5c31\u662f\u5047\u8bbe\u5411\u91cfq\u548ck\u7684\u5404\u4e2a\u5206\u91cf\u662f\u76f8\u4e92\u72ec\u7acb\u7684\u968f\u673a\u53d8\u91cf, X = q_i, Y = k_i, X\u548cY\u5404\u81ea\u6709d_k\u4e2a\u5206\u91cf, \u4e5f\u5c31\u662f\u5411\u91cf\u7684\u7ef4\u5ea6\u7b49\u4e8ed_k, \u6709E(X) = E(Y) = 0, \u4ee5\u53caD(X) = D(Y) = 1. \u53ef\u4ee5\u5f97\u5230E(XY) = E(X)E(Y) = 0 * 0 = 0 \u540c\u7406, \u5bf9\u4e8eD(XY)\u63a8\u5bfc\u5982\u4e0b: \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \u6839\u636e\u671f\u671b\u548c\u65b9\u5dee\u7684\u6027\u8d28, \u5bf9\u4e8e\u4e92\u76f8\u72ec\u7acb\u7684\u53d8\u91cf\u6ee1\u8db3\u4e0b\u5f0f: E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) \u4e0a\u8ff0\u516c\u5f0f\uff0c\u7b80\u8bfb\u4e3a\uff1a\u548c\u7684\u671f\u671b\uff0c\u7b49\u4e8e\u671f\u671b\u7684\u548c\uff1b\u548c\u7684\u65b9\u5dee\u7b49\u4e8e\u65b9\u5dee\u7684\u548c \u6839\u636e\u4e0a\u9762\u7684\u516c\u5f0f, \u53ef\u4ee5\u5f88\u8f7b\u677e\u7684\u5f97\u51faq*k\u7684\u5747\u503c\u4e3aE(qk) = 0, D(qk) = d_k. \u6240\u4ee5\u65b9\u5dee\u8d8a\u5927, \u5bf9\u5e94\u7684qk\u7684\u70b9\u79ef\u5c31\u8d8a\u5927, \u8fd9\u6837softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c31\u4f1a\u66f4\u504f\u5411\u6700\u5927\u503c\u6240\u5728\u7684\u5206\u91cf. \u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5c06\u70b9\u79ef\u9664\u4ee5sqrt(d_k), \u5c06\u65b9\u5dee\u5728\u6570\u5b66\u4e0a\u91cd\u65b0\"\u62c9\u56de1\", \u5982\u4e0b\u6240\u793a: D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 \u6700\u7ec8\u7684\u7ed3\u8bba: \u901a\u8fc7\u6570\u5b66\u4e0a\u7684\u6280\u5de7\u5c06\u65b9\u5dee\u63a7\u5236\u57281, \u4e5f\u5c31\u6709\u6548\u7684\u63a7\u5236\u4e86\u70b9\u79ef\u7ed3\u679c\u7684\u53d1\u6563, \u4e5f\u5c31\u63a7\u5236\u4e86\u5bf9\u5e94\u7684\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898! 5 \u5c0f\u7ed3 \u00b6 self-attention\u673a\u5236\u7684\u91cd\u70b9\u662f\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u53c2\u4e0e\u89c4\u5219\u8fd0\u7b97, \u8fd9\u91cc\u9762Q=K=V. self-attention\u6700\u5927\u7684\u4f18\u52bf\u662f\u53ef\u4ee5\u65b9\u4fbf\u6709\u6548\u7684\u63d0\u53d6\u8fdc\u8ddd\u79bb\u4f9d\u8d56\u7684\u7279\u5f81\u548c\u7ed3\u6784\u4fe1\u606f, \u4e0d\u5fc5\u5411RNN\u90a3\u6837\u4f9d\u6b21\u8ba1\u7b97\u4ea7\u751f\u4f20\u9012\u635f\u8017. \u5173\u4e8eself-attention\u91c7\u7528\u4e09\u5143\u7ec4\u7684\u539f\u56e0, \u7ecf\u5178\u5b9e\u73b0\u7684\u65b9\u5f0f\u6570\u5b66\u610f\u4e49\u660e\u786e, \u7406\u7531\u5145\u5206, \u81f3\u4e8e\u5176\u4ed6\u65b9\u5f0f\u7684\u53ef\u884c\u6027\u6682\u65f6\u6ca1\u6709\u8bba\u6587\u505a\u5145\u5206\u7684\u5bf9\u6bd4\u8bd5\u9a8c\u7814\u7a76. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. softmax\u51fd\u6570\u672c\u8d28\u662f\u5bf9\u8f93\u5165\u7684\u6570\u636e\u5206\u5e03\u505a\u4e00\u6b21\u5f52\u4e00\u5316\u5904\u7406, \u4f46\u662f\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927. \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6, softmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u5177\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b\u89c1\u8bb2\u4e49\u6b63\u6587\u90e8\u5206, \u6ce8\u610f\u8981\u5206\u4e24\u79cd\u60c5\u51b5\u8ba8\u8bba, \u5206\u522b\u5904\u7406. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0. \u7ed3\u5408\u7b2c\u4e00\u6b65, \u7b2c\u4e8c\u6b65\u7684\u7ed3\u8bba, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u6700\u7ec8\u7684\u68af\u5ea6\u77e9\u9635\u63a5\u8fd1\u4e8e\u96f6\u77e9\u9635, \u8fd9\u6837\u5728\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\u7684\u65f6\u5019\u5c31\u4f1a\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u73b0\u8c61. \u5b66\u4e60\u4e86\u7ef4\u5ea6\u548c\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb\u63a8\u5bfc. \u901a\u8fc7\u671f\u671b\u548c\u65b9\u5dee\u7684\u63a8\u5bfc\u7406\u89e3\u4e86\u4e3a\u4ec0\u4e48\u70b9\u79ef\u4f1a\u9020\u6210\u65b9\u5dee\u53d8\u5927. \u7406\u89e3\u4e86\u901a\u8fc7\u6570\u5b66\u6280\u5de7\u9664\u4ee5sqrt(d_k)\u5c31\u53ef\u4ee5\u8ba9\u65b9\u5dee\u6062\u590d\u62101.","title":"3 Self attention\u673a\u5236\u8be6\u89e3"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#_1","text":"\u638c\u63e1self-attention\u7684\u673a\u5236\u548c\u539f\u7406. \u638c\u63e1\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u6765\u8ba1\u7b97self-attention. \u7406\u89e3softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. \u7406\u89e3softmax\u51fd\u6570\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u68af\u5ea6\u6c42\u5bfc\u7684\u6570\u5b66\u8fc7\u7a0b. \u7406\u89e3softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u539f\u56e0. \u7406\u89e3self-attention\u8ba1\u7b97\u89c4\u5219\u4e2d\u5f52\u4e00\u5316\u7684\u539f\u56e0. \u601d\u8003\u98981\uff1a Transformer\u4e2d\u4e00\u76f4\u5f3a\u8c03\u7684self-attention\u662f\u4ec0\u4e48? \u4e3a\u4ec0\u4e48\u80fd\u53d1\u6325\u5982\u6b64\u5927\u7684\u4f5c\u7528? \u8ba1\u7b97\u7684\u65f6\u5019\u5982\u679c\u4e0d\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V), \u800c\u4ec5\u4ec5\u4f7f\u7528(Q, V)\u6216\u8005(K, V)\u6216\u8005(V)\u884c\u4e0d\u884c? \u601d\u8003\u98982\uff1aself-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#1-self-attention","text":"self-attention\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u8eab\u548c\u81ea\u8eab\u8fdb\u884c\u5173\u8054\u7684attention\u673a\u5236, \u4ece\u800c\u5f97\u5230\u66f4\u597d\u7684representation\u6765\u8868\u8fbe\u81ea\u8eab. self-attention\u662fattention\u673a\u5236\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u5728self-attention\u4e2d, Q=K=V, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd(token)\u90fd\u548c\u8be5\u5e8f\u5217\u4e2d\u7684\u5176\u4ed6\u6240\u6709\u5355\u8bcd(token)\u8fdb\u884cattention\u89c4\u5219\u7684\u8ba1\u7b97. attention\u673a\u5236\u8ba1\u7b97\u7684\u7279\u70b9\u5728\u4e8e, \u53ef\u4ee5\u76f4\u63a5\u8de8\u8d8a\u4e00\u53e5\u8bdd\u4e2d\u4e0d\u540c\u8ddd\u79bb\u7684token, \u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u5b66\u4e60\u5230\u5e8f\u5217\u7684\u77e5\u8bc6\u4f9d\u8d56\u548c\u8bed\u5e8f\u7ed3\u6784. \u4ece\u4e0a\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230, self-attention\u53ef\u4ee5\u8fdc\u8ddd\u79bb\u7684\u6355\u6349\u5230\u8bed\u4e49\u5c42\u9762\u7684\u7279\u5f81(its\u7684\u6307\u4ee3\u5bf9\u8c61\u662fLaw). \u5e94\u7528\u4f20\u7edf\u7684RNN, LSTM, \u5728\u83b7\u53d6\u957f\u8ddd\u79bb\u8bed\u4e49\u7279\u5f81\u548c\u7ed3\u6784\u7279\u5f81\u7684\u65f6\u5019, \u9700\u8981\u6309\u7167\u5e8f\u5217\u987a\u5e8f\u4f9d\u6b21\u8ba1\u7b97, \u8ddd\u79bb\u8d8a\u8fdc\u7684\u8054\u7cfb\u4fe1\u606f\u7684\u635f\u8017\u8d8a\u5927, \u6709\u6548\u63d0\u53d6\u548c\u6355\u83b7\u7684\u53ef\u80fd\u6027\u8d8a\u5c0f. \u4f46\u662f\u5e94\u7528self-attention\u65f6, \u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u4f1a\u76f4\u63a5\u5c06\u53e5\u5b50\u4e2d\u4efb\u610f\u4e24\u4e2atoken\u7684\u8054\u7cfb\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u6b65\u9aa4\u76f4\u63a5\u8054\u7cfb\u8d77\u6765, \u5173\u4e8eself-attention\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528(Q, K, V)\u4e09\u5143\u7ec4\u800c\u4e0d\u662f\u5176\u4ed6\u5f62\u5f0f: \u9996\u5148\u4e00\u6761\u5c31\u662f\u4ece\u5206\u6790\u7684\u89d2\u5ea6\u770b, \u67e5\u8be2Query\u662f\u4e00\u6761\u72ec\u7acb\u7684\u5e8f\u5217\u4fe1\u606f, \u901a\u8fc7\u5173\u952e\u8bcdKey\u7684\u63d0\u793a\u4f5c\u7528, \u5f97\u5230\u6700\u7ec8\u8bed\u4e49\u7684\u771f\u5b9e\u503cValue\u8868\u8fbe, \u6570\u5b66\u610f\u4e49\u66f4\u5145\u5206, \u5b8c\u5907. \u8fd9\u91cc\u4e0d\u4f7f\u7528(K, V)\u6216\u8005(V)\u6ca1\u6709\u4ec0\u4e48\u5fc5\u987b\u7684\u7406\u7531, \u4e5f\u6ca1\u6709\u76f8\u5173\u7684\u8bba\u6587\u6765\u4e25\u683c\u9610\u8ff0\u6bd4\u8f83\u8bd5\u9a8c\u7684\u7ed3\u679c\u5dee\u5f02, \u6240\u4ee5\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u653e\u6027\u95ee\u9898\u672a\u6765\u53bb\u63a2\u7d22, \u53ea\u8981\u660e\u786e\u5728\u7ecf\u5178self-attention\u5b9e\u73b0\u4e2d\u7528\u7684\u662f\u4e09\u5143\u7ec4\u5c31\u597d. self-attention\u516c\u5f0f\u4e2d\u7684\u5f52\u4e00\u5316\u6709\u4ec0\u4e48\u4f5c\u7528? \u4e3a\u4ec0\u4e48\u8981\u6dfb\u52a0scaled?","title":"1 Self-attention\u7684\u673a\u5236\u548c\u539f\u7406"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#2-self-attention","text":"\u8bad\u7ec3\u4e0a\u7684\u610f\u4e49: \u968f\u7740\u8bcd\u5d4c\u5165\u7ef4\u5ea6d_k\u7684\u589e\u5927, q * k \u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4e5f\u4f1a\u589e\u5927, \u5728\u8bad\u7ec3\u65f6\u4f1a\u5c06\u5e26\u6709\u9971\u548c\u533a\u95f4\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u6bd4\u5982\uff1asigmoid\u6fc0\u6d3b\u51fd\u6570\u3001tanh\u6fc0\u6d3b\u51fd\u6570\u3001\u903b\u8f91\u56de\u5f52softmax\uff09\u63a8\u5165\u68af\u5ea6\u975e\u5e38\u5c0f\u7684\u533a\u57df, \u53ef\u80fd\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u7684\u73b0\u8c61, \u9020\u6210\u6a21\u578b\u6536\u655b\u56f0\u96be. \u6570\u5b66\u4e0a\u7684\u610f\u4e49: \u5047\u8bbeq\u548ck\u7684\u7edf\u8ba1\u53d8\u91cf\u662f\u6ee1\u8db3\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u72ec\u7acb\u968f\u673a\u53d8\u91cf, \u610f\u5473\u7740q\u548ck\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1. \u90a3\u4e48q\u548ck\u7684\u70b9\u79ef\u7ed3\u679c\u5c31\u662f\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3ad_k, \u4e3a\u4e86\u62b5\u6d88\u8fd9\u79cd\u65b9\u5dee\u88ab\u653e\u5927d_k\u500d\u7684\u5f71\u54cd, \u5728\u8ba1\u7b97\u4e2d\u4e3b\u52a8\u5c06\u70b9\u79ef\u7f29\u653e1/sqrt(d_k), \u8fd9\u6837\u70b9\u79ef\u540e\u7684\u7ed3\u679c\u4f9d\u7136\u6ee1\u8db3\u5747\u503c\u4e3a0, \u65b9\u5dee\u4e3a1.","title":"2 Self-attention\u4e2d\u7684\u5f52\u4e00\u5316\u6982\u8ff0"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#3-softmax","text":"\u8fd9\u91cc\u6211\u4eec\u52063\u4e2a\u6b65\u9aa4\u6765\u89e3\u91casoftmax\u7684\u68af\u5ea6\u95ee\u9898: \u7b2c\u4e00\u6b65: softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684. \u7b2c\u4e8c\u6b65: softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u7b2c\u4e09\u6b65: softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0.","title":"3 softmax\u7684\u68af\u5ea6\u53d8\u5316"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#31-softmax","text":"\u5bf9\u4e8e\u4e00\u4e2a\u8f93\u5165\u5411\u91cfx, softmax\u51fd\u6570\u5c06\u5176\u505a\u4e86\u4e00\u4e2a\u5f52\u4e00\u5316\u7684\u6620\u5c04, \u9996\u5148\u901a\u8fc7\u81ea\u7136\u5e95\u6570e\u5c06\u8f93\u5165\u5143\u7d20\u4e4b\u95f4\u7684\u5dee\u8ddd\u5148\"\u62c9\u5927\", \u7136\u540e\u518d\u5f52\u4e00\u5316\u4e3a\u4e00\u4e2a\u65b0\u7684\u5206\u5e03. \u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u5047\u8bbe\u67d0\u4e2a\u8f93\u5165x\u4e2d\u6700\u5927\u7684\u5143\u7d20\u4e0b\u6807\u662fk, \u5982\u679c\u8f93\u5165\u7684\u6570\u91cf\u7ea7\u53d8\u5927(\u5c31\u662fx\u4e2d\u7684\u6bcf\u4e2a\u5206\u91cf\u7edd\u5bf9\u503c\u90fd\u5f88\u5927), \u90a3\u4e48\u5728\u6570\u5b66\u4e0a\u4f1a\u9020\u6210y_k\u7684\u503c\u975e\u5e38\u63a5\u8fd11. \u5177\u4f53\u7528\u4e00\u4e2a\u4f8b\u5b50\u6765\u6f14\u793a, \u5047\u8bbe\u8f93\u5165\u7684\u5411\u91cfx = [a, a, 2a], \u90a3\u4e48\u968f\u4fbf\u7ed9\u51e0\u4e2a\u4e0d\u540c\u6570\u91cf\u7ea7\u7684\u503c\u6765\u770b\u770b\u5bf9y3\u4ea7\u751f\u7684\u5f71\u54cd a = 1\u65f6, y3 = 0.5761168847658291 # e^2 / (e^1 + e^1 + e^2)) a = 10\u65f6, y3 = 0.9999092083843412 # e^20 / (e^10 + e^10 + e^20)) a = 100\u65f6, y3 = 1.0 # e^200 / (e^100 + e^100 + e^200)) \u91c7\u7528\u4e00\u6bb5\u5b9e\u4f8b\u4ee3\u7801\u5c06a\u5728\u4e0d\u540c\u53d6\u503c\u4e0b, \u5bf9\u5e94\u7684y3\u5168\u90e8\u753b\u51fa\u6765, \u4ee5\u66f2\u7ebf\u7684\u5f62\u5f0f\u5c55\u793a: from math import exp from matplotlib import pyplot as plt import numpy as np f = lambda x : exp ( x * 2 ) / ( exp ( x ) + exp ( x ) + exp ( x * 2 )) x = np . linspace ( 0 , 100 , 100 ) y_3 = [ f ( x_i ) for x_i in x ] plt . plot ( x , y_3 ) plt . show () \u5f97\u5230\u5982\u4e0b\u7684\u66f2\u7ebf: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927 \u7ed3\u8bba\uff1a \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6\uff0csoftmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e\u3002\u901a\u4fd7\u7684\u8bb2\uff1a\u6570\u636e\u7684\u65b9\u5dee\u53d8\u5927\uff08\u79bb\u6563\u7a0b\u5ea6\u53d8\u5927\uff09\uff0c\u6700\u5927\u503c\u5f3a\u5360\u4e86\u6240\u6709\u6982\u7387\u3002","title":"3.1 softmax\u51fd\u6570\u7684\u8f93\u5165\u5206\u5e03\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#32-softmax","text":"softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u5bb9\u6613\u68af\u5ea6\u6d88\u5931\uff0c\u6240\u4ee5\u8981\u770b\u4e00\u770bsoftmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u662f\u5982\u4f55\u6c42\u5bfc\u7684\u3002 \u9996\u5148\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u548c\u8f93\u51fa: \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u8bbeX=[x_1,x_2,\\cdots,x_n],Y=softmax(X)=[y_1,y_2,\\cdots,y_n]\\\\\\\\ \u5219y_i=\\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}},\u663e\u7136\\sum_{i=1}^{n}y_i=1 \u53cd\u5411\u4f20\u64ad\u5c31\u662f\u8f93\u51fa\u7aef\u7684\u635f\u5931\u51fd\u6570\u5bf9\u8f93\u5165\u7aef\u6c42\u504f\u5bfc\u7684\u8fc7\u7a0b, \u8fd9\u91cc\u8981\u5206\u4e24\u79cd\u60c5\u51b5, \u7b2c\u4e00\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \\begin{align*} (1)\u5f53i=j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial{y_i}}{\\partial{x_i}}\\\\\\\\ &= \\frac{\\partial}{\\partial{x_i}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})-e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}\\cdot(\\sum_ke^{x_k})}{(\\sum_ke^{x_k})^2}-\\frac{e^{x_i}\\cdot e^{x_i}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &=\\frac{e^{x_i}}{\\sum_ke^{x_k}}-\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_i}}{\\sum_ke^{x_k}}\\\\\\\\ &=y_i-y_i\\cdot y_i \\\\\\\\ &=y_i(1-y_i) \\end{align*} \u7b2c\u4e8c\u79cd\u5982\u4e0b\u6240\u793a: \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \\begin{align*} (2)\u5f53i\\neq j\u65f6\\\\\\\\ \\frac{\\partial{y_i}}{\\partial{x_j}} &= \\frac{\\partial}{\\partial{x_j}}{(\\frac{e^{x_i}}{\\sum_k e^{x_k}})} \\\\\\\\ &= \\frac{(e^{x_i})^{\\prime} (\\sum_k e^{x_k}) - e^{x_i}(\\sum_ke^{x_k})^{\\prime}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= \\frac{0\\cdot (\\sum_k e^{x_k}) - e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}\\cdot e^{x_j}}{(\\sum_ke^{x_k})^2}\\\\\\\\ &= -\\frac{e^{x_i}}{\\sum_ke^{x_k}}\\cdot \\frac{ e^{x_j}}{\\sum_ke^{x_k}}\\\\\\\\ &=-y_i\\cdot y_j \\end{align*} \u7ecf\u8fc7\u5bf9\u4e24\u79cd\u60c5\u51b5\u5206\u522b\u7684\u6c42\u5bfc\u8ba1\u7b97, \u53ef\u4ee5\u5f97\u51fa\u6700\u7ec8\u7684\u7ed3\u8bba\u5982\u4e0b: \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \\begin{align*} \u7efc\u4e0a\u6240\u8ff0\uff1a\\frac{\\partial y_i}{\\partial x_j} &= \\begin{cases} y_i-y_i\\cdot y_i, & \\text {i=j} \\\\\\\\ 0-y_i\\cdot y_j, & \\text{i $\\neq$ j} \\end{cases} \\\\\\\\ \u6240\u4ee5\uff1a\\frac{\\partial Y}{\\partial X} &= diag(Y)-Y^T\\cdot Y \\;\\;\\;(\u5f53Y\u7684shape\u4e3a(1,n)\u65f6) \\end{align*} \u628a\u62bd\u8c61\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u6620\u5c04\u6210\u77e9\u9635\u8868\u793a\uff0c\u89c13.3\u8282\u8868\u793a\uff08i=j\u65f6\uff0c\u4e24\u4e2a\u77e9\u9635\u5bf9\u89d2\u7ebf - \u5bf9\u89d2\u7ebf \uff1bi!=j\u65f6\uff0c\u5bf9\u5e94\u4f4d\u7f6e\u76f8\u51cf\uff09\u3002","title":"3.2 softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#33-softmax","text":"\u6839\u636e\u7b2c\u4e8c\u6b65\u4e2dsoftmax\u51fd\u6570\u7684\u6c42\u5bfc\u7ed3\u679c, \u53ef\u4ee5\u5c06\u6700\u7ec8\u7684\u7ed3\u679c\u4ee5\u77e9\u9635\u5f62\u5f0f\u5c55\u5f00\u5982\u4e0b: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} \\hat y_1 & 0 & \\cdots & 0 \\\\ 0 & \\hat y_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\hat y_d \\end{bmatrix} - \\begin{bmatrix} \\hat y_1^2 & \\hat y_1 \\hat y_2 & \\cdots & \\hat y_1 \\hat y_d \\\\ \\hat y_2 \\hat y_1 & \\hat y_2^2 & \\cdots & \\hat y_2 \\hat y_d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\hat y_d \\hat y_1 & \\hat y_d \\hat y_2 & \\cdots & \\hat y_d^2 \\end{bmatrix} \u6839\u636e\u7b2c\u4e00\u6b65\u4e2d\u7684\u8ba8\u8bba\u7ed3\u679c, \u5f53\u8f93\u5165x\u7684\u5206\u91cf\u503c\u8f83\u5927\u65f6, softmax\u51fd\u6570\u4f1a\u5c06\u5927\u90e8\u5206\u6982\u7387\u5206\u914d\u7ed9\u6700\u5927\u7684\u5143\u7d20, \u5047\u8bbe\u6700\u5927\u5143\u7d20\u662fx1, \u90a3\u4e48softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c06\u4ea7\u751f\u4e00\u4e2a\u63a5\u8fd1one-hot\u7684\u7ed3\u679c\u5f20\u91cfy_ = [1, 0, 0,..., 0], \u6b64\u65f6\u7ed3\u679c\u77e9\u9635\u53d8\u4e3a: \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \\frac{\\partial g(X)}{\\partial X}\\approx \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\\\\\ 0 & 0 & \\cdots & 0 \\\\\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\ 0 & 0 & \\cdots & 0 \\end{bmatrix}=0 \u7ed3\u8bba: \u7efc\u4e0a\u53ef\u4ee5\u5f97\u51fa, \u6240\u6709\u7684\u68af\u5ea6\u90fd\u6d88\u5931\u4e3a0(\u63a5\u8fd1\u4e8e0), \u53c2\u6570\u51e0\u4e4e\u65e0\u6cd5\u66f4\u65b0, \u6a21\u578b\u6536\u655b\u56f0\u96be.","title":"3.3 softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#4","text":"\u9488\u5bf9\u4e3a\u4ec0\u4e48\u7ef4\u5ea6\u4f1a\u5f71\u54cd\u70b9\u79ef\u7684\u5927\u5c0f, \u539f\u59cb\u8bba\u6587\u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u70b9\u89e3\u91ca\u5982\u4e0b: To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their doct product, q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k. \u6211\u4eec\u5206\u4e24\u6b65\u5bf9\u5176\u8fdb\u884c\u4e00\u4e2a\u63a8\u5bfc, \u9996\u5148\u5c31\u662f\u5047\u8bbe\u5411\u91cfq\u548ck\u7684\u5404\u4e2a\u5206\u91cf\u662f\u76f8\u4e92\u72ec\u7acb\u7684\u968f\u673a\u53d8\u91cf, X = q_i, Y = k_i, X\u548cY\u5404\u81ea\u6709d_k\u4e2a\u5206\u91cf, \u4e5f\u5c31\u662f\u5411\u91cf\u7684\u7ef4\u5ea6\u7b49\u4e8ed_k, \u6709E(X) = E(Y) = 0, \u4ee5\u53caD(X) = D(Y) = 1. \u53ef\u4ee5\u5f97\u5230E(XY) = E(X)E(Y) = 0 * 0 = 0 \u540c\u7406, \u5bf9\u4e8eD(XY)\u63a8\u5bfc\u5982\u4e0b: \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \\begin{align*} D(XY) & = E(X^2\\cdot Y^2)-[E(XY)]^2 \\\\\\\\ &=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\\\\\ &=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\\\\\ &=D(X)D(Y)-[E(X)E(Y)]^2 \\\\\\\\ &=1 \\times 1- (0 \\times 0)^2 \\\\\\\\ &=1 \\end{align*} \u6839\u636e\u671f\u671b\u548c\u65b9\u5dee\u7684\u6027\u8d28, \u5bf9\u4e8e\u4e92\u76f8\u72ec\u7acb\u7684\u53d8\u91cf\u6ee1\u8db3\u4e0b\u5f0f: E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) E(\\sum_iZ_i) =\\sum_iE(Z_i),\\\\\\\\ D(\\sum_iZ_i) =\\sum_iD(Z_i) \u4e0a\u8ff0\u516c\u5f0f\uff0c\u7b80\u8bfb\u4e3a\uff1a\u548c\u7684\u671f\u671b\uff0c\u7b49\u4e8e\u671f\u671b\u7684\u548c\uff1b\u548c\u7684\u65b9\u5dee\u7b49\u4e8e\u65b9\u5dee\u7684\u548c \u6839\u636e\u4e0a\u9762\u7684\u516c\u5f0f, \u53ef\u4ee5\u5f88\u8f7b\u677e\u7684\u5f97\u51faq*k\u7684\u5747\u503c\u4e3aE(qk) = 0, D(qk) = d_k. \u6240\u4ee5\u65b9\u5dee\u8d8a\u5927, \u5bf9\u5e94\u7684qk\u7684\u70b9\u79ef\u5c31\u8d8a\u5927, \u8fd9\u6837softmax\u7684\u8f93\u51fa\u5206\u5e03\u5c31\u4f1a\u66f4\u504f\u5411\u6700\u5927\u503c\u6240\u5728\u7684\u5206\u91cf. \u4e00\u4e2a\u6280\u5de7\u5c31\u662f\u5c06\u70b9\u79ef\u9664\u4ee5sqrt(d_k), \u5c06\u65b9\u5dee\u5728\u6570\u5b66\u4e0a\u91cd\u65b0\"\u62c9\u56de1\", \u5982\u4e0b\u6240\u793a: D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 D(\\frac{q\\cdot k}{\\sqrt{d_k}})=\\frac{d_k}{(\\sqrt{d_k})^2}=1 \u6700\u7ec8\u7684\u7ed3\u8bba: \u901a\u8fc7\u6570\u5b66\u4e0a\u7684\u6280\u5de7\u5c06\u65b9\u5dee\u63a7\u5236\u57281, \u4e5f\u5c31\u6709\u6548\u7684\u63a7\u5236\u4e86\u70b9\u79ef\u7ed3\u679c\u7684\u53d1\u6563, \u4e5f\u5c31\u63a7\u5236\u4e86\u5bf9\u5e94\u7684\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898!","title":"4 \u7ef4\u5ea6\u4e0e\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb"},{"location":"07_mkdocs_review_model/3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html#5","text":"self-attention\u673a\u5236\u7684\u91cd\u70b9\u662f\u4f7f\u7528\u4e09\u5143\u7ec4(Q, K, V)\u53c2\u4e0e\u89c4\u5219\u8fd0\u7b97, \u8fd9\u91cc\u9762Q=K=V. self-attention\u6700\u5927\u7684\u4f18\u52bf\u662f\u53ef\u4ee5\u65b9\u4fbf\u6709\u6548\u7684\u63d0\u53d6\u8fdc\u8ddd\u79bb\u4f9d\u8d56\u7684\u7279\u5f81\u548c\u7ed3\u6784\u4fe1\u606f, \u4e0d\u5fc5\u5411RNN\u90a3\u6837\u4f9d\u6b21\u8ba1\u7b97\u4ea7\u751f\u4f20\u9012\u635f\u8017. \u5173\u4e8eself-attention\u91c7\u7528\u4e09\u5143\u7ec4\u7684\u539f\u56e0, \u7ecf\u5178\u5b9e\u73b0\u7684\u65b9\u5f0f\u6570\u5b66\u610f\u4e49\u660e\u786e, \u7406\u7531\u5145\u5206, \u81f3\u4e8e\u5176\u4ed6\u65b9\u5f0f\u7684\u53ef\u884c\u6027\u6682\u65f6\u6ca1\u6709\u8bba\u6587\u505a\u5145\u5206\u7684\u5bf9\u6bd4\u8bd5\u9a8c\u7814\u7a76. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u7684\u8f93\u5165\u662f\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u7684. softmax\u51fd\u6570\u672c\u8d28\u662f\u5bf9\u8f93\u5165\u7684\u6570\u636e\u5206\u5e03\u505a\u4e00\u6b21\u5f52\u4e00\u5316\u5904\u7406, \u4f46\u662f\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u5bf9softmax\u6700\u7ec8\u7684\u5206\u5e03\u5f71\u54cd\u975e\u5e38\u4e4b\u5927. \u5728\u8f93\u5165\u5143\u7d20\u7684\u6570\u91cf\u7ea7\u8f83\u5927\u65f6, softmax\u51fd\u6570\u51e0\u4e4e\u5c06\u5168\u90e8\u7684\u6982\u7387\u5206\u5e03\u90fd\u5206\u914d\u7ed9\u4e86\u6700\u5927\u503c\u5206\u91cf\u6240\u5bf9\u5e94\u7684\u6807\u7b7e. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u5728\u53cd\u5411\u4f20\u64ad\u7684\u8fc7\u7a0b\u4e2d\u662f\u5982\u4f55\u68af\u5ea6\u6c42\u5bfc\u7684. \u5177\u4f53\u7684\u63a8\u5bfc\u8fc7\u7a0b\u89c1\u8bb2\u4e49\u6b63\u6587\u90e8\u5206, \u6ce8\u610f\u8981\u5206\u4e24\u79cd\u60c5\u51b5\u8ba8\u8bba, \u5206\u522b\u5904\u7406. \u5b66\u4e60\u4e86softmax\u51fd\u6570\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u73b0\u8c61\u7684\u539f\u56e0. \u7ed3\u5408\u7b2c\u4e00\u6b65, \u7b2c\u4e8c\u6b65\u7684\u7ed3\u8bba, \u53ef\u4ee5\u5f88\u6e05\u695a\u7684\u770b\u5230\u6700\u7ec8\u7684\u68af\u5ea6\u77e9\u9635\u63a5\u8fd1\u4e8e\u96f6\u77e9\u9635, \u8fd9\u6837\u5728\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\u7684\u65f6\u5019\u5c31\u4f1a\u4ea7\u751f\u68af\u5ea6\u6d88\u5931\u73b0\u8c61. \u5b66\u4e60\u4e86\u7ef4\u5ea6\u548c\u70b9\u79ef\u5927\u5c0f\u7684\u5173\u7cfb\u63a8\u5bfc. \u901a\u8fc7\u671f\u671b\u548c\u65b9\u5dee\u7684\u63a8\u5bfc\u7406\u89e3\u4e86\u4e3a\u4ec0\u4e48\u70b9\u79ef\u4f1a\u9020\u6210\u65b9\u5dee\u53d8\u5927. \u7406\u89e3\u4e86\u901a\u8fc7\u6570\u5b66\u6280\u5de7\u9664\u4ee5sqrt(d_k)\u5c31\u53ef\u4ee5\u8ba9\u65b9\u5dee\u6062\u590d\u62101.","title":"5 \u5c0f\u7ed3"},{"location":"07_mkdocs_review_model/4%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Transformer\u4e2d\u5e94\u7528\u591a\u5934\u6ce8\u610f\u529b\u7684\u539f\u56e0. \u638c\u63e1Transformer\u4e2d\u591a\u5934\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u65b9\u5f0f. \u601d\u8003\u9898\uff1aTransformer\u4e3a\u4ec0\u4e48\u9700\u8981\u8fdb\u884cMulti-head Attention? Multi-head Attention\u7684\u8ba1\u7b97\u8fc7\u7a0b\u662f\u4ec0\u4e48? 1 \u91c7\u7528Multi-head Attention\u7684\u539f\u56e0 \u00b6 \u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u8fdb\u884cMulti-head Attention\u7684\u539f\u56e0\u662f\u5c06\u6a21\u578b\u5206\u4e3a\u591a\u4e2a\u5934, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f, \u6700\u540e\u518d\u5c06\u5404\u4e2a\u65b9\u9762\u7684\u4fe1\u606f\u7efc\u5408\u8d77\u6765\u5f97\u5230\u66f4\u597d\u7684\u6548\u679c. \u591a\u4e2a\u5934\u8fdb\u884cattention\u8ba1\u7b97\u6700\u540e\u518d\u7efc\u5408\u8d77\u6765, \u7c7b\u4f3c\u4e8eCNN\u4e2d\u91c7\u7528\u591a\u4e2a\u5377\u79ef\u6838\u7684\u4f5c\u7528, \u4e0d\u540c\u7684\u5377\u79ef\u6838\u63d0\u53d6\u4e0d\u540c\u7684\u7279\u5f81, \u5173\u6ce8\u4e0d\u540c\u7684\u90e8\u5206, \u6700\u540e\u518d\u8fdb\u884c\u878d\u5408. \u76f4\u89c2\u4e0a\u8bb2, \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f. 2 Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f \u00b6 Multi-head Attention\u548c\u5355\u4e00head\u7684Attention\u552f\u4e00\u7684\u533a\u522b\u5c31\u5728\u4e8e, \u5176\u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684embedding_dim=512\u8fdb\u884c\u5207\u5272\u6210head=8, \u8fd9\u6837\u6bcf\u4e00\u4e2ahead\u7684\u5d4c\u5165\u7ef4\u5ea6\u5c31\u662f512/8=64, \u540e\u7eed\u7684Attention\u8ba1\u7b97\u516c\u5f0f\u5b8c\u5168\u4e00\u81f4, \u53ea\u4e0d\u8fc7\u662f\u572864\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u77e9\u9635\u8fd0\u7b97\u800c\u5df2. \u5728head=8\u4e2a\u5934\u4e0a\u5206\u522b\u8fdb\u884c\u6ce8\u610f\u529b\u89c4\u5219\u7684\u8fd0\u7b97\u540e, \u7b80\u5355\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u5bf9\u7ed3\u679c\u5f20\u91cf\u8fdb\u884c\u878d\u5408\u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u7ed3\u679c. 3 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u91c7\u7528Multi-head Attention\u7684\u539f\u56e0. \u5c06\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u5934, \u5206\u522b\u8fdb\u884cAttention\u8ba1\u7b97, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f\u7279\u5f81, \u66f4\u597d\u7684\u63d0\u5347\u6a21\u578b\u7684\u6548\u679c. \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f. \u5b66\u4e60\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f. \u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6embedding_dim\u8fdb\u884c\u5207\u5272, \u5207\u5272\u540e\u7684\u8ba1\u7b97\u89c4\u5219\u548c\u5355\u4e00head\u5b8c\u5168\u4e00\u81f4. \u5728\u4e0d\u540c\u7684head\u4e0a\u5e94\u7528\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u540e, \u5f97\u5230\u7684\u7ed3\u679c\u5f20\u91cf\u76f4\u63a5\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u8fdb\u884c\u878d\u5408, \u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u7ed3\u679c\u5f20\u91cf.","title":"4 Multi head Attention\u8be6\u89e3"},{"location":"07_mkdocs_review_model/4%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#_1","text":"\u638c\u63e1Transformer\u4e2d\u5e94\u7528\u591a\u5934\u6ce8\u610f\u529b\u7684\u539f\u56e0. \u638c\u63e1Transformer\u4e2d\u591a\u5934\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u65b9\u5f0f. \u601d\u8003\u9898\uff1aTransformer\u4e3a\u4ec0\u4e48\u9700\u8981\u8fdb\u884cMulti-head Attention? Multi-head Attention\u7684\u8ba1\u7b97\u8fc7\u7a0b\u662f\u4ec0\u4e48?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"07_mkdocs_review_model/4%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#1-multi-head-attention","text":"\u539f\u59cb\u8bba\u6587\u4e2d\u63d0\u5230\u8fdb\u884cMulti-head Attention\u7684\u539f\u56e0\u662f\u5c06\u6a21\u578b\u5206\u4e3a\u591a\u4e2a\u5934, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f, \u6700\u540e\u518d\u5c06\u5404\u4e2a\u65b9\u9762\u7684\u4fe1\u606f\u7efc\u5408\u8d77\u6765\u5f97\u5230\u66f4\u597d\u7684\u6548\u679c. \u591a\u4e2a\u5934\u8fdb\u884cattention\u8ba1\u7b97\u6700\u540e\u518d\u7efc\u5408\u8d77\u6765, \u7c7b\u4f3c\u4e8eCNN\u4e2d\u91c7\u7528\u591a\u4e2a\u5377\u79ef\u6838\u7684\u4f5c\u7528, \u4e0d\u540c\u7684\u5377\u79ef\u6838\u63d0\u53d6\u4e0d\u540c\u7684\u7279\u5f81, \u5173\u6ce8\u4e0d\u540c\u7684\u90e8\u5206, \u6700\u540e\u518d\u8fdb\u884c\u878d\u5408. \u76f4\u89c2\u4e0a\u8bb2, \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f.","title":"1 \u91c7\u7528Multi-head Attention\u7684\u539f\u56e0"},{"location":"07_mkdocs_review_model/4%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#2-multi-head-attention","text":"Multi-head Attention\u548c\u5355\u4e00head\u7684Attention\u552f\u4e00\u7684\u533a\u522b\u5c31\u5728\u4e8e, \u5176\u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684embedding_dim=512\u8fdb\u884c\u5207\u5272\u6210head=8, \u8fd9\u6837\u6bcf\u4e00\u4e2ahead\u7684\u5d4c\u5165\u7ef4\u5ea6\u5c31\u662f512/8=64, \u540e\u7eed\u7684Attention\u8ba1\u7b97\u516c\u5f0f\u5b8c\u5168\u4e00\u81f4, \u53ea\u4e0d\u8fc7\u662f\u572864\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4e00\u7cfb\u5217\u7684\u77e9\u9635\u8fd0\u7b97\u800c\u5df2. \u5728head=8\u4e2a\u5934\u4e0a\u5206\u522b\u8fdb\u884c\u6ce8\u610f\u529b\u89c4\u5219\u7684\u8fd0\u7b97\u540e, \u7b80\u5355\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u5bf9\u7ed3\u679c\u5f20\u91cf\u8fdb\u884c\u878d\u5408\u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u7ed3\u679c.","title":"2 Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f"},{"location":"07_mkdocs_review_model/4%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html#3","text":"\u5b66\u4e60\u4e86Transformer\u67b6\u6784\u91c7\u7528Multi-head Attention\u7684\u539f\u56e0. \u5c06\u6a21\u578b\u5212\u5206\u4e3a\u591a\u4e2a\u5934, \u5206\u522b\u8fdb\u884cAttention\u8ba1\u7b97, \u53ef\u4ee5\u5f62\u6210\u591a\u4e2a\u5b50\u7a7a\u95f4, \u8ba9\u6a21\u578b\u53bb\u5173\u6ce8\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f\u7279\u5f81, \u66f4\u597d\u7684\u63d0\u5347\u6a21\u578b\u7684\u6548\u679c. \u591a\u5934\u6ce8\u610f\u529b\u6709\u52a9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5230\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u4fe1\u606f. \u5b66\u4e60\u4e86Multi-head Attention\u7684\u8ba1\u7b97\u65b9\u5f0f. \u5bf9\u7279\u5f81\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5206\u5272, \u4e00\u822c\u662f\u5bf9\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6embedding_dim\u8fdb\u884c\u5207\u5272, \u5207\u5272\u540e\u7684\u8ba1\u7b97\u89c4\u5219\u548c\u5355\u4e00head\u5b8c\u5168\u4e00\u81f4. \u5728\u4e0d\u540c\u7684head\u4e0a\u5e94\u7528\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u89c4\u5219\u540e, \u5f97\u5230\u7684\u7ed3\u679c\u5f20\u91cf\u76f4\u63a5\u91c7\u7528\u62fc\u63a5concat\u7684\u65b9\u5f0f\u8fdb\u884c\u878d\u5408, \u5c31\u5f97\u5230\u4e86Multi-head Attention\u7684\u7ed3\u679c\u5f20\u91cf.","title":"3 \u5c0f\u7ed3"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html","text":"\u5b66\u4e60\u76ee\u6807 \u00b6 \u638c\u63e1Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u80cc\u540e\u7684\u539f\u56e0. \u638c\u63e1Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684. \u7406\u89e3\u4e3a\u4ec0\u4e48\u91c7\u7528\u8fd9\u6837\u7684\u65b9\u5f0f\u53ef\u4ee5\u5b9e\u73b0Transformer\u7684\u5e76\u884c\u5316. \u638c\u63e1Transformer\u53ef\u4ee5\u66ff\u4ee3seq2seq\u7684\u6838\u5fc3\u539f\u56e0. \u601d\u8003\u9898\uff1atransformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684? \u5177\u4f53\u4f53\u73b0\u5728\u54ea\u91cc? \u601d\u8003\u9898\uff1aTransformer\u76f8\u6bd4\u4e8eRNN/LSTM\u6709\u4ec0\u4e48\u4f18\u52bf? \u4e3a\u4ec0\u4e48? \u601d\u8003\u9898\uff1a\u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq? 1 Transformer\u7684\u5e76\u884c\u8ba1\u7b97 \u00b6 \u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e00\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b. \u5bf9\u4e8eRNN\u6765\u8bf4, \u4efb\u610f\u65f6\u523bt\u7684\u8f93\u5165\u662f\u65f6\u523bt\u7684\u8f93\u5165x(t)\u548c\u4e0a\u4e00\u65f6\u523b\u7684\u9690\u85cf\u5c42\u8f93\u51fah(t-1), \u7ecf\u8fc7\u8fd0\u7b97\u540e\u5f97\u5230\u5f53\u524d\u65f6\u523b\u9690\u85cf\u5c42\u7684\u8f93\u51fah(t), \u8fd9\u4e2ah(t)\u4e5f\u5373\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523bt+1\u7684\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u8fd9\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u662fRNN\u7684\u672c\u8d28\u7279\u5f81, RNN\u7684\u5386\u53f2\u4fe1\u606f\u662f\u9700\u8981\u901a\u8fc7\u8fd9\u4e2a\u65f6\u95f4\u6b65\u4e00\u6b65\u4e00\u6b65\u5411\u540e\u4f20\u9012\u7684. \u800c\u8fd9\u5c31\u610f\u5473\u7740RNN\u5e8f\u5217\u540e\u9762\u7684\u4fe1\u606f\u53ea\u80fd\u7b49\u5230\u524d\u9762\u7684\u8ba1\u7b97\u7ed3\u675f\u540e, \u5c06\u5386\u53f2\u4fe1\u606f\u901a\u8fc7hidden state\u4f20\u9012\u7ed9\u540e\u9762\u624d\u80fd\u5f00\u59cb\u8ba1\u7b97, \u5f62\u6210\u94fe\u5f0f\u7684\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u5b9e\u73b0\u5e76\u884c. \u5bf9\u4e8eTransformer\u7ed3\u6784\u6765\u8bf4, \u5728self-attention\u5c42, \u65e0\u8bba\u5e8f\u5217\u7684\u957f\u5ea6\u662f\u591a\u5c11, \u90fd\u53ef\u4ee5\u4e00\u6b21\u6027\u8ba1\u7b97\u6240\u6709\u5355\u8bcd\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u5173\u7cfb, \u8fd9\u4e2aattention\u7684\u8ba1\u7b97\u662f\u540c\u6b65\u7684, \u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c. 2 Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u8fc7\u7a0b \u00b6 2.1 Transformer\u67b6\u6784\u4e2dEncoder\u7684\u5e76\u884c\u5316 \u00b6 \u9996\u5148Transformer\u7684\u5e76\u884c\u5316\u4e3b\u8981\u4f53\u73b0\u5728Encoder\u6a21\u5757\u4e0a. \u4e0a\u56fe\u6700\u5e95\u5c42\u7eff\u8272\u7684\u90e8\u5206, \u6574\u4e2a\u5e8f\u5217\u6240\u6709\u7684token\u53ef\u4ee5\u5e76\u884c\u7684\u8fdb\u884cEmbedding\u64cd\u4f5c, \u8fd9\u4e00\u5c42\u7684\u5904\u7406\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684. \u4e0a\u56fe\u7b2c\u4e8c\u5c42\u571f\u9ec4\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662fTransformer\u4e2d\u6700\u91cd\u8981\u7684self-attention\u90e8\u5206, \u8fd9\u91cc\u5bf9\u4e8e\u4efb\u610f\u4e00\u4e2a\u5355\u8bcd\u6bd4\u5982x1, \u8981\u8ba1\u7b97x1\u5bf9\u4e8e\u5176\u4ed6\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5206\u5e03, \u5f97\u5230z1. \u8fd9\u4e2a\u8fc7\u7a0b\u662f\u5177\u6709\u4f9d\u8d56\u6027\u7684, \u5fc5\u987b\u7b49\u5230\u5e8f\u5217\u4e2d\u6240\u6709\u7684\u5355\u8bcd\u5b8c\u6210Embedding\u624d\u53ef\u4ee5\u8fdb\u884c. \u56e0\u6b64\u8fd9\u4e00\u6b65\u662f\u4e0d\u80fd\u5e76\u884c\u5904\u7406\u7684. \u4f46\u662f\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\u770b, \u6211\u4eec\u771f\u5b9e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u5e03\u7684\u65f6\u5019, \u91c7\u7528\u7684\u90fd\u662f\u77e9\u9635\u8fd0\u7b97, \u4e5f\u5c31\u662f\u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u8ba1\u7b97\u51fa\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5f20\u91cf, \u4ece\u8fd9\u4e2a\u89d2\u5ea6\u770b\u4e5f\u7b97\u662f\u5b9e\u73b0\u4e86\u5e76\u884c, \u53ea\u662f\u77e9\u9635\u8fd0\u7b97\u7684\"\u5e76\u884c\"\u548c\u8bcd\u5d4c\u5165\u7684\"\u5e76\u884c\"\u6982\u5ff5\u4e0a\u4e0d\u540c\u800c\u5df2. \u4e0a\u56fe\u7b2c\u4e09\u5c42\u84dd\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u5bf9\u4e8e\u4e0d\u540c\u7684\u5411\u91cfz\u4e4b\u95f4\u4e5f\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684, \u6240\u4ee5\u8fd9\u4e00\u5c42\u662f\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316\u5904\u7406\u7684. \u4e5f\u5c31\u662f\u6240\u6709\u7684\u5411\u91cfz\u8f93\u5165Feed Forward\u7f51\u7edc\u7684\u8ba1\u7b97\u53ef\u4ee5\u540c\u6b65\u8fdb\u884c, \u4e92\u4e0d\u5e72\u6270. 2.2 Transformer\u67b6\u6784\u4e2dDecoder\u7684\u5e76\u884c\u5316 \u00b6 \u5176\u6b21Transformer\u7684\u5e76\u884c\u5316\u4e5f\u90e8\u5206\u7684\u4f53\u73b0\u5728Decoder\u6a21\u5757\u4e0a. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u5176\u4e2dSelf-Attention\u548cEncoder-Decoder Attention\u4e24\u4e2a\u5b50\u5c42\u7684\u5e76\u884c\u5316\u4e5f\u662f\u5728\u8fdb\u884c\u77e9\u9635\u4e58\u6cd5, \u548cEncoder\u7684\u7406\u89e3\u662f\u4e00\u81f4\u7684. \u5728\u8fdb\u884cEmbedding\u548cFeed Forward\u7684\u5904\u7406\u65f6, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb, \u6240\u4ee5\u4e5f\u662f\u53ef\u4ee5\u5b8c\u5168\u5e76\u884c\u5316\u5904\u7406\u7684, \u8fd9\u91cc\u548cEncoder\u7684\u7406\u89e3\u4e5f\u662f\u4e00\u81f4\u7684. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u9636\u6bb5\u57fa\u672c\u4e0a\u4e0d\u8ba4\u4e3a\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u56e0\u4e3a\u7b2c\u4e00\u4e2atime step\u7684\u8f93\u5165\u53ea\u662f\u4e00\u4e2a\"SOS\", \u540e\u7eed\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u4e5f\u53ea\u662f\u4f9d\u6b21\u6dfb\u52a0\u4e4b\u524d\u6240\u6709\u7684\u9884\u6d4btoken. \u6ce8\u610f: \u6700\u91cd\u8981\u7684\u533a\u522b\u662f\u8bad\u7ec3\u9636\u6bb5\u76ee\u6807\u6587\u672c\u5982\u679c\u670920\u4e2atoken, \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u662f\u4e00\u6b21\u6027\u7684\u8f93\u5165\u7ed9Decoder\u7aef, \u53ef\u4ee5\u505a\u5230\u4e00\u4e9b\u5b50\u5c42\u7684\u5e76\u884c\u5316\u5904\u7406. \u4f46\u662f\u5728\u9884\u6d4b\u9636\u6bb5, \u5982\u679c\u9884\u6d4b\u7684\u7ed3\u679c\u8bed\u53e5\u603b\u5171\u670920\u4e2atoken, \u5219\u9700\u8981\u91cd\u590d\u5904\u740620\u6b21\u5faa\u73af\u7684\u8fc7\u7a0b, \u6bcf\u6b21\u7684\u8f93\u5165\u6dfb\u52a0\u8fdb\u53bb\u4e00\u4e2atoken, \u6bcf\u6b21\u7684\u8f93\u5165\u5e8f\u5217\u6bd4\u4e0a\u4e00\u6b21\u591a\u4e00\u4e2atoken, \u6240\u4ee5\u4e0d\u8ba4\u4e3a\u662f\u5e76\u884c\u5904\u7406. 3 Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b \u00b6 \u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e8c\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b. Transformer\u56e0\u4e3a\u91c7\u7528\u4e86Multi-head Attention\u7ed3\u6784\u548c\u8ba1\u7b97\u673a\u5236, \u62e5\u6709\u6bd4RNN/LSTM\u66f4\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b, \u8fd9\u91cc\u5e76\u4e0d\u4ec5\u4ec5\u7531\u7406\u8bba\u5206\u6790\u5f97\u6765, \u800c\u662f\u5927\u91cf\u7684\u8bd5\u9a8c\u6570\u636e\u548c\u5bf9\u6bd4\u7ed3\u679c, \u6e05\u695a\u7684\u5c55\u793a\u4e86Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b\u8fdc\u8fdc\u80dc\u4e8eRNN/LSTM. \u6ce8\u610f: \u4e0d\u662f\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u5c31\u8d8a\u65e0\u654c, \u5728\u5f88\u591a\u5177\u4f53\u7684\u5e94\u7528\u4e2dRNN/LSTM\u4f9d\u7136\u5927\u6709\u7528\u6b66\u4e4b\u5730, \u8981\u5177\u4f53\u95ee\u9898\u5177\u4f53\u5206\u6790. 4 \u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq? \u00b6 4.1 seq2seq\u7684\u4e24\u5927\u7f3a\u9677 \u00b6 seq2seq\u67b6\u6784\u7684\u7b2c\u4e00\u5927\u7f3a\u9677\u662f\u5c06Encoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u8bed\u4e49\u5411\u91cf\u4e2d, \u7528\u8fd9\u4e2a\u56fa\u5b9a\u7684\u5411\u91cf\u6765\u4ee3\u8868\u7f16\u7801\u5668\u7aef\u7684\u5168\u90e8\u4fe1\u606f. \u8fd9\u6837\u65e2\u4f1a\u9020\u6210\u4fe1\u606f\u7684\u635f\u8017, \u4e5f\u65e0\u6cd5\u8ba9Decoder\u7aef\u5728\u89e3\u7801\u7684\u65f6\u5019\u53bb\u7528\u6ce8\u610f\u529b\u805a\u7126\u54ea\u4e9b\u662f\u66f4\u91cd\u8981\u7684\u4fe1\u606f. seq2seq\u67b6\u6784\u7684\u7b2c\u4e8c\u5927\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c, \u672c\u8d28\u4e0a\u548cRNN/LSTM\u65e0\u6cd5\u5e76\u884c\u7684\u539f\u56e0\u4e00\u6837. 4.2 Transformer\u7684\u6539\u8fdb \u00b6 Transformer\u67b6\u6784\u540c\u65f6\u89e3\u51b3\u4e86seq2seq\u7684\u4e24\u5927\u7f3a\u9677, \u65e2\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97, \u53c8\u5e94\u7528Multi-head Attention\u673a\u5236\u6765\u89e3\u51b3Encoder\u56fa\u5b9a\u7f16\u7801\u7684\u95ee\u9898, \u8ba9Decoder\u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u53bb\u5173\u6ce8\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u6700\u91cd\u8981\u7684\u90a3\u4e9b\u90e8\u5206. 5 \u5c0f\u7ed3 \u00b6 \u5b66\u4e60\u4e86Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u539f\u56e0. \u7b2c\u4e00\u5927\u4f18\u52bf\u662f\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf. \u7b2c\u4e8c\u5927\u4f18\u52bf\u662f\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dEncoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Encoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u6d4b\u8bd5\u9636\u6bb5\u90fd\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u76f8\u540c\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dDecoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u7684Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u8ba1\u7b97\u4e0d\u80fd\u5e76\u884c\u5316\u5904\u7406. \u5b66\u4e60\u4e86seq2seq\u67b6\u6784\u7684\u4e24\u5927\u7f3a\u9677. \u7b2c\u4e00\u4e2a\u7f3a\u9677\u662fEncoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u88ab\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u7684\u8f93\u51fa\u5f20\u91cf, \u5f53\u5e8f\u5217\u957f\u5ea6\u8f83\u957f\u65f6\u4f1a\u9020\u6210\u6bd4\u8f83\u4e25\u91cd\u7684\u4fe1\u606f\u635f\u8017. \u7b2c\u4e8c\u4e2a\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c\u8ba1\u7b97. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u5bf9seq2seq\u4e24\u5927\u7f3a\u9677\u7684\u6539\u8fdb. Transformer\u5e94\u7528Multi-head Attention\u673a\u5236\u8ba9\u7f16\u7801\u5668\u4fe1\u606f\u53ef\u4ee5\u66f4\u597d\u7684\u5c55\u793a\u7ed9\u89e3\u7801\u5668. Transformer\u53ef\u4ee5\u5b9e\u73b0Encoder\u7aef\u7684\u5e76\u884c\u8ba1\u7b97.","title":"5 Transformer\u4f18\u52bf"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#_1","text":"\u638c\u63e1Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u80cc\u540e\u7684\u539f\u56e0. \u638c\u63e1Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684. \u7406\u89e3\u4e3a\u4ec0\u4e48\u91c7\u7528\u8fd9\u6837\u7684\u65b9\u5f0f\u53ef\u4ee5\u5b9e\u73b0Transformer\u7684\u5e76\u884c\u5316. \u638c\u63e1Transformer\u53ef\u4ee5\u66ff\u4ee3seq2seq\u7684\u6838\u5fc3\u539f\u56e0. \u601d\u8003\u9898\uff1atransformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u662f\u5982\u4f55\u8fdb\u884c\u7684? \u5177\u4f53\u4f53\u73b0\u5728\u54ea\u91cc? \u601d\u8003\u9898\uff1aTransformer\u76f8\u6bd4\u4e8eRNN/LSTM\u6709\u4ec0\u4e48\u4f18\u52bf? \u4e3a\u4ec0\u4e48? \u601d\u8003\u9898\uff1a\u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq?","title":"\u5b66\u4e60\u76ee\u6807"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#1-transformer","text":"\u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e00\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b. \u5bf9\u4e8eRNN\u6765\u8bf4, \u4efb\u610f\u65f6\u523bt\u7684\u8f93\u5165\u662f\u65f6\u523bt\u7684\u8f93\u5165x(t)\u548c\u4e0a\u4e00\u65f6\u523b\u7684\u9690\u85cf\u5c42\u8f93\u51fah(t-1), \u7ecf\u8fc7\u8fd0\u7b97\u540e\u5f97\u5230\u5f53\u524d\u65f6\u523b\u9690\u85cf\u5c42\u7684\u8f93\u51fah(t), \u8fd9\u4e2ah(t)\u4e5f\u5373\u5c06\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523bt+1\u7684\u8f93\u5165\u7684\u4e00\u90e8\u5206. \u8fd9\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u662fRNN\u7684\u672c\u8d28\u7279\u5f81, RNN\u7684\u5386\u53f2\u4fe1\u606f\u662f\u9700\u8981\u901a\u8fc7\u8fd9\u4e2a\u65f6\u95f4\u6b65\u4e00\u6b65\u4e00\u6b65\u5411\u540e\u4f20\u9012\u7684. \u800c\u8fd9\u5c31\u610f\u5473\u7740RNN\u5e8f\u5217\u540e\u9762\u7684\u4fe1\u606f\u53ea\u80fd\u7b49\u5230\u524d\u9762\u7684\u8ba1\u7b97\u7ed3\u675f\u540e, \u5c06\u5386\u53f2\u4fe1\u606f\u901a\u8fc7hidden state\u4f20\u9012\u7ed9\u540e\u9762\u624d\u80fd\u5f00\u59cb\u8ba1\u7b97, \u5f62\u6210\u94fe\u5f0f\u7684\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u5b9e\u73b0\u5e76\u884c. \u5bf9\u4e8eTransformer\u7ed3\u6784\u6765\u8bf4, \u5728self-attention\u5c42, \u65e0\u8bba\u5e8f\u5217\u7684\u957f\u5ea6\u662f\u591a\u5c11, \u90fd\u53ef\u4ee5\u4e00\u6b21\u6027\u8ba1\u7b97\u6240\u6709\u5355\u8bcd\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u5173\u7cfb, \u8fd9\u4e2aattention\u7684\u8ba1\u7b97\u662f\u540c\u6b65\u7684, \u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c.","title":"1 Transformer\u7684\u5e76\u884c\u8ba1\u7b97"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#2-transformer","text":"","title":"2 Transformer\u67b6\u6784\u7684\u5e76\u884c\u5316\u8fc7\u7a0b"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#21-transformerencoder","text":"\u9996\u5148Transformer\u7684\u5e76\u884c\u5316\u4e3b\u8981\u4f53\u73b0\u5728Encoder\u6a21\u5757\u4e0a. \u4e0a\u56fe\u6700\u5e95\u5c42\u7eff\u8272\u7684\u90e8\u5206, \u6574\u4e2a\u5e8f\u5217\u6240\u6709\u7684token\u53ef\u4ee5\u5e76\u884c\u7684\u8fdb\u884cEmbedding\u64cd\u4f5c, \u8fd9\u4e00\u5c42\u7684\u5904\u7406\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684. \u4e0a\u56fe\u7b2c\u4e8c\u5c42\u571f\u9ec4\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662fTransformer\u4e2d\u6700\u91cd\u8981\u7684self-attention\u90e8\u5206, \u8fd9\u91cc\u5bf9\u4e8e\u4efb\u610f\u4e00\u4e2a\u5355\u8bcd\u6bd4\u5982x1, \u8981\u8ba1\u7b97x1\u5bf9\u4e8e\u5176\u4ed6\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5206\u5e03, \u5f97\u5230z1. \u8fd9\u4e2a\u8fc7\u7a0b\u662f\u5177\u6709\u4f9d\u8d56\u6027\u7684, \u5fc5\u987b\u7b49\u5230\u5e8f\u5217\u4e2d\u6240\u6709\u7684\u5355\u8bcd\u5b8c\u6210Embedding\u624d\u53ef\u4ee5\u8fdb\u884c. \u56e0\u6b64\u8fd9\u4e00\u6b65\u662f\u4e0d\u80fd\u5e76\u884c\u5904\u7406\u7684. \u4f46\u662f\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\u770b, \u6211\u4eec\u771f\u5b9e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u5e03\u7684\u65f6\u5019, \u91c7\u7528\u7684\u90fd\u662f\u77e9\u9635\u8fd0\u7b97, \u4e5f\u5c31\u662f\u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u8ba1\u7b97\u51fa\u6240\u6709token\u7684\u6ce8\u610f\u529b\u5f20\u91cf, \u4ece\u8fd9\u4e2a\u89d2\u5ea6\u770b\u4e5f\u7b97\u662f\u5b9e\u73b0\u4e86\u5e76\u884c, \u53ea\u662f\u77e9\u9635\u8fd0\u7b97\u7684\"\u5e76\u884c\"\u548c\u8bcd\u5d4c\u5165\u7684\"\u5e76\u884c\"\u6982\u5ff5\u4e0a\u4e0d\u540c\u800c\u5df2. \u4e0a\u56fe\u7b2c\u4e09\u5c42\u84dd\u8272\u7684\u90e8\u5206, \u4e5f\u5c31\u662f\u524d\u9988\u5168\u8fde\u63a5\u5c42, \u5bf9\u4e8e\u4e0d\u540c\u7684\u5411\u91cfz\u4e4b\u95f4\u4e5f\u662f\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\u7684, \u6240\u4ee5\u8fd9\u4e00\u5c42\u662f\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316\u5904\u7406\u7684. \u4e5f\u5c31\u662f\u6240\u6709\u7684\u5411\u91cfz\u8f93\u5165Feed Forward\u7f51\u7edc\u7684\u8ba1\u7b97\u53ef\u4ee5\u540c\u6b65\u8fdb\u884c, \u4e92\u4e0d\u5e72\u6270.","title":"2.1 Transformer\u67b6\u6784\u4e2dEncoder\u7684\u5e76\u884c\u5316"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#22-transformerdecoder","text":"\u5176\u6b21Transformer\u7684\u5e76\u884c\u5316\u4e5f\u90e8\u5206\u7684\u4f53\u73b0\u5728Decoder\u6a21\u5757\u4e0a. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u5176\u4e2dSelf-Attention\u548cEncoder-Decoder Attention\u4e24\u4e2a\u5b50\u5c42\u7684\u5e76\u884c\u5316\u4e5f\u662f\u5728\u8fdb\u884c\u77e9\u9635\u4e58\u6cd5, \u548cEncoder\u7684\u7406\u89e3\u662f\u4e00\u81f4\u7684. \u5728\u8fdb\u884cEmbedding\u548cFeed Forward\u7684\u5904\u7406\u65f6, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb, \u6240\u4ee5\u4e5f\u662f\u53ef\u4ee5\u5b8c\u5168\u5e76\u884c\u5316\u5904\u7406\u7684, \u8fd9\u91cc\u548cEncoder\u7684\u7406\u89e3\u4e5f\u662f\u4e00\u81f4\u7684. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u9636\u6bb5\u57fa\u672c\u4e0a\u4e0d\u8ba4\u4e3a\u91c7\u7528\u4e86\u5e76\u884c\u5316\u5904\u7406. \u56e0\u4e3a\u7b2c\u4e00\u4e2atime step\u7684\u8f93\u5165\u53ea\u662f\u4e00\u4e2a\"SOS\", \u540e\u7eed\u6bcf\u4e00\u4e2atime step\u7684\u8f93\u5165\u4e5f\u53ea\u662f\u4f9d\u6b21\u6dfb\u52a0\u4e4b\u524d\u6240\u6709\u7684\u9884\u6d4btoken. \u6ce8\u610f: \u6700\u91cd\u8981\u7684\u533a\u522b\u662f\u8bad\u7ec3\u9636\u6bb5\u76ee\u6807\u6587\u672c\u5982\u679c\u670920\u4e2atoken, \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u662f\u4e00\u6b21\u6027\u7684\u8f93\u5165\u7ed9Decoder\u7aef, \u53ef\u4ee5\u505a\u5230\u4e00\u4e9b\u5b50\u5c42\u7684\u5e76\u884c\u5316\u5904\u7406. \u4f46\u662f\u5728\u9884\u6d4b\u9636\u6bb5, \u5982\u679c\u9884\u6d4b\u7684\u7ed3\u679c\u8bed\u53e5\u603b\u5171\u670920\u4e2atoken, \u5219\u9700\u8981\u91cd\u590d\u5904\u740620\u6b21\u5faa\u73af\u7684\u8fc7\u7a0b, \u6bcf\u6b21\u7684\u8f93\u5165\u6dfb\u52a0\u8fdb\u53bb\u4e00\u4e2atoken, \u6bcf\u6b21\u7684\u8f93\u5165\u5e8f\u5217\u6bd4\u4e0a\u4e00\u6b21\u591a\u4e00\u4e2atoken, \u6240\u4ee5\u4e0d\u8ba4\u4e3a\u662f\u5e76\u884c\u5904\u7406.","title":"2.2 Transformer\u67b6\u6784\u4e2dDecoder\u7684\u5e76\u884c\u5316"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#3-transformer","text":"\u5bf9\u4e8eTransformer\u6bd4\u4f20\u7edf\u5e8f\u5217\u6a21\u578bRNN/LSTM\u5177\u5907\u4f18\u52bf\u7684\u7b2c\u4e8c\u5927\u539f\u56e0\u5c31\u662f\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b. Transformer\u56e0\u4e3a\u91c7\u7528\u4e86Multi-head Attention\u7ed3\u6784\u548c\u8ba1\u7b97\u673a\u5236, \u62e5\u6709\u6bd4RNN/LSTM\u66f4\u5f3a\u5927\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b, \u8fd9\u91cc\u5e76\u4e0d\u4ec5\u4ec5\u7531\u7406\u8bba\u5206\u6790\u5f97\u6765, \u800c\u662f\u5927\u91cf\u7684\u8bd5\u9a8c\u6570\u636e\u548c\u5bf9\u6bd4\u7ed3\u679c, \u6e05\u695a\u7684\u5c55\u793a\u4e86Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b\u8fdc\u8fdc\u80dc\u4e8eRNN/LSTM. \u6ce8\u610f: \u4e0d\u662f\u8d8a\u5148\u8fdb\u7684\u6a21\u578b\u5c31\u8d8a\u65e0\u654c, \u5728\u5f88\u591a\u5177\u4f53\u7684\u5e94\u7528\u4e2dRNN/LSTM\u4f9d\u7136\u5927\u6709\u7528\u6b66\u4e4b\u5730, \u8981\u5177\u4f53\u95ee\u9898\u5177\u4f53\u5206\u6790.","title":"3 Transformer\u7684\u7279\u5f81\u62bd\u53d6\u80fd\u529b"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#4-transformerseq2seq","text":"","title":"4 \u4e3a\u4ec0\u4e48\u8bf4Transformer\u53ef\u4ee5\u4ee3\u66ffseq2seq?"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#41-seq2seq","text":"seq2seq\u67b6\u6784\u7684\u7b2c\u4e00\u5927\u7f3a\u9677\u662f\u5c06Encoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u8bed\u4e49\u5411\u91cf\u4e2d, \u7528\u8fd9\u4e2a\u56fa\u5b9a\u7684\u5411\u91cf\u6765\u4ee3\u8868\u7f16\u7801\u5668\u7aef\u7684\u5168\u90e8\u4fe1\u606f. \u8fd9\u6837\u65e2\u4f1a\u9020\u6210\u4fe1\u606f\u7684\u635f\u8017, \u4e5f\u65e0\u6cd5\u8ba9Decoder\u7aef\u5728\u89e3\u7801\u7684\u65f6\u5019\u53bb\u7528\u6ce8\u610f\u529b\u805a\u7126\u54ea\u4e9b\u662f\u66f4\u91cd\u8981\u7684\u4fe1\u606f. seq2seq\u67b6\u6784\u7684\u7b2c\u4e8c\u5927\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c, \u672c\u8d28\u4e0a\u548cRNN/LSTM\u65e0\u6cd5\u5e76\u884c\u7684\u539f\u56e0\u4e00\u6837.","title":"4.1 seq2seq\u7684\u4e24\u5927\u7f3a\u9677"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#42-transformer","text":"Transformer\u67b6\u6784\u540c\u65f6\u89e3\u51b3\u4e86seq2seq\u7684\u4e24\u5927\u7f3a\u9677, \u65e2\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97, \u53c8\u5e94\u7528Multi-head Attention\u673a\u5236\u6765\u89e3\u51b3Encoder\u56fa\u5b9a\u7f16\u7801\u7684\u95ee\u9898, \u8ba9Decoder\u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u53bb\u5173\u6ce8\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u6700\u91cd\u8981\u7684\u90a3\u4e9b\u90e8\u5206.","title":"4.2 Transformer\u7684\u6539\u8fdb"},{"location":"07_mkdocs_review_model/5%20Transformer%E4%BC%98%E5%8A%BF.html#5","text":"\u5b66\u4e60\u4e86Transformer\u76f8\u6bd4\u4e8eRNN/LSTM\u7684\u4f18\u52bf\u548c\u539f\u56e0. \u7b2c\u4e00\u5927\u4f18\u52bf\u662f\u5e76\u884c\u8ba1\u7b97\u7684\u4f18\u52bf. \u7b2c\u4e8c\u5927\u4f18\u52bf\u662f\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u5f3a. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dEncoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Encoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u6d4b\u8bd5\u9636\u6bb5\u90fd\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u76f8\u540c\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Encoder\u6a21\u5757\u5728self-attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u4e2dDecoder\u6a21\u5757\u7684\u5e76\u884c\u5316\u673a\u5236. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728\u8bad\u7ec3\u9636\u6bb5\u7684Embedding\u5c42, Feed Forward\u5c42, Add & Norm\u5c42\u90fd\u662f\u53ef\u4ee5\u5e76\u884c\u5316\u7684. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u5404\u4e2atoken\u4e4b\u95f4\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb, \u65e0\u6cd5\u72ec\u7acb\u8ba1\u7b97, \u4e0d\u662f\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u5e76\u884c\u5316. Decoder\u6a21\u5757\u5728self-attention\u5c42, \u4ee5\u53caEncoder-Decoder Attention\u5c42, \u56e0\u4e3a\u91c7\u7528\u4e86\u77e9\u9635\u8fd0\u7b97\u7684\u5b9e\u73b0\u65b9\u5f0f, \u53ef\u4ee5\u4e00\u6b21\u6027\u7684\u5b8c\u6210\u6240\u6709\u6ce8\u610f\u529b\u5f20\u91cf\u7684\u8ba1\u7b97, \u4e5f\u662f\u53e6\u4e00\u79cd\"\u5e76\u884c\u5316\"\u7684\u4f53\u73b0. Decoder\u6a21\u5757\u5728\u9884\u6d4b\u8ba1\u7b97\u4e0d\u80fd\u5e76\u884c\u5316\u5904\u7406. \u5b66\u4e60\u4e86seq2seq\u67b6\u6784\u7684\u4e24\u5927\u7f3a\u9677. \u7b2c\u4e00\u4e2a\u7f3a\u9677\u662fEncoder\u7aef\u7684\u6240\u6709\u4fe1\u606f\u88ab\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u7684\u8f93\u51fa\u5f20\u91cf, \u5f53\u5e8f\u5217\u957f\u5ea6\u8f83\u957f\u65f6\u4f1a\u9020\u6210\u6bd4\u8f83\u4e25\u91cd\u7684\u4fe1\u606f\u635f\u8017. \u7b2c\u4e8c\u4e2a\u7f3a\u9677\u662f\u65e0\u6cd5\u5e76\u884c\u8ba1\u7b97. \u5b66\u4e60\u4e86Transformer\u67b6\u6784\u5bf9seq2seq\u4e24\u5927\u7f3a\u9677\u7684\u6539\u8fdb. Transformer\u5e94\u7528Multi-head Attention\u673a\u5236\u8ba9\u7f16\u7801\u5668\u4fe1\u606f\u53ef\u4ee5\u66f4\u597d\u7684\u5c55\u793a\u7ed9\u89e3\u7801\u5668. Transformer\u53ef\u4ee5\u5b9e\u73b0Encoder\u7aef\u7684\u5e76\u884c\u8ba1\u7b97.","title":"5 \u5c0f\u7ed3"}]}