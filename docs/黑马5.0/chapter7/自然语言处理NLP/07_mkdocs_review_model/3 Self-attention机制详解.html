
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../img/logo.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.6">
    
    
      
        <title>3 Self attention机制详解 - 自然语言处理基础V4.0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.cd566b2a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href=".." title="自然语言处理基础V4.0" class="md-header__button md-logo" aria-label="自然语言处理基础V4.0" data-md-component="logo">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            自然语言处理基础V4.0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3 Self attention机制详解
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
    <img src="../assets/images/logo.svg" height="45px" alt="logo">

  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="自然语言处理基础V4.0" class="md-nav__button md-logo" aria-label="自然语言处理基础V4.0" data-md-component="logo">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    自然语言处理基础V4.0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          第一章 自然语言处理入门
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="第一章 自然语言处理入门" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          第一章 自然语言处理入门
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_mkdocs_NLP/1_%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.html" class="md-nav__link">
        1 自然语言处理入门
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          第二章 文本预处理
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="第二章 文本预处理" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          第二章 文本预处理
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_mkdocs_preprocess/1%20%E8%AE%A4%E8%AF%86%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86.html" class="md-nav__link">
        1 认识文本预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_mkdocs_preprocess/2%20%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95.html" class="md-nav__link">
        2 文本处理的基本方法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_mkdocs_preprocess/3%20%E6%96%87%E6%9C%AC%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95.html" class="md-nav__link">
        3 文本张量表示方法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_mkdocs_preprocess/4%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.html" class="md-nav__link">
        4 文本数据分析
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_mkdocs_preprocess/5%20%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86.html" class="md-nav__link">
        5 文本特征处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_mkdocs_preprocess/6%20%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.html" class="md-nav__link">
        6 文本数据增强
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_mkdocs_preprocess/7%20jieba%E8%AF%8D%E6%80%A7%E5%AF%B9%E7%85%A7%E8%A1%A8.html" class="md-nav__link">
        7 jieba词性对照表
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          第三章 RNN及其变体
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="第三章 RNN及其变体" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          第三章 RNN及其变体
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/1%20%E8%AE%A4%E8%AF%86RNN%E6%A8%A1%E5%9E%8B.html" class="md-nav__link">
        1 认识RNN模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/2%20%E4%BC%A0%E7%BB%9FRNN%E6%A8%A1%E5%9E%8B.html" class="md-nav__link">
        2 传统RNN模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/3%20LSTM%E6%A8%A1%E5%9E%8B.html" class="md-nav__link">
        3 LSTM模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/4%20GRU%E6%A8%A1%E5%9E%8B.html" class="md-nav__link">
        4 GRU模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/5%20RNN%E6%A1%88%E4%BE%8B-%E4%BA%BA%E5%90%8D%E5%88%86%E7%B1%BB%E5%99%A8.html" class="md-nav__link">
        5 RNN案例 人名分类器
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/6%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D1.html" class="md-nav__link">
        6 注意力机制介绍1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/7%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D2.html" class="md-nav__link">
        7 注意力机制介绍2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_mkdocs_RNN/8%20RNN%E6%A1%88%E4%BE%8B-seq2seq%E8%8B%B1%E8%AF%91%E6%B3%95.html" class="md-nav__link">
        8 RNN案例 seq2seq英译法
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          第四章 Transformer
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="第四章 Transformer" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          第四章 Transformer
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_mkdocs_transformer/1%20Transformer%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        1 Transformer背景介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_mkdocs_transformer/2%20%E8%AE%A4%E8%AF%86Transformer%E6%9E%B6%E6%9E%84.html" class="md-nav__link">
        2 认识Transformer架构
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_mkdocs_transformer/3%20%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html" class="md-nav__link">
        3 输入部分实现
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_mkdocs_transformer/4%20%E7%BC%96%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html" class="md-nav__link">
        4 编码器部分实现
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_mkdocs_transformer/5%20%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html" class="md-nav__link">
        5 解码器部分实现
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_mkdocs_transformer/6%20%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86%E5%AE%9E%E7%8E%B0.html" class="md-nav__link">
        6 输出部分实现
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_mkdocs_transformer/7%20%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA.html" class="md-nav__link">
        7 模型构建
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          第五章 迁移学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="第五章 迁移学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          第五章 迁移学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/1%20fasttext%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        1 fasttext工具介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/2%20fasttext%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.html" class="md-nav__link">
        2 fasttext模型架构
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/3%20fasttext%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html" class="md-nav__link">
        3 fasttext文本分类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/4%20%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F.html" class="md-nav__link">
        4 训练词向量
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/5%20%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%81%E7%A7%BB.html" class="md-nav__link">
        5 词向量迁移
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/6%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5.html" class="md-nav__link">
        6 迁移学习概念
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/7%20NLP%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.html" class="md-nav__link">
        7 NLP中的常用预训练模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/8%20Transformers%E5%BA%93%E4%BD%BF%E7%94%A8.html" class="md-nav__link">
        8 Transformers库使用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/9%20%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.html" class="md-nav__link">
        9 迁移学习实践
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_mkdocs_translearning/10%20NLP%E4%B8%AD%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%28%E6%8B%93%E5%B1%95%E8%B5%84%E6%96%99%29.html" class="md-nav__link">
        10 NLP中的标准数据集(拓展资料)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          第六章 Bert系列模型
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="第六章 Bert系列模型" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          第六章 Bert系列模型
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_mkdocs_bert_pretrained_model/1%20BERT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        1 BERT模型介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_mkdocs_bert_pretrained_model/2%20BERT%E6%A8%A1%E5%9E%8B%E7%89%B9%E7%82%B9.html" class="md-nav__link">
        2 BERT模型特点
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_mkdocs_bert_pretrained_model/3%20BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        3 BERT系列模型介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_mkdocs_bert_pretrained_model/4%20ELMo%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        4 ELMo模型介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_mkdocs_bert_pretrained_model/5%20GPT%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        5 GPT模型介绍
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_mkdocs_bert_pretrained_model/6%20BERT%20GPT%20ELMo%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AF%B9%E6%AF%94.html" class="md-nav__link">
        6 BERT GPT ELMo模型的对比
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          第七章 Transformer精选问答(拓展资料)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="第七章 Transformer精选问答(拓展资料)" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          第七章 Transformer精选问答(拓展资料)
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="1%20Transformer%20%E5%90%84%E5%AD%90%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.html" class="md-nav__link">
        1 Transformer 各子模块作用
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html" class="md-nav__link">
        2 Transformer Decoder模块
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          3 Self attention机制详解
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="3%20Self-attention%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3.html" class="md-nav__link md-nav__link--active">
        3 Self attention机制详解
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    学习目标
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-self-attention" class="md-nav__link">
    1 Self-attention的机制和原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-self-attention" class="md-nav__link">
    2 Self-attention中的归一化概述
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-softmax" class="md-nav__link">
    3 softmax的梯度变化
  </a>
  
    <nav class="md-nav" aria-label="3 softmax的梯度变化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-softmax" class="md-nav__link">
    3.1 softmax函数的输入分布是如何影响输出的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-softmax" class="md-nav__link">
    3.2 softmax函数在反向传播的过程中是如何梯度求导的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-softmax" class="md-nav__link">
    3.3 softmax函数出现梯度消失现象的原因
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4 维度与点积大小的关系
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5 小结
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="4%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html" class="md-nav__link">
        4 Multi head Attention详解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="5%20Transformer%E4%BC%98%E5%8A%BF.html" class="md-nav__link">
        5 Transformer优势
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    学习目标
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-self-attention" class="md-nav__link">
    1 Self-attention的机制和原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-self-attention" class="md-nav__link">
    2 Self-attention中的归一化概述
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-softmax" class="md-nav__link">
    3 softmax的梯度变化
  </a>
  
    <nav class="md-nav" aria-label="3 softmax的梯度变化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-softmax" class="md-nav__link">
    3.1 softmax函数的输入分布是如何影响输出的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-softmax" class="md-nav__link">
    3.2 softmax函数在反向传播的过程中是如何梯度求导的
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-softmax" class="md-nav__link">
    3.3 softmax函数出现梯度消失现象的原因
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4 维度与点积大小的关系
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5 小结
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

  <h1>3 Self attention机制详解</h1>

<h3 id="_1">学习目标<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<ul>
<li>掌握self-attention的机制和原理.</li>
<li>掌握为什么要使用三元组(Q, K, V)来计算self-attention.</li>
<li>理解softmax函数的输入是如何影响输出分布的.</li>
<li>理解softmax函数反向传播进行梯度求导的数学过程.</li>
<li>理解softmax函数出现梯度消失的原因.</li>
<li>理解self-attention计算规则中归一化的原因.</li>
</ul>
<blockquote>
<p>思考题1： Transformer中一直强调的self-attention是什么? 为什么能发挥如此大的作用? 计算的时候如果不使用三元组(Q, K, V), 而仅仅使用(Q, V)或者(K, V)或者(V)行不行?</p>
<p>思考题2：self-attention公式中的归一化有什么作用? 为什么要添加scaled?</p>
</blockquote>
<h2 id="1-self-attention">1 Self-attention的机制和原理<a class="headerlink" href="#1-self-attention" title="Permanent link">&para;</a></h2>
<p>self-attention是一种通过自身和自身进行关联的attention机制, 从而得到更好的representation来表达自身.</p>
<p>self-attention是attention机制的一种特殊情况，在self-attention中, Q=K=V, 序列中的每个单词(token)都和该序列中的其他所有单词(token)进行attention规则的计算.</p>
<p>attention机制计算的特点在于, 可以直接跨越一句话中不同距离的token, 可以远距离的学习到序列的知识依赖和语序结构.</p>
<p><center><img src="./img/picture_8.png" height="auto" width="auto"/></center></p>
<blockquote>
<ul>
<li>
<p>从上图中可以看到, self-attention可以远距离的捕捉到语义层面的特征(its的指代对象是Law).</p>
</li>
<li>
<p>应用传统的RNN, LSTM, 在获取长距离语义特征和结构特征的时候, 需要按照序列顺序依次计算, 距离越远的联系信息的损耗越大, 有效提取和捕获的可能性越小.</p>
</li>
<li>
<p>但是应用self-attention时, 计算过程中会直接将句子中任意两个token的联系通过一个计算步骤直接联系起来, </p>
</li>
</ul>
</blockquote>
<p>关于self-attention为什么要使用(Q, K, V)三元组而不是其他形式:</p>
<ul>
<li>首先一条就是从分析的角度看, 查询Query是一条独立的序列信息, 通过关键词Key的提示作用, 得到最终语义的真实值Value表达, 数学意义更充分, 完备.</li>
<li>这里不使用(K, V)或者(V)没有什么必须的理由, 也没有相关的论文来严格阐述比较试验的结果差异, 所以可以作为开放性问题未来去探索, 只要明确在经典self-attention实现中用的是三元组就好.</li>
</ul>
<p>self-attention公式中的归一化有什么作用? 为什么要添加scaled?</p>
<h2 id="2-self-attention">2 Self-attention中的归一化概述<a class="headerlink" href="#2-self-attention" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>训练上的意义: 随着词嵌入维度d_k的增大, q * k 点积后的结果也会增大, 在训练时会将带有饱和区间的激活函数（比如：sigmoid激活函数、tanh激活函数、逻辑回归softmax）推入梯度非常小的区域, 可能出现梯度消失的现象, 造成模型收敛困难.</p>
</li>
<li>
<p>数学上的意义: 假设q和k的统计变量是满足标准正态分布的独立随机变量, 意味着q和k满足均值为0, 方差为1. 那么q和k的点积结果就是均值为0, 方差为d_k, 为了抵消这种方差被放大d_k倍的影响, 在计算中主动将点积缩放1/sqrt(d_k), 这样点积后的结果依然满足均值为0, 方差为1.</p>
</li>
</ul>
<h2 id="3-softmax">3 softmax的梯度变化<a class="headerlink" href="#3-softmax" title="Permanent link">&para;</a></h2>
<p>这里我们分3个步骤来解释softmax的梯度问题:</p>
<ul>
<li>第一步: softmax函数的输入分布是如何影响输出的.</li>
<li>第二步: softmax函数在反向传播的过程中是如何梯度求导的.</li>
<li>第三步: softmax函数出现梯度消失现象的原因.</li>
</ul>
<h3 id="31-softmax">3.1 softmax函数的输入分布是如何影响输出的<a class="headerlink" href="#31-softmax" title="Permanent link">&para;</a></h3>
<ul>
<li>对于一个输入向量x, softmax函数将其做了一个归一化的映射, 首先通过自然底数e将输入元素之间的差距先"拉大", 然后再归一化为一个新的分布. 在这个过程中假设某个输入x中最大的元素下标是k, 如果输入的数量级变大(就是x中的每个分量绝对值都很大), 那么在数学上会造成y_k的值非常接近1.</li>
<li>具体用一个例子来演示, 假设输入的向量x = [a, a, 2a], 那么随便给几个不同数量级的值来看看对y3产生的影响</li>
</ul>
<div class="highlight"><pre><span></span><code>a = 1时,   y3 = 0.5761168847658291  # e^2 / (e^1 + e^1 + e^2))
a = 10时,  y3 = 0.9999092083843412  # e^20 / (e^10 + e^10 + e^20))
a = 100时, y3 = 1.0                 # e^200 / (e^100 + e^100 + e^200))
</code></pre></div>
<blockquote>
<ul>
<li>采用一段实例代码将a在不同取值下, 对应的y3全部画出来, 以曲线的形式展示:</li>
</ul>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_3</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<blockquote>
<ul>
<li>得到如下的曲线:</li>
</ul>
</blockquote>
<p><center><img src="./img/picture_13.png" height="auto" width="auto"/></center></p>
<blockquote>
<ul>
<li>
<p>从上图可以很清楚的看到输入元素的数量级对softmax最终的分布影响非常之大</p>
</li>
<li>
<p>结论： 在输入元素的数量级较大时，softmax函数几乎将全部的概率分布都分配给了最大值分量所对应的标签。通俗的讲：数据的方差变大（离散程度变大），最大值强占了所有概率。</p>
</li>
</ul>
</blockquote>
<h3 id="32-softmax">3.2 softmax函数在反向传播的过程中是如何梯度求导的<a class="headerlink" href="#32-softmax" title="Permanent link">&para;</a></h3>
<p>softmax函数在反向传播中容易梯度消失，所以要看一看softmax函数在反向传播中是如何求导的。</p>
<p>首先定义神经网络的输入和输出:</p>
<blockquote></blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
设X=[x_1,x_2,\cdots,x_n],Y=softmax(X)=[y_1,y_2,\cdots,y_n]\\\\
则y_i=\frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}},显然\sum_{i=1}^{n}y_i=1
</div>
<script type="math/tex; mode=display">
设X=[x_1,x_2,\cdots,x_n],Y=softmax(X)=[y_1,y_2,\cdots,y_n]\\\\
则y_i=\frac{e^{x_i}}{\sum_{j=1}^{n}e^{x_j}},显然\sum_{i=1}^{n}y_i=1
</script>
</div>
<p>反向传播就是输出端的损失函数对输入端求偏导的过程, 这里要分两种情况, 第一种如下所示:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align*}
(1)当i=j时\\\\
\frac{\partial{y_i}}{\partial{x_j}} &amp;= \frac{\partial{y_i}}{\partial{x_i}}\\\\
&amp;= \frac{\partial}{\partial{x_i}}{(\frac{e^{x_i}}{\sum_k e^{x_k}})} \\\\
&amp;= \frac{(e^{x_i})^{\prime} (\sum_k e^{x_k}) - e^{x_i}(\sum_ke^{x_k})^{\prime}}{(\sum_ke^{x_k})^2}\\\\
&amp;=\frac{e^{x_i}\cdot(\sum_ke^{x_k})-e^{x_i}\cdot e^{x_i}}{(\sum_ke^{x_k})^2}\\\\
&amp;=\frac{e^{x_i}\cdot(\sum_ke^{x_k})}{(\sum_ke^{x_k})^2}-\frac{e^{x_i}\cdot e^{x_i}}{(\sum_ke^{x_k})^2}\\\\
&amp;=\frac{e^{x_i}}{\sum_ke^{x_k}}-\frac{e^{x_i}}{\sum_ke^{x_k}}\cdot \frac{ e^{x_i}}{\sum_ke^{x_k}}\\\\
&amp;=y_i-y_i\cdot y_i \\\\
&amp;=y_i(1-y_i) 
\end{align*}
</div>
<script type="math/tex; mode=display">
\begin{align*}
(1)当i=j时\\\\
\frac{\partial{y_i}}{\partial{x_j}} &= \frac{\partial{y_i}}{\partial{x_i}}\\\\
&= \frac{\partial}{\partial{x_i}}{(\frac{e^{x_i}}{\sum_k e^{x_k}})} \\\\
&= \frac{(e^{x_i})^{\prime} (\sum_k e^{x_k}) - e^{x_i}(\sum_ke^{x_k})^{\prime}}{(\sum_ke^{x_k})^2}\\\\
&=\frac{e^{x_i}\cdot(\sum_ke^{x_k})-e^{x_i}\cdot e^{x_i}}{(\sum_ke^{x_k})^2}\\\\
&=\frac{e^{x_i}\cdot(\sum_ke^{x_k})}{(\sum_ke^{x_k})^2}-\frac{e^{x_i}\cdot e^{x_i}}{(\sum_ke^{x_k})^2}\\\\
&=\frac{e^{x_i}}{\sum_ke^{x_k}}-\frac{e^{x_i}}{\sum_ke^{x_k}}\cdot \frac{ e^{x_i}}{\sum_ke^{x_k}}\\\\
&=y_i-y_i\cdot y_i \\\\
&=y_i(1-y_i) 
\end{align*}
</script>
</div>
<p>第二种如下所示:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align*}
(2)当i\neq j时\\\\
\frac{\partial{y_i}}{\partial{x_j}} &amp;= \frac{\partial}{\partial{x_j}}{(\frac{e^{x_i}}{\sum_k e^{x_k}})} \\\\
&amp;= \frac{(e^{x_i})^{\prime} (\sum_k e^{x_k}) - e^{x_i}(\sum_ke^{x_k})^{\prime}}{(\sum_ke^{x_k})^2}\\\\
&amp;= \frac{0\cdot (\sum_k e^{x_k}) - e^{x_i}\cdot e^{x_j}}{(\sum_ke^{x_k})^2}\\\\
&amp;= -\frac{e^{x_i}\cdot e^{x_j}}{(\sum_ke^{x_k})^2}\\\\
&amp;= -\frac{e^{x_i}}{\sum_ke^{x_k}}\cdot \frac{ e^{x_j}}{\sum_ke^{x_k}}\\\\
&amp;=-y_i\cdot y_j 
\end{align*}
</div>
<script type="math/tex; mode=display">
\begin{align*}
(2)当i\neq j时\\\\
\frac{\partial{y_i}}{\partial{x_j}} &= \frac{\partial}{\partial{x_j}}{(\frac{e^{x_i}}{\sum_k e^{x_k}})} \\\\
&= \frac{(e^{x_i})^{\prime} (\sum_k e^{x_k}) - e^{x_i}(\sum_ke^{x_k})^{\prime}}{(\sum_ke^{x_k})^2}\\\\
&= \frac{0\cdot (\sum_k e^{x_k}) - e^{x_i}\cdot e^{x_j}}{(\sum_ke^{x_k})^2}\\\\
&= -\frac{e^{x_i}\cdot e^{x_j}}{(\sum_ke^{x_k})^2}\\\\
&= -\frac{e^{x_i}}{\sum_ke^{x_k}}\cdot \frac{ e^{x_j}}{\sum_ke^{x_k}}\\\\
&=-y_i\cdot y_j 
\end{align*}
</script>
</div>
<p>经过对两种情况分别的求导计算, 可以得出最终的结论如下:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align*}
综上所述：\frac{\partial y_i}{\partial x_j} &amp;=  \begin{cases} y_i-y_i\cdot y_i, &amp; \text {i=j} \\\\ 0-y_i\cdot y_j, &amp; \text{i $\neq$ j} \end{cases} \\\\
所以：\frac{\partial Y}{\partial X} &amp;= diag(Y)-Y^T\cdot Y \;\;\;(当Y的shape为(1,n)时)
\end{align*}
</div>
<script type="math/tex; mode=display">
\begin{align*}
综上所述：\frac{\partial y_i}{\partial x_j} &=  \begin{cases} y_i-y_i\cdot y_i, & \text {i=j} \\\\ 0-y_i\cdot y_j, & \text{i $\neq$ j} \end{cases} \\\\
所以：\frac{\partial Y}{\partial X} &= diag(Y)-Y^T\cdot Y \;\;\;(当Y的shape为(1,n)时)
\end{align*}
</script>
</div>
<p>把抽象的数学公式，映射成矩阵表示，见3.3节表示（i=j时，两个矩阵对角线 - 对角线 ；i!=j时，对应位置相减）。</p>
<h3 id="33-softmax">3.3 softmax函数出现梯度消失现象的原因<a class="headerlink" href="#33-softmax" title="Permanent link">&para;</a></h3>
<blockquote>
<ul>
<li>根据第二步中softmax函数的求导结果, 可以将最终的结果以矩阵形式展开如下:</li>
</ul>
</blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial g(X)}{\partial X}\approx \begin{bmatrix} \hat y_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \hat y_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \hat y_d  \end{bmatrix} - \begin{bmatrix} \hat y_1^2 &amp; \hat y_1 \hat y_2 &amp; \cdots &amp; \hat y_1 \hat y_d \\ \hat y_2 \hat y_1  &amp; \hat y_2^2 &amp; \cdots &amp; \hat y_2 \hat y_d \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \hat y_d \hat y_1 &amp; \hat y_d \hat y_2 &amp; \cdots &amp; \hat y_d^2  \end{bmatrix}
</div>
<script type="math/tex; mode=display">
\frac{\partial g(X)}{\partial X}\approx \begin{bmatrix} \hat y_1 & 0 & \cdots & 0 \\ 0 & \hat y_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \hat y_d  \end{bmatrix} - \begin{bmatrix} \hat y_1^2 & \hat y_1 \hat y_2 & \cdots & \hat y_1 \hat y_d \\ \hat y_2 \hat y_1  & \hat y_2^2 & \cdots & \hat y_2 \hat y_d \\ \vdots & \vdots & \ddots & \vdots \\ \hat y_d \hat y_1 & \hat y_d \hat y_2 & \cdots & \hat y_d^2  \end{bmatrix}
</script>
</div>
<blockquote>
<ul>
<li>根据第一步中的讨论结果, 当输入x的分量值较大时, softmax函数会将大部分概率分配给最大的元素, 假设最大元素是x1, 那么softmax的输出分布将产生一个接近one-hot的结果张量y_ = [1, 0, 0,..., 0], 此时结果矩阵变为:</li>
</ul>
</blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial g(X)}{\partial X}\approx \begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \cdots &amp; 0 \\\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\\ 0 &amp; 0 &amp; \cdots &amp; 0  \end{bmatrix} - \begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \cdots &amp; 0 \\\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\\ 0 &amp; 0 &amp; \cdots &amp; 0  \end{bmatrix}=0
</div>
<script type="math/tex; mode=display">
\frac{\partial g(X)}{\partial X}\approx \begin{bmatrix} 1 & 0 & \cdots & 0 \\\\ 0 & 0 & \cdots & 0 \\\\ \vdots & \vdots & \ddots & \vdots \\\\ 0 & 0 & \cdots & 0  \end{bmatrix} - \begin{bmatrix} 1 & 0 & \cdots & 0 \\\\ 0 & 0 & \cdots & 0 \\\\ \vdots & \vdots & \ddots & \vdots \\\\ 0 & 0 & \cdots & 0  \end{bmatrix}=0
</script>
</div>
<blockquote>
<ul>
<li>结论: 综上可以得出, 所有的梯度都消失为0(接近于0), 参数几乎无法更新, 模型收敛困难.</li>
</ul>
</blockquote>
<h2 id="4">4 维度与点积大小的关系<a class="headerlink" href="#4" title="Permanent link">&para;</a></h2>
<ul>
<li>针对为什么维度会影响点积的大小, 原始论文中有这样的一点解释如下:</li>
</ul>
<div class="highlight"><pre><span></span><code>To illustrate why the dot products get large, assume that the components of q and k 
are independent random variables with mean 0 and variance 1. Then their doct product,
q*k = (q1k1+q2k2+......+q(d_k)k(d_k)), has mean 0 and variance d_k.
</code></pre></div>
<blockquote>
<ul>
<li>
<p>我们分两步对其进行一个推导, 首先就是假设向量q和k的各个分量是相互独立的随机变量, X = q_i, Y = k_i, X和Y各自有d_k个分量, 也就是向量的维度等于d_k, 有E(X) = E(Y) = 0, 以及D(X) = D(Y) = 1.</p>
</li>
<li>
<p>可以得到E(XY) = E(X)E(Y) = 0 * 0 = 0</p>
</li>
<li>
<p>同理, 对于D(XY)推导如下:</p>
</li>
</ul>
</blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align*}
D(XY) &amp; = E(X^2\cdot Y^2)-[E(XY)]^2 \\\\
&amp;=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\
&amp;=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\
&amp;=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\
&amp;=D(X)D(Y)-[E(X)E(Y)]^2 \\\\
&amp;=1 \times 1- (0 \times 0)^2 \\\\
&amp;=1
\end{align*}
</div>
<script type="math/tex; mode=display">
\begin{align*}
D(XY) & = E(X^2\cdot Y^2)-[E(XY)]^2 \\\\
&=E(X^2)E(Y^2)-[E(X)E(Y)]^2 \\\\
&=E(X^2-0^2)E(Y^2-0^2)-[E(X)E(Y)]^2 \\\\
&=E(X^2-[E(X)]^2)E(Y^2-[E(Y)]^2)-[E(X)E(Y)]^2 \\\\
&=D(X)D(Y)-[E(X)E(Y)]^2 \\\\
&=1 \times 1- (0 \times 0)^2 \\\\
&=1
\end{align*}
</script>
</div>
<blockquote>
<ul>
<li>根据期望和方差的性质, 对于互相独立的变量满足下式:</li>
</ul>
</blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
E(\sum_iZ_i) =\sum_iE(Z_i),\\\\
D(\sum_iZ_i) =\sum_iD(Z_i)
</div>
<script type="math/tex; mode=display">
E(\sum_iZ_i) =\sum_iE(Z_i),\\\\
D(\sum_iZ_i) =\sum_iD(Z_i)
</script>
</div>
<blockquote>
<ul>
<li>上述公式，简读为：和的期望，等于期望的和；和的方差等于方差的和 </li>
<li>
<p>根据上面的公式, 可以很轻松的得出q*k的均值为E(qk) = 0, D(qk) = d_k.</p>
</li>
<li>
<p>所以方差越大, 对应的qk的点积就越大, 这样softmax的输出分布就会更偏向最大值所在的分量.</p>
</li>
<li>
<p>一个技巧就是将点积除以sqrt(d_k), 将方差在数学上重新"拉回1", 如下所示:</p>
</li>
</ul>
</blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
D(\frac{q\cdot k}{\sqrt{d_k}})=\frac{d_k}{(\sqrt{d_k})^2}=1
</div>
<script type="math/tex; mode=display">
D(\frac{q\cdot k}{\sqrt{d_k}})=\frac{d_k}{(\sqrt{d_k})^2}=1
</script>
</div>
<blockquote>
<ul>
<li>最终的结论: 通过数学上的技巧将方差控制在1, 也就有效的控制了点积结果的发散, 也就控制了对应的梯度消失的问题!</li>
</ul>
</blockquote>
<h2 id="5">5 小结<a class="headerlink" href="#5" title="Permanent link">&para;</a></h2>
<ul>
<li>self-attention机制的重点是使用三元组(Q, K, V)参与规则运算, 这里面Q=K=V.</li>
<li>self-attention最大的优势是可以方便有效的提取远距离依赖的特征和结构信息, 不必向RNN那样依次计算产生传递损耗.</li>
<li>关于self-attention采用三元组的原因, 经典实现的方式数学意义明确, 理由充分, 至于其他方式的可行性暂时没有论文做充分的对比试验研究.</li>
<li>学习了softmax函数的输入是如何影响输出分布的.<ul>
<li>softmax函数本质是对输入的数据分布做一次归一化处理, 但是输入元素的数量级对softmax最终的分布影响非常之大.</li>
<li>在输入元素的数量级较大时, softmax函数几乎将全部的概率分布都分配给了最大值分量所对应的标签.</li>
</ul>
</li>
<li>
<p>学习了softmax函数在反向传播的过程中是如何梯度求导的.</p>
<ul>
<li>具体的推导过程见讲义正文部分, 注意要分两种情况讨论, 分别处理.</li>
</ul>
</li>
<li>
<p>学习了softmax函数出现梯度消失现象的原因.</p>
<ul>
<li>结合第一步, 第二步的结论, 可以很清楚的看到最终的梯度矩阵接近于零矩阵, 这样在进行参数更新的时候就会产生梯度消失现象.</li>
</ul>
</li>
<li>
<p>学习了维度和点积大小的关系推导.</p>
<ul>
<li>通过期望和方差的推导理解了为什么点积会造成方差变大.</li>
<li>理解了通过数学技巧除以sqrt(d_k)就可以让方差恢复成1.</li>
</ul>
</li>
</ul>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="2%20Transformer%20Decoder%E6%A8%A1%E5%9D%97.html" class="md-footer__link md-footer__link--prev" aria-label="上一页: 2 Transformer Decoder模块" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              2 Transformer Decoder模块
            </div>
          </div>
        </a>
      
      
        
        <a href="4%20Multi-head%20Attention%E8%AF%A6%E8%A7%A3.html" class="md-footer__link md-footer__link--next" aria-label="下一页: 4 Multi head Attention详解" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              4 Multi head Attention详解
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "search": "../assets/javascripts/workers/search.22074ed6.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.1514a9a0.min.js"></script>
      
        <script src="../js/extra.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>