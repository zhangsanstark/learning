
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="01-LLM%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html">
      
      
        <link rel="next" href="../02-%E4%B8%BB%E6%B5%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/01-GPT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B.html">
      
      
      <link rel="icon" href="../img/AI.jpg">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.24">
    
    
      
        <title>1.2 LLM主要架构类别 - 大模型技术开发与应用V5.0</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llm" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href=".." title="大模型技术开发与应用V5.0" class="md-header__button md-logo" aria-label="大模型技术开发与应用V5.0" data-md-component="logo">
      
  <img src="../img/AI.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            大模型技术开发与应用V5.0
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1.2 LLM主要架构类别
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="大模型技术开发与应用V5.0" class="md-nav__button md-logo" aria-label="大模型技术开发与应用V5.0" data-md-component="logo">
      
  <img src="../img/AI.jpg" alt="logo">

    </a>
    大模型技术开发与应用V5.0
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    1:大模型背景简介
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            1:大模型背景简介
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="01-LLM%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1.1 LLM基础知识
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    1.2 LLM主要架构类别
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="02-LLM%E4%B8%BB%E8%A6%81%E7%B1%BB%E5%88%AB%E6%9E%B6%E6%9E%84.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    1.2 LLM主要架构类别
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      学习目标
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llm_1" class="md-nav__link">
    <span class="md-ellipsis">
      LLM主要类别
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencoder-modelae" class="md-nav__link">
    <span class="md-ellipsis">
      自编码模型 (AutoEncoder model，AE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="自编码模型 (AutoEncoder model，AE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1. 代表模型 BERT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 代表模型 BERT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 BERT的架构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 Embedding模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 双向Transformer模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 预微调模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 BERT的预训练任务
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.5 BERT的预训练任务">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#151-masked-lm" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.1 任务一: Masked LM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#152-next-sentence-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.2 任务二: Next Sentence Prediction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16" class="md-nav__link">
    <span class="md-ellipsis">
      1.6 数据集
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1.7 BERT模型的特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ae" class="md-nav__link">
    <span class="md-ellipsis">
      2. AE模型总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoregressive-modelar" class="md-nav__link">
    <span class="md-ellipsis">
      自回归模型 (Autoregressive model，AR)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="自回归模型 (Autoregressive model，AR)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1. 代表模型 GPT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 代表模型 GPT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 GPT模型架构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 GPT训练过程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.2 GPT训练过程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.1 无监督的预训练语言模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-fine-tunning" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.2 有监督的下游任务fine-tunning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#123" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.3 整体训练过程架构图
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 GPT数据集
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 GPT模型的特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ar" class="md-nav__link">
    <span class="md-ellipsis">
      2. AR模型总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-to-sequence-model" class="md-nav__link">
    <span class="md-ellipsis">
      序列到序列（Sequence to Sequence Model）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="序列到序列（Sequence to Sequence Model）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1. 代表模型T5
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 代表模型T5">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 T5模型架构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 T5 训练过程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 T5数据集
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 T5模型的特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-encoder-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      2. encoder-decoder模型总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#-decoder-only" class="md-nav__link">
    <span class="md-ellipsis">
      目前大模型主流模型架构-Decoder-only
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      小结总结
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    2:主流大模型介绍
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            2:主流大模型介绍
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-%E4%B8%BB%E6%B5%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/01-GPT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.1 GPT系列模型
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-%E4%B8%BB%E6%B5%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/02-LLM%E4%B8%BB%E6%B5%81%E5%BC%80%E6%BA%90%E4%BB%A3%E8%A1%A8%E6%A8%A1%E5%9E%8B.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2.2 LLM主流开源代表模型
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    3:大模型提示词工程应用实战
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            3:大模型提示词工程应用实战
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8D%97.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.1 大模型Prompt工程指南
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/02-%E9%87%91%E8%9E%8D%E8%A1%8C%E4%B8%9A%E5%8A%A8%E6%80%81%E6%96%B9%E5%90%91%E8%AF%84%E4%BC%B0%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.2 金融行业动态方向评估项目介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/03-LLM%E5%AE%9E%E7%8E%B0%E9%87%91%E8%9E%8D%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.3 LLM实现金融文本分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/04-LLM%E5%AE%9E%E7%8E%B0%E9%87%91%E8%9E%8D%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.4 LLM实现金融文本信息抽取
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/05-LLM%E5%AE%9E%E7%8E%B0%E9%87%91%E8%9E%8D%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3.5 LLM实现金融文本匹配
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    4:大模型微调主要方式
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            4:大模型微调主要方式
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BPrompt-Tuning%E6%96%B9%E6%B3%95.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4.1 大模型Prompt-Tuning方法
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E4%B8%BB%E8%A6%81%E6%96%B9%E5%BC%8F/02-%E5%A4%A7%E6%A8%A1%E5%9E%8BPEFT%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4.2 大模型PEFT微调方法
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    5:基于GPT2预训练模型搭建医疗问诊机器人
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            5:基于GPT2预训练模型搭建医疗问诊机器人
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-%E5%9F%BA%E4%BA%8EGPT2%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%8C%BB%E7%96%97%E9%97%AE%E8%AF%8A%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AE%9E%E7%8E%B0.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5.1 医疗问诊机器人实现
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    6:新零售行业评价决策系统
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            6:新零售行业评价决策系统
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/01-%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.1 项目背景介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/02-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.2 BERT+PET方式文本分类介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/03-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.3 BERT+PET方式数据预处理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/04-BERT%2BPET%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.4 BERT+PET方式模型代码实现和训练
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/05-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.5 BERT+P-Tuning方式文本分类介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/06-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.6 BERT+P-Tuning方式数据预处理
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-%E6%96%B0%E9%9B%B6%E5%94%AE%E8%A1%8C%E4%B8%9A%E8%AF%84%E4%BB%B7%E5%86%B3%E7%AD%96%E7%B3%BB%E7%BB%9F/07-BERT%2BP-Tuning%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.7 BERT+P-Tuning方式模型代码实现和训练
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    7:基于ChatGLM微调实现信息抽取+文本分类的多任务实战
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            7:基于ChatGLM微调实现信息抽取+文本分类的多任务实战
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/01-%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7.1 项目整体简介
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/02-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7.2 多任务数据预处理方式
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/03-LoRA%E6%96%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%AE%AD%E7%BB%83.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7.3 LoRA方式微调ChatGLM模型代码实现和训练
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-%E5%9F%BA%E4%BA%8EChatGLM%E5%BE%AE%E8%B0%83%E5%AE%9E%E7%8E%B0%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%2B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AE%9E%E6%88%98/%E8%B6%8B%E5%8A%A8%E4%BA%91%E4%BD%BF%E7%94%A8%E3%80%8A%E8%A1%A5%E5%85%85%E3%80%8B.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7.4 趋动云使用《扩展》
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    8:基于LangChain+ChatGLM-6B实现本地知识RAG问答机器人
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            8:基于LangChain+ChatGLM-6B实现本地知识RAG问答机器人
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-%E5%9F%BA%E4%BA%8ELangChain%2BChatGLM%E5%AE%9E%E7%8E%B0%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E6%9C%BA%E5%99%A8%E4%BA%BA/01-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E6%A1%86%E6%9E%B6LangChain%E8%AF%A6%E8%A7%A3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8.1 LangChain介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-%E5%9F%BA%E4%BA%8ELangChain%2BChatGLM%E5%AE%9E%E7%8E%B0%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E6%9C%BA%E5%99%A8%E4%BA%BA/02-LangChain%20%2B%20ChatGLM%20%E5%AE%9E%E7%8E%B0%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8.2 LangChain+ChatGLM-6B实现本地知识库问答
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    9:大模型Agent的应用
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            9:大模型Agent的应用
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E5%BA%94%E7%94%A8/01-%E5%A4%A7%E6%A8%A1%E5%9E%8BFunction%20Call%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9.1 大模型functioncall的原理及其应用
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E5%BA%94%E7%94%A8/02-GPTs%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9.2 GPTs的原理及应用
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E5%BA%94%E7%94%A8/03-Assistant%20API%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9.3 Assistant API的原理及应用
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-%E5%A4%A7%E6%A8%A1%E5%9E%8BAgent%E5%BA%94%E7%94%A8/04-Agent%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%BA%94%E7%94%A8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9.4 Agent原理介绍与应用
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      学习目标
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llm_1" class="md-nav__link">
    <span class="md-ellipsis">
      LLM主要类别
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencoder-modelae" class="md-nav__link">
    <span class="md-ellipsis">
      自编码模型 (AutoEncoder model，AE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="自编码模型 (AutoEncoder model，AE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1. 代表模型 BERT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 代表模型 BERT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 BERT的架构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 Embedding模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 双向Transformer模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 预微调模块
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 BERT的预训练任务
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.5 BERT的预训练任务">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#151-masked-lm" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.1 任务一: Masked LM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#152-next-sentence-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.2 任务二: Next Sentence Prediction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16" class="md-nav__link">
    <span class="md-ellipsis">
      1.6 数据集
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17-bert" class="md-nav__link">
    <span class="md-ellipsis">
      1.7 BERT模型的特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ae" class="md-nav__link">
    <span class="md-ellipsis">
      2. AE模型总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoregressive-modelar" class="md-nav__link">
    <span class="md-ellipsis">
      自回归模型 (Autoregressive model，AR)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="自回归模型 (Autoregressive model，AR)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1. 代表模型 GPT
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 代表模型 GPT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 GPT模型架构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 GPT训练过程
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.2 GPT训练过程">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.1 无监督的预训练语言模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-fine-tunning" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.2 有监督的下游任务fine-tunning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#123" class="md-nav__link">
    <span class="md-ellipsis">
      1.2.3 整体训练过程架构图
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 GPT数据集
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 GPT模型的特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-ar" class="md-nav__link">
    <span class="md-ellipsis">
      2. AR模型总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequence-to-sequence-model" class="md-nav__link">
    <span class="md-ellipsis">
      序列到序列（Sequence to Sequence Model）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="序列到序列（Sequence to Sequence Model）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1. 代表模型T5
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 代表模型T5">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 T5模型架构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 T5 训练过程
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 T5数据集
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-t5" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 T5模型的特点
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-encoder-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      2. encoder-decoder模型总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#-decoder-only" class="md-nav__link">
    <span class="md-ellipsis">
      目前大模型主流模型架构-Decoder-only
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      小结总结
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="llm">LLM主要类别架构介绍<a class="headerlink" href="#llm" title="Permanent link">&para;</a></h1>
<hr />
<h3 id="_1">学习目标<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<ul>
<li>了解LLM主要类别架构.</li>
<li>掌握BERT、GPT、T5等模型原理</li>
</ul>
<hr />
<h3 id="llm_1">LLM主要类别<a class="headerlink" href="#llm_1" title="Permanent link">&para;</a></h3>
<div align=center><img src="./assets/1-1-2.png" style="zoom:70%" ><img/></div>

<p>LLM本身基于transformer架构。自2017年，attention is all you need诞生起，原始的transformer模型为不同领域的模型提供了灵感和启发。基于原始的Transformer框架，衍生出了一系列模型，一些模型仅仅使用encoder或decoder，有些模型同时使用encoder+decoder。</p>
<div align=center><img src="./assets/1-2-1.png" style="zoom:70%" ><img/></div>

<p>LLM分类一般分为三种：自编码模型（encoder）、自回归模型(decoder)和序列到序列模型(encoder-decoder)。</p>
<hr />
<h3 id="autoencoder-modelae">自编码模型 (AutoEncoder  model，AE)<a class="headerlink" href="#autoencoder-modelae" title="Permanent link">&para;</a></h3>
<p>AE模型，代表作BERT，其特点为：Encoder-Only, 基本原理：是在输入中随机MASK掉一部分单词，根据上下文预测这个词。AE模型通常用于内容理解任务，比如自然语言理解（NLU）中的分类任务：情感分析、提取式问答。</p>
<hr />
<h4 id="1-bert">1. 代表模型 BERT<a class="headerlink" href="#1-bert" title="Permanent link">&para;</a></h4>
<p>BERT是2018年10月由Google AI研究院提出的一种预训练模型.</p>
<ul>
<li>BERT的全称是Bidirectional Encoder Representation from Transformers.</li>
<li>BERT在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类, 并且在11种不同NLP测试中创出SOTA表现. 包括将GLUE基准推高至80.4% (绝对改进7.6%), MultiNLI准确度达到86.7% (绝对改进5.6%). 成为NLP发展史上的里程碑式的模型成就.</li>
</ul>
<hr />
<h5 id="11-bert">1.1 BERT的架构<a class="headerlink" href="#11-bert" title="Permanent link">&para;</a></h5>
<p>总体架构: 如下图所示, 最左边的就是BERT的架构图, 可以很清楚的看到BERT采用了Transformer Encoder block进行连接, 因为是一个典型的双向编码模型.</p>
<div align=center><img src="./assets/1-2-3.png" style="zoom:60%" ><img/></div>

<p>从上面的架构图中可以看到, 宏观上BERT分三个主要模块:</p>
<ul>
<li>最底层黄色标记的Embedding模块.</li>
<li>中间层蓝色标记的Transformer模块.</li>
<li>最上层绿色标记的预微调模块.</li>
</ul>
<hr />
<h5 id="12-embedding">1.2 Embedding模块<a class="headerlink" href="#12-embedding" title="Permanent link">&para;</a></h5>
<p>BERT中的该模块是由三种Embedding共同组成而成, 如下图</p>
<div align=center><img src="./assets/1-2-7.png" style="zoom:90%" ><img/></div>

<blockquote>
<ul>
<li>Token Embeddings 是词嵌入张量, 第一个单词是CLS标志, 可以用于之后的分类任务.</li>
<li>Segment Embeddings 是句子分段嵌入张量, 是为了服务后续的两个句子为输入的预训练任务.</li>
<li>Position Embeddings 是位置编码张量, 此处注意和传统的Transformer不同, 不是三角函数计算的固定位置编码, 而是通过学习得出来的.</li>
<li>整个Embedding模块的输出张量就是这3个张量的直接加和结果.</li>
</ul>
</blockquote>
<hr />
<h5 id="13-transformer">1.3 双向Transformer模块<a class="headerlink" href="#13-transformer" title="Permanent link">&para;</a></h5>
<p>BERT中只使用了经典Transformer架构中的Encoder部分, 完全舍弃了Decoder部分. 而两大预训练任务也集中体现在训练Transformer模块中.</p>
<hr />
<h5 id="14">1.4 预微调模块<a class="headerlink" href="#14" title="Permanent link">&para;</a></h5>
<ul>
<li>经过中间层Transformer的处理后, BERT的最后一层根据任务的不同需求而做不同的调整即可.</li>
<li>比如对于sequence-level的分类任务, BERT直接取第一个[CLS] token 的final hidden state, 再加一层全连接层后进行softmax来预测最终的标签.</li>
</ul>
<blockquote>
<ul>
<li>对于不同的任务, 微调都集中在预微调模块, 几种重要的NLP微调任务架构图展示如下</li>
</ul>
</blockquote>
<div align=center><img src="./assets/1-2-8.png" style="zoom:90%" ><img/></div>

<blockquote>
<ul>
<li>从上图中可以发现, 在面对特定任务时, 只需要对预微调层进行微调, 就可以利用Transformer强大的注意力机制来模拟很多下游任务, 并得到SOTA的结果. (句子对关系判断, 单文本主题分类, 问答任务(QA), 单句贴标签(NER))</li>
<li>若干可选的超参数建议如下:</li>
</ul>
</blockquote>
<div class="highlight"><pre><span></span><code>Batch size: 16, 32
Learning rate (Adam): 5e-5, 3e-5, 2e-5
Epochs: 3, 4
</code></pre></div>
<hr />
<h5 id="15-bert">1.5 BERT的预训练任务<a class="headerlink" href="#15-bert" title="Permanent link">&para;</a></h5>
<p>BERT包含两个预训练任务:</p>
<ul>
<li>任务一: Masked LM (带mask的语言模型训练)</li>
<li>任务二: Next Sentence Prediction (下一句话预测任务)</li>
</ul>
<hr />
<h6 id="151-masked-lm">1.5.1 任务一: Masked LM<a class="headerlink" href="#151-masked-lm" title="Permanent link">&para;</a></h6>
<p>带mask的语言模型训练</p>
<ul>
<li>关于传统的语言模型训练, 都是采用left-to-right, 或者left-to-right + right-to-left结合的方式, 但这种单向方式或者拼接的方式提取特征的能力有限. 为此BERT提出一个深度双向表达模型(deep bidirectional representation). 即采用MASK任务来训练模型.</li>
<li>1: 在原始训练文本中, 随机的抽取15%的token作为参与MASK任务的对象.</li>
<li>2: 在这些被选中的token中, 数据生成器并不是把它们全部变成[MASK], 而是有下列3种情况.</li>
<li>2.1: 在80%的概率下, 用[MASK]标记替换该token, 比如my dog is hairy -&gt; my dog is [MASK]</li>
<li>2.2: 在10%的概率下, 用一个随机的单词替换token, 比如my dog is hairy -&gt; my dog is apple</li>
<li>2.3: 在10%的概率下, 保持该token不变, 比如my dog is hairy -&gt; my dog is hairy</li>
<li>3: 模型在训练的过程中, 并不知道它将要预测哪些单词? 哪些单词是原始的样子? 哪些单词被遮掩成了[MASK]? 哪些单词被替换成了其他单词? 正是在这样一种高度不确定的情况下, 反倒逼着模型快速学习该token的分布式上下文的语义, 尽最大努力学习原始语言说话的样子. 同时因为原始文本中只有15%的token参与了MASK操作, 并不会破坏原语言的表达能力和语言规则.</li>
</ul>
<hr />
<h6 id="152-next-sentence-prediction">1.5.2 任务二: Next Sentence Prediction<a class="headerlink" href="#152-next-sentence-prediction" title="Permanent link">&para;</a></h6>
<p>下一句话预测任务</p>
<ul>
<li>在NLP中有一类重要的问题比如QA(Quention-Answer), NLI(Natural Language Inference), 需要模型能够很好的理解两个句子之间的关系, 从而需要在模型的训练中引入对应的任务. 在BERT中引入的就是Next Sentence Prediction任务. 采用的方式是输入句子对(A, B), 模型来预测句子B是不是句子A的真实的下一句话.</li>
<li>1: 所有参与任务训练的语句都被选中作为句子A.</li>
<li>1.1: 其中50%的B是原始文本中真实跟随A的下一句话. (标记为IsNext, 代表正样本)</li>
<li>1.2: 其中50%的B是原始文本中随机抽取的一句话. (标记为NotNext, 代表负样本)</li>
<li>2: 在任务二中, BERT模型可以在测试集上取得97%-98%的准确率.</li>
</ul>
<hr />
<h5 id="16">1.6 数据集<a class="headerlink" href="#16" title="Permanent link">&para;</a></h5>
<p>BooksCorpus (800M words) + English Wikipedia (2,500M words)</p>
<hr />
<h5 id="17-bert">1.7 BERT模型的特点<a class="headerlink" href="#17-bert" title="Permanent link">&para;</a></h5>
<p>模型的一些关键参数为：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>取值</th>
</tr>
</thead>
<tbody>
<tr>
<td>transformer 层数</td>
<td>12</td>
</tr>
<tr>
<td>特征维度</td>
<td>768</td>
</tr>
<tr>
<td>transformer head 数</td>
<td>12</td>
</tr>
<tr>
<td>总参数量</td>
<td>1.15 亿</td>
</tr>
</tbody>
</table>
<h4 id="2-ae">2. AE模型总结<a class="headerlink" href="#2-ae" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>优点：BERT使用双向transformer，在语言理解相关的任务中表现很好。</p>
</li>
<li>
<p>缺点：</p>
</li>
<li>
<ul>
<li>输入噪声：BERT在预训练过程中使用【mask】符号对输入进行处理，这些符号在下游的finetune任务中永远不会出现，这会导致**预训练-微调差异**。而AR模型不会依赖于任何被mask的输入，因此不会遇到这类问题。</li>
</ul>
</li>
<li>更适合用于语言嵌入表达, 语言理解方面的任务, 不适合用于生成式的任务</li>
</ul>
<hr />
<h3 id="autoregressive-modelar">自回归模型 (Autoregressive model，AR)<a class="headerlink" href="#autoregressive-modelar" title="Permanent link">&para;</a></h3>
<p>AR模型，代表作GPT，其特点为：Decoder-Only，基本原理：从左往右学习的模型，只能利用上文或者下文的信息，比如：AR模型从一系列time steps中学习，并将上一步的结果作为回归模型的输入，以预测下一个time step的值。AR模型通常用于生成式任务，在长文本的生成能力很强，比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答。</p>
<hr />
<h4 id="1-gpt">1. 代表模型 GPT<a class="headerlink" href="#1-gpt" title="Permanent link">&para;</a></h4>
<p>2018年6月, OpenAI公司发表了论文“Improving Language Understanding by Generative Pre-training”《用生成式预训练提高模型的语言理解力》, 推出了具有1.17亿个参数的GPT（Generative Pre-training , 生成式预训练）模型. </p>
<p>与BERT最大的区别在于GPT采用了传统的语言模型方法进行预训练, 即使用单词的上文来预测单词, 而BERT是采用了双向上下文的信息共同来预测单词.正是因为训练方法上的区别, 使得GPT更擅长处理自然语言生成任务(NLG), 而BERT更擅长处理自然语言理解任务(NLU).</p>
<hr />
<h5 id="11-gpt">1.1 GPT模型架构<a class="headerlink" href="#11-gpt" title="Permanent link">&para;</a></h5>
<p>看三个语言模型的对比架构图, 中间的就是GPT:</p>
<div align=center><img src="./assets/1-2-3.png" style="zoom:60%" ><img/></div>

<blockquote>
<ul>
<li>
<p>从上图可以很清楚的看到GPT采用的是单向Transformer模型, 例如给定一个句子[u1, u2, ..., un], GPT在预测单词ui的时候只会利用[u1, u2, ..., u(i-1)]的信息, 而BERT会同时利用上下文的信息[u1, u2, ..., u(i-1), u(i+1), ..., un].</p>
</li>
<li>
<p>作为两大模型的直接对比, BERT采用了Transformer的Encoder模块, 而GPT采用了Transformer的Decoder模块. 并且GPT的Decoder Block和经典Transformer Decoder Block还有所不同, 如下图所示:</p>
</li>
</ul>
</blockquote>
<div align=center><img src="./assets/1-2-4.png" style="zoom:40%" ><img/></div>

<blockquote>
<ul>
<li>
<p>如上图所示, 经典的Transformer Decoder Block包含3个子层, 分别是Masked Multi-Head Attention层, encoder-decoder attention层, 以及Feed Forward层. 但是在GPT中取消了第二个encoder-decoder attention子层, 只保留Masked Multi-Head Attention层, 和Feed Forward层.</p>
</li>
<li>
<p>注意: 对比于经典的Transformer架构, 解码器模块采用了6个Decoder Block; GPT的架构中采用了12个Decoder Block.</p>
</li>
</ul>
</blockquote>
<div align=center><img src="./assets/1-2-5.png" style="zoom:50%" ><img/></div>

<hr />
<h5 id="12-gpt">1.2 GPT训练过程<a class="headerlink" href="#12-gpt" title="Permanent link">&para;</a></h5>
<p>GPT的训练包括两阶段过程: <strong>预训练 + 微调</strong></p>
<ul>
<li>第一阶段: 无监督的预训练语言模型.</li>
<li>第二阶段: 有监督的下游任务fine-tunning.</li>
</ul>
<hr />
<h6 id="121">1.2.1 无监督的预训练语言模型<a class="headerlink" href="#121" title="Permanent link">&para;</a></h6>
<ul>
<li>给定句子U = [u1, u2, ..., un], GPT训练语言模型时的目标是最大化下面的似然函数:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L_1(U)=\sum_i\log P(u_i|u_{i-k},\cdots,u_{i-1};\Theta)
</div>
<script type="math/tex; mode=display">
L_1(U)=\sum_i\log P(u_i|u_{i-k},\cdots,u_{i-1};\Theta)
</script>
</div>
<ul>
<li>上述公式具体来说是要预测每个词ui的概率，这个概率是基于它前面 ui-k 到 ui−1 个词，以及模型 Θ。这里的 k 表示上文的窗口大小，理论上来讲 k 取的越大，模型所能获取的上文信息越充足，模型的能力越强。</li>
<li>GPT是一个单向语言模型,模型对输入U 进行特征嵌入得到 transformer 第一层的输h0，再经过多层 transformer 特征编码，使用最后一层的输出即可得到当前预测的概率分布，计算过程如下：</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
h_0 = UW_e + W_p
</div>
<script type="math/tex; mode=display">
h_0 = UW_e + W_p
</script>
</div>
<blockquote>
<p>其中Wp是单词的位置编码, We是单词本身的word embedding. Wp的形状是[max_seq_len, embedding_dim], We的形状是[vocab_size, embedding_dim].</p>
</blockquote>
<ul>
<li>得到输入张量h0后, 要将h0传入GPT的Decoder Block中, 依次得到ht:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
h_t = transformer\_block(h_{l-1})\;\;\;\;l\in[1,t]
</div>
<script type="math/tex; mode=display">
h_t = transformer\_block(h_{l-1})\;\;\;\;l\in[1,t]
</script>
</div>
<ul>
<li>最后通过得到的ht来预测下一个单词:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
P(u)=softmax(h_tW_e^T)
</div>
<script type="math/tex; mode=display">
P(u)=softmax(h_tW_e^T)
</script>
</div>
<hr />
<h6 id="122-fine-tunning">1.2.2 有监督的下游任务fine-tunning<a class="headerlink" href="#122-fine-tunning" title="Permanent link">&para;</a></h6>
<ul>
<li>GPT经过预训练后, 会针对具体的下游任务对模型进行微调. 微调采用的是有监督学习, 训练样本包括单词序列[x1, x2, ..., xn]和label y. GPT微调的目标任务是根据单词序列[x1, x2, ..., xn]预测标签y.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
P(y|x^1,\cdots,x^m)=softmax(h_l^mW_y)
</div>
<script type="math/tex; mode=display">
P(y|x^1,\cdots,x^m)=softmax(h_l^mW_y)
</script>
</div>
<blockquote>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">W_y</span><script type="math/tex">W_y</script></span>表示预测输出的矩阵参数, 微调任务的目标是最大化下面的函数:</p>
</blockquote>
<div class="arithmatex">
<div class="MathJax_Preview">
L_2=\sum_{(x,y)}\log P(y|x^1,\cdots,x^m)
</div>
<script type="math/tex; mode=display">
L_2=\sum_{(x,y)}\log P(y|x^1,\cdots,x^m)
</script>
</div>
<ul>
<li>综合两个阶段的目标任务函数, 可知GPT的最终优化函数为:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L_3 = L_2 + \lambda L_1
</div>
<script type="math/tex; mode=display">
L_3 = L_2 + \lambda L_1
</script>
</div>
<hr />
<h6 id="123">1.2.3 整体训练过程架构图<a class="headerlink" href="#123" title="Permanent link">&para;</a></h6>
<p>根据下游任务适配的过程分两步: 1、根据任务定义不同输入, 2、对不同任务增加不同的分类层. </p>
<ul>
<li>具体定义可以参见下图: </li>
</ul>
<div align=center><img src="./assets/1-2-6.png" style="zoom:55%" ><img/></div>

<ul>
<li>分类任务（Classification）: 将起始和终止token加入到原始序列两端, 输入transformer中得到特征向量, 最后经过一个全连接得到预测的概率分布；</li>
<li>文本蕴涵（Entailment）: 将前提（premise）和假设（hypothesis）通过分隔符（Delimiter）隔开, 两端加上起始和终止token. 再依次通过transformer和全连接得到预测结果；</li>
<li>文本相似度（Similarity）: 输入的两个句子, 正向和反向各拼接一次, 然后分别输入给transformer, 得到的特征向量拼接后再送给全连接得到预测结果；</li>
<li>问答和常识推理（Multiple-Choice）: 将 N个选项的问题抽象化为N个二分类问题, 即每个选项分别和内容进行拼接, 然后各送入transformer和全连接中, 最后选择置信度最高的作为预测结果</li>
</ul>
<hr />
<p>总的来说，都是通过在序列前后添加 Start 和 Extract 特殊标识符来表示开始和结束，序列之间添加必要的 Delim 标识符来表示分隔，当然实际使用时不会直接用 “Start/Extract/Delim” 这几个词，而是使用某些特殊符号。基于不同下游任务构造的输入序列，使用预训练的 GPT 模型进行特征编码，然后使用序列最后一个 token 的特征向量进行预测。</p>
<p>可以看到，不论下游任务的输入序列怎么变，最后的预测层怎么变，中间的特征抽取模块都是不变的，具有很好的迁移能力。</p>
<hr />
<h5 id="13-gpt">1.3  GPT数据集<a class="headerlink" href="#13-gpt" title="Permanent link">&para;</a></h5>
<p>GPT使用了BooksCorpus数据集, 文本大小约 5 GB，包含 7400w+ 的句子。这个数据集由 7000 本独立的、不同风格类型的书籍组成, 选择该部分数据集的原因: </p>
<ul>
<li>书籍文本包含大量高质量长句，保证模型学习长距离信息依赖。</li>
<li>这些书籍因为没有发布, 所以很难在下游数据集上见到, 更能验证模型的泛化能力. </li>
</ul>
<hr />
<h5 id="14-gpt">1.4 GPT模型的特点<a class="headerlink" href="#14-gpt" title="Permanent link">&para;</a></h5>
<p>模型的一些关键参数为：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>取值</th>
</tr>
</thead>
<tbody>
<tr>
<td>transformer 层数</td>
<td>12</td>
</tr>
<tr>
<td>特征维度</td>
<td>768</td>
</tr>
<tr>
<td>transformer head 数</td>
<td>12</td>
</tr>
<tr>
<td>总参数量</td>
<td>1.17 亿</td>
</tr>
</tbody>
</table>
<p>优点</p>
<ul>
<li>在有监督学习的12个任务中, GPT在9个任务上的表现超过了state-of-the-art的模型</li>
<li>利用Transformer做特征抽取, 能够捕捉到更长的记忆信息, 且较传统的 RNN 更易于并行化</li>
</ul>
<p>缺点</p>
<ul>
<li>GPT 最大的问题就是传统的语言模型是单向的. </li>
<li>针对不同的任务, 需要不同的数据集进行模型微调, 相对比较麻烦</li>
</ul>
<hr />
<h4 id="2-ar">2. AR模型总结<a class="headerlink" href="#2-ar" title="Permanent link">&para;</a></h4>
<p>优点：AR模型擅长生成式NLP任务。AR模型使用注意力机制，预测下一个token，因此自然适用于文本生成。此外，AR模型可以简单地将训练目标设置为预测语料库中的下一个token，因此生成数据相对容易。</p>
<p>缺点：AR模型只能用于前向或者后向建模，不能同时使用双向的上下文信息，不能完全捕捉token的内在联系。</p>
<hr />
<h3 id="sequence-to-sequence-model">序列到序列（Sequence to Sequence Model）<a class="headerlink" href="#sequence-to-sequence-model" title="Permanent link">&para;</a></h3>
<p>encoder-decoder模型同时使用编码器和解码器。它将每个task视作序列到序列的转换/生成（比如，文本到文本，文本到图像或者图像到文本的多模态任务）。对于文本分类任务来说，编码器将文本作为输入，解码器生成文本标签。Encoder-decoder模型通常用于需要内容理解和生成的任务，比如机器翻译。</p>
<hr />
<h4 id="1-t5">1. 代表模型T5<a class="headerlink" href="#1-t5" title="Permanent link">&para;</a></h4>
<p>T5 由谷歌的 Raffel 等人于 2020年7月提出，相关论文为“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”. 该模型的目的为构建任务统一框架：将所有NLP任务都视为文本转换任务。</p>
<div align=center><img src="./assets/1-2-9.png" style="zoom:85%" ><img/></div>

<p>比如英德翻译，只需将训练数据集的输入部分前加上“translate English to German（给我从英语翻译成德语）” 就行。假设需要翻译"That is good"，那么先转换成 "translate English to German：That is good." 输入模型，之后就可以直接输出德语翻译 “Das ist gut.”。 对于需要输出连续值的 STS-B（文本语义相似度任务）， 也是直接输出文本。</p>
<p>通过这样的方式就能将 NLP 任务都转换成 Text-to-Text 形式，也就可以**用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务。**</p>
<hr />
<h5 id="11-t5">1.1 T5模型架构<a class="headerlink" href="#11-t5" title="Permanent link">&para;</a></h5>
<p>T5模型结构与原始的Transformer基本一致,除了做了以下几点改动：</p>
<ul>
<li>作者采用了一种简化版的Layer Normalization，去除了Layer Norm 的bias；将Layer Norm放在残差连接外面。</li>
<li>位置编码：T5使用了一种简化版的相对位置编码，即每个位置编码都是一个标量，被加到 logits 上用于计算注意力权重。各层共享位置编码，但是在同一层内，不同的注意力头的位置编码都是独立学习的。一定数量的位置Embedding，每一个对应一个可能的 key-query 位置差。作者学习了32个Embedding，至多适用于长度为128的位置差，超过位置差的位置编码都使用相同的Embedding。</li>
</ul>
<hr />
<h5 id="12-t5">1.2 T5 训练过程<a class="headerlink" href="#12-t5" title="Permanent link">&para;</a></h5>
<p>自监督预训练：采用类似于BERT模型的MLM预训练任务。</p>
<p>多任务预训练：除了使用大规模数据进行无监督预训练，T5模型还可以利用不同任务的标注数据进行有监督的多任务预训练，例如SQuAD问答和机器翻译等任务。</p>
<h5 id="13-t5">1.3 T5数据集<a class="headerlink" href="#13-t5" title="Permanent link">&para;</a></h5>
<p>作者对公开爬取的网页数据集Common Crawl进行了过滤，去掉一些重复的、低质量的，看着像代码的文本等，并且最后只保留英文文本，得到数据集**C4: the Colossal Clean Crawled Corpus**。</p>
<h5 id="14-t5">1.4 T5模型的特点<a class="headerlink" href="#14-t5" title="Permanent link">&para;</a></h5>
<p>模型的一些关键参数为：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>取值</th>
</tr>
</thead>
<tbody>
<tr>
<td>transformer 层数</td>
<td>24</td>
</tr>
<tr>
<td>特征维度</td>
<td>768</td>
</tr>
<tr>
<td>transformer head 数</td>
<td>12</td>
</tr>
<tr>
<td>总参数量</td>
<td>2.2 亿</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="2-encoder-decoder">2. encoder-decoder模型总结<a class="headerlink" href="#2-encoder-decoder" title="Permanent link">&para;</a></h4>
<p>优点：T5模型可以处理多种NLP任务，并且可以通过微调来适应不同的应用场景，具有良好的可扩展性；相比其他语言生成模型（如GPT-2、GPT3等），T5模型的参数数量相对较少，训练速度更快，且可以在相对较小的数据集上进行训练。</p>
<p>缺点：由于T5模型使用了大量的Transformer结构，在训练时需要大量的计算资源和时间; 模型的可解释性不足。</p>
<h3 id="-decoder-only">目前大模型主流模型架构-Decoder-only<a class="headerlink" href="#-decoder-only" title="Permanent link">&para;</a></h3>
<p>LLM之所以主要都用Decoder-only架构，除了训练效率和工程实现上的优势外，在理论上是因为Encoder的双向注意力会存在低秩问题，这可能会削弱模型表达能力，就生成任务而言，引入双向注意力并无实质好处。而Encoder-Decoder架构之所以能够在某些场景下表现更好，大概只是因为它多了一倍参数。所以，在同等参数量、同等推理成本下，Decoder-only架构就是最优选择了。</p>
<h3 id="_2">小结总结<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<ul>
<li>本小节主要介绍LLM的主要类别架构：自回归模型、自编码模型和序列到序列模型。</li>
<li>分别对不同类型架构的代表模型如：BERT、GPT、T5等相关模型进行介绍</li>
</ul>
<hr />
<hr />
<hr />
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../assets/javascripts/bundle.081f42fc.min.js"></script>
      
        <script src="../js/extra.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>